,journalName,journalId,doi,title,pubDay,pubMonth,pubYear,codeLink,codeText,pubDate
0,Scientific Data,41597,10.1038/s41597-024-03158-7,Mapping Road Surface Type of Kenya Using OpenStreetMap and High-resolution Google Satellite Imagery,3,4,2024,https://github.com/Dsayddd/RoadSurface,The data files and the python scripts used for model training are available online through GitHub repository: .,2024-04-03
0,Scientific Data,41597,10.1038/s41597-024-03214-2,High spatial resolution elevation change dataset derived from ICESat-2 crossover points on the Tibetan Plateau,17,4,2024,https://github.com/snowhydro/icesat-cross-point,The script used to process the ICESat-2 data and extract ICESat-2 crossover points from it is available at the following link: .,2024-04-17
0,Scientific Data,41597,10.1038/s41597-024-03272-6,Genome-wide profiling of angiogenic -regulatory elements unravels -regulatory SNPs for vascular abnormality,8,5,2024,,No custom code was used.,2024-05-08
0,Scientific Data,41597,10.1038/s41597-024-03358-1,CheXmask: a large-scale dataset of anatomical segmentation masks for multi-center chest x-ray images,17,5,2024,https://github.com/ngaggion/CheXmask-Database,"The code associated with this study is available in our Github repository: . The repository encompasses Python 3 code for various components, including data preparation, data post-processing, technical validation, and the deep learning models. The data preparation section includes scripts for preprocessing the images. The data post-processing section provides scripts for converting the segmentation masks from run-length encoding to a binary mask format, examples of how to read the model and the necessary code to revert the pre-processing steps for each dataset. The technical validation section includes the code for the individual RCA analysis and the processing of the physician results. Additionally, the repository includes the code for the deep learning models used for image segmentation, including the HybridGNet model architecture, weights, training and inference scripts. The software prerequisites for running the code are outlined in the repository’s README file.",2024-05-17
0,Scientific Data,41597,10.1038/s41597-024-03159-6,"Globe-LFMC 2.0, an enhanced and updated dataset for live fuel moisture content research",4,4,2024,,"[{'italic': 'figshare', 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR15', '#text': '15'}}, '#text': 'The code for the detection of potential outliers and the extraction of land cover and meteorological data was developed using Python 3.9.7. The corresponding Jupyter Notebooks are available at the  repository.'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR15', '#text': '15'}}, '#text': 'The outlier detection code uses the Globe-LFMC-2.0 file as input.'}, 'When executing the land cover and meteorological data extraction code, it is essential to have downloaded the required input data first.']",2024-04-04
0,Scientific Data,41597,10.1038/s41597-024-03215-1,Three  assembled wild cacao genomes from the Upper Amazon,11,4,2024,,No code was developed for implementing a software.,2024-04-11
0,Scientific Data,41597,10.1038/s41597-024-03273-5,Mapping annual 10-m soybean cropland with spatiotemporal sample migration,2,5,2024,https://github.com/ZihangLou/ChinaSoybean10,"The programs used to generate the datasets and all the results were ESRI ArcGIS (10.6), Python (3.7 or 3.8) and Google Earth Engine (GEE). The scripts utilized for ChinaSoybean10 described in this paper can be accessed at .",2024-05-02
0,Scientific Data,41597,10.1038/s41597-024-03359-0,REAL-Colon: A dataset for developing real-world AI applications in colonoscopy,25,5,2024,https://github.com/cosmoimd/real-colon-dataset; https://github.com/cosmoimd/DeepLearningExamples,"To facilitate the process of downloading and exploring the dataset, we have made available a set of useful Python codes on our GitHub repository at . These scripts facilitate easy access to the data and assist in its analysis, enabling users to reproduce all the plots presented in this paper. Code for the training and testing of the polyp detection models can be found separately at .",2024-05-25
0,Scientific Data,41597,10.1038/s41597-024-03301-4,InsectSound1000 An insect sound dataset for deep learning based acoustic insect recognition,9,5,2024,,"[{'ext-link': {'@xlink:href': 'https://github.com/Jelt0/InsectSound1000Tools', '@ext-link-type': 'uri', '#text': 'https://github.com/Jelt0/InsectSound1000Tools'}, '#text': 'The Python script used to extract the sound samples collected from the InsectSound1000 dataset from the raw, long-term recordings is available online on the InsetSound1000 GitHub page (). The script can easily be modified for extraction, e.g., samples of different lengths, different sample rates or even different thresholds for activity detection.'}, 'Furthermore, the accommodating GitHub repository contains a Jupyter Notebook to split the InsectSound1000 dataset into training, validation and test subsets while ensuring that recording dates do not overlap in the different subsets.']",2024-05-09
0,Scientific Data,41597,10.1038/s41597-024-03330-z,"A large and diverse brain organoid dataset of 1,400 cross-laboratory images of 64 trackable brain organoids",20,5,2024,https://github.com/LabTrivedi/MOrgAna,"The code for training MOrgAna and the SegFormer is publicly available on GitHub:  and. The data splits for MOrgAna and SegFormer training and evaluation, the configuration files for SegFormer training, the CellProfiler project as well as the workflow for the Technical Validation are publicly available on GitHub and co-deposited on Zenodo.",2024-05-20
0,Scientific Data,41597,10.1038/s41597-024-03160-z,"Three years of weekly DEMs, aerial orthomosaics and surveyed shoreline positions at Waikīkī Beach, Hawai‘i",29,3,2024,,There is no custom code for this project.,2024-03-29
0,Scientific Data,41597,10.1038/s41597-024-03217-z,Dataset of weekly intra-treatment diffusion weighted imaging in head and neck cancer patients treated with MR-Linac,11,5,2024,https://github.com/kwahid/Weekly_DWI_Data_Descriptor,Codes used for data annotation are available through GitHub: ().,2024-05-11
0,Scientific Data,41597,10.1038/s41597-024-03275-3,"A new commercial boundary dataset for metropolitan areas in the USA and Canada, built from open data",24,4,2024,https://github.com/schoolofcities/commercial-boundaries,"The codes used in this study are available from Figshare or the School of cities GitHub (). In addition, modified metropolitan areas, a saved Pytorch state file and commercial boundary result data can also be obtained from those pages. Readers can easily replicate the identified commercial boundary using the given codes and datasets.",2024-04-24
0,Scientific Data,41597,10.1038/s41597-024-03302-3,Parsimonious estimation of hourly surface ozone concentration across China during 2015–2020,14,5,2024,https://github.com/Wenxiu0902/Ozone_prediction,"The code is available on GitHub () primarily using Python and R languages. It includes data preprocessing, model training, testing, prediction, and visualization sections. Additionally, sample model input data is also provided.",2024-05-14
0,Scientific Data,41597,10.1038/s41597-024-03190-7,"RASMI: Global ranges of building material intensities differentiated by region, structure, and function",23,4,2024,,"[{'ext-link': {'@xlink:href': 'https://github.com/TomerFishman/MaterialIntensityEstimator', '@ext-link-type': 'uri', '#text': 'https://github.com/TomerFishman/MaterialIntensityEstimator'}, 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR58', '#text': '58'}}, '#text': 'The entire workflow is available in the RASMI GitHub repository (): the methods for creating the MI value ranges are implemented in Python 3 code, and the random forest classification of structure types is implemented in the Orange machine learning and data mining suite.'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR59', '#text': '59'}}, '#text': 'The GitHub repository also stores the resulting data and supporting figures. Released versions of the data are archived in Zenodo. The Zenodo version used for this data descriptor is v20230905.'}]",2024-04-23
0,Scientific Data,41597,10.1038/s41597-024-03218-y,Large-scale annotated dataset for cochlear hair cell detection and classification,23,4,2024,https://github.com/indzhykulianlab/hcat-data,"All associated code for downloading, loading, and preprocessing this dataset may be found at: .",2024-04-23
0,Scientific Data,41597,10.1038/s41597-024-03247-7,CROPGRIDS: a global geo-referenced dataset of 173 crops,22,4,2024,https://doi.org/10.6084/m9.figshare.22491997,All data processing and testing described in Methods and Technical Validation sections were conducted using MATLAB version R2021a. Main codes used to construct CROPGRIDS are distributed in the “CODES.zip” folder (Table ) along with CROPGRIDS dataset available for public download from the  repository at .,2024-04-22
0,Scientific Data,41597,10.1038/s41597-024-03303-2,A dataset for measuring the impact of research data and their curation,3,5,2024,https://github.com/ICPSR/mica-data-descriptor; https://doi.org/10.5281/zenodo.8432666,The code used to produce the MICA project dataset is available on GitHub at  and through Zenodo with the identifier . Data manipulation and pre-processing were performed in Python. Data curation for distribution was performed in SPSS.,2024-05-03
0,Scientific Data,41597,10.1038/s41597-024-03219-x,A baseline structure inventory with critical attribution for the US and its territories,16,5,2024,,"[{'ext-link': {'@xlink:href': 'https://github.com/btswan87/isosceles', '@ext-link-type': 'uri', '#text': 'https://github.com/btswan87/isosceles'}, '#text': 'The sample selection process used ISOSCELES, a program written in Python 2.7 using the open source packages GDAL, OGR, SciPy, Numpy, Sci-kit Learn, and Pandas. It is available at .'}, 'Main geospatial data operations and manipulations use open packages, including Python, dask, sqalchemy2, geopandas, pandas, SciPy, Sci-kit Learn, psycopg2-binary, sqlalchemy, postgres, GDAL, OGR, DBeaver, and PostgreSQL. Regularization was performed using ArcPy.', 'Database is a Docker image from CrunchyData with Postgres 14.2 and PostGIS 3.1.']",2024-05-16
0,Scientific Data,41597,10.1038/s41597-024-03304-1,A unified dataset for pre-processed climate indicators weighted by gridded economic activity,24,5,2024,https://github.com/CoMoS-SA/climaterepo,"Python code running the  dashboard and scripts for aggregating data are available at . The  leverages . We employed  to process the data, exploiting package  for the weighted aggregations.",2024-05-24
0,Scientific Data,41597,10.1038/s41597-024-03391-0,Maritime Freight Carbon Emission in the U.S. using AIS data from 2018 to 2022,25,5,2024,,"The data calculation was primarily performed by PyCharm Community Edition 2022.1.3, with Python 3.11 runtime environment (hosted on an Intel i5-1135G7 CPU @ 2.4 GHz, 16 GB RAM, and Windows 11 operating system). The custom code includes parameters that are integral to the dataset generation process. Specifically, the minimum stay time (“thre_sail”) is set to 1 hour, the minimum sailing time (“thre_stop”) is set to 1 hour, and the distance to the nearest port(“thre_shold”) is set to 0.2°. Researchers and interested parties can access the code through the provided, subject to the authors’ permission. The detailed codes can be found in Supplementary information.",2024-05-25
0,Scientific Data,41597,10.1038/s41597-024-03249-5,A dataset of storm surge reconstructions in the Western North Pacific using CNN,22,4,2024,,The python code that was used to train the model to generate the dataset descripted above can be found at Figshare. Further questions can be directed towards Jianlong Feng (fjl181988@tust.edu.cn).,2024-04-22
0,Scientific Data,41597,10.1038/s41597-024-03334-9,Transcriptome dynamics of  in response to abiotic stresses by Iso-seq and RNA-seq data,9,5,2024,,"The study utilized publicly available software with clear methodological descriptions of their parameters. In cases where no specific parameters were provided, we opted to use the default parameters as suggested by the software developer.",2024-05-09
0,Scientific Data,41597,10.1038/s41597-024-03392-z,ACcoding: A graph-based dataset for online judge programming,29,5,2024,https://github.com/KarryBramley/ACcoding-Dataset,All the code is freely accessible in .,2024-05-29
0,Scientific Data,41597,10.1038/s41597-024-03193-4,Cataract-1K Dataset for Deep-Learning-Assisted Analysis of Cataract Surgery Videos,12,4,2024,https://github.com/Negin-Ghamsarian/Cataract-1K,"We provide all code for mask creation using JSON annotations and phase extraction using CSV files, as well as the training IDs for four-fold validation and usage instructions in the GitHub repository of the paper ().",2024-04-12
0,Scientific Data,41597,10.1038/s41597-024-03220-4,Phenomics and transcriptomic profiling of fruit development in distinct apple varieties,16,4,2024,,"[{'ext-link': {'@xlink:href': 'https://www.rosaceae.org/species/malus/malus_x_domestica/genome_GDDH13_v1.1', '@ext-link-type': 'uri', '#text': 'https://www.rosaceae.org/species/malus/malus_x_domestica/genome_GDDH13_v1.1'}, '#text': 'The following are the commands for data processing. The analysis is deployed on CentOS 7 platform. All software versions have been specified in the Methods section. The reference genome version we used is GDDH13_v1.1, detailed annotation and gene prediction information can be found here ().'}, '1. Quality control', '$ fastp -i sample_raw_1.fq.gz -o sample_clean_1.fq.gz -I sample_raw_2.fq.gz -O sample_clean_2.fq.gz -r --length_required 60 -f 12', '2. Read mapping', '$ hisat2 --dta --summary-file sample.summary.txt --new-summary --min-introlen 20 --max-introlen 5000 reference.genome -1 sample_clean_1.fq.gz -2 sample_clean_2.fq.gz -S sample.sam', '3. Convert and sort', '$ samtools sort sample.bam sample.sam', '4. Normalize', '$ stringtie -G reference.gff3 -e -B -o sample.gtf -A sample.tab sample.bam']",2024-04-16
0,Scientific Data,41597,10.1038/s41597-024-03364-3,A daily gap-free normalized difference vegetation index dataset from 1981 to 2023 in China,22,5,2024,https://github.com/mainearth/Daily-Gap-free-NDVI-Code.git,The Python codes for generating and processing the daily gap-free NDVI data in China can be accessed through GitHub ().,2024-05-22
0,Scientific Data,41597,10.1038/s41597-024-03420-y,3D motion analysis dataset of healthy young adult volunteers walking and running on overground and treadmill,30,5,2024,https://www.vicon.com/,A custom Python code used to read data is freely available on the dataset (Python Folder). All processing code used by Vicon are available for free on Vicon website ().,2024-05-30
0,Scientific Data,41597,10.1038/s41597-024-03250-y,The Imperial College Storm Model (IRIS) Dataset,24,4,2024,https://github.com/njsparks/iris,The IRIS code is publicly available () and a release of the version described here has been archived.,2024-04-24
0,Scientific Data,41597,10.1038/s41597-024-03307-y,The SPECTRAL Perfusion Arm Clamping dAtaset (SPECTRALPACA) for video-rate functional imaging of the skin,25,5,2024,,The SPECTRALPACA dataset includes the Python code that facilitates loading of the spectral data. This code and its usage instructions can be found in the dataset archive in the folder named .,2024-05-25
0,Scientific Data,41597,10.1038/s41597-024-03137-y,A multimodal driver monitoring benchmark dataset for driver modeling in assisted driving automation,30,3,2024,https://doi.org/10.7910/DVN/SG9TMD,"The custom code “DataExtraction.py” is available from the same repository, (). The code consists of functions to generate  from  and  as well as functions to create images and videos of seat pressure distribution heatmaps. It also includes functions for extracting zipped files of the dataset. The code is tested and run with Python 3.11.",2024-03-30
0,Scientific Data,41597,10.1038/s41597-024-03222-2,A multimodal physiological dataset for driving behaviour analysis,12,4,2024,https://github.com/zwqzwq0/MPDB,"Readers can access the tutorials and code of our original and preprocessed datasets on Github (). Two folders called preprocessing and classification can be found, which contain MATLAB code for preprocessing and python code for classification.",2024-04-12
0,Scientific Data,41597,10.1038/s41597-024-03196-1,A document-level information extraction pipeline for layered cathode materials for sodium-ion batteries,11,4,2024,https://github.com/GGNoWayBack/cathodedataextractor,The source code of the document-level information extraction pipeline is available at .,2024-04-11
0,Scientific Data,41597,10.1038/s41597-024-03223-1,A Prolonged Artificial Nighttime-light Dataset of China (1984-2020),22,4,2024,https://github.com/xian1234/NTLSTM; https://www.mathworks.com/matlabcentral/fileexchange/119308-modest,"The programs used to generate all the results were Python 3.7, MATLAB (R2018b), and ArcGIS (10.4). The code and scripts used for training, testing, and predicting the NTL data are available in the open GitHub repository “”, and the code for calibrating and validating the data is available at “”.",2024-04-22
0,Scientific Data,41597,10.1038/s41597-024-03252-w,"FastMRI Prostate: A public, biparametric MRI dataset to advance machine learning for prostate cancer imaging",20,4,2024,,"[{'ext-link': {'@xlink:href': 'https://github.com/cai2r/fastMRI_prostate', '@ext-link-type': 'uri', '#text': 'https://github.com/cai2r/fastMRI_prostate'}, '#text': 'Code to load, reconstruct, and visualize fastMRI prostate data, as well as code to run training, validation, and testing for the deep-learning-based classification and reconstruction, may be found at .'}, {'ext-link': {'@xlink:href': 'https://github.com/facebookresearch/fastMRI', '@ext-link-type': 'uri', '#text': 'https://github.com/facebookresearch/fastMRI'}, '#text': 'The fastMRI repository (which also contains code for the VarNet) may be found at .'}]",2024-04-20
0,Scientific Data,41597,10.1038/s41597-024-03281-5,Binary dataset for machine learning applications to tropical cyclone formation prediction,3,5,2024,https://github.com/kieucq/tcg-binary-dataset,Both the code and dataset presented herein are fully accessible on our GitHub repository at . All Python codes follow the standard GNU Open Source Licence.,2024-05-03
0,Scientific Data,41597,10.1038/s41597-024-03367-0,Metricizing policy texts: Comprehensive dataset on China’s Agri-policy intensity spanning 1982–2023,22,5,2024,https://github.com/YNAU-WYH/Agricultural-policy-dataset.git,The code data utilized for computations and analyses within this paper is accessible via the GitHub repository at ().,2024-05-22
0,Scientific Data,41597,10.1038/s41597-024-03224-0,Bias-corrected NESM3 global dataset for dynamical downscaling under 1.5 °C and 2 °C global warming scenarios,20,4,2024,https://github.com/Mengzhuo-Zhang/BC_scripts_NESM3; https://www.ncl.ucar.edu; https://code.mpimet.mpg.de/projects/cdo,"The code used to produce the bias-corrected NESM3 simulations is publicly available on the GitHub repository:  (last access: 21 January 2024). The code consists of an NCL (version 6.6.2, ) script to compute non-linear trends of the variables and a few CDO (version 1.7.0, ) scripts to regrid data and correct NESM3 bias.",2024-04-20
0,Scientific Data,41597,10.1038/s41597-024-03282-4,High resolution Tibetan Plateau regional reanalysis 1961-present,3,5,2024,https://github.com/wrf-model/WRF/tags; https://www2.mmm.ucar.edu/wrf/users; https://github.com/PayphoneChoou/TPRR_CODE,The TPRR setup is available through the Weather Research and Forecasting (WRF) model. WRF is provided through a git repository () available at the model’s website (). The users can download the source code of WRF model in the git repository anonymously. The Python scripts used in this study for data post processing can be available through the following git repository: .,2024-05-03
0,Scientific Data,41597,10.1038/s41597-024-03339-4,Comparing built-up area datasets to assess urban exposure to coastal hazards in Europe,15,5,2024,https://doi.org/10.6084/m9.figshare.c.6839949,"The python code to create the DLECZ, classify the settlement types and create built-up area density maps from building vector data in order to validate the results, is available at .",2024-05-15
0,Scientific Data,41597,10.1038/s41597-024-03397-8,"Time-of-Flight MRA of Intracranial Aneurysms with Interval Surveillance, Clinical Segmentation and Annotations",30,5,2024,,"The described dataset was manually organised according to BIDS (v1.8.0). DICOM files were converted to NIFTI format using MRIcroGL dcm2niix software (NITRC, v1.0.20220720). Masks and annotations were prepared using 3DSlicer software (v5.2.1). Python (Spyder, v5.4.3) was used to order scans by imaging modality and scanning dates, and for the stripping of unnecessary NIFTI slices in some patients where portions of the skull not related to the Circle of Willis were also imaged. Code used for these purposes is not published as it is not required for generation or processing of the published dataset.",2024-05-30
0,Scientific Data,41597,10.1038/s41597-024-03169-4,Semantic integration of diverse data in materials science: Assessing Orowan strengthening,30,4,2024,demo-orowan; PMD Core Ontology (PMDco); Tensile Test Ontology (TTO); Precipitate Geometry Ontology (PGO),The script and the ontologies utilized in this study are openly accessible and can be obtained from the following GitHub repositories:• • • •,2024-04-30
0,Scientific Data,41597,10.1038/s41597-024-03225-z,BOLD: Blood-gas and Oximetry Linked Dataset,24,5,2024,,"['All code used for data extraction, processing, visualization, and technical validation is available as SQL queries (Google’s Bigquery syntax) and Jupyter notebooks in the corresponding PhysioNet page and on GitHub.', {'ext-link': {'@xlink:href': 'https://github.com/joamats/pulse-ox-dataset', '@ext-link-type': 'uri', '#text': 'https://github.com/joamats/pulse-ox-dataset'}}, {'italic': ['2_CONSORT_diagram.ipynb', '3_tableones.ipynb', '4_technical_validation.ipynb, and 5_missingness.ipynb'], '#text': '1.\u2009\u2009\u2009\u2009The publicly-available scripts are structured as follows: 1. The folders MIMIC-III, MIMIC-IV, and eICU-CRD contain the SQL queries to fetch the data, alongside auxiliary tables that need to be created first in a user’s BigQuery environment. 2. The source folder contains the Jupyter notebook (1_dataset.ipynb) to create the dataset, which is calling the main SQL scripts needed to create the final CSV file. It also contains the notebooks , ,  to recreate all the analyses.'}, {'italic': ['2_CONSORT_diagram.ipynb', '3_tableones.ipynb', '4_technical_validation.ipynb, and 5_missingness.ipynb'], '#text': '2.\u2009\u2009\u2009\u2009The source folder contains the Jupyter notebook (1_dataset.ipynb) to create the dataset, which is calling the main SQL scripts needed to create the final CSV file. It also contains the notebooks , ,  to recreate all the analyses.'}]",2024-05-24
0,Scientific Data,41597,10.1038/s41597-024-03254-8,A dataset for fatigue estimation during shoulder internal and external rotation movements using wearables,27,4,2024,,We supply the raw data in.csv files and have not used any ad-hoc code to process them.,2024-04-27
0,Scientific Data,41597,10.1038/s41597-024-03283-3,Multispectral analysis-ready satellite data for three East African mountain ecosystems,9,5,2024,https://doi.org/10.17192/FDR/166,The code used to process the Sentinel-2 data with  version 3.7.10 can be obtained via data_UMR .,2024-05-09
0,Scientific Data,41597,10.1038/s41597-024-03310-3,Transient dataset of household appliances with Intensive switching events,14,5,2024,,"['We used Python to write programmes to process and validate the dataset. Here are some of the programmes we used:', '• Check.py: This programme is used to validate and check the accuracy of the dataset.', '• ShowWave.py: This file is used to view the waveform of the current when the appliance is started.', '• ShowWaveFFT.py: This file is used to perform a Fast Fourier Transform on the current and voltage waveforms.', '• LWIP_NILMF417IGT_MakeFile_CRC_new: This folder contains the embedded programme used in the data acquisition section.', '• Upper_computer_monitoring_system.mp4: This file is the video of the monitoring programme of the upper computer during the aggregation data collection.', {'ext-link': [{'@xlink:href': 'https://github.com/TagEnd/TDHA-Acquisition-System-Submit', '@ext-link-type': 'uri', '#text': 'https://github.com/TagEnd/TDHA-Acquisition-System-Submit'}, {'@xlink:href': 'https://www.scidb.cn/en/detail?dataSetId=876623ff38634ccb8426b07146720914&version=V2', '@ext-link-type': 'uri', '#text': 'https://www.scidb.cn/en/detail?dataSetId=876623ff38634ccb8426b07146720914&version=V2'}, {'@xlink:href': 'http://f-lab.ncepu.edu.cn/TDHA', '@ext-link-type': 'uri', '#text': 'http://f-lab.ncepu.edu.cn/TDHA'}], '#text': 'These programmes are helpful in efficiently processing and validating the datasets to ensure the correctness and usability of the data. The source code of this programme has been posted on  and the TDHA dataset has been posted both on the Science Databank  and on our custom platform .'}]",2024-05-14
0,Scientific Data,41597,10.1038/s41597-024-03398-7,ChineseEEG: A Chinese Linguistic Corpora EEG Dataset for Semantic Alignment and Neural Decoding,29,5,2024,https://github.com/ncclabsustech/Chinese_reading_task_eeg_processing,"The code for all modules is openly available on GitHub (). All scripts were developed in Python 3.10. Package openpyxl v3.1.2 was utilized to export segmented text in Excel (.xlsx) files, and egi-pynetstation v1.0.1, g3pylib v0.1.1, psychopy v2023.2.3 were used to implement the scripts for EGI device control, Tobii eye-tracker control, stimuli presentation respectively. In the data pre-processing scripts, MNE v1.6.0, pybv v0.7.5, pyprep v0.4.3, mne-iclabel v0.5.1 were used to implement the pre-processing pipeline, while mne-bids v0.14 was used to organize the data into BIDS format. The text embeddings were calculated using Hugging Face transformers v4.36.2. For more details about code usage, please refer to the GitHub repository.",2024-05-29
0,Scientific Data,41597,10.1038/s41597-024-03311-2,ROSPaCe: Intrusion Detection Dataset for a ROS2-Based Cyber-Physical System and IoT Networks,10,5,2024,,"[{'italic': 'codes', 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR47', '#text': '47'}}, '#text': 'The software code to run the entire experimental campaign can be downloaded from the  folder of our open software repository. Each subfolder corresponds to a specific activity of the dataset creation procedure reported in this paper.'}, 'The ROSPaCe proposed versions are available in a public repository and can be freely downloaded. The repository contains a folder for each of the datasets, and an additional CSV file that enlists the features included.']",2024-05-10
0,Scientific Data,41597,10.1038/s41597-024-03312-1,The Poses for Equine Research Dataset (PFERD),15,5,2024,,"['We make available functions for users to use our datasets:', '• Loading c3d files and the hSMAL model with the captured parameters to visualize the mocap data and the fitted results.', '• Projecting the reconstructed model in image planes with provided camera information.', {'ext-link': {'@xlink:href': 'https://github.com/Celiali/test.git', '@ext-link-type': 'uri', '#text': 'https://github.com/Celiali/PFERD.git'}, '#text': '• Quantitative evaluation using the mocap data and silhouette subsets. Further detail about environment settings and code usage can be found in .'}]",2024-05-15
0,Scientific Data,41597,10.1038/s41597-024-03171-w,An open source knowledge graph ecosystem for the life sciences,11,4,2024,https://github.com/callahantiff/PheKnowLator; https://pypi.org/project/pkt-kg; https://github.com/callahantiff/PheKnowLator/tree/master/builds/deploy/triple-store#readme; https://github.com/callahantiff/PheKnowLator/releases/tag/v2.1.0; https://zenodo.org/record/4685943,The PheKnowLator ecosystem coding resources are described in detail in Supplementary Table  by ecosystem component. The PKT-KG algorithm is publicly available through GitHub () and PyPI (). The SPARQL Endpoint deployment code and documentation are also available through GitHub: . A list of the computational resources used to evaluate the PheKnowLator ecosystem is provided in Supplementary Table . The code used to scrape the GitHub API is available from Zenodo. The survey of open-source KG construction tools is also available on Zenodo. The v2.1.0 PheKnowLator code is available on GitHub () and from Zenodo ().,2024-04-11
0,Scientific Data,41597,10.1038/s41597-024-03371-4,Exploring SureChEMBL from a drug discovery perspective,16,5,2024,https://github.com/Fraunhofer-ITMP/patent-clinical-candidate-characteristics,The Python scripts and Jupyter notebooks supporting the conclusions of this study can be accessed and downloaded via GitHub (). The repository is structured into “Data” and “Notebook” sections. The “Data” section is the exact replica of the Zenodo dump mentioned previously. The “Notebook” section includes all the analyses presented in this study organized based on the sections within the results.,2024-05-16
0,Scientific Data,41597,10.1038/s41597-024-03172-9,Morphometric dataset of  for non-invasive sex identification using machine learning,5,4,2024,https://www.kaggle.com/code/ariffazlanalymann/ml-varanus-morphology-sex-prediction,"The Python code utilised in this study is available on Kaggle, via the link: .",2024-04-05
0,Scientific Data,41597,10.1038/s41597-024-03287-z,A global time series of traffic volumes on extra-urban roads,8,5,2024,,The Python and R code used to create the road networks and perform the analyses is provided together with the output data (see section “Data Records”).,2024-05-08
0,Scientific Data,41597,10.1038/s41597-024-03088-4,Fisheries Inspection in Portuguese Waters from 2015 to 2023,10,4,2024,,"[{'ext-link': [{'@xlink:href': 'https://github.com/ricardomourarpm/Fishery_Inspection_PT_2017_23', '@ext-link-type': 'uri', '#text': 'https://github.com/ricardomourarpm/Fishery_Inspection_PT_2017_23'}, {'@xlink:href': 'https://colab.research.google.com/', '@ext-link-type': 'uri', '#text': 'https://colab.research.google.com/'}, {'@xlink:href': 'https://jupyter.org/install.html', '@ext-link-type': 'uri', '#text': 'https://jupyter.org/install.html'}], '#text': 'The code used for the extraction, translation, pre-processing, and protection of the vessel identification is available in a GitHub repository (). To run the provided code, it is possible to run it locally using Python in a Jupyter Notebook or even use the Anaconda Distribution, or you can use it directly online using, e.g., the Google Colaboratory (). The Anaconda Distribution () is an excellent choice for scientific computing and data science as it includes Python, Jupyter Notebook, and other commonly used packages.'}, {'italic': ['4_1_k_identifiers_analysis.py', '5_Anonimization.py'], 'ext-link': {'@xlink:href': 'https://github.com/ricardomourarpm/Fishery_Inspection_PT_2017_23', '@ext-link-type': 'uri', '#text': 'https://github.com/ricardomourarpm/Fishery_Inspection_PT_2017_23'}, '#text': 'One must use synthetically generated or protected data to run the codes since the original dataset cannot be shared for privacy and confidentiality reasons. Moreover, some values/variable vectors in files  and  are not real (), as they should be withheld from the general public to ensure data protection and privacy.'}]",2024-04-10
0,Scientific Data,41597,10.1038/s41597-024-03144-z,MultiSenseBadminton: Wearable Sensor–Based Biomechanical Dataset for Evaluation of Badminton Performance,5,4,2024,https://github.com/dailyminiii/MultiSenseBadminton,"Software is available on GitHub and can be accessed via the following link. . This comprehensive software package includes examples for reading and parsing HDF5 files, performing data preprocessing by extracting and filtering, and displaying the results. Additionally, it offers functionality for training a deep-learning model using the preprocessed data, generating a T-SNE plot based on the preprocessed data, and creating a visualization video based on the raw data presented in Fig. .",2024-04-05
0,Scientific Data,41597,10.1038/s41597-024-03344-7,A dataset of ambient sensors in a meeting room for activity recognition,21,5,2024,,"[{'ext-link': [{'@xlink:href': 'https://github.com/cdsnlab/ScientificData', '@ext-link-type': 'uri', '#text': 'https://github.com/cdsnlab/ScientificData'}, {'@xlink:href': '10.6084/m9.figshare.24558619', '@ext-link-type': 'doi', '#text': 'https://doi.org/10.6084/m9.figshare.24558619'}, {'@xlink:href': 'https://zenodo.org/records/7763477', '@ext-link-type': 'uri', '#text': 'https://zenodo.org/records/7763477'}], 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR31', '#text': '31'}}, '#text': 'The extraction of collected data, annotation of activities, conversion of collected data into formatted data record files, and analysis of sensor data utilize Python 3.7 and a variety of Python libraries. The codes are available on our lab’s GitHub site (). The DOO-RE dataset files and outcomes of data collection are hosted on the public repositories, accessible at figshare () and Zenodo (), with further inquiries welcome via contacting the corresponding authors. For optimal viewing, it is advised to open DOO-RE’s sensor data files, which are in CSV format, using Excel. JSON format metadata files can be easily viewed or modified using any text editor.'}, 'Our MQTT-based IoT system (JAVA-based), named Lapras, gathers data from ambient sensors, actuators, and a camera. This system archives the raw data in a MongoDB database and uses Python to preprocess and extract sensor data for the creation of DOO-RE. Information and code for this collected system can also be requested through the lab’s website site or the corresponding author. We are open to assisting and providing information about DOO-RE, with the exception of requests for privacy-sensitive content, such as video recordings.']",2024-05-21
0,Scientific Data,41597,10.1038/s41597-024-03230-2,"HiMIC-Monthly: A 1 km high-resolution atmospheric moisture index collection over China, 2003–2020",24,4,2024,https://doi.org/10.5281/zenodo.8352539,Sample codes for developing the HiMIC-Monthly dataset are available from Zenodo at .,2024-04-24
0,Scientific Data,41597,10.1038/s41597-024-03317-w,A Dataset for Evaluating Contextualized Representation of Biomedical Concepts in Language Models,4,5,2024,https://github.com/hrouhizadeh/BioWiC,"The entire process, including the development of the dataset and the conduction of experiments, was implemented using the Python programming language. The complete code and dataset are hosted on GitHub at: .",2024-05-04
0,Scientific Data,41597,10.1038/s41597-024-03375-0,transcriptome assembly of a lipoxygenase knock-down strain in the diatom,22,5,2024,,"The article includes a list of software programs, such as de novo transcriptome assembly, pre- and post-assembly procedures, and transcriptome annotation, all of which are specified alongside their respective versions within the Methods section. If specific parameter details are not provided, the programs were used with their default settings.",2024-05-22
0,Scientific Data,41597,10.1038/s41597-024-03147-w,Satellite-based time-series of sea-surface temperature since 1980 for climate applications,29,3,2024,,"[{'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR114', '#text': '114'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR127', '#text': '127'}}], 'ext-link': {'@xlink:href': 'https://github.com/bcdev/MMS', '@ext-link-type': 'uri', '#text': 'https://github.com/bcdev/MMS'}, '#text': 'The Multi-sensor Matchup System is available from .'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR43', '#text': '43'}}, '#text': 'SLSTR pre-processing code to regrid Vis/NIR channels to match infrared bands.'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR128', '#text': '128'}}, '#text': 'Code used to validate level 3 and 4 products is available'}]",2024-03-29
0,Scientific Data,41597,10.1038/s41597-024-03261-9,A road surface reconstruction dataset for autonomous driving,6,5,2024,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR38', '#text': '38'}}, 'ext-link': {'@xlink:href': 'https://github.com/ztsrxh/RSRD_dev_toolkit', '@ext-link-type': 'uri', '#text': 'https://github.com/ztsrxh/RSRD_dev_toolkit'}, 'italic': ['projection.py', 'read_imu_rtk.py', 'data_reader.py', 'Dataloader', 'cam_extrinsic.py'], '#text': 'We provide a development kit programmed with Python language for this dataset, which contains scripts for visualizing and parsing the dataset. The toolkit is available at the code repository (). The  provides functions for reading calibration parameters, reading disparity and depth maps, projecting points onto images and pixels onto points, as well as their visualization. The  shows an example that parses the motion information and convert them into relative location under ENU coordinate. The  implements the  in PyTorch that provides training samples. The  implements the calculation of camera extrinsic parameter between two time clocks. The extrinsic is presented as the translation and rotation matrices from the current time to the origin.'}, 'The code has MIT license for unrestricted usage.']",2024-05-06
0,Scientific Data,41597,10.1038/s41597-024-03290-4,A high-resolution dataset of water bodies distribution over the Tibetan Plateau,4,5,2024,https://github.com/Siyu1993/WaterPreprocessing,"Codes for the dataset pre-processing are written using python, including TIFF read, morphological opening-and-closing operation, TIFF write and mosaic process. The codes are available at: . Then the image could be visualized in QGIS software (V3.16).",2024-05-04
0,Scientific Data,41597,10.1038/s41597-024-03204-4,High-quality faba bean reference transcripts generated using PacBio and Illumina RNA-seq data,9,4,2024,,"No specific code was developed in this work. The parameters of bioinformatics tools and all software used for data processing were described in the Methods section and Supplementary table . If no detailed parameters are mentioned, the default parameters were used.",2024-04-09
0,Scientific Data,41597,10.1038/s41597-024-03319-8,Typical and extreme weather datasets for studying the resilience of buildings to climate change and heatwaves,23,5,2024,https://zenodo.org/record/7300024#.ZBbi4XbMI2x,The source codes to generate these datasets from CORDEX climate data can be found at: .,2024-05-23
0,Scientific Data,41597,10.1038/s41597-024-03149-8,A unified dataset for the city-scale traffic assignment model in 20 U.S. cities,29,3,2024,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR48', '#text': '48'}}, '#text': 'The guidelines for data retrieval and utilization have been uploaded to GitHub. The specific contents comprise:'}, {'bold': 'Input data introduction.ipynb', '#text': '1. : A brief introduction and data demonstration about the input data for the traffic assignment process in the dataset.'}, {'bold': 'A guide for TransCAD users.md', '#text': '2. : It is a guide for users who want to view and modify the dataset in the Graphical User Interface (GUI) of TransCAD.'}, {'bold': 'AequilibraE_assignmnet.py', '#text': '3. : A Python code file for users who want to get access to the traffic assignment results by using the AqeuilibraE.'}]",2024-03-29
0,Scientific Data,41597,10.1038/s41597-024-03234-y,An EEG Dataset of Neural Signatures in a Competitive Two-Player Game Encouraging Deceptive Behavior,16,4,2024,https://github.com/yiyuchen-lab/DeceptionGame,"The EEG preprocessing, ERP analysis code and code used for classification is avaliable at .",2024-04-16
0,Scientific Data,41597,10.1038/s41597-024-03378-x,"Global high-resolution growth projections dataset for rooftop area consistent with the shared socioeconomic pathways, 2020–2050",30,5,2024,https://doi.org/10.5281/zenodo.11085013,We have documented within the Data Descriptor the Pseudocodes that support the methodology of this study. Codes used for inferencing results along with XGBoost model generated in this study are hosted at Zenodo ().,2024-05-30
0,Scientific Data,41597,10.1038/s41597-024-03179-2,Dataset of building locations in Poland in the 1970s and 1980s,5,4,2024,https://github.com/Szubbi/WallToWallMapingBuildingsPoland,Data processing was performed using the ESRI Arcpy Python Library. Spatial analysis and map production was done using ArcGIS Pro software. Plots were generated with Matplotlib Python Library. Code written for maps processing and detections postprocessing is available at GitHub repository: .,2024-04-05
0,Scientific Data,41597,10.1038/s41597-024-03264-6,A chromosome-level haplotype-resolved genome assembly of oriental tobacco budworm (,6,5,2024,,"['All bioinformatic tools were executed following their respective protocols and manuals. The software version used was described in Methods. Below is detailed parameter information about some bioinformatics tools.', {'bold': 'Genome size estimation'}, 'jellyfish count -C -m 21 -s 50000000000 -t 32 reads_R*.fq -o reads.jf', 'jellyfish histo -t 32 reads.jf >reads.histo', 'genomescope.R -i reads.histo -o output_dir -k 21', {'bold': 'Genome assembly'}, 'hifiasm -o hass --primary -t 48 --h1 hic_read1.fq.gz --h2 hic_read2.fq.gz \\', '--ul ont.reads.fq.gz hifi_reads.fastq.gz 2\u2009>\u2009asm.log', 'yak count -k31 -b37 -t16 -o pat.yak paternal.fq.gz', 'yak count -k31 -b37 -t16 -o mat.yak maternal.fq.gz', 'hifiasm -o hass -t 48 -1 pat.yak -2 mat.yak /dev/null 2\u2009>\u2009asm.trio.log', {'bold': 'Purge haplotigs'}, 'minimap2 -t 48 -ax map-hifi hass.p_ctg.fa hifi_reads.fastq.gz --secondary\u2009=\u2009no | samtools sort -@ 48 -m 1\u2009G -o hifi_read.aln.bam -T tmp.align', 'purge_haplotigs hist -b hifi_read.aln.bam -g hass.p_ctg.fa -t 48', 'purge_haplotigs cov -i hifi_read.aln.bam.gencov -l 15 -m 68 -h 140', 'purge_haplotigs purge -g hass.p_ctg.fa -c coverage_stats.csv -t 48', {'bold': 'Genome sequences correction'}, 'yak count -t 48 -k 21 -b 37 -o k21.yak femal.illumina.reads.gz', 'yak count -t 48 -k 31 -b 37 -o k31.yak femal.illumina.reads.gz', 'nextPolish2 -t 48 -o curated.np2.fasta hifi_read.aln.bam curated.fasta k21.yak k32.yak', {'bold': 'Hi-C data analysis'}, 'juicer.sh -s DpnII -g hass -z curated.np2.fasta -t 60 -p chrom.sizes', {'bold': 'Busco analysis'}, 'busco -m genome -i genome.fasta -l insecta_odb10 -o busco_out --cpu 45 –offline', {'bold': 'HiFi reads mapping'}, 'minimap2 -t 48 -ax map-hifi genome.fasta hifi_reads.fastq.gz\u2009>\u2009hifi_read.aln.sam', {'bold': 'Transcript assembling'}, 'hisat2 -p 48 -q -x genome.index -1 $j.1.fq.gz -2 $j.2.fq.gz -S $j.sam', 'samtools view -bS -@ 10 -o $j.bam $j.sam', 'samtools sort -@ 10 -o $j.sorted.bam $j.bam', 'stringtie $j.sorted.bam -p 16 -o $j.gtf', 'ls *.gtf\u2009>\u2009gtf.list', 'taco_run -p 16 gtf.list', {'bold': 'Repeat annotation'}, 'EDTA.pl --genome genome.fa --cds transcript.cds --sensitive 1 --threads 45 --anno 1 --overwrite 1 --species others --force 1', 'RepeatMasker -lib repeat.lib -pa 48 -html -xsmall -gff genome.fa\u2009>\u2009repeatmasker.log', {'bold': 'Gene prediction'}, 'braker.pl --species\u2009=\u2009hass I am running a few minutes late; my previous meeting is running over.', '--genome\u2009=\u2009genome.fa.mod.MAKER.masked I am running a few minutes late; my previous meeting is running over.', '--bam rna.aln.bam \\', '--prot_seq\u2009=\u2009Arthropoda.10.pep.fa \\', '--gff3 --threads\u2009=\u200948 --workingdir\u2009=\u2009braker3_out --min_contig\u2009=\u200910000 --overwrite --addUTR\u2009=\u2009on', {'bold': 'Genome annotation'}, 'emapper.py -i pep.fa -o pep.fa --itype proteins --cpu 32 --excel --evalue 1.0e-5', 'pfam_scan.pl -fasta pep.fa -dir PfamScan/data/35.0 -outfile pfam_out.tbl -e_seq1.0e-5 -e_dom 1.0e-5 -cpu 8', 'blastp -query pep.fa -db tremble_invertebrates -evalue 1.0e-5 -num_threads 16 -out blastp.tremble.out -max_target_seqs. 1 -outfmt 6 -subject_besthit']",2024-05-06
0,Scientific Data,41597,10.1038/s41597-024-03320-1,The BELSAR dataset: Mono- and bistatic full-pol L-band SAR for agriculture and hydrology,20,5,2024,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR15', '#text': '15'}}, '#text': 'The codes used to produce the integrated dataset from SAR, vegetation and soil data have been uploaded to figshare along with it. These contain a number of tools that can be easily adapted and reused to use the BELSAR data for other purposes.'}, 'To rebuild the integrated dataset from the BELSAR-Campaign data, the contents of the ESA repository must first be downloaded. Then, running the following python command:', {'monospace': 'python integrated_dataset.py path_output path_radardata path_insitu'}, {'italic': ['path_output', 'path_radardata', 'path_insitu', 'RadarData', 'Insitu', 'in situ'], '#text': 'where , , and  are a user-defined path to a directory where output files will be written and the paths to  and , respectively, will extract zonal statistics from the SAR data and match them to the corresponding  vegetation and soil measurements andto generate the integrated dataset. For test purposes, this scripts can be run for one or a subset of SAR acquisitions by adding their indices, from 0 to 319, as arguments to the python command, e.g.,'}, {'monospace': 'python integrated_dataset.py path_output path_radardata path_insitu 319'}, 'for the last acquisition and', {'monospace': 'python integrated_dataset.py path_output path_radardata path_insitu 317 319'}, 'for the last three acquisitions.']",2024-05-20
0,Scientific Data,41597,10.1038/s41597-024-03406-w,"The O3 guidelines: open data, open code, and open infrastructure for sustainable curated scientific resources",29,5,2024,,,2024-05-29
0,Scientific Data,41597,10.1038/s41597-024-03236-w,Sea-surface CO maps for the Bay of Bengal based on advanced machine learning algorithms,13,4,2024,https://github.com/APJ1812/INCOIS_pCO2,The code used to create the final product (different machine learning models) is available at . The study uses general machine learning codes available in Python.,2024-04-13
0,Scientific Data,41597,10.1038/s41597-024-03321-0,"A tree-based corpus annotated with Cyber-Syndrome, symptoms, and acupoints",10,5,2024,https://github.com/Xiduoduosci/CS_A_corpus,"Write data analysis code using Python and install packages such as Nltk, Numpy, and Pandas to assist. The code runs on the local computer. The data annotation in this paper uses the Brat tool (version: Brat-1.3p1), running on a Linux system. The code is mainly used for CS-A corpus generation and the quality analysis of the corpus. The code has been uploaded to the  repository and is accessible using the following link: .",2024-05-10
0,Scientific Data,41597,10.1038/s41597-024-03122-5,Registered multi-device/staining histology image dataset for domain-agnostic machine learning models,3,4,2024,https://github.com/p024eb/PLISM-registration,All codes used in the image registration between WSI and smartphone images described in the manuscript were written in Python 3 and are available through our GitHub repository (). We have provided all the necessary libraries and python scripts that allow the tracing of our results.,2024-04-03
0,Scientific Data,41597,10.1038/s41597-024-03180-9,Large language model enhanced corpus of CO reduction electrocatalysts and synthesis procedures,6,4,2024,https://github.com/cxqwindy/CO2_reduction_electrocatalysts_db; rxn4chemistry; chemdataextractor.org; radimrehurek.com; PyMuPDF; www.pytorch.org; scikit-learn.org,"The scripts utilized to parse articles and extract entities are home-written codes which are publicly available at the github repository . The underlying machine-learning libraries used in this project are all open-source: rxn4chemistry(), ChemDataExtractor (), gensim (), PyMuPDF(), Pytorch () and scikit-learn ().",2024-04-06
0,Scientific Data,41597,10.1038/s41597-024-03208-0,The Plegma dataset: Domestic appliance-level and aggregate electricity demand with metadata from Greece,12,4,2024,https://github.com/sathanasoulias/Plegma-Dataset,"The dataset can be efficiently managed, visualized and preprocessed using four Jupyter notebooks. These notebooks are accessible for download at  To ensure the proper functioning of these notebooks, it is necessary to have Python version 3 along with the Pandas, Plotly and Numpy libraries installed. Moreover, the primary Javascript functions used in the data collection process (Z-wave service and DataBroker service) are located in the data_collection folder giving more details about the implementation of such a system.",2024-04-12
0,Scientific Data,41597,10.1038/s41597-024-03322-z,A chromosome-level genome assembly of an avivorous bat species (),10,5,2024,https://github.com/life404/genome-NycAvi.git,"In this study, all analyses were conducted following the manuals and tutorials of software and pipeline. The detailed software versions are specified in the methods section. Unless specified otherwise, default or author-recommended parameters were used for software and analysis pipeline. Detailed information about the parameters and custom scripts utilized in this research can be obtained by downloading them from .",2024-05-10
0,Scientific Data,41597,10.1038/s41597-024-03380-3,Long non-coding RNAs expression and regulation across different brain regions in primates,28,5,2024,https://github.com/NavandarM/PrimBrainLnc.git; https://github.com/NavandarM/lncRNA-expression-and-regulation-across-different-brain-regions-in-primates.git,The source code for database is available in GitHub at . The commands for tools used for lncRNA prediction and scripts for its downstream analysis is available at .,2024-05-28
0,Scientific Data,41597,10.1038/s41597-024-03267-3,Normative volumes and relaxation times at 3T during brain development,25,4,2024,,"Code for generating derivative tables from the Brain Quantifier outputs, converting model slopes to nifti files, and automate the generation of scan screenshots was implemented in python. Modeling and statistics were done in R. All scripts are available in the dataset “code” folder. T1 and T2 maps as well as volumetric results were obtained using research applications for which access was granted by research collaboration agreements between the authors.",2024-04-25
0,Scientific Data,41597,10.1038/s41597-024-03296-y,County-level intensity of carbon emissions from crop farming in China during 2000–2019,6,5,2024,,Python and ArcGIS are the software used to generate all the results. The code used for the downscaling calculations is available on the Figshare.,2024-05-06
0,Scientific Data,41597,10.1038/s41597-024-03323-y,ATOMIX benchmark datasets for dissipation rate measurements using shear probes,21,5,2024,https://github.com/SCOR-ATOMIX/ShearProbes_BenchmarkDescriptor; https://doi.org/10.5281/zenodo.10610150,"Matlab computer software used to read the data and produce the figures from the netCDF files, together with a Python script to check the required content of an ATOMIX netCDF file is available from the ATOMIX Shear Probes GitHub repository (). The present paper is based on version 1.0, available from Fer ., at .",2024-05-21
0,Scientific Data,41597,10.1038/s41597-024-03153-y,Standardised Versioning of Datasets: a FAIR–compliant Proposal,9,4,2024,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR29', '#text': '29'}}, '#text': 'The authors declare that the data supporting the findings of this study are available within the paper and its Supplementary Information files. Should raw data files be needed in another format, they are available from the original sources, allocated in the third-party repositories listed below:'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR31', '#text': '31'}}, '#text': '• DS 01, SML2010, available from;'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR32', '#text': '32'}}, '#text': '• DS 02, Hungarian chickenpox cases, available from;'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR33', '#text': '33'}}, '#text': '• DS 03, Global land temperature, available from;'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR34', '#text': '34'}}, '#text': '• DS 04, Sales prediction, available from;'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR35', '#text': '35'}}, '#text': '• DS 05, Air quality, available from;'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR36', '#text': '36'}}, '#text': '• DS 06, Ozone level detection, available from; and'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR37', '#text': '37'}}, '#text': '• DS 07, Dublin footfall counts 2022, available from.'}]",2024-04-09
0,Scientific Data,41597,10.1038/s41597-024-03297-x,"PacBio Hi-Fi genome assembly of , a model for the study of multipartite mutualism in insects",4,5,2024,,"All software and pipelines were executed according to the manual and protocols of the published bioinformatic tools. The version and code/parameters of software have been described in the Methods section. Metabolic network reconstructions were carried out using pathway tools 27.0 (April 12, 2023), with annual updates planned.",2024-05-04
0,Scientific Data,41597,10.1038/s41597-024-03353-6,An fNIRS dataset for driving risk cognition of passengers in highly automated driving scenarios,28,5,2024,https://github.com/benchidefeng/fNIRS-experiment-for-automated-driving-scenarios.git,The relational codes and example mentioned in this study and a brief description (readme.md) have been uploaded in github  Or Please contact the corresponding author with any further queries regarding code availability.,2024-05-28
0,Scientific Data,41597,10.1038/s41597-024-03069-7,"An integrated metagenomic, metabolomic and transcriptomic survey of  across genotypes and environments",5,4,2024,,"The software used for processing the data is described in the methods. A custom Python code, manual curation, and MetAtlas were used for analysis of LC-MS data. Metagenomic analyses used the DOE-JGI Metagenome Annotation Pipeline (v.5.0).",2024-04-05
0,Scientific Data,41597,10.1038/s41597-024-03183-6,Exploring Alashan Ground Squirrel () Diversity: Metagenomic and Transcriptomic Datasets from the Helan Mountains,21,5,2024,,"Tables ,  detail all the software and versions used in this study for transcriptomics and metagenomics, respectively. Unless specific parameter details are provided, the programs were utilised with their default parameters.",2024-05-21
0,Scientific Data,41597,10.1038/s41597-024-03325-w,Cloud micro- and macrophysical properties from ground-based remote sensing during the MOSAiC drift experiment,16,5,2024,,"The presented dataset was processed based on CloudnetPy version 1.39.0. However, to account for the Arctic clouds, modifications such as merging the observational data and the LLS processing needed to be done. The adjusted Cloudnet source code is archived via Zenodo.",2024-05-16
0,Scientific Data,41597,10.1038/s41597-024-03184-5,From home energy management systems to energy communities: methods and data,6,4,2024,,"[{'ext-link': {'@xlink:href': 'https://github.com/aebruano/HEMStoEC', '@ext-link-type': 'uri', '#text': 'https://github.com/aebruano/HEMStoEC'}, 'italic': ['extract_quadro_10(‘2023_06_11_00_00_00’', '2023_06_11_00_00_00.mat', '2023_06_11_00_00_0'], '#text': 'All code for the generation of the dataset was written in Matlab R2022 and can be found at . Daily information is received by the data acquisition system in a zipped file, which should be placed in the same directory (denoted as root directory) of the function files. A sample can be found in 2023_06_11_00_00_00.zip. The README and the VARS files provide information about the format of the files enclosed in the zip file. Matlab data is extracted from the unzipped file using the Matlab function extract_quadro_10.m. The command ) creates a Matlab data file  inside the 0 directory. Gaps are identified and data is interpolated using the function Validate_Quadro_4.m.'}, {'italic': ['2023_06_11_00_00_00_cor.mat', '2023_06_11_00_00_0', 'Validate_Quadro_4(‘2023_06_11_00_00_00’,‘2023_06_11_23_23_59’)', '2023_06_11_00_00_00 to 2023_06_11_23_23_59 excl pst 15', 'min est 1 hr_cor.mat', 'Factor.mat'], '#text': 'A data file  is created, again inside the 0 directory, upon the command .Data with a common time basis is achieved using the Matlab function convert_quadro_10_cor.m. Using the command convert_quadro_10_cor(‘2023_06_11_00_00_00’,‘2023_06_11_23_23_59’,”, minutes(15),hours(1)), the data file \u2009 is created, this time in the root directory. A matlab file, , needs to be placed in the root directory.'}]",2024-04-06
0,Scientific Data,41597,10.1038/s41597-024-03240-0,"A construction waste landfill dataset of two districts in Beijing, China from high resolution satellite images",16,4,2024,,"['The data and predictive models presented in this study are publicly available:', {'bold': '• Dataset', 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR30', '#text': '30'}}, 'ext-link': {'@xlink:href': 'https://zenodo.org/record/8333888', '@ext-link-type': 'uri', '#text': 'https://zenodo.org/record/8333888'}, '#text': '. You can download the images from the Zenodo repository. After downloading, place the train and val files from the Deep Learning Datasets folder into the data folder of the CWLD semantic segmentation model.'}, {'bold': '• CWLD semantic segmentation model', 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR41', '#text': '41'}}, 'ext-link': [{'@xlink:href': 'https://zenodo.org/records/10911443', '@ext-link-type': 'uri', '#text': 'https://zenodo.org/records/10911443'}, {'@xlink:href': 'https://github.com/huangleinxidimejd/CWLD_Model', '@ext-link-type': 'uri', '#text': 'https://github.com/huangleinxidimejd/CWLD_Model'}], '#text': '. Code scripts and project instructions on how to use this dataset to train segmentation models are available for download in the Zenodo repository, and weight files for trained models are also provided for readers to try out the models without training. Visit . The requirements.txt file provides the libraries needed to run the project, and the README.md file describes in detail the deployment process and functionality of each module, as well as the role of the various toolkits in utils. In addition, the model and the corresponding code for executing the model are available on the GitHub platform at .'}]",2024-04-16
0,Scientific Data,41597,10.1038/s41597-024-03299-9,Cryo2StructData: A Large Labeled Cryo-EM Density Map Dataset for AI-based Modeling of Protein Structures,6,5,2024,https://github.com/BioinfoMachineLearning/cryo2struct; https://dataverse.harvard.edu/dataverse/Cryo2StructData; https://doi.org/10.7910/DVN/FCDG0W; https://doi.org/10.7910/DVN/SXNYRE; https://doi.org/10.7910/DVN/CGUENL; https://doi.org/10.7910/DVN/DTV4JF; https://doi.org/10.7910/DVN/2GSSC9; https://doi.org/10.7910/DVN/JMN60H,"The source code and instructions to reproduce our results are freely available at . To keep the data files of Cryo2StructData permanent, we published all data to the Harvard Dataverse (), an online data management and sharing platform with a permanent Digital Object Identifier number for each dataset. The Cryo2StructData Dataverse comprises the Full Cryo2StructData, referred to as  (), along with its associated trained deep transformer model and data splits, referred as  (). Similarly, within the Cryo2StructData Dataverse, we find the Small Subsample of the complete Cryo2StructData, denoted as  (), accompanied by its respective trained deep transformer model and data splits, recognized as  (). Finally, the test dataset has been made available as  (). The metadata of Cryo2StructData is available at .",2024-05-06
0,Scientific Data,41597,10.1038/s41597-024-03384-z,A labelled dataset to classify direct deforestation drivers from Earth Observation imagery in Cameroon,31,5,2024,,"[{'ext-link': {'@xlink:href': 'https://github.com/aedebus/Cam-ForestNet', '@ext-link-type': 'uri', '#text': 'https://github.com/aedebus/Cam-ForestNet'}, '#text': 'The code used to prepare data is available on Github, in the ‘prepare_files’ folder and the code to format the folders in ‘model’\u2009>\u2009’data’\u2009>\u2009’ForestNetDataset’: . The folders are organised to be ready-to-use with our classification model, Cam-ForestNet, by simply unzipping the relevant ‘my_examples’ folder in ‘model’\u2009>\u2009’data’\u2009>\u2009’ForestNetDataset’.'}, 'QGIS 3.24.0, Python 3.8./3.5., and the Google Earth Engine Python API were used. Ubuntu 20.04.3 LTS was used to run the code.']",2024-05-31
0,Scientific Data,41597,10.1038/s41597-024-03212-4,Automated BigSMILES conversion workflow and dataset for homopolymeric macromolecules,11,4,2024,,"[{'sup': [{'xref': [{'@ref-type': 'bibr', '@rid': 'CR7', '#text': '7'}, {'@ref-type': 'bibr', '@rid': 'CR30', '#text': '30'}], '#text': ','}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR38', '#text': '38'}}], '#text': 'The collected SMILES datasets can be accessed in the corresponding supplementary information files and Zenodo.'}, {'ext-link': {'@xlink:href': 'https://github.com/CDAL-SChoi/BigSMILES_homopolymer', '@ext-link-type': 'uri', '#text': 'https://github.com/CDAL-SChoi/BigSMILES_homopolymer'}, '#text': 'The Python codes of the SMILES to BigSMILES conversion algorithm, BigSMILES to SMILES conversion algorithm, and examples of the analysis conducted in this study, including technical validations, are available at GitHub: ().'}]",2024-04-11
0,Scientific Data,41597,10.1038/s41597-024-03327-8,Analysis of AlphaMissense data in different protein groups and structural context,14,5,2024,https://doi.org/10.5281/zenodo.10023059,"Python scripts and IPython notebooks can be accessed at Zenodo (). These files also include scripts, which were used to generate the data in Table  and to create Figs. , . They are stored along with data files in the pub.zip archive and listed in detail in the Methods section and the README.md file. The coloram.py script is a PyMOL plugin for coloring structures in PyMOL.",2024-05-14
0,Scientific Data,41597,10.1038/s41597-024-03356-3,A longitudinal electrophysiological and behavior dataset for PD rat in response to deep brain stimulation,15,5,2024,https://doi.org/10.12751/g-node.lzvqb5,The code for stimulus artifact removal and power spectrum analysis is available on G-Node. ().,2024-05-15
0,Scientific Data,41597,10.1038/s41597-024-03385-y,Head model dataset for mixed reality navigation in neurosurgical interventions for intracranial lesions,25,5,2024,,"The creation of the dataset was entirely based on the open-source software platform, 3D Slicer, without the use of custom code.",2024-05-25
0,Scientific Data,41597,10.1038/s41597-024-03128-z,Physiological data for affective computing in HRI with anthropomorphic service robots: the AFFECT-HRI data set,4,4,2024,,The published data set contains raw anonymized data. All anonymization steps can be found in Section . The SAM mapping can be found in . No additional code was used to generate the data set.,2024-04-04
0,Scientific Data,41597,10.1038/s41597-024-03186-3,A database of groundwater wells in the United States,4,4,2024,,All codes for data standardization are available at the HydroShare data respository.,2024-04-04
0,Scientific Data,41597,10.1038/s41597-024-03271-7,A global dataset of terrestrial evapotranspiration and soil moisture dynamics from 1982 to 2020,3,5,2024,https://github.com/kunlz/Codes.longterm.SiTHv2.product; https://github.com/kunlz/SiTHv2,"The codes to process the data and generate the figures, and the details of selected flux sites are available at . The model codes of SiTHv2 are available at .",2024-05-03
0,Scientific Data,41597,10.1038/s41597-024-03328-7,Global WaterPack - The development of global surface water over the past 20 years at daily temporal resolution,9,5,2024,https://www.python.org/downloads/windows/; https://www.nv5geospatialsoftware.com/Products/IDL; https://gdal.org/index.html; https://download.geoservice.dlr.de/GWP/files/code/,"The generation of the GWP is conducted using multiple processing steps including data acquisition and preparation, classification, interpolation, as well as enhancement of overestimated pixels. Global MODIS data was downloaded and stored in computing environments of DLR’s Earth Observation Center. Further processing tasks have been performed in internal CPU and GPU clusters available at DLR’s Earth Observation Center using DLR proprietary software along with specialized Python (v3.8, ) and IDL (v8.0, ) scripts. Due to the utilization of proprietary tools, it is not possible to openly disclose the implemented processing pipeline to the public. The calculation of the global mosaics at different temporal scales which are available for download at the Geoservice of the DLR Earth Observation Center were carried out using GDAL (Geospatial Data Abstraction Library v.3.6, ). The corresponding scripts are available at .",2024-05-09
0,Scientific Data,41597,10.1038/s41597-024-03386-x,"A large-scale multivariate soccer athlete health, performance, and position monitoring dataset",30,5,2024,http://www.github.com/simula/soccermon,"We provide a sample codebase that can be used to import and structure the raw data from the  dataset, as a public software repository on GitHub: .",2024-05-30
0,Scientific Data,41597,10.1038/s41597-023-02880-y,Mobility networks in Greater Mexico City,18,1,2024,https://doi.org/10.17605/OSF.IO/GWQ6U,"DuckDB and Python were employed for constructing the mobility networks. The entire network dataset, along with the code utilized for network construction and supplementary tables can be found at: .",2024-01-18
0,Scientific Data,41597,10.1038/s41597-024-03129-y,EUPRO - A reference database on project-based R&D collaboration networks,12,3,2024,https://figshare.com/articles/software/cordis_wrapper_rb/24711738,AIT has developed the web scraping script for data collection of parts of EUPRO; it can be retrieved from: .,2024-03-12
0,Scientific Data,41597,10.1038/s41597-024-02959-0,ezBIDS: Guided standardization of neuroimaging data interoperable with major data archives and platforms,8,2,2024,https://github.com/brainlife/ezbids,All code is publicly available on our GitHub repository: .,2024-02-08
0,Scientific Data,41597,10.1038/s41597-024-03100-x,HEvOD: A database of hurricane evacuation orders in the United States,5,3,2024,,Tweets were extracted from Twitter using publicly available scripts and the ‘Twitter API v2 for academic research’. No other code was used in the development of the database.,2024-03-05
0,Scientific Data,41597,10.1038/s41597-024-02930-z,Urban water and electricity demand data for understanding climate change impacts on the water-energy nexus,23,1,2024,,"The code for pre-processing the data, as well as the subsequent analyses, is available online. The code was written in R version 4.1.2 and last ran on April 16, 2022.",2024-01-23
0,Scientific Data,41597,10.1038/s41597-024-02989-8,"Chromosome-level genome assembly and annotation of the yellow grouper,",31,1,2024,,"No custom code was used in this study. All bioinformatics tools, commands and pipelines used in data processing were executed following the manual and protocols provided by the respective software developers. The versions of the software used, along with their corresponding parameters, have been thoroughly described in the Methods section.",2024-01-31
0,Scientific Data,41597,10.1038/s41597-024-03074-w,High resolution climate change observations and projections for the evaluation of heat-related extremes,1,3,2024,https://github.com/emilylynnwilliams/CHC-CMIP6_SourceCode,"The equations used to calculate RH, VPD, and WBGT are available in R on GitHub (). The CHC-CMIP6 dataset was processed using code written in the Interactive Data Language and Python.",2024-03-01
0,Scientific Data,41597,10.1038/s41597-024-02931-y,An fMRI Dataset on Social Reward Processing and Decision Making in Younger and Older Adults,1,2,2024,,"[{'ext-link': [{'@xlink:href': 'https://github.com/DVS-Lab/srndna-datapaper', '@ext-link-type': 'uri', '#text': 'https://github.com/DVS-Lab/srndna-datapaper'}, {'@xlink:href': 'https://zenodo.org/doi/10.5281/zenodo.10456520', '@ext-link-type': 'uri', '#text': 'https://zenodo.org/doi/10.5281/zenodo.10456520'}], '#text': 'All code is available on GitHub without restrictions (). A snapshot of the repository has been placed on Zenodo for permanent archiving ().'}, 'This repository includes a README.md file that describes how the dataset was generated. This repository also includes stimulus presentation scripts and the sourcedata generated by those scripts.']",2024-02-01
0,Scientific Data,41597,10.1038/s41597-024-02960-7,Pig-eRNAdb: a comprehensive enhancer and eRNA dataset of pigs,1,2,2024,https://github.com/WangYF33/CNNEE,All CNNEE code for enhancer prediction and eRNA identification is publicly available at .,2024-02-01
0,Scientific Data,41597,10.1038/s41597-023-02882-w,High-resolution repeat topography of drifting ice floes in the Arctic Ocean from terrestrial laser scanning,13,1,2024,https://zenodo.org/record/8120858,pydar is available at Zenodo ().,2024-01-13
0,Scientific Data,41597,10.1038/s41597-024-03017-5,A 10-m annual grazing intensity dataset in 2015–2021 for the largest temperate meadow steppe in China,10,2,2024,https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html,RF was run with scikit-learn () under Python 3.7. The pre-processing and harmonized code of Landsat-7/8 and Sentinle-2 has been uploaded to Figshare.,2024-02-10
0,Scientific Data,41597,10.1038/s41597-024-02932-x,An 8-model ensemble of CMIP6-derived ocean surface wave climate,20,1,2024,https://github.com/NOAA-EMC/WW3; https://github.com/PCMDI/pcmdi_metrics.git,"The wind wave climate ensemble was produced using WAVEWATCH III v6.07.1 release available at . The wind wave climate ensemble performance was evaluated through the combined use of the Climate Data Operator (CDO) suite and the PCMDI Metrics Package (PMP) Python library, publicly available at .",2024-01-20
0,Scientific Data,41597,10.1038/s41597-024-03018-4,Homologous Recombination Deficiency Unrelated to Platinum and PARP Inhibitor Response in Cell Line Libraries,6,2,2024,https://github.com/shirotak/CellLine_HRD_DrugRes,Codes to reproduce the results of this work are available on the Figshare and the GitHub project page ().,2024-02-06
0,Scientific Data,41597,10.1038/s41597-024-03132-3,Engagnition: A multi-dimensional dataset for engagement recognition of children with autism spectrum disorder,15,3,2024,https://github.com/dailyminiii/Engagnition,The Engagnition software is available to the public through its official repository (). This repository mainly includes the code for analyzing data distribution and technical validation.,2024-03-15
0,Scientific Data,41597,10.1038/s41597-024-02933-w,A dataset of oracle characters for benchmarking machine learning algorithms,18,1,2024,https://github.com/wm-bupt/oracle-mnist,Oracle-MNIST are freely available online at GitHub (). Tutorials for loading the dataset and code for training and testing oracle character recognition models are also publicly available without restriction.,2024-01-18
0,Scientific Data,41597,10.1038/s41597-023-02855-z,An ensemble of bias-adjusted CMIP6 climate simulations based on a high-resolution North American reanalysis,11,1,2024,https://github.com/Ouranosinc/ESPO-G/releases/tag/ESPO-G6-R2v1.0.0; https://github.com/Ouranosinc/ESPO-G,The code to reproduce the dataset ESPO-G6-R2 dataset and the figures from this paper are available in the release ESPO-G6-R2 v1.0.0 () of the ESPO-G GitHub repository (). The code works with  version 0.41.0 and  version 0.5.13.,2024-01-11
0,Scientific Data,41597,10.1038/s41597-024-03019-3,Beyond MD17: the reactive xxMD dataset,20,2,2024,,,2024-02-20
0,Scientific Data,41597,10.1038/s41597-024-03133-2,Mapping of secondary forest age in China using stacked generalization and Landsat time series,16,3,2024,https://github.com/Zhangshaoy/SFAC.git; https://zsy11600.users.earthengine.app/view/sfac,"The codes used in data generation and processing are in two formats, JavaScript used in GEE and Python. The codes are available in GitHub at: (). Each repository includes a guide for the use of the codes. An online visualization map using the GEE experimental app is also provided: ().",2024-03-16
0,Scientific Data,41597,10.1038/s41597-024-02934-9,An 800-year record of benthic foraminifer images and 2D morphometrics from the Santa Barbara Basin,30,1,2024,www.github.com/HullLab/AutoMorph; https://github.com/GregDMeyer/classifier,"Images were processed using  software, which is described in Hsiang . and freely available on GitHub at . The classifier software used to tag images with taxonomic identifications can be found on GitHub at .",2024-01-30
0,Scientific Data,41597,10.1038/s41597-024-02963-4,"High frequency Lunar Penetrating Radar quality control, editing and processing of Chang’E-4 lunar mission",24,1,2024,https://github.com/Giacomo-Roncoroni/LPR_CE4,The codes for the described algorithm will be available in Figshare and at .,2024-01-24
0,Scientific Data,41597,10.1038/s41597-024-03105-6,Will we ever be able to accurately predict solubility?,18,3,2024,,No custom code has been used.,2024-03-18
0,Scientific Data,41597,10.1038/s41597-024-02935-8,A detailed database of sub-annual Spanish demographic statistics: 2005–2021,16,1,2024,https://doi.org/10.3886/E192045V2,"The data analysis methods, software and associated parameters used in this study are described in the section of Methods. The code used to derive all datasets from the raw microdata can be downloaded at .",2024-01-16
0,Scientific Data,41597,10.1038/s41597-023-02886-6,Electricity and natural gas tariffs at United States wastewater treatment plants,23,1,2024,https://github.com/we3lab/wwtp-energy-tariffs,The data is provided as Excel and CSV spreadsheets that can be used without code for manipulation. Sample Python scripts are available on GitHub to ease analysis and demonstrate technical validation procedures ().,2024-01-23
0,Scientific Data,41597,10.1038/s41597-024-03106-5,"PEARL-Neuro Database: EEG, fMRI, health and lifestyle data of middle-aged people at risk of dementia",7,3,2024,,No custom code was used. We share raw data.,2024-03-07
0,Scientific Data,41597,10.1038/s41597-024-03164-9,Harnessing Big Data in Critical Care: Exploring a new European Dataset,28,3,2024,https://github.com/nrodemund/sicdb,"All publicly available code can be accessed from the SICdb GitHub Code Repository (). However, due to the partial use of the code to appropriately remove sensitive patient information in accordance with HIPAA regulations, not all codes are fully publicly accessible. Furthermore, the GDPR restricts the sharing of certain code components to ensure the highest level of anonymization.",2024-03-28
0,Scientific Data,41597,10.1038/s41597-024-02994-x,A 10-m national-scale map of ground-mounted photovoltaic power stations in China of 2020,13,2,2024,https://github.com/MrSuperNiu/PV_ScientificData_Classification_Code,"The GEE code for PV power stations classification based on Sentinel-2 imagery and DEM data is available at . The code is written in JavaScript, including all the mentioned steps in this paper, including feature calculation, random forest training, etc.",2024-02-13
0,Scientific Data,41597,10.1038/s41597-023-02887-5,High-resolution temporal gravity field data products: Monthly mass grids and spherical harmonics from 1994 to 2021,13,1,2024,,"There is no customized code in generation or processing of datasets. For setting up and training the Deep Learning Models, the publicly available codes in Python language from TensorFlow and Keras libraries were used. The trend error mitigation and all the figure plots in the paper were implemented using the existing routines/functions in MATLAB software.",2024-01-13
0,Scientific Data,41597,10.1038/s41597-024-03021-9,A large open access dataset of brain metastasis 3D segmentations on MRI with clinical and imaging information,29,2,2024,https://cbica.github.io/CaPTk/preprocessing_brats.html,The image pre-processing code used to build the dataset can be found at the following link: .,2024-02-29
0,Scientific Data,41597,10.1038/s41597-023-02744-5,Simulated outcomes for durotomy repair in minimally invasive spine surgery,10,1,2024,,"['The SOSpine dataset was prepared as described in the methods sections with minimal custom processing.', {'ext-link': {'@xlink:href': 'https://github.com/AlexeyAB/darknet', '@ext-link-type': 'uri', '#text': 'https://github.com/AlexeyAB/darknet'}, '#text': 'Open-source software used for computer vision training and validation can be found on Github ().'}, {'ext-link': {'@xlink:href': 'https://github.com/alanbalugu/SOSpine', '@ext-link-type': 'uri', '#text': 'https://github.com/alanbalugu/SOSpine'}, '#text': 'Python code used to generate the figures and perform the technical validation can be found within the FigShare project as described above within the SOSpine.zip file. Python code and necessary dataset files can also be found on GitHub ().'}]",2024-01-10
0,Scientific Data,41597,10.1038/s41597-024-02908-x,Annotated dataset for training deep learning models to detect astrocytes in human brain tissue,19,1,2024,https://github.com/qbeer/coco-froc-analysis,In order to do the evaluation we made the Python package () accessible. We generated FROC curves with this tool and generally it is possible to use it for binary evaluation for COCO formatted data.,2024-01-19
0,Scientific Data,41597,10.1038/s41597-024-03051-3,Water footprints and crop water use of 175 individual crops for 1990–2019 simulated with a global crop model,14,2,2024,,The source code for AquaCrop-OSPy v6.1—the crop model upon which ACEA is based—is freely available via github.com/aquacropos/aquacrop. The source code and most inputs for ACEA (version 2.0) are available via Zenodo. Note that some input datasets are not included but can be directly obtained from the original sources instead. You can find brief instructions and references to input datasets in the readme file.,2024-02-14
0,Scientific Data,41597,10.1038/s41597-024-02909-w,Chromosome-level genome assembly of,25,1,2024,,"['nextDenovo: input_type\u2009=\u2009raw, read_type\u2009=\u2009ont, read_cutoff\u2009=\u20091k, seed_cutoff\u2009=\u200934747, sort_options\u2009=\u2009-m 20\u2009g -t 14, minimap2_options_raw\u2009=\u2009-t 14, pa_correction\u2009=\u20098, correction_options\u2009=\u2009-p 14, minimap2_options_cns\u2009=\u2009-t 14, minimap2_options_map\u2009=\u2009-t 14, nextgraph_options\u2009=\u2009-a 1', 'NextPolish: sgs_options\u2009=\u2009-max_depth 100 -bwa, lgs_options\u2009=\u2009-min_read_len 1k -max_depth 100, lgs_minimap2_options\u2009=\u2009-x map-ont', 'TEsorter: -db rexdb-plant', 'Repeatmasker: -pa 14 -s -xsmall', 'Blastp: E-value ≤ 1e-5', 'Swiss-Prot: E-value\u2009≤\u20091e−5', 'Nr: E-value\u2009≤\u20091e−5', 'Orthofinder: -S diamond -M msa -T fasttree', 'trimAl: -gt 0.6 -cons 60', 'RAxML: raxmlHPC-PTHREADS -m PROTGAMMAJTT -f a -p 123 -x 123 -# 100', 'Wgdi: pvalue\u2009=\u20090.05', 'Other commands and pipelines used in data processing were executed using their corresponding default parameters.']",2024-01-25
0,Scientific Data,41597,10.1038/s41597-024-03052-2,A Clinical Breathomics Dataset,14,2,2024,https://doi.org/10.6084/m9.figshare.23522490.v6,The in-house R and Python scripts for GC-MS and heat map analysis are available in the figshare repository ().,2024-02-14
0,Scientific Data,41597,10.1038/s41597-024-03081-x,An Integrated Multi-Source Dataset for Measuring Settlement Evolution in the United States from 1810 to 2020,7,3,2024,https://github.com/YoonjungAhn/HISTPLUS,Code for analysis and validation is available at .,2024-03-07
0,Scientific Data,41597,10.1038/s41597-024-03138-x,Harmonized disposable income dataset for Europe at subnational level,21,3,2024,,"The code supporting the analyses is accessible in the Zenodo through Gitlab repository, under the GNU Affero General Public License v3.0.",2024-03-21
0,Scientific Data,41597,10.1038/s41597-024-03053-1,County-scale dataset indicating the effects of disasters on crops in Taiwan from 2003 to 2022,14,2,2024,https://github.com/YuanChihSu/Crop_Disaster_Dataset,Most of the weather data used in this study were downloaded using a Python script. Only weather data obtained from agricultural weather stations were manually downloaded. All datasets were processed and analyzed using SAS. The Python and SAS codes are available at . A full list of weather station codes and altitudes is also provided.,2024-02-14
0,Scientific Data,41597,10.1038/s41597-024-02998-7,A 30-m annual corn residue coverage dataset from 2013 to 2021 in Northeast China,16,2,2024,https://doi.org/10.6084/m9.figshare.23993517.v4,The programs used to generate all the results were Python (3.10) JavaScript and ArcGIS (10.8). Analysis scripts used in this study will be available at .,2024-02-16
0,Scientific Data,41597,10.1038/s41597-024-03025-5,A synthetic digital city dataset for robustness and generalisation of depth estimation models,16,3,2024,https://github.com/ReparkHjc/SDCD,"A git repository is publicly available at , in this repository several python scripts for visualisation, benchmarking and data pre-processing are available.",2024-03-16
0,Scientific Data,41597,10.1038/s41597-024-03054-0,Sea water temperature and light intensity at high-Arctic subtidal shallows – 16 years perspective,22,2,2024,https://github.com/8ernabemoreno/Isfjorden-shallows_longterm-seawater-temp-lux,"The code that accompanies this data descriptor is publicly available in the GitHub repository . It contains Python code that might be useful for basic level users to create CF-NetCDF (.nc) files from .csv, and (ii) minimally process long-term data (e.g., annual, and monthly means).",2024-02-22
0,Scientific Data,41597,10.1038/s41597-024-03083-9,Cancer-Alterome: a literature-mined resource for regulatory events caused by genetic alterations in cancer,2,3,2024,https://github.com/bionlp-hzau/Cancer-Alterome; https://ftp.ncbi.nlm.nih.gov/entrez/entrezdirect; https://github.com/OntoGene/OGER; https://github.com/ncbi-nlp/PhenoTagger; https://www.ncbi.nlm.nih.gov/research/pubtator/; https://github.com/YaoXinZhi/BERT-CRF-for-BioNLP-OST2019-AGAC-Task1; https://github.com/YaoXinZhi/BERT-for-BioNLP-OST2019-AGAC-Task2; http://www.pytorch.org; http://scikit-learn.ory,"The scripts utilized to parse literature and extract events are home-written codes which are publicly available at GitHub repository . The underlying python3 libraries used in this project are all open-source: E-direct (), OGER++ (), PhenoTagger (), PubTator (), AGAC-based model (AGAC-NER:  and AGAC-RE: ), Pytorch () and sci-kit-learn (). More details on the guidelines of code usage are given in Supplementary .",2024-03-02
0,Scientific Data,41597,10.1038/s41597-023-02891-9,"Occupant behavior, thermal environment, and appliance electricity use of a single-family apartment in China",11,1,2024,,"All data visualizations in this paper were created using Python and its publicly available libraries, including Pandas, NumPy, Matplotlib, Seaborn, and Missingno. A coding guide has been compiled in a Jupyter notebook and is provided along with the dataset in the Figshare repository.",2024-01-11
0,Scientific Data,41597,10.1038/s41597-024-03055-z,High-resolution land use/cover forecasts for Switzerland in the 21st century,23,2,2024,https://doi.org/10.16904/envidat.458,"Code used to produce the LULC forecasts is available on our EnviDat repository (). The script is based on input data which are not included as they are not freely available, therefore it will not run as it is. These input data and their sources are listed in Supplementary Table ; access to them can be requested to their owners. Alternatively, the code can be modified to run on other data.",2024-02-23
0,Scientific Data,41597,10.1038/s41597-024-03027-3,AndroDex: Android Dex Images of Obfuscated Malware,16,2,2024,,"[{'italic': 'AndroDex', '#text': 'The dataset  is publicly available and can be accessed via the following links:'}, {'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR18', '#text': '18'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR17', '#text': '17'}}], 'ext-link': [{'@xlink:href': '10.6084/m9.figshare.23931477.v1', '@ext-link-type': 'doi', '#text': 'https://doi.org/10.6084/m9.figshare.23931477.v1'}, {'@xlink:href': '10.6084/m9.figshare.23931204.v1', '@ext-link-type': 'doi', '#text': 'https://doi.org/10.6084/m9.figshare.23931204.v1'}], '#text': 'Binaries of all files along with the code to convert images of any size are available at: , whereas images converted are available at.'}, {'italic': 'androdex', '#text': 'All the files are password protected to make sure that none of the files were deleted by server and the password is .'}, {'italic': ['AVPass obfuscation technique', 'Obfuscapk technique'], 'ext-link': [{'@xlink:href': 'https://github.com/sslab-gatech/avpass', '@ext-link-type': 'uri', '#text': 'https://github.com/sslab-gatech/avpass'}, {'@xlink:href': 'https://github.com/ClaudiuGeorgiu/Obfuscapk', '@ext-link-type': 'uri', '#text': 'https://github.com/ClaudiuGeorgiu/Obfuscapk'}], '#text': 'The  that applied is available at () whereas  is available at ().'}]",2024-02-16
0,Scientific Data,41597,10.1038/s41597-024-02913-0,A 31-year (1990–2020) global gridded population dataset generated by cluster analysis and statistical learning,24,1,2024,https://github.com/lulingliu/GlobPOP,The fully reproducible codes are publicly available at GitHub ().,2024-01-24
0,Scientific Data,41597,10.1038/s41597-024-02971-4,City-level building operation and end-use carbon emissions dataset from China for 2015–2020,26,1,2024,,All emission calculations involved in the method chapter are completed in Excel.,2024-01-26
0,Scientific Data,41597,10.1038/s41597-024-03086-6,Leuven-Haifa High-Resolution Fundus Image Dataset for Retinal Blood Vessel Segmentation and Glaucoma Diagnosis,29,2,2024,https://pvbm.readthedocs.io/en/latest/,"We provide the  code used to extract the microvascular biomarkers within the RDR repository as a notebook file. The PVBM toolbox code is also available via . For reproducibility and convenience in case any user wants to customize the extraction, all the .py files and a  file are available.",2024-02-29
0,Scientific Data,41597,10.1038/s41597-024-02914-z,Benchmark for welding gun fault prediction with multivariate time series data,18,1,2024,,Step-by-step guidance and the source code for machine learning benchmarks can be found on Zenodo.,2024-01-18
0,Scientific Data,41597,10.1038/s41597-024-02943-8,Dataset of human-single neuron activity during a Sternberg working memory task,18,1,2024,https://github.com/rutishauserlab/workingmem-release-NWB,All code associated with this project is available as open source. The code is available on GitHub (). MATLAB scripts are included in this repository to reproduce all figures shown and to illustrate how to use the data.,2024-01-18
0,Scientific Data,41597,10.1038/s41597-024-02972-3,Extended-wavelength diffuse reflectance spectroscopy dataset of animal tissues for bone-related biomedical applications,26,1,2024,,"[{'ext-link': {'@xlink:href': 'https://github.com/Biophotonics-Tyndall/PUB-DataDescriptorCode.git', '@ext-link-type': 'uri', '#text': 'https://github.com/Biophotonics-Tyndall/PUB-DataDescriptorCode.git'}, '#text': 'Example codes are publicly available in a Github repository at: .'}, 'The repository contains one MATLAB script (DRS_cal_proc_example.m) demonstrating DRS data processing and systematic artifact correction, one Python function (calibDRS.py) for SNV transformation, and three MATLAB functions to namely calculate area-under-curve (AUC_DRS_fnc.m), splice two spectra via spline interpolation (DRS_splice_fnc.m) and calculate SNV transformation (snv_DRS_fnc.m) for DRS data.']",2024-01-26
0,Scientific Data,41597,10.1038/s41597-023-02865-x,Nearshore wave buoy data from southeastern Australia for coastal research and management,12,2,2024,,"All data preparation, processing and analysis, and the generation of plots and data tables were carried out using MATLAB. The code used to process buoy data from displacements to the wave parameter time series data presented here is provided with the data packages on the SEED portal in html format and can be viewed as they appear in MATLAB by anyone without access to that proprietary software.",2024-02-12
0,Scientific Data,41597,10.1038/s41597-023-02780-1,Remotely sensed above-ground storage tank dataset for object detection and infrastructure assessment,12,1,2024,https://github.com/celinerobi/ast-data-pipeline,The raw aerial imagery and annotation tools used in this study are publicly accessible. The source code developed by the authors to process the imagery and develop the tank inventory dataset are available on GitHub ().,2024-01-12
0,Scientific Data,41597,10.1038/s41597-024-03029-1,"Multimodal single-neuron, intracranial EEG, and fMRI brain responses during movie watching in human patients",16,2,2024,https://github.com/rutishauserlab/bmovie-release-NWB-BIDS; https://github.com/mvdoc/budapest-fmri-data,"All code is available in the GitHub repository (). The python code includes scripts to read and plot the data from the NWB files and perform the analyses presented in this data descriptor. The code relies heavily on open-source Python packages such as numpy, scipy, pynwb, mne-python, nilearn, and pycortex. The movie annotation files are also provided in the GitHub repository under ‘assets/annotations’ folder. The scripts related to the estimation of tSNR and ISC were adapted from the code provided in the GitHub repository associated with the budapest-fmri-data study (see: ).",2024-02-16
0,Scientific Data,41597,10.1038/s41597-024-03058-w,Harmonized diffusion MRI data and white matter measures from the Adolescent Brain Cognitive Development Study,27,2,2024,,"['The open-access dMRI data processing software used in this study can be accessed on GitHub. The repositories contain the processing pipeline and scripts used for harmonizing and processing the dMRI data, as well as for performing whole brain tractography and white matter parcellation. Researchers and clinicians interested in utilizing these tools for their own research or clinical applications can easily download and customize the software to fit their specific needs.', 'Additionally, the GitHub repositories include detailed documentation on how to use the software, as well as examples of how to run the scripts on sample data. The repositories also provide information on the dependencies required to run the software, ensuring that researchers have access to all the necessary tools to use the software effectively.', 'By making the dMRI data processing software openly accessible on GitHub, we hope to encourage further research and clinical applications of the software and facilitate collaboration across the scientific community. Please refer to the following GitHub links for each of the specific dMRI data processing software:', 'a) Convolutional neural network dMRI brain segmentation', {'ext-link': {'@xlink:href': 'https://github.com/pnlbwh/CNN-Diffusion-MRIBrain-Segmentation', '@ext-link-type': 'uri', '#text': 'https://github.com/pnlbwh/CNN-Diffusion-MRIBrain-Segmentation'}, '#text': 'b)'}, {'ext-link': [{'@xlink:href': 'https://github.com/pnlbwh/dMRIharmonization', '@ext-link-type': 'uri', '#text': 'https://github.com/pnlbwh/dMRIharmonization'}, {'@xlink:href': 'https://github.com/pnlbwh/multi-shell-dMRIharmonization', '@ext-link-type': 'uri', '#text': 'https://github.com/pnlbwh/multi-shell-dMRIharmonization'}], '#text': 'c) DMRI data harmonization: , in this study we used the multi-shell version of this script:'}, {'ext-link': {'@xlink:href': 'https://github.com/pnlbwh/ukftractography', '@ext-link-type': 'uri', '#text': 'https://github.com/pnlbwh/ukftractography'}, '#text': 'd) UKF two tensor whole brain tractography:'}, {'ext-link': {'@xlink:href': 'https://github.com/SlicerDMRI/whitematteranalysis', '@ext-link-type': 'uri', '#text': 'https://github.com/SlicerDMRI/whitematteranalysis'}, '#text': 'e) White Matter Analysis:'}, {'ext-link': {'@xlink:href': 'http://dmri.slicer.org', '@ext-link-type': 'uri', '#text': 'http://dmri.slicer.org'}, '#text': 'f) SlicerDMRI:'}]",2024-02-27
0,Scientific Data,41597,10.1038/s41597-024-03000-0,The Dresden in vivo OCT dataset for automatic middle ear segmentation,26,2,2024,https://gitlab.com/nct_tso_public/diome,"Scripts for segmentation merging and visualization, statistics calculation and fan-shape correction are publicly available at . All the scripts are written in Python 3.11 and are public under the MIT license.",2024-02-26
0,Scientific Data,41597,10.1038/s41597-023-02895-5,Development of Groundwater Levels Dataset for Chile since 1970,5,2,2024,https://osf.io/swdg9,The code to process the DTW records is archived at the OSF repository:,2024-02-05
0,Scientific Data,41597,10.1038/s41597-024-02945-6,Manually annotated and curated Dataset of diverse Weed Species in Maize and Sorghum for Computer Vision,23,1,2024,https://github.com/grimmlab/MFWD,The code to download the dataset is publicly available for download on GitHub: .,2024-01-23
0,Scientific Data,41597,10.1038/s41597-024-02974-1,Marine picoplankton metagenomes and MAGs from eleven vertical profiles obtained by the Malaspina Expedition,1,2,2024,,"[{'ext-link': {'@xlink:href': 'https://gitlab.com/malaspina-public/picoplankton-vertical-profiles', '@ext-link-type': 'uri', '#text': 'https://gitlab.com/malaspina-public/picoplankton-vertical-profiles'}, '#text': 'All the software used to process the data set presented here is publicly available and distributed by their developers. All versions have been specified in the main text, along with the options used when departing from defaults. Custom scripts used in intermediate or summarizing steps are available at .'}, {'ext-link': {'@xlink:href': 'https://github.com/felipehcoutinho/QueroBins', '@ext-link-type': 'uri', '#text': 'https://github.com/felipehcoutinho/QueroBins'}, '#text': 'Code for bin decontamination step can be found at .'}]",2024-02-01
0,Scientific Data,41597,10.1038/s41597-023-02896-4,Wind and structural loads data measured on parabolic trough solar collectors at an operational power plant,19,1,2024,https://github.com/NREL/NSO_processing_scripts,"The Python processing routines for the met masts, lidar and loads data are publicly available at .",2024-01-19
0,Scientific Data,41597,10.1038/s41597-024-03089-3,A city-level dataset of heavy metal emissions into the atmosphere across China from 2015–2020,29,2,2024,https://github.com/Olivia-2012/HMEAs_DataSet,"Data processing was performed in Python 3.10, and data used for the computation of HMEAs at city level are available can be accessed at Github repository located at . We implemented the procedure described in the Methods section.",2024-02-29
0,Scientific Data,41597,10.1038/s41597-024-02975-0,A clinical microscopy dataset to develop a deep learning diagnostic test for urinary tract infection,1,2,2024,https://github.com/casus/UMOD,All code is available from  under MIT open source licence.,2024-02-01
0,Scientific Data,41597,10.1038/s41597-023-02897-3,A database of thermally activated delayed fluorescent molecules auto-generated from scientific literature with ChemDataExtractor,17,1,2024,https://github.com/Dingyun-Huang/chemdataextractorTADF,The code used to generate the databases in this work can be found at . The repository contains ChemDataExtractor v2.1 which has been modified for text-mining TADF properties; iPython notebooks that demonstrate an example data-extraction pipeline and data-cleaning and post-processing are also provided.,2024-01-17
0,Scientific Data,41597,10.1038/s41597-024-03031-7,The Allen Ancient DNA Resource (AADR) a curated compendium of ancient human genomes,10,2,2024,https://github.com/DReichLab/adna-workflow,"The pipeline used for processing raw data generated within the Reich lab is available in the ‘Workflow Description Language’ (WDL) here: , and includes individual python scripts for components of the pipeline.",2024-02-10
0,Scientific Data,41597,10.1038/s41597-024-03117-2,NuInsSeg: A fully annotated dataset for nuclei instance segmentation in H&E-stained histological images,14,3,2024,https://www.kaggle.com/datasets/ipateam/nuinsseg; https://github.com/masih4/NuInsSeg,"The dataset and required code to generate the dataset are publicly available on Zenodo, Kaggle (), and GitHub (), respectively.",2024-03-14
0,Scientific Data,41597,10.1038/s41597-024-03146-x,The extrachromosomal circular DNA atlas of aged and young mouse brains,27,3,2024,https://github.com/XiaoningHong/MouseBrain_ScientificData,The codes used to analyze the data in this study are available in the GitHub repository at the following URL: ().,2024-03-27
0,Scientific Data,41597,10.1038/s41597-024-02918-9,RailFOD23: A dataset for foreign object detection on railroad transmission lines,16,1,2024,https://github.com/CV-Altrai2023/RailFOD23,We released and shared the code for our data synthesis().,2024-01-16
0,Scientific Data,41597,10.1038/s41597-024-02976-z,miR-Blood – a small RNA atlas of human blood components,2,2,2024,https://github.com/gitHBDX/mirblood-code,"The code used for data pre-processing has been deposited on . The following software versions were used: unitas v1.7.7, SeqMap v1.0.13, SPORTS v1.1, Bowtie v1.3, SCANPY v1.8.2, Python v3.10.6, Plotly v5.10.0, Plotly Express v0.4.1, SciPy v1.9.1, Seaborn v0.12.2, and UpSetPlot v0.8.0.",2024-02-02
0,Scientific Data,41597,10.1038/s41597-023-02898-2,Refractiveindex.info database of optical constants,18,1,2024,,"[{'italic': ['refractiveindex.info', 'Figshare', 'nk', 'n'], 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR5', '#text': '5'}}, 'sub': '2', '#text': 'The dataset described here, which represents the core of the  database, is available at . It presently (as of December 2023) contains 3135 data records on 605 materials in the part of the dataset corresponding to linear optical properties (), and 193 records on 89 materials in the part corresponding to nonlinear optical properties ().'}, {'italic': ['refractiveindex.info', 'refractiveindex.info-database', 'refractiveindex.info-scripts'], 'ext-link': [{'@xlink:href': 'https://creativecommons.org/publicdomain/zero/1.0', '@ext-link-type': 'uri', '#text': 'https://creativecommons.org/publicdomain/zero/1.0'}, {'@xlink:href': 'https://github.com/polyanskiy/refractiveindex.info-database', '@ext-link-type': 'uri', '#text': 'https://github.com/polyanskiy/refractiveindex.info-database'}, {'@xlink:href': 'https://github.com/polyanskiy/refractiveindex.info-scripts', '@ext-link-type': 'uri', '#text': 'https://github.com/polyanskiy/refractiveindex.info-scripts'}], '#text': 'The code that underpins the  database is made accessible under the Creative Commons Zero (CC0) license (). This license facilitates the unrestricted use, distribution, and modification of the code, making it widely accessible for various applications. The entire codebase, including detailed documentation, is publicly available on the  GitHub project (). This repository is regularly updated, ensuring it evolves to meet the ongoing needs of both the scientific and engineering sectors. For additional utility, users can explore the  project on GitHub (), which offers scripts for deriving optical constants from established models and tools for converting Zemax glass catalogs to the dataset’s YAML format.'}, {'italic': ['refractiveindex.info', 'refractiveindex.info'], 'sup': {'xref': [{'@ref-type': 'bibr', '@rid': 'CR6', '#text': '6'}, {'@ref-type': 'bibr', '@rid': 'CR7', '#text': '7'}, {'@ref-type': 'bibr', '@rid': 'CR8', '#text': '8'}, {'@ref-type': 'bibr', '@rid': 'CR9', '#text': '9'}, {'@ref-type': 'bibr', '@rid': 'CR10', '#text': '10'}, {'@ref-type': 'bibr', '@rid': 'CR11', '#text': '11'}, {'@ref-type': 'bibr', '@rid': 'CR12', '#text': '12'}, {'@ref-type': 'bibr', '@rid': 'CR13', '#text': '13'}, {'@ref-type': 'bibr', '@rid': 'CR14', '#text': '14'}, {'@ref-type': 'bibr', '@rid': 'CR15', '#text': '15'}, {'@ref-type': 'bibr', '@rid': 'CR16', '#text': '16'}, {'@ref-type': 'bibr', '@rid': 'CR17', '#text': '17'}, {'@ref-type': 'bibr', '@rid': 'CR18', '#text': '18'}, {'@ref-type': 'bibr', '@rid': 'CR19', '#text': '19'}, {'@ref-type': 'bibr', '@rid': 'CR20', '#text': '20'}, {'@ref-type': 'bibr', '@rid': 'CR21', '#text': '21'}, {'@ref-type': 'bibr', '@rid': 'CR22', '#text': '22'}, {'@ref-type': 'bibr', '@rid': 'CR23', '#text': '23'}, {'@ref-type': 'bibr', '@rid': 'CR24', '#text': '24'}, {'@ref-type': 'bibr', '@rid': 'CR25', '#text': '25'}, {'@ref-type': 'bibr', '@rid': 'CR26', '#text': '26'}, {'@ref-type': 'bibr', '@rid': 'CR27', '#text': '27'}, {'@ref-type': 'bibr', '@rid': 'CR28', '#text': '28'}, {'@ref-type': 'bibr', '@rid': 'CR29', '#text': '29'}, {'@ref-type': 'bibr', '@rid': 'CR30', '#text': '30'}, {'@ref-type': 'bibr', '@rid': 'CR31', '#text': '31'}, {'@ref-type': 'bibr', '@rid': 'CR32', '#text': '32'}, {'@ref-type': 'bibr', '@rid': 'CR33', '#text': '33'}, {'@ref-type': 'bibr', '@rid': 'CR34', '#text': '34'}, {'@ref-type': 'bibr', '@rid': 'CR35', '#text': '35'}, {'@ref-type': 'bibr', '@rid': 'CR36', '#text': '36'}, {'@ref-type': 'bibr', '@rid': 'CR37', '#text': '37'}, {'@ref-type': 'bibr', '@rid': 'CR38', '#text': '38'}, {'@ref-type': 'bibr', '@rid': 'CR39', '#text': '39'}, {'@ref-type': 'bibr', '@rid': 'CR40', '#text': '40'}, {'@ref-type': 'bibr', '@rid': 'CR41', '#text': '41'}, {'@ref-type': 'bibr', '@rid': 'CR42', '#text': '42'}, {'@ref-type': 'bibr', '@rid': 'CR43', '#text': '43'}, {'@ref-type': 'bibr', '@rid': 'CR44', '#text': '44'}, {'@ref-type': 'bibr', '@rid': 'CR45', '#text': '45'}, {'@ref-type': 'bibr', '@rid': 'CR46', '#text': '46'}, {'@ref-type': 'bibr', '@rid': 'CR47', '#text': '47'}, {'@ref-type': 'bibr', '@rid': 'CR48', '#text': '48'}, {'@ref-type': 'bibr', '@rid': 'CR49', '#text': '49'}, {'@ref-type': 'bibr', '@rid': 'CR50', '#text': '50'}, {'@ref-type': 'bibr', '@rid': 'CR51', '#text': '51'}, {'@ref-type': 'bibr', '@rid': 'CR52', '#text': '52'}, {'@ref-type': 'bibr', '@rid': 'CR53', '#text': '53'}, {'@ref-type': 'bibr', '@rid': 'CR54', '#text': '54'}, {'@ref-type': 'bibr', '@rid': 'CR55', '#text': '55'}, {'@ref-type': 'bibr', '@rid': 'CR56', '#text': '56'}, {'@ref-type': 'bibr', '@rid': 'CR57', '#text': '57'}, {'@ref-type': 'bibr', '@rid': 'CR58', '#text': '58'}, {'@ref-type': 'bibr', '@rid': 'CR59', '#text': '59'}, {'@ref-type': 'bibr', '@rid': 'CR60', '#text': '60'}, {'@ref-type': 'bibr', '@rid': 'CR61', '#text': '61'}, {'@ref-type': 'bibr', '@rid': 'CR62', '#text': '62'}, {'@ref-type': 'bibr', '@rid': 'CR63', '#text': '63'}, {'@ref-type': 'bibr', '@rid': 'CR64', '#text': '64'}, {'@ref-type': 'bibr', '@rid': 'CR65', '#text': '65'}, {'@ref-type': 'bibr', '@rid': 'CR66', '#text': '66'}, {'@ref-type': 'bibr', '@rid': 'CR67', '#text': '67'}, {'@ref-type': 'bibr', '@rid': 'CR68', '#text': '68'}, {'@ref-type': 'bibr', '@rid': 'CR69', '#text': '69'}, {'@ref-type': 'bibr', '@rid': 'CR70', '#text': '70'}, {'@ref-type': 'bibr', '@rid': 'CR71', '#text': '71'}, {'@ref-type': 'bibr', '@rid': 'CR72', '#text': '72'}, {'@ref-type': 'bibr', '@rid': 'CR73', '#text': '73'}, {'@ref-type': 'bibr', '@rid': 'CR74', '#text': '74'}, {'@ref-type': 'bibr', '@rid': 'CR75', '#text': '75'}, {'@ref-type': 'bibr', '@rid': 'CR76', '#text': '76'}, {'@ref-type': 'bibr', '@rid': 'CR77', '#text': '77'}, {'@ref-type': 'bibr', '@rid': 'CR78', '#text': '78'}, {'@ref-type': 'bibr', '@rid': 'CR79', '#text': '79'}, {'@ref-type': 'bibr', '@rid': 'CR80', '#text': '80'}, {'@ref-type': 'bibr', '@rid': 'CR81', '#text': '81'}, {'@ref-type': 'bibr', '@rid': 'CR82', '#text': '82'}, {'@ref-type': 'bibr', '@rid': 'CR83', '#text': '83'}, {'@ref-type': 'bibr', '@rid': 'CR84', '#text': '84'}, {'@ref-type': 'bibr', '@rid': 'CR85', '#text': '85'}, {'@ref-type': 'bibr', '@rid': 'CR86', '#text': '86'}, {'@ref-type': 'bibr', '@rid': 'CR87', '#text': '87'}, {'@ref-type': 'bibr', '@rid': 'CR88', '#text': '88'}, {'@ref-type': 'bibr', '@rid': 'CR89', '#text': '89'}, {'@ref-type': 'bibr', '@rid': 'CR90', '#text': '90'}, {'@ref-type': 'bibr', '@rid': 'CR91', '#text': '91'}, {'@ref-type': 'bibr', '@rid': 'CR92', '#text': '92'}, {'@ref-type': 'bibr', '@rid': 'CR93', '#text': '93'}, {'@ref-type': 'bibr', '@rid': 'CR94', '#text': '94'}, {'@ref-type': 'bibr', '@rid': 'CR95', '#text': '95'}, {'@ref-type': 'bibr', '@rid': 'CR96', '#text': '96'}, {'@ref-type': 'bibr', '@rid': 'CR97', '#text': '97'}, {'@ref-type': 'bibr', '@rid': 'CR98', '#text': '98'}, {'@ref-type': 'bibr', '@rid': 'CR99', '#text': '99'}, {'@ref-type': 'bibr', '@rid': 'CR100', '#text': '100'}, {'@ref-type': 'bibr', '@rid': 'CR101', '#text': '101'}, {'@ref-type': 'bibr', '@rid': 'CR102', '#text': '102'}, {'@ref-type': 'bibr', '@rid': 'CR103', '#text': '103'}, {'@ref-type': 'bibr', '@rid': 'CR104', '#text': '104'}, {'@ref-type': 'bibr', '@rid': 'CR105', '#text': '105'}, {'@ref-type': 'bibr', '@rid': 'CR106', '#text': '106'}, {'@ref-type': 'bibr', '@rid': 'CR107', '#text': '107'}, {'@ref-type': 'bibr', '@rid': 'CR108', '#text': '108'}, {'@ref-type': 'bibr', '@rid': 'CR109', '#text': '109'}, {'@ref-type': 'bibr', '@rid': 'CR110', '#text': '110'}, {'@ref-type': 'bibr', '@rid': 'CR111', '#text': '111'}, {'@ref-type': 'bibr', '@rid': 'CR112', '#text': '112'}, {'@ref-type': 'bibr', '@rid': 'CR113', '#text': '113'}, {'@ref-type': 'bibr', '@rid': 'CR114', '#text': '114'}, {'@ref-type': 'bibr', '@rid': 'CR115', '#text': '115'}, {'@ref-type': 'bibr', '@rid': 'CR116', '#text': '116'}, {'@ref-type': 'bibr', '@rid': 'CR117', '#text': '117'}, {'@ref-type': 'bibr', '@rid': 'CR118', '#text': '118'}, {'@ref-type': 'bibr', '@rid': 'CR119', '#text': '119'}, {'@ref-type': 'bibr', '@rid': 'CR120', '#text': '120'}, {'@ref-type': 'bibr', '@rid': 'CR121', '#text': '121'}, {'@ref-type': 'bibr', '@rid': 'CR122', '#text': '122'}, {'@ref-type': 'bibr', '@rid': 'CR123', '#text': '123'}, {'@ref-type': 'bibr', '@rid': 'CR124', '#text': '124'}, {'@ref-type': 'bibr', '@rid': 'CR125', '#text': '125'}, {'@ref-type': 'bibr', '@rid': 'CR126', '#text': '126'}, {'@ref-type': 'bibr', '@rid': 'CR127', '#text': '127'}, {'@ref-type': 'bibr', '@rid': 'CR128', '#text': '128'}, {'@ref-type': 'bibr', '@rid': 'CR129', '#text': '129'}, {'@ref-type': 'bibr', '@rid': 'CR130', '#text': '130'}, {'@ref-type': 'bibr', '@rid': 'CR131', '#text': '131'}, {'@ref-type': 'bibr', '@rid': 'CR132', '#text': '132'}, {'@ref-type': 'bibr', '@rid': 'CR133', '#text': '133'}, {'@ref-type': 'bibr', '@rid': 'CR134', '#text': '134'}, {'@ref-type': 'bibr', '@rid': 'CR135', '#text': '135'}, {'@ref-type': 'bibr', '@rid': 'CR136', '#text': '136'}, {'@ref-type': 'bibr', '@rid': 'CR137', '#text': '137'}, {'@ref-type': 'bibr', '@rid': 'CR138', '#text': '138'}, {'@ref-type': 'bibr', '@rid': 'CR139', '#text': '139'}, {'@ref-type': 'bibr', '@rid': 'CR140', '#text': '140'}, {'@ref-type': 'bibr', '@rid': 'CR141', '#text': '141'}, {'@ref-type': 'bibr', '@rid': 'CR142', '#text': '142'}, {'@ref-type': 'bibr', '@rid': 'CR143', '#text': '143'}, {'@ref-type': 'bibr', '@rid': 'CR144', '#text': '144'}, {'@ref-type': 'bibr', '@rid': 'CR145', '#text': '145'}, {'@ref-type': 'bibr', '@rid': 'CR146', '#text': '146'}, {'@ref-type': 'bibr', '@rid': 'CR147', '#text': '147'}, {'@ref-type': 'bibr', '@rid': 'CR148', '#text': '148'}, {'@ref-type': 'bibr', '@rid': 'CR149', '#text': '149'}, {'@ref-type': 'bibr', '@rid': 'CR150', '#text': '150'}, {'@ref-type': 'bibr', '@rid': 'CR151', '#text': '151'}, {'@ref-type': 'bibr', '@rid': 'CR152', '#text': '152'}, {'@ref-type': 'bibr', '@rid': 'CR153', '#text': '153'}, {'@ref-type': 'bibr', '@rid': 'CR154', '#text': '154'}, {'@ref-type': 'bibr', '@rid': 'CR155', '#text': '155'}, {'@ref-type': 'bibr', '@rid': 'CR156', '#text': '156'}, {'@ref-type': 'bibr', '@rid': 'CR157', '#text': '157'}, {'@ref-type': 'bibr', '@rid': 'CR158', '#text': '158'}, {'@ref-type': 'bibr', '@rid': 'CR159', '#text': '159'}, {'@ref-type': 'bibr', '@rid': 'CR160', '#text': '160'}, {'@ref-type': 'bibr', '@rid': 'CR161', '#text': '161'}, {'@ref-type': 'bibr', '@rid': 'CR162', '#text': '162'}, {'@ref-type': 'bibr', '@rid': 'CR163', '#text': '163'}, {'@ref-type': 'bibr', '@rid': 'CR164', '#text': '164'}, {'@ref-type': 'bibr', '@rid': 'CR165', '#text': '165'}, {'@ref-type': 'bibr', '@rid': 'CR166', '#text': '166'}, {'@ref-type': 'bibr', '@rid': 'CR167', '#text': '167'}, {'@ref-type': 'bibr', '@rid': 'CR168', '#text': '168'}, {'@ref-type': 'bibr', '@rid': 'CR169', '#text': '169'}, {'@ref-type': 'bibr', '@rid': 'CR170', '#text': '170'}, {'@ref-type': 'bibr', '@rid': 'CR171', '#text': '171'}, {'@ref-type': 'bibr', '@rid': 'CR172', '#text': '172'}, {'@ref-type': 'bibr', '@rid': 'CR173', '#text': '173'}, {'@ref-type': 'bibr', '@rid': 'CR174', '#text': '174'}, {'@ref-type': 'bibr', '@rid': 'CR175', '#text': '175'}, {'@ref-type': 'bibr', '@rid': 'CR176', '#text': '176'}, {'@ref-type': 'bibr', '@rid': 'CR177', '#text': '177'}, {'@ref-type': 'bibr', '@rid': 'CR178', '#text': '178'}, {'@ref-type': 'bibr', '@rid': 'CR179', '#text': '179'}, {'@ref-type': 'bibr', '@rid': 'CR180', '#text': '180'}, {'@ref-type': 'bibr', '@rid': 'CR181', '#text': '181'}, {'@ref-type': 'bibr', '@rid': 'CR182', '#text': '182'}, {'@ref-type': 'bibr', '@rid': 'CR183', '#text': '183'}, {'@ref-type': 'bibr', '@rid': 'CR184', '#text': '184'}, {'@ref-type': 'bibr', '@rid': 'CR185', '#text': '185'}, {'@ref-type': 'bibr', '@rid': 'CR186', '#text': '186'}, {'@ref-type': 'bibr', '@rid': 'CR187', '#text': '187'}, {'@ref-type': 'bibr', '@rid': 'CR188', '#text': '188'}, {'@ref-type': 'bibr', '@rid': 'CR189', '#text': '189'}, {'@ref-type': 'bibr', '@rid': 'CR190', '#text': '190'}, {'@ref-type': 'bibr', '@rid': 'CR191', '#text': '191'}, {'@ref-type': 'bibr', '@rid': 'CR192', '#text': '192'}, {'@ref-type': 'bibr', '@rid': 'CR193', '#text': '193'}, {'@ref-type': 'bibr', '@rid': 'CR194', '#text': '194'}, {'@ref-type': 'bibr', '@rid': 'CR195', '#text': '195'}, {'@ref-type': 'bibr', '@rid': 'CR196', '#text': '196'}, {'@ref-type': 'bibr', '@rid': 'CR197', '#text': '197'}, {'@ref-type': 'bibr', '@rid': 'CR198', '#text': '198'}, {'@ref-type': 'bibr', '@rid': 'CR199', '#text': '199'}, {'@ref-type': 'bibr', '@rid': 'CR200', '#text': '200'}, {'@ref-type': 'bibr', '@rid': 'CR201', '#text': '201'}, {'@ref-type': 'bibr', '@rid': 'CR202', '#text': '202'}, {'@ref-type': 'bibr', '@rid': 'CR203', '#text': '203'}, {'@ref-type': 'bibr', '@rid': 'CR204', '#text': '204'}, {'@ref-type': 'bibr', '@rid': 'CR205', '#text': '205'}, {'@ref-type': 'bibr', '@rid': 'CR206', '#text': '206'}, {'@ref-type': 'bibr', '@rid': 'CR207', '#text': '207'}, {'@ref-type': 'bibr', '@rid': 'CR208', '#text': '208'}, {'@ref-type': 'bibr', '@rid': 'CR209', '#text': '209'}, {'@ref-type': 'bibr', '@rid': 'CR210', '#text': '210'}, {'@ref-type': 'bibr', '@rid': 'CR211', '#text': '211'}, {'@ref-type': 'bibr', '@rid': 'CR212', '#text': '212'}, {'@ref-type': 'bibr', '@rid': 'CR213', '#text': '213'}, {'@ref-type': 'bibr', '@rid': 'CR214', '#text': '214'}, {'@ref-type': 'bibr', '@rid': 'CR215', '#text': '215'}, {'@ref-type': 'bibr', '@rid': 'CR216', '#text': '216'}, {'@ref-type': 'bibr', '@rid': 'CR217', '#text': '217'}, {'@ref-type': 'bibr', '@rid': 'CR218', '#text': '218'}, {'@ref-type': 'bibr', '@rid': 'CR219', '#text': '219'}, {'@ref-type': 'bibr', '@rid': 'CR220', '#text': '220'}, {'@ref-type': 'bibr', '@rid': 'CR221', '#text': '221'}, {'@ref-type': 'bibr', '@rid': 'CR222', '#text': '222'}, {'@ref-type': 'bibr', '@rid': 'CR223', '#text': '223'}, {'@ref-type': 'bibr', '@rid': 'CR224', '#text': '224'}, {'@ref-type': 'bibr', '@rid': 'CR225', '#text': '225'}, {'@ref-type': 'bibr', '@rid': 'CR226', '#text': '226'}, {'@ref-type': 'bibr', '@rid': 'CR227', '#text': '227'}, {'@ref-type': 'bibr', '@rid': 'CR228', '#text': '228'}, {'@ref-type': 'bibr', '@rid': 'CR229', '#text': '229'}, {'@ref-type': 'bibr', '@rid': 'CR230', '#text': '230'}, {'@ref-type': 'bibr', '@rid': 'CR231', '#text': '231'}, {'@ref-type': 'bibr', '@rid': 'CR232', '#text': '232'}, {'@ref-type': 'bibr', '@rid': 'CR233', '#text': '233'}, {'@ref-type': 'bibr', '@rid': 'CR234', '#text': '234'}, {'@ref-type': 'bibr', '@rid': 'CR235', '#text': '235'}, {'@ref-type': 'bibr', '@rid': 'CR236', '#text': '236'}, {'@ref-type': 'bibr', '@rid': 'CR237', '#text': '237'}, {'@ref-type': 'bibr', '@rid': 'CR238', '#text': '238'}, {'@ref-type': 'bibr', '@rid': 'CR239', '#text': '239'}, {'@ref-type': 'bibr', '@rid': 'CR240', '#text': '240'}, {'@ref-type': 'bibr', '@rid': 'CR241', '#text': '241'}, {'@ref-type': 'bibr', '@rid': 'CR242', '#text': '242'}, {'@ref-type': 'bibr', '@rid': 'CR243', '#text': '243'}, {'@ref-type': 'bibr', '@rid': 'CR244', '#text': '244'}, {'@ref-type': 'bibr', '@rid': 'CR245', '#text': '245'}, {'@ref-type': 'bibr', '@rid': 'CR246', '#text': '246'}, {'@ref-type': 'bibr', '@rid': 'CR247', '#text': '247'}, {'@ref-type': 'bibr', '@rid': 'CR248', '#text': '248'}, {'@ref-type': 'bibr', '@rid': 'CR249', '#text': '249'}, {'@ref-type': 'bibr', '@rid': 'CR250', '#text': '250'}, {'@ref-type': 'bibr', '@rid': 'CR251', '#text': '251'}, {'@ref-type': 'bibr', '@rid': 'CR252', '#text': '252'}, {'@ref-type': 'bibr', '@rid': 'CR253', '#text': '253'}, {'@ref-type': 'bibr', '@rid': 'CR254', '#text': '254'}, {'@ref-type': 'bibr', '@rid': 'CR255', '#text': '255'}, {'@ref-type': 'bibr', '@rid': 'CR256', '#text': '256'}, {'@ref-type': 'bibr', '@rid': 'CR257', '#text': '257'}, {'@ref-type': 'bibr', '@rid': 'CR258', '#text': '258'}, {'@ref-type': 'bibr', '@rid': 'CR259', '#text': '259'}, {'@ref-type': 'bibr', '@rid': 'CR260', '#text': '260'}, {'@ref-type': 'bibr', '@rid': 'CR261', '#text': '261'}, {'@ref-type': 'bibr', '@rid': 'CR262', '#text': '262'}, {'@ref-type': 'bibr', '@rid': 'CR263', '#text': '263'}, {'@ref-type': 'bibr', '@rid': 'CR264', '#text': '264'}, {'@ref-type': 'bibr', '@rid': 'CR265', '#text': '265'}, {'@ref-type': 'bibr', '@rid': 'CR266', '#text': '266'}, {'@ref-type': 'bibr', '@rid': 'CR267', '#text': '267'}, {'@ref-type': 'bibr', '@rid': 'CR268', '#text': '268'}, {'@ref-type': 'bibr', '@rid': 'CR269', '#text': '269'}, {'@ref-type': 'bibr', '@rid': 'CR270', '#text': '270'}, {'@ref-type': 'bibr', '@rid': 'CR271', '#text': '271'}, {'@ref-type': 'bibr', '@rid': 'CR272', '#text': '272'}, {'@ref-type': 'bibr', '@rid': 'CR273', '#text': '273'}, {'@ref-type': 'bibr', '@rid': 'CR274', '#text': '274'}, {'@ref-type': 'bibr', '@rid': 'CR275', '#text': '275'}, {'@ref-type': 'bibr', '@rid': 'CR276', '#text': '276'}, {'@ref-type': 'bibr', '@rid': 'CR277', '#text': '277'}, {'@ref-type': 'bibr', '@rid': 'CR278', '#text': '278'}, {'@ref-type': 'bibr', '@rid': 'CR279', '#text': '279'}, {'@ref-type': 'bibr', '@rid': 'CR280', '#text': '280'}, {'@ref-type': 'bibr', '@rid': 'CR281', '#text': '281'}, {'@ref-type': 'bibr', '@rid': 'CR282', '#text': '282'}, {'@ref-type': 'bibr', '@rid': 'CR283', '#text': '283'}, {'@ref-type': 'bibr', '@rid': 'CR284', '#text': '284'}, {'@ref-type': 'bibr', '@rid': 'CR285', '#text': '285'}, {'@ref-type': 'bibr', '@rid': 'CR286', '#text': '286'}, {'@ref-type': 'bibr', '@rid': 'CR287', '#text': '287'}, {'@ref-type': 'bibr', '@rid': 'CR288', '#text': '288'}, {'@ref-type': 'bibr', '@rid': 'CR289', '#text': '289'}, {'@ref-type': 'bibr', '@rid': 'CR290', '#text': '290'}, {'@ref-type': 'bibr', '@rid': 'CR291', '#text': '291'}, {'@ref-type': 'bibr', '@rid': 'CR292', '#text': '292'}, {'@ref-type': 'bibr', '@rid': 'CR293', '#text': '293'}, {'@ref-type': 'bibr', '@rid': 'CR294', '#text': '294'}, {'@ref-type': 'bibr', '@rid': 'CR295', '#text': '295'}, {'@ref-type': 'bibr', '@rid': 'CR296', '#text': '296'}, {'@ref-type': 'bibr', '@rid': 'CR297', '#text': '297'}, {'@ref-type': 'bibr', '@rid': 'CR298', '#text': '298'}, {'@ref-type': 'bibr', '@rid': 'CR299', '#text': '299'}, {'@ref-type': 'bibr', '@rid': 'CR300', '#text': '300'}, {'@ref-type': 'bibr', '@rid': 'CR301', '#text': '301'}, {'@ref-type': 'bibr', '@rid': 'CR302', '#text': '302'}, {'@ref-type': 'bibr', '@rid': 'CR303', '#text': '303'}, {'@ref-type': 'bibr', '@rid': 'CR304', '#text': '304'}, {'@ref-type': 'bibr', '@rid': 'CR305', '#text': '305'}, {'@ref-type': 'bibr', '@rid': 'CR306', '#text': '306'}, {'@ref-type': 'bibr', '@rid': 'CR307', '#text': '307'}, {'@ref-type': 'bibr', '@rid': 'CR308', '#text': '308'}, {'@ref-type': 'bibr', '@rid': 'CR309', '#text': '309'}, {'@ref-type': 'bibr', '@rid': 'CR310', '#text': '310'}, {'@ref-type': 'bibr', '@rid': 'CR311', '#text': '311'}, {'@ref-type': 'bibr', '@rid': 'CR312', '#text': '312'}, {'@ref-type': 'bibr', '@rid': 'CR313', '#text': '313'}, {'@ref-type': 'bibr', '@rid': 'CR314', '#text': '314'}, {'@ref-type': 'bibr', '@rid': 'CR315', '#text': '315'}, {'@ref-type': 'bibr', '@rid': 'CR316', '#text': '316'}, {'@ref-type': 'bibr', '@rid': 'CR317', '#text': '317'}, {'@ref-type': 'bibr', '@rid': 'CR318', '#text': '318'}, {'@ref-type': 'bibr', '@rid': 'CR319', '#text': '319'}, {'@ref-type': 'bibr', '@rid': 'CR320', '#text': '320'}, {'@ref-type': 'bibr', '@rid': 'CR321', '#text': '321'}, {'@ref-type': 'bibr', '@rid': 'CR322', '#text': '322'}, {'@ref-type': 'bibr', '@rid': 'CR323', '#text': '323'}, {'@ref-type': 'bibr', '@rid': 'CR324', '#text': '324'}, {'@ref-type': 'bibr', '@rid': 'CR325', '#text': '325'}, {'@ref-type': 'bibr', '@rid': 'CR326', '#text': '326'}, {'@ref-type': 'bibr', '@rid': 'CR327', '#text': '327'}, {'@ref-type': 'bibr', '@rid': 'CR328', '#text': '328'}, {'@ref-type': 'bibr', '@rid': 'CR329', '#text': '329'}, {'@ref-type': 'bibr', '@rid': 'CR330', '#text': '330'}, {'@ref-type': 'bibr', '@rid': 'CR331', '#text': '331'}, {'@ref-type': 'bibr', '@rid': 'CR332', '#text': '332'}, {'@ref-type': 'bibr', '@rid': 'CR333', '#text': '333'}, {'@ref-type': 'bibr', '@rid': 'CR334', '#text': '334'}, {'@ref-type': 'bibr', '@rid': 'CR335', '#text': '335'}, {'@ref-type': 'bibr', '@rid': 'CR336', '#text': '336'}, {'@ref-type': 'bibr', '@rid': 'CR337', '#text': '337'}, {'@ref-type': 'bibr', '@rid': 'CR338', '#text': '338'}, {'@ref-type': 'bibr', '@rid': 'CR339', '#text': '339'}, {'@ref-type': 'bibr', '@rid': 'CR340', '#text': '340'}, {'@ref-type': 'bibr', '@rid': 'CR341', '#text': '341'}, {'@ref-type': 'bibr', '@rid': 'CR342', '#text': '342'}, {'@ref-type': 'bibr', '@rid': 'CR343', '#text': '343'}, {'@ref-type': 'bibr', '@rid': 'CR344', '#text': '344'}, {'@ref-type': 'bibr', '@rid': 'CR345', '#text': '345'}, {'@ref-type': 'bibr', '@rid': 'CR346', '#text': '346'}, {'@ref-type': 'bibr', '@rid': 'CR347', '#text': '347'}, {'@ref-type': 'bibr', '@rid': 'CR348', '#text': '348'}, {'@ref-type': 'bibr', '@rid': 'CR349', '#text': '349'}, {'@ref-type': 'bibr', '@rid': 'CR350', '#text': '350'}, {'@ref-type': 'bibr', '@rid': 'CR351', '#text': '351'}, {'@ref-type': 'bibr', '@rid': 'CR352', '#text': '352'}, {'@ref-type': 'bibr', '@rid': 'CR353', '#text': '353'}, {'@ref-type': 'bibr', '@rid': 'CR354', '#text': '354'}, {'@ref-type': 'bibr', '@rid': 'CR355', '#text': '355'}, {'@ref-type': 'bibr', '@rid': 'CR356', '#text': '356'}, {'@ref-type': 'bibr', '@rid': 'CR357', '#text': '357'}, {'@ref-type': 'bibr', '@rid': 'CR358', '#text': '358'}, {'@ref-type': 'bibr', '@rid': 'CR359', '#text': '359'}, {'@ref-type': 'bibr', '@rid': 'CR360', '#text': '360'}, {'@ref-type': 'bibr', '@rid': 'CR361', '#text': '361'}, {'@ref-type': 'bibr', '@rid': 'CR362', '#text': '362'}, {'@ref-type': 'bibr', '@rid': 'CR363', '#text': '363'}, {'@ref-type': 'bibr', '@rid': 'CR364', '#text': '364'}, {'@ref-type': 'bibr', '@rid': 'CR365', '#text': '365'}, {'@ref-type': 'bibr', '@rid': 'CR366', '#text': '366'}, {'@ref-type': 'bibr', '@rid': 'CR367', '#text': '367'}, {'@ref-type': 'bibr', '@rid': 'CR368', '#text': '368'}, {'@ref-type': 'bibr', '@rid': 'CR369', '#text': '369'}, {'@ref-type': 'bibr', '@rid': 'CR370', '#text': '370'}, {'@ref-type': 'bibr', '@rid': 'CR371', '#text': '371'}, {'@ref-type': 'bibr', '@rid': 'CR372', '#text': '372'}, {'@ref-type': 'bibr', '@rid': 'CR373', '#text': '373'}, {'@ref-type': 'bibr', '@rid': 'CR374', '#text': '374'}, {'@ref-type': 'bibr', '@rid': 'CR375', '#text': '375'}, {'@ref-type': 'bibr', '@rid': 'CR376', '#text': '376'}, {'@ref-type': 'bibr', '@rid': 'CR377', '#text': '377'}, {'@ref-type': 'bibr', '@rid': 'CR378', '#text': '378'}, {'@ref-type': 'bibr', '@rid': 'CR379', '#text': '379'}, {'@ref-type': 'bibr', '@rid': 'CR380', '#text': '380'}, {'@ref-type': 'bibr', '@rid': 'CR381', '#text': '381'}, {'@ref-type': 'bibr', '@rid': 'CR382', '#text': '382'}, {'@ref-type': 'bibr', '@rid': 'CR383', '#text': '383'}, {'@ref-type': 'bibr', '@rid': 'CR384', '#text': '384'}, {'@ref-type': 'bibr', '@rid': 'CR385', '#text': '385'}, {'@ref-type': 'bibr', '@rid': 'CR386', '#text': '386'}, {'@ref-type': 'bibr', '@rid': 'CR387', '#text': '387'}, {'@ref-type': 'bibr', '@rid': 'CR388', '#text': '388'}, {'@ref-type': 'bibr', '@rid': 'CR389', '#text': '389'}, {'@ref-type': 'bibr', '@rid': 'CR390', '#text': '390'}, {'@ref-type': 'bibr', '@rid': 'CR391', '#text': '391'}, {'@ref-type': 'bibr', '@rid': 'CR392', '#text': '392'}, {'@ref-type': 'bibr', '@rid': 'CR393', '#text': '393'}, {'@ref-type': 'bibr', '@rid': 'CR394', '#text': '394'}, {'@ref-type': 'bibr', '@rid': 'CR395', '#text': '395'}, {'@ref-type': 'bibr', '@rid': 'CR396', '#text': '396'}, {'@ref-type': 'bibr', '@rid': 'CR397', '#text': '397'}, {'@ref-type': 'bibr', '@rid': 'CR398', '#text': '398'}, {'@ref-type': 'bibr', '@rid': 'CR399', '#text': '399'}, {'@ref-type': 'bibr', '@rid': 'CR400', '#text': '400'}, {'@ref-type': 'bibr', '@rid': 'CR401', '#text': '401'}, {'@ref-type': 'bibr', '@rid': 'CR402', '#text': '402'}, {'@ref-type': 'bibr', '@rid': 'CR403', '#text': '403'}, {'@ref-type': 'bibr', '@rid': 'CR404', '#text': '404'}, {'@ref-type': 'bibr', '@rid': 'CR405', '#text': '405'}, {'@ref-type': 'bibr', '@rid': 'CR406', '#text': '406'}, {'@ref-type': 'bibr', '@rid': 'CR407', '#text': '407'}, {'@ref-type': 'bibr', '@rid': 'CR408', '#text': '408'}, {'@ref-type': 'bibr', '@rid': 'CR409', '#text': '409'}, {'@ref-type': 'bibr', '@rid': 'CR410', '#text': '410'}, {'@ref-type': 'bibr', '@rid': 'CR411', '#text': '411'}, {'@ref-type': 'bibr', '@rid': 'CR412', '#text': '412'}, {'@ref-type': 'bibr', '@rid': 'CR413', '#text': '413'}, {'@ref-type': 'bibr', '@rid': 'CR414', '#text': '414'}, {'@ref-type': 'bibr', '@rid': 'CR415', '#text': '415'}, {'@ref-type': 'bibr', '@rid': 'CR416', '#text': '416'}, {'@ref-type': 'bibr', '@rid': 'CR417', '#text': '417'}, {'@ref-type': 'bibr', '@rid': 'CR418', '#text': '418'}, {'@ref-type': 'bibr', '@rid': 'CR419', '#text': '419'}, {'@ref-type': 'bibr', '@rid': 'CR420', '#text': '420'}, {'@ref-type': 'bibr', '@rid': 'CR421', '#text': '421'}, {'@ref-type': 'bibr', '@rid': 'CR422', '#text': '422'}, {'@ref-type': 'bibr', '@rid': 'CR423', '#text': '423'}, {'@ref-type': 'bibr', '@rid': 'CR424', '#text': '424'}, {'@ref-type': 'bibr', '@rid': 'CR425', '#text': '425'}, {'@ref-type': 'bibr', '@rid': 'CR426', '#text': '426'}, {'@ref-type': 'bibr', '@rid': 'CR427', '#text': '427'}, {'@ref-type': 'bibr', '@rid': 'CR428', '#text': '428'}, {'@ref-type': 'bibr', '@rid': 'CR429', '#text': '429'}, {'@ref-type': 'bibr', '@rid': 'CR430', '#text': '430'}, {'@ref-type': 'bibr', '@rid': 'CR431', '#text': '431'}, {'@ref-type': 'bibr', '@rid': 'CR432', '#text': '432'}, {'@ref-type': 'bibr', '@rid': 'CR433', '#text': '433'}, {'@ref-type': 'bibr', '@rid': 'CR434', '#text': '434'}, {'@ref-type': 'bibr', '@rid': 'CR435', '#text': '435'}, {'@ref-type': 'bibr', '@rid': 'CR436', '#text': '436'}, {'@ref-type': 'bibr', '@rid': 'CR437', '#text': '437'}, {'@ref-type': 'bibr', '@rid': 'CR438', '#text': '438'}, {'@ref-type': 'bibr', '@rid': 'CR439', '#text': '439'}, {'@ref-type': 'bibr', '@rid': 'CR440', '#text': '440'}, {'@ref-type': 'bibr', '@rid': 'CR441', '#text': '441'}, {'@ref-type': 'bibr', '@rid': 'CR442', '#text': '442'}, {'@ref-type': 'bibr', '@rid': 'CR443', '#text': '443'}, {'@ref-type': 'bibr', '@rid': 'CR444', '#text': '444'}, {'@ref-type': 'bibr', '@rid': 'CR445', '#text': '445'}, {'@ref-type': 'bibr', '@rid': 'CR446', '#text': '446'}, {'@ref-type': 'bibr', '@rid': 'CR447', '#text': '447'}, {'@ref-type': 'bibr', '@rid': 'CR448', '#text': '448'}, {'@ref-type': 'bibr', '@rid': 'CR449', '#text': '449'}, {'@ref-type': 'bibr', '@rid': 'CR450', '#text': '450'}, {'@ref-type': 'bibr', '@rid': 'CR451', '#text': '451'}, {'@ref-type': 'bibr', '@rid': 'CR452', '#text': '452'}, {'@ref-type': 'bibr', '@rid': 'CR453', '#text': '453'}, {'@ref-type': 'bibr', '@rid': 'CR454', '#text': '454'}, {'@ref-type': 'bibr', '@rid': 'CR455', '#text': '455'}, {'@ref-type': 'bibr', '@rid': 'CR456', '#text': '456'}, {'@ref-type': 'bibr', '@rid': 'CR457', '#text': '457'}, {'@ref-type': 'bibr', '@rid': 'CR458', '#text': '458'}, {'@ref-type': 'bibr', '@rid': 'CR459', '#text': '459'}, {'@ref-type': 'bibr', '@rid': 'CR460', '#text': '460'}, {'@ref-type': 'bibr', '@rid': 'CR461', '#text': '461'}, {'@ref-type': 'bibr', '@rid': 'CR462', '#text': '462'}, {'@ref-type': 'bibr', '@rid': 'CR463', '#text': '463'}, {'@ref-type': 'bibr', '@rid': 'CR464', '#text': '464'}, {'@ref-type': 'bibr', '@rid': 'CR465', '#text': '465'}, {'@ref-type': 'bibr', '@rid': 'CR466', '#text': '466'}, {'@ref-type': 'bibr', '@rid': 'CR467', '#text': '467'}, {'@ref-type': 'bibr', '@rid': 'CR468', '#text': '468'}, {'@ref-type': 'bibr', '@rid': 'CR469', '#text': '469'}, {'@ref-type': 'bibr', '@rid': 'CR470', '#text': '470'}, {'@ref-type': 'bibr', '@rid': 'CR471', '#text': '471'}], '#text': ', , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , –'}, '#text': 'It is essential to note that the data encapsulated within the  dataset is meticulously curated from publicly available sources. This includes peer-reviewed journals, authoritative books, and manufacturer datasheets, ensuring that the dataset is not only expansive but also anchored in reliability and veracity. Each data record within the dataset explicitly cites the source, offering users a pathway to delve deeper into the original data and its context. All journal papers from which data are presently used in the  dataset, excluding those without a DOI identifier, are included in the following reference list.'}, {'italic': 'refractiveindex.info', '#text': 'By integrating a comprehensive data collection, adopting a standard-based data file format, ensuring ongoing updates, and maintaining open access, the  emerges as an essential tool for researchers, engineers, and students delving into the complex world of optical constants and material properties.'}]",2024-01-18
0,Scientific Data,41597,10.1038/s41597-024-03061-1,A simulated ‘sandbox’ for exploring the modifiable areal unit problem in aggregation and disaggregation,24,2,2024,https://github.com/jjniev01/areal_sandbox,"The code utilised in producing this dataset was originally a series of individual scripts in R and, for submitting jobs, to the HPC, in . We have compiled these scripts, including job submission scripts, into a single ordered  notebook to ease comprehension and replicability. All packages indicated in the notebook utilised the most recent version available on November 1, 2021. The code notebook is available at the following Github repository release: .",2024-02-24
0,Scientific Data,41597,10.1038/s41597-024-02948-3,Global seasonal prediction of fire danger,25,1,2024,https://github.com/ecmwf-projects/geff; https://github.com/fdg10371/Jupyter_notebooks,The fire indices have been generated using the open source GEFF modelling system v4.1(). The code to reproduce the results of this manuscript is openly available on a public repository: .,2024-01-25
0,Scientific Data,41597,10.1038/s41597-024-03033-5,China’s low-carbon policy intensity dataset from national- to prefecture-level over 2007–2022,16,2,2024,,"Code used for constructing the low-carbon policy intensity is written in Python 3.10.8 and Stata 15, and has been uploaded to figshare.",2024-02-16
0,Scientific Data,41597,10.1038/s41597-024-02978-x,Patient level dataset to study the effect of COVID-19 in people with Multiple Sclerosis,31,1,2024,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR12', '#text': '12'}}, '#text': 'The dataset acquisition pipeline was developed using Python programming language, and the data is provided in a CSV format, which makes it compatible with a variety of data analysis tools and software packages. The pipeline utilized Python libraries, including matplotlib 3.6.0, pandas 1.5.3, NumPy 1.24, and SciPy 1.0 for data aggregation, statistical analysis (e.g., bias checks using chi-squared test), and visualization. Jupyter Notebook 5.0 served as the interface for the pipeline.'}, {'ext-link': {'@xlink:href': 'https://github.com/hky365/Global-Data-Sharing-Initiative-.git', '@ext-link-type': 'uri', '#text': 'https://github.com/hky365/Global-Data-Sharing-Initiative-.git'}, '#text': 'To assist the user community in both the collection and analysis of the data, the code and tools developed for this dataset are available through GitHub. Users can access the repository, which contains the Python scripts, at . This can help users to reproduce the analyses, adapt the code for their specific needs, and collaborate with other researchers.'}]",2024-01-31
0,Scientific Data,41597,10.1038/s41597-024-03005-9,"Fluorescent Neuronal Cells v2: multi-task, multi-format annotations for deep learning in microscopy",10,2,2024,,"[{'ext-link': {'@xlink:href': 'https://github.com/clissa/fluocells-scientific-data', '@ext-link-type': 'uri', '#text': 'https://github.com/clissa/fluocells-scientific-data'}, '#text': 'The code associated with this work is available on GitHub (). The repository contains utils to:'}, {'bold': 'data operations', 'monospace': 'dataOps/: i', 'italic': ['TIFF', 'PNG'], 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR24', '#text': '24'}}, '#text': '• perform  () converting raw  images into  with metadata, ii) recreating expected data folders structure, iii) convert VIA annotation to binary masks, iv) encode binary masks into various annotation formats and types, v) preprocess yellow masks from previous FNC version)'}, {'bold': 'modelling', 'monospace': ['fluocells/models/:', 'compute_metrics.py, evaluate.py', 'training.py:'], 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR6', '#text': '6'}}, '#text': '• implement deep learning  strategies ( contains network blocks to implement c-ResUnet architecture;  and  contain utils to implement model training and evaluation)'}, {'monospace': 'notebooks/:', '#text': '• explore, analyze and evaluate models interactively ( contains jupyter notebooks with examples of how to deal with standard stages of data analysis, namely i) exploratory data analysis, ii) implementation of model architecture and training pipeline, and iii) experiments'}]",2024-02-10
0,Scientific Data,41597,10.1038/s41597-024-02921-0,National-scale remotely sensed lake trophic state from 1984 through 2020,16,1,2024,,"[{'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR54', '#text': '54'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR71', '#text': '71'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR72', '#text': '72'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR73', '#text': '73'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR74', '#text': '74'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR75', '#text': '75'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR76', '#text': '76'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR77', '#text': '77'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR78', '#text': '78'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR79', '#text': '79'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR80', '#text': '80'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR81', '#text': '81'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR82', '#text': '82'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR47', '#text': '47'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR83', '#text': '83'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR84', '#text': '84'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR85', '#text': '85'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR86', '#text': '86'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR87', '#text': '87'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR88', '#text': '88'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR89', '#text': '89'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR90', '#text': '90'}}], '#text': 'All data harmonization, modeling, and validation procedures for the LTS-US dataset were scripted in the R Statistical Environment, using the tidyverse, lubridate, data.table, sf, keras, tensorflow, caret, CAST, yaml, reticulate, xgboost, nnet, viridis, trend, multiROC, ggpubr, fastshap, maps, ggtext, and ggforce packages.'}, {'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR91', '#text': '91'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR54', '#text': '54'}}], '#text': 'To enhance reproducibility, all scripts are designed to work within a single pipeline that uses the targets package. The targets pipeline is divided into four main components: “1_aggregate”, “2_train”, “3_predict”, and “4_qc”. Each component corresponds to one of the steps presented above and can be customized by users to fit their specific needs. The associated pipeline setup and user guide can be found on the Environmental Data Initiative, where the “README_targets.pdf” file details directory architecture and how to execute the pipeline. When downloading the “scripts.zip” folder to access the targets pipeline, future users should be aware that empty files within the directory are necessary for running the pipeline, as those folders will become populated each time the pipeline is run.'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR54', '#text': '54'}}, '#text': 'To ensure reproducibility across operating platforms, all scripts for the pipeline can be executed within a container. Running the pipeline within the container allows users to execute the entire pipeline without the need to make small, yet important, edits to the code, or to configure their own operating environment to conform to the pipeline’s requirements. For example, recent versions of the sf package default to using the s2 spherical geometry engine instead of the Graphic Environment Operating System (GEOS), which assumes planar coordinates. End users on a system with one version of the sf library might need to adjust the code to use the correct geometry engine, whereas users with another version might be able to run the pipeline without any adjustments. The container crystallizes a known-working set of libraries, both at the system level (e.g., GEOS, GDAL, PROJ) and at the R level (e.g., sf), so that anybody can run the code without reconfiguring their own environment. This also provides future proofing by ensuring that the inevitable changes to other libraries over time do not lead to errors. To help end users, who are less familiar with running containerized code, a tutorial for installing and executing the pipeline within the container is located in the Environmental Data Initiative repository as a compressed entity (see “README_container.pdf”). The EDI repository also contains both a rendered (“lake_trophic_status_docker_image.tar.gz”; ~3.5 GB) and unrendered (“lts_container.zip”; ~4.0 KB) docker image. While the document “README_container.pdf” details information for running both the rendered and unrendered images, future users can choose either format depending on their familiarity with rendering Docker images and their capacity to download larger Docker images.'}]",2024-01-16
0,Scientific Data,41597,10.1038/s41597-024-02950-9,An open dataset on individual perceptions of transport policies,22,1,2024,https://github.com/Urban-Analytics/UTM-Hanoi,"The code used for exploratory data analysis, validation and visualization in this study is openly available for access and use. The codebase, which includes Jupyter Notebooks, Python scripts, and relevant libraries, is hosted on a public GitHub repository (). The code is distributed under the MIT License, allowing for modification, distribution, and reuse, as long as proper credit is given to the original authors and the license terms are followed.",2024-01-22
0,Scientific Data,41597,10.1038/s41597-024-03036-2,A large dataset of annotated incident reports on medication errors,29,2,2024,,"The machine annotator code is written in Python 3.9, using libraries for pandas, NumPy, multiprocessing, tqdm, transformers, SentencePiece, neologdn, etc. and can be operated on Google Colab or a local integrated development environment. The annotation guidelines, deposited dataset and machine annotator are available at figshare. The manually reviewed and machine-annotated datasets used for technical validation are also available at figshare.",2024-02-29
0,Scientific Data,41597,10.1038/s41597-024-03065-x,A high altitude respiration and SpO2 dataset for assessing the human response to hypoxia,27,2,2024,,"['For the purpose of acquiring physiological data, the BerryMed patient monitor was employed. And the data export function was developed based on the C/C++ source code provided by the manufacturer. As there was no license from BerryMed to make their source code public, only the code we modified was provided. All physiological data is stored in header-less CSV format. To fully grasp the structure of the data file and integrate it into a specific project, please refer to the introduction provided in the Usage Notes section.', {'ext-link': [{'@xlink:href': 'https://github.com/oca-john/Harespod', '@ext-link-type': 'uri', '#text': 'https://github.com/oca-john/Harespod'}, {'@xlink:href': 'https://gitee.com/oca-john/Harespod', '@ext-link-type': 'uri', '#text': 'https://gitee.com/oca-john/Harespod'}], '#text': 'Before analysis, the raw data underwent upsampling, filtering, and cropping procedures. For data collation and analysis, Python libraries like Pandas (Version 1.5.3), Scipy (Version 1.8.0), NeuroKit2 (Version 0.2.3), and Statsmodels (Version 0.14.0) were employed. Furthermore, Matplotlib (Version 3.7.0), Seaborn (Version 0.12.2) and NeuroKit2 were used for data visualization. To record the timestamp required to achieve the desired altitude during the experiment, the researcher employed a script program. The datetime library was utilized to acquire the timestamp and calculate the time periods. The source code is available on Github () and Gitee ().'}]",2024-02-27
0,Scientific Data,41597,10.1038/s41597-024-02922-z,Mass spectrometry-based proteomics data from thousands of HeLa control samples,23,1,2024,github.com/RasmussenLab/hela_qc_mnt_data,"The code used for preparing the PRIDE data upload, the creation of curated data views on the University of Copenhagen FTP large file storage called ERDA, the workflows for sample raw file processing are available on . The software used for processing is provided as a python package in the provided GitHub repository.",2024-01-23
0,Scientific Data,41597,10.1038/s41597-023-02844-2,Fine-grained urban blue-green-gray landscape dataset for 36 Chinese cities based on deep learning network,4,3,2024,https://github.com/Zhiyu-Xu/Fine-grained-urban-blue-green-gray-landscape-dataset-for-36-Chinese-cities,"The programs used to generate the dataset were ENVI (5.3), ESRI ArcGIS (10.6) and Pytorch deep learning framework. All used codes to generate the dataset are available in the following GitHub ().",2024-03-04
0,Scientific Data,41597,10.1038/s41597-024-02923-y,Hourly values of an advanced human-biometeorological index for diverse populations from 1991 to 2020 in Greece,16,1,2024,,The code used to produce the presented human thermal bioclimate dataset is publicly available at the Zenodo repository. It is free to re-use and modify with attribution under the Creative Commons Attribution 4.0 International (CC BY 4.0) license.,2024-01-16
0,Scientific Data,41597,10.1038/s41597-024-02981-2,Preoperative CT and survival data for patients undergoing resection of colorectal liver metastases,6,2,2024,https://github.com/lassoan/LabelmapToDICOMSeg,Code for converting DICOM images with segmentation masks to standard DICOM segmentation objects is available on GitHub: .,2024-02-06
0,Scientific Data,41597,10.1038/s41597-024-03009-5,A benchmark GaoFen-7 dataset for building extraction from satellite images,10,2,2024,https://doi.org/10.6084/m9.figshare.24305557,"The programs and software used to generate all the results were Python and ESRI ArcMap 10.7. All DL models (FCN 8S, SegNet, UNet, RefineNet, LinkNet, Attention UNet, and HRNet) as well as the dataset are publicly available through the figshare repository ().",2024-02-10
0,Scientific Data,41597,10.1038/s41597-024-03067-9,Wind turbine database for intelligent operation and maintenance strategies,29,2,2024,https://github.com/alecuba16/fuhrlander,"The turbine dataset was generated by aggregating the SCADA data obtained from the entire wind farm. It consists of five wind turbines, all of them of the same model and manufacturer: Fuhrländer FL2500 2.5 MW. To facilitate the manipulation and pre-processing of the data, we have developed functions in the programming languages R and MATLAB to serve as an interface. These functions efficiently transform the raw data into a structured table format. In this format, each variable corresponds to a column, while each entry represents a five-minute interval of data recorded in the rows. The database and the code are freely available at and at the GitHub page .",2024-02-29
0,Scientific Data,41597,10.1038/s41597-024-03123-4,Multi-omics dataset of bovine mammary epithelial cells stimulated by ten different essential amino acids,12,3,2024,https://www.bioinformatics.babraham.ac.uk/projects/fastqc/; http://proteowizard.sourceforge.net; https://xcmsonline.scripps.edu/; https://sciex.com/products/software/multiquant-software; https://www.metaboanalyst.ca,"FastQC (version 0.11.3, ) was adopted to check the quality of raw FASTQ sequencing files. Metabolite profiling was analysed with ProteoWizard package (), XCMS Online software (), SIMCA 13.0 (Umetrics AB, Umea, Sweden) software, MultiQuant software () and MetaboAnalyst plotform (), respectively.",2024-03-12
0,Scientific Data,41597,10.1038/s41597-024-02924-x,A database of computed Raman spectra of inorganic compounds with accurate hybrid functionals,22,1,2024,,"Data and custom code are available on GitHub repository for the automation of Raman predictions, post-processing, and database construction, under a CC BY 4.0 License. The  code CRYSTAL is commercially available, and the TZVP basis sets used (see Supplementary Section  of the SI) can be accessed from the CRYSTAL website.",2024-01-22
0,Scientific Data,41597,10.1038/s41597-024-02953-6,AVDOS-VR: Affective Video Database with Physiological Signals and Continuous Ratings Collected Remotely in VR,25,1,2024,,"Data processing was carried out in Python (v3.9) and all code developed for its pre-processing, transformation and analysis is user-friendly, documented, and freely available via our Github repository.",2024-01-25
0,Scientific Data,41597,10.1038/s41597-024-03039-z,MatKG: An autonomously generated knowledge graph in Material Science,17,2,2024,,The code base is made available in as noted above.,2024-02-17
0,Scientific Data,41597,10.1038/s41597-024-03068-8,A comprehensive dataset of environmentally contaminated sites in the state of São Paulo in Brazil,2,3,2024,,"[{'ext-link': {'@xlink:href': 'https://github.com/nsamlani/Code_Digitization_CETESB', '@ext-link-type': 'uri', '#text': 'https://github.com/nsamlani/Code_Digitization_CETESB'}, '#text': 'The code files used to extract the dataset are available in the following Github repository: .'}, 'The forms have changed in content and in layout over the years. Therefore, we include the bounding box coordinates for the text and the checkboxes every year (grouped text files named “bounding_boxes_text.txt” and “bounding_boxes_checkboxes.txt” for text and checkboxes, respectively). This information needs to be included in the corresponding codes to allow the reproduction of the outputs presented in this dataset.']",2024-03-02
0,Scientific Data,41597,10.1038/s41597-024-02926-9,Quantum computing dataset of maximum independent set problem on king lattice of over hundred Rydberg atoms,23,1,2024,https://doi.org/10.6084/m9.figshare.23911368; https://github.com/GiggleLiu/MISAlgorithms.jl,"The raw data can be utilized to generate fluorescence data (‘flouAreshape’) and digitized data (‘floudigreshape’) using the provided code (‘Digitze_Data.m’) available on  (). The data processing involved in Technical Validations entails the classification of randomly generated graphs by verifying group isomorphism and obtaining the true solution for the MIS problem. This task is accomplished using custom Python code, also provided. The MIS solver is included in the ‘MISSolver’ class, saved in the Python script ‘MISSolver.py’. This solver is based on the  code () and its corresponding reference. The ‘GraphTable’ class, found in Data Records, is stored in the Python script ‘GraphTable.py’. Instances of ‘GraphTable’ are saved using the Python  module. The ‘GraphLinkedList’ class and a ‘GraphNode’ class are preserved in the Python script ‘GraphLinkedList.py’. The code for saving the experimented solution to the ‘GraphTable’ instances is implemented in the Python Jupyter notebook ‘Save_Graphtable.pynb’. An example code for reading the ‘.pkl’ instance is provided in ‘Open_Graphtable_Example.pynb’.",2024-01-23
0,Scientific Data,41597,10.1038/s41597-024-02984-z,Curated benchmark dataset for ultrasound based breast lesion analysis,31,1,2024,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR28', '#text': '28'}}, '#text': 'The custom code for importing dataset into variables in Matlab environment (Mathworks, USA) and Python programming language is available at github repository.'}, {'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR29', '#text': '29'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR30', '#text': '30'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR31', '#text': '31'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR32', '#text': '32'}}], '#text': 'The code used for processing DICOM images was based on the cornerstone3D, dicomParser and Nanodicom libraries. The code used for image annotation was based on markerjs2 library.'}]",2024-01-31
0,Scientific Data,41597,10.1038/s41597-023-02877-7,Mock community taxonomic classification performance of publicly available shotgun metagenomics pipelines,17,1,2024,https://github.com/mvee18/benchmarkingpaper,All relevant code used in these analyses can be found at  and in the figshare repository. The README in either repository provides additional useful information for the usage and description of files.,2024-01-17
0,Scientific Data,41597,10.1038/s41597-024-03011-x,Chromosome level genome assembly of the Etruscan shrew,7,2,2024,,All code used in this project is publicly available. All relevant software and references are listed in Methods and Technical vation.,2024-02-07
0,Scientific Data,41597,10.1038/s41597-024-03155-w,A Dataset of Electrical Components for Mesh Segmentation and Computational Geometry Research,22,3,2024,https://github.com/bensch98/eec-analysis,"Figures ,  were generated using the data provided in the data set using the open source library Open3D. To facilitate the reproduction of these figures, a copy of the raw data is included in the repository’s corresponding directory. Data set preparation was done with a combination of libraries, including Open3D, numpy and bpy. Blender version 3.3.0 was used to label the 234 triangle meshes. Further on, various functions were programmed to interact with the data set. These include functions for scaling labels up and down, cropping triangle meshes by labels, converting vertex labels to triangle or edge labels and calculating surface, volume and cluster centroids of the cropped out meshes like the labelled regions. Additionally, based on the technology, category, and subcategory, the components can be filtered. All the software used in the study are open source available at .",2024-03-22
0,Scientific Data,41597,10.1038/s41597-024-02927-8,COMPAS-2: a dataset of -condensed hetero-polycyclic aromatic systems,19,1,2024,https://gitlab.com/porannegroup/compas,"All code is available on the Poranne Group repository on GitLab: , licensed under a CC-BY license. Further details are provided in the repository’s online README.md file.",2024-01-19
0,Scientific Data,41597,10.1038/s41597-023-02905-6,A real-world dataset of group emotion experiences based on physiological data,23,1,2024,,"['The raw data in “xlsx” or “HDF5” format was transformed into dictionaries containing the relevant data in matrices stored in a “pickle” format.', {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR60', '#text': '60'}}, 'ext-link': {'@xlink:href': 'https://github.com/scipy/scipy', '@ext-link-type': 'uri', '#text': 'https://github.com/scipy/scipy'}, '#text': 'For the physiological data pre-processing, we rely on the “biosppy” library, which contains modules for filtering the EDA and PPG signals, peak extraction and EDA decomposition into the EDR and EDL components. For the statistical tests analysis, we used the “SciPy” library ().'}, 'For further information regarding the raw or transformed data, code incompatibilities, or others, we welcome the reader to contact our corresponding author.', {'italic': 'Zenodo', 'ext-link': {'@xlink:href': 'https://zenodo.org/record/8136135', '@ext-link-type': 'uri', '#text': 'https://zenodo.org/record/8136135'}, '#text': 'The processing was done in Python 3.7.4, and the required code is available in the “5_Scripts” folder on  () so it can be easily replicated.'}]",2024-01-23
0,Scientific Data,41597,10.1038/s41597-023-02421-7,Efficacy of MRI data harmonization in the age of machine learning: a multicenter study across 36 datasets,23,1,2024,,"[{'italic': 'harmonizer', 'ext-link': {'@xlink:href': 'https://github.com/Imaging-AI-for-Health-virtual-lab/harmonizer', '@ext-link-type': 'uri', '#text': 'https://github.com/Imaging-AI-for-Health-virtual-lab/harmonizer'}, '#text': 'The source code of the efficacy measurement and  transformer is publicly available in a GitHub repository at . The following are the versions of software and Python libraries used to obtain the results presented in this study:'}, {'sub': ['1', '1'], '#text': '- FreeSurfer version 7.1.1. For T-weighted images belonging to ICBM and NKI2 datasets, we used FreeSurfer version 5.3. ABIDEI T-weighted images were already processed using FreeSurfer version 5.1.'}, '- fractalbrain toolkit version 1.1', '- neuroHarmonize v. 2.1.0 package', '- eXtreme Gradient Boosting (XGBoost) version 0.90.']",2024-01-23
0,Scientific Data,41597,10.1038/s41597-024-02928-7,A comparative wordlist for investigating distant relations among languages in Lowland South America,18,1,2024,https://github.com/pano-tacanan-history/blumpanotacana,"All code that has been used during the creation of this dataset is published on Zenodo (v0.2) and curated on GitHub (). For converting the data to CLDF, we have used the Python tools cldfbench (v1.13.0) using the pylexibank plugin (v3.4.0). The dataset is linked to Concepticon (v3.1.0), Glottolog (v4.7), and CLTS (v2.2.0). The code for integrating data with other datasets via SQL is presented in the main README.md. The scripts that were used to create the plots and to compute the coverage and synonymy is part of the ‘analysis’ folder, where another README.md file leads through the replication of all necessary steps. The code for the initial addition of IDS data is added to ‘raw/archive/‘ for documentation. This list was then filtered while finalizing the concept list. All the orthography profiles that are used during the conversion of graphemes are part of ‘etc/orthography’.",2024-01-18
0,Scientific Data,41597,10.1038/s41597-024-02957-2,Acting Emotions: a comprehensive dataset of elicited emotions,31,1,2024,,"[{'ext-link': {'@xlink:href': 'https://github.com/DECEIVER-dot/BITalino-Toolbox', '@ext-link-type': 'uri', '#text': 'https://github.com/DECEIVER-dot/BITalino-Toolbox'}, '#text': 'The code used for the experiment is publicly available at platform and cloud-based service for software development and version control GitHub. .'}, 'We developed the code for these applications in MaxMSP. All required packages are listed in the requirements.txt file. This repository contains three applications developed in MaxMSP that enable direct communication with an IRCAM R-IoT module embedded in a BITalino board. The applications provide various functionalities, including biosignal data recording, Bluetooth connectivity, and interactive annotation while viewing video recordings of experiments. Please note that the software is designed to work with the MaxMSP programming environment and requires external libraries for specific functionalities.', 'The first application allows seamless communication via a USB connection with the IRCAM R-IoT module on the BITalino board. It lets the user record biosignal data directly into a CSV file format with two different sample rates. The recorded data is saved in its raw form without normalization or interpolation. Please be aware that the current software version does not include data normalization or interpolation.', 'The second application builds upon the functionality of the first application but adds Bluetooth connectivity as an alternative communication method. With this application, the user can connect the IRCAM R-IoT module on the BITalino board and the computer using Bluetooth. It provides the same data recording capabilities as the USB version, but motion data is not recorded with this application version.', 'Experiment Annotation with Video Recording (Mira and iOS) The third application is designed for interactive annotation while viewing video recordings of experiments. Developed in MaxMSP, it requires the installation of external libraries for the Mira interface, which provides enhanced interaction capabilities. Additionally, this application relies on an iOS device to run the software effectively. Using this application, researchers or experimenters can annotate the video recordings in real-time, allowing for precise and synchronized annotation of events or observations.', {'ext-link': {'@xlink:href': 'https://github.com/PIA-Group/BioSPPy', '@ext-link-type': 'uri', '#text': 'https://github.com/PIA-Group/BioSPPy'}, '#text': 'We implemented code in Python using the BioSPPy library to extract relevant information from biosignals. BioSPPy is a Python library for biosignal processing providing a set of algorithms for processing and analyzing physiological signals, such as electrocardiography, electrodermal activity, and electromyography, to name a few. BioSPPy simplifies extracting relevant information from biosignals and enables researchers and developers to focus on their analysis tasks. BioSPPy offers various modules and functionalities, including signal processing and feature Extraction such as HRV, EDA, and EMG analysis. BioSPPy is an open-source library and can be easily installed using Python package managers like pip or conda. Its modular design and user-friendly API make it accessible to beginners and experienced biosignal processing researchers. For more information, documentation, and code examples, please visit the official BioSPPy GitHub repository at ().'}, {'ext-link': {'@xlink:href': 'https://posit.co/download/rstudio-desktop/', '@ext-link-type': 'uri', '#text': 'https://posit.co/download/rstudio-desktop/'}, '#text': 'RStudio () code was developed for conducting tests and interclass correlation analysis. The provided code allows users to perform various statistical tests and calculate interclass correlation coefficients using R programming language. Please note that the file naming and location should be adjusted according to the user’s computer file structure. We include the required external libraries for running the tests.'}]",2024-01-31
0,Scientific Data,41597,10.1038/s41597-024-03042-4,Mapping urban form into local climate zones for the continental US from 1986–2020,13,2,2024,https://github.com/QiMengEnv/CONUS_Longitudinal_LCZ,"Python scripts for training data sampling, earth observation and census input feature collection, random forest model fine tuning and mode prediction on Google Earth Engine are available at . All data processing and visualizations are done in Python 3.9. The post-classification processing is done in JavaScript on Google Earth Engine Code Editor and is also available with the same URL.",2024-02-13
0,Scientific Data,41597,10.1038/s41597-024-02958-1,"Ethnicity data resource in population-wide health records: completeness, coverage and granularity of diversity",22,2,2024,https://github.com/BHFDSC/CCU037_01,All code for data preparation and analysis are publicly available on GitHub ().,2024-02-22
0,Scientific Data,41597,10.1038/s41597-024-02987-w,The Swedish National Facility for Magnetoencephalography Parkinson’s disease dataset,31,1,2024,https://github.com/natmegsweden/NatMEG-PD_data; https://github.com/mcvinding/warpimg,The scripts used to anonymise and arrange the data according to BIDS format are available at . The scripts for the MRI warping procedure are available from .,2024-01-31
0,Scientific Data,41597,10.1038/s41597-023-02637-7,Mapping the spatial heterogeneity of global land use and land cover from 2020 to 2100 at a 1 km resolution,28,10,2023,https://github.com/JGCRI/gcam-core; https://github.com/HPSCIL/Patch-generating_Land_Use_Simulation_Model,The computing codes of the GCAM model along with parameter settings are available at . The interface software of the PLUS model can be obtained from .,2023-10-28
0,Scientific Data,41597,10.1038/s41597-023-02666-2,A dataset for benchmarking Neotropical anuran calls identification in passive acoustic monitoring,6,11,2023,https://doi.org/10.5281/zenodo.8342596; https://github.com/soundclim/anuraset,"The dataset and the raw data are hosted in Zenodo  under the CC0 license. All the code for reproducing the experimental protocol, the building and preprocessing of the dataset, and the use of the baseline model are available in the repository  under the MIT license. We open the Python code to fast development of new deep learning models and experiments in Pytorch.",2023-11-06
0,Scientific Data,41597,10.1038/s41597-023-02695-x,Improved global 250 m 8-day NDVI and EVI products from 2000–2021 using the LSTM model,14,11,2023,https://github.com/Xiongkovsky/glass_vis_lstm_code,The Python codes for generating and processing data and be accessed through GitHub ().,2023-11-14
0,Scientific Data,41597,10.1038/s41597-023-02808-6,Labelled dataset for Ultra-Low Temperature Freezer to aid dynamic modelling & fault detection and diagnostics,9,12,2023,https://lab.compute.dtu.dk/taohu/ult-freezers-labelled-dataset-sci-data.git,The Python code for ETL pipelines is available on the open-access GitLab at .,2023-12-09
0,Scientific Data,41597,10.1038/s41597-023-02696-w,Reconstructing aerosol optical depth using spatiotemporal Long Short-Term Memory convolutional autoencoder,30,11,2023,https://github.com/lu-liang-geo/AOD-reconstruction,All code for processing the raw MCD19A2 HDF-EOS files as well as reconstructing the missing data is available on GitHub:  or Figshare. The code is all provided in Python using open-source libraries for reproducibility.,2023-11-30
0,Scientific Data,41597,10.1038/s41597-023-02752-5,Introducing MEG-MASC a high-quality magneto-encephalography dataset for evaluating natural speech processing,4,12,2023,https://github.com/kingjr/meg-masc/,The code is available on .,2023-12-04
0,Scientific Data,41597,10.1038/s41597-023-02781-0,"HALD, a human aging and longevity knowledge graph for precision gerontology and geroscience analyses",1,12,2023,https://github.com/zexuwu/hald,All code used in this paper can be downloaded on GitHub at .,2023-12-01
0,Scientific Data,41597,10.1038/s41597-023-02582-5,Tethered Balloon-Borne Turbulence Measurements in Winter and Spring during the MOSAiC Expedition,19,10,2023,,The algorithm for the calculation of energy dissipation rates was written in Python. It was developed for the described turbulence records and is freely available on Zenodo.,2023-10-19
0,Scientific Data,41597,10.1038/s41597-023-02809-5,A dataset of proteomic changes during human heat stress and heat acclimation,7,12,2023,https://acclimation.statgen.org/,The source code for this dataset is publicly available through the Figshare repository. The data may also be visualized with an interactive web visualization tool:,2023-12-07
0,Scientific Data,41597,10.1038/s41597-023-02838-0,Chromosome-level genome assembly of the Stoliczka’s Asian trident bat (),15,12,2023,,"All commands and pipelines used in the data processing were all executed according to the manuals and protocols of the corresponding bioinformatics software. If no detailed parameters were provided, default parameters were used. The version of the software has been specified in the Methods section. No custom programming or coding was used.",2023-12-15
0,Scientific Data,41597,10.1038/s41597-023-02639-5,Indoor UWB Positioning and Position Tracking Data Set,26,10,2023,,"The code that was used to preprocess the raw recorded data such as procedures to fix tag positions, remove range outliers, range offset compensation, removing measurements with A6 in  and creating cleaned-up data set is published as a separate GitHub repository and archived with Zenodo to provide permanent access to a usable instance of code. All code is available under the terms of Apache-2.0 License.",2023-10-26
0,Scientific Data,41597,10.1038/s41597-023-02697-9,The EPOS multi-disciplinary Data Portal for integrated access to solid Earth science datasets,8,11,2023,https://epos-eu.github.io/epos-open-source/; https://github.com/epos-eu/SHAPEness-Metadata-Editor; https://github.com/epos-eu/EPOS-DCAT-AP,"The code developed for the EPOS Data Portal system is available on the GitHub repository of EPOS . Other code that contributed to the development of the system is already available on such repository, as in the case of the SHAPEness metadata Editor () and the specifications for EPOS-DCAT-AP extension ().",2023-11-08
0,Scientific Data,41597,10.1038/s41597-023-02782-z,Chromosome-level genome assembly of chub mackerel () from the Indo-Pacific Ocean,8,12,2023,,"['The software versions, settings and parameters used are described below:', '1. GenomeScope v2.0; p\u2009=\u20092, k\u2009=\u200921', '2. HiFiasm v0.15.4-r343; ran on Galaxy with default parameters, with the exception of purging level\u2009=\u20090 (none).', '3. QUAST v5.0.2; python quast.py [Assembly file name]', '4. BUSCO v5.4.7; busco -i [Assembly file name] -l vertebrata_odb10 -m genome', {'italic': 'k', '#text': '5. Meryl v1.3; (meryldb generation) Meryl was run on all four raw read files separately to generate a meryl database for that sequencing run, and then the four meryl databases were merged using the “union-sum” function, to make a meryl database for all the reads. The  value was 21 for all runs.'}, '6. Merqury v1.3; ran on Galaxy with following parameters; Evaluation mode: Default mode, k-mer counts database: fScoJap1.meryldb.meryldb, Number of assemblies: One assembly (“Two assemblies” for running on c1 & c2 simultaneously), Genome assembly: [Assembly file name]', '7. purge_dups v1.2.5; ran on Galaxy using workflow “VGP purge assembly with purge_dups pipeline”; Hifiasm Primary assembly: fScoJap1_c1.fasta, Hifiasm alternate assembly: [fScoJap1_c2.fasta]', '8. salsa v2.3; ran on Galaxy with parameters; Initial assembly file: p1.fastq, Bed alignment: Aligned bed format files of Hi-C data (fScoJap1_S_2476_8_R1_001.fasta, fScoJap1_S_2476_8_R2_001.fasta)', '9. gEVAL v2.2.0;', '10. RepeatMasker v4.1.5; ran with following parameters; Repeat library source: Dfam 3.7, Species: zebra fish; Search engine: RMBlast v2.14.0\u2009+\u2009; Sensitive search option.', '11. tidk v0.2.1; tidk find -c Scombriformes -f [GCF_027409825.1_fScoJap1.pri_genomic.fna] -w 10000', '12. primrose v1.3.0; primrose [fScoJap1_HiFi.bam fScoJap1_5mC-HiFi.bam]', '13. pbmm2 v1.10.0; pbmm2 index [GCF_027409825.1_fScoJap1.pri_genomic.fna] fScoJap1_5mC-HiFi.bam fScoJap_5mC-HiFi.mmi; pbmm2 align [fScoJap1_5mC-HiFi.mmi fScoJap_5mC-HiFi.bam] [fScoJap1_5mC-HiFi_aligned_sorted.bam]–sort', '14. pb-CpG-tools v1.1.0; python aligned_bam_to_cpg_scores.py -b [fScoJap_5mC_HiFi_aligned_sorted.bam] -f [GCF_027409825.1_fScoJap1.pri_genomic.fna] -o cpg_regions -p model -d /pileup_calling_model/', '15. EMBOSS v6.5.7.0; newcpgreport -window 100 -shift 1 -minlen 200 -minoe 0.6 -minpc 50. [GCF_027409825.1_fScoJap1.pri_genomic.fna]', '16. TBtools-II v1.113; ran in GUI through Graphics\u2009>\u2009Comparative Genomics\u2009>\u2009One Step MCScanX option with following parameters; Input Genome Sequence File (.fa) of Species One: GCF_027409825.1_fScoJap1.pri_genomic.fna, Input Gene Structure Annotation File (.gff/.gtf3) of Species One: GCF_027409825.1_fScoJap1.pri_genomic.gff, Input Genome Sequence File (.fa) of Species Two: GCF_027409825.1_fScoJap1.pri_genomic.fna, Input Gene Structure Annotation File (.gff/.gtf3) of Species Two: GCF_027409825.1_fScoJap1.pri_genomic.gff, CPU for BlastP: 2, E-value: 1e-10, Num of BlastHits: 5', '17. BUSCO v4.1.4; ran on RefSeq annotation “GCF_027409825.1-RS_2023_01” with following parameters; Lineage: actinopterygii_odb10, Mode: Protein', 'No custom scripts or code was used in validation of the dataset.']",2023-12-08
0,Scientific Data,41597,10.1038/s41597-023-02854-0,Open source and reproducible and inexpensive infrastructure for data challenges and education,2,1,2024,https://github.com/cuamc-dop-ids/hptbi-hackathon; https://doi.org/10.5281/zenodo.8400499,"We provided participants with a template repository on GitHub () that included a skeleton for working with either R or Python. The R package, the participant template repository, and administrator repository have been uploaded to Zenodo () as well.",2024-01-02
0,Scientific Data,41597,10.1038/s41597-023-02669-z,Database of segmentations and surface models of bones of the entire lower body created from cadaver CT scans,3,11,2023,https://github.com/MCM-Fischer/VSDFullBodyBoneModels; https://doi.org/10.5281/zenodo.8316730,The code used to create and analyze the datasets is openly accessible via  and versioned at Zenondo: .,2023-11-03
0,Scientific Data,41597,10.1038/s41597-023-02698-8,FIKElectricity: A Electricity Consumption Dataset from Three Restaurant Kitchens in Portugal,8,11,2023,https://github.com/feelab-info/eGaugeDataAcquisition; http://www.raspbian.org/; https://doi.org/10.17605/OSF.IO/K3G8N,The code used to collect and store the individual consumption data is available at . The code was developed using Python3.6 and deployed on a Linux machine (Raspbian see )). The Python3 code to reproduce the examples presented in this paper is available on the dataset repository at .,2023-11-08
0,Scientific Data,41597,10.1038/s41597-023-02755-2,High-throughput density functional theory screening of double transition metal MXene precursors,25,11,2023,https://nanohub.org/tools/vaspingestor; https://github.com/katnykiel/vasp_ingestor,"The DFT results database, code used to generate figures, and tool for ingesting new data into this database are available at . This code is also available at .",2023-11-25
0,Scientific Data,41597,10.1038/s41597-023-02612-2,A benchmark dataset for machine learning in ecotoxicology,18,10,2023,https://renkulab.io/gitlab/mltox/adore,"The code used to load and process the input data and generate the output dataset was created and run in Python 3.9 and is made available on . The repository contains code on how to load the data, prepare it for modeling, ., create one-hot and multi-hot-encodings for categorical features, and apply the train-test-split for 5-fold cross-validation. A good starting point are the files in the folder  for random forests ( and ).",2023-10-18
0,Scientific Data,41597,10.1038/s41597-023-02641-x,Enhancing radiomics and Deep Learning systems through the standardization of medical imaging workflows,21,10,2023,,,2023-10-21
0,Scientific Data,41597,10.1038/s41597-023-02657-3,Labeled temperate hardwood tree stomatal image datasets from seven taxa of  and 17 hardwood species,2,1,2024,https://github.com/JiaxinWang123/ScientificData_Labeled_Hardwood_Images; https://github.com/JiaxinWang123/StoManager1,"To ensure that the dataset can be easily reproduced and expanded upon in the future, we have made all the Python and R code used to generate and validate the resource available on a code repository (). StoManager1’s source code and an online demonstration are available on GitHub (), along with a user-friendly Windows application on Zenodo.",2024-01-02
0,Scientific Data,41597,10.1038/s41597-023-02756-1,A global daily evapotranspiration deficit index dataset for quantifying drought severity from 1979 to 2022,24,11,2023,https://github.com/XiaZhang1113/Daily-drought-index–DEDI; https://www.python.org/; https://code.mpimet.mpg.de/,The code used to calculate the DEDI dataset is available via GitHub () under the MIT license. The scripts are written with the open-source Python language version 3.8.6 () and the Climate Data Operators (CDO) version 1.9.10 (). Any updates will be published on GitHub.,2023-11-24
0,Scientific Data,41597,10.1038/s41597-023-02642-w,Hyperspectral images of grapevine leaves including healthy leaves and leaves with biotic and abiotic symptoms,26,10,2023,,"A Python code example is provided in the  folder. This example helps the reader to understand how to open hyperspectral images, to extract spectra, or to display the results obtained after a Principal Component Analysis (PCA) applied on the first image and a PCA on all average spectra where one average spectrum is obtained per leaf/image.",2023-10-26
0,Scientific Data,41597,10.1038/s41597-023-02757-0,The OREGANO knowledge graph for computational drug repurposing,6,12,2023,https://gitub.u-bordeaux.fr/erias/oregano,The code for the integration and the knowledge graph are available on the GitHub of the OREGANO project in the  folder ().,2023-12-06
0,Scientific Data,41597,10.1038/s41597-023-02786-9,High-resolution electric power load data of an industrial park with multiple types of buildings in China,6,12,2023,https://github.com/Industrialpark/SEMLab_HFUT-Building-Electricpowerloaddata,"The code implementation is done in the R programming language version 4.1.0 and MATLAB R2018a. The custom code used for data processing, technical validation, visualization is available on the github page ().",2023-12-06
0,Scientific Data,41597,10.1038/s41597-023-02842-4,"MarFERReT, an open-source, version-controlled reference library of marine microbial eukaryote functional genes",21,12,2023,https://github.com/armbrustlab/marferret; https://zenodo.org/records/10278540,"Code, documentation, and tutorials for this project are available on the MarFERReT repository: . This repository has also been archived for the v.1.1 release and the archived code is available on Zenodo here: . Information on the software versions and parameters used in this publication are included in the MarFERReT containerized build on the repository and in the archived code.",2023-12-21
0,Scientific Data,41597,10.1038/s41597-023-02614-0,A comparison of neuroelectrophysiology databases,19,10,2023,,"['Notes about software discussed in the paper', '1. Note that DANDI utilizes the OMERO-Zarr, a software package for efficient storage and retrieval of large microscopy datasets.', '2. Note that DataLad datasets are standard git/git-annex repositories, and these tools may be used directly in cases where the DataLad tool is not desired or available.', '3. While Jupyter alone is not optimal for use with electrophysiology data, it can be used with other Python libraries such as MNE-Python to load, preprocess, and plot example EEG data in a Jupyter notebook through vscode.', {'ext-link': {'@xlink:href': 'http://nwbexplorer.opensourcebrain.org/', '@ext-link-type': 'uri', '#text': 'http://nwbexplorer.opensourcebrain.org/'}, '#text': '4. For NWB explorer, see  for more information.'}, {'ext-link': {'@xlink:href': 'https://bioimagesuiteweb.github.io/webapp/viewer.html', '@ext-link-type': 'uri', '#text': 'https://bioimagesuiteweb.github.io/webapp/viewer.html'}, '#text': '5. For BioImageSuite/Viewer, see  for more information.'}, {'ext-link': {'@xlink:href': 'https://kitware.github.io/itk-vtk-viewer/docs/', '@ext-link-type': 'uri', '#text': 'https://kitware.github.io/itk-vtk-viewer/docs/'}, '#text': '6. For itk/vtk viewer, see  for more information.'}]",2023-10-19
0,Scientific Data,41597,10.1038/s41597-023-02672-4,Dataset of  coaxial monitoring and print’s cross-section images by Direct Energy Deposition fabrication,7,11,2023,,"The source codes used for data generation and initial image processing for melt-pool detection and geometrical analysis are also available along with the data. These codes can serve as a useful tool for future integration. The descriptions of the inputs and outputs of the used image processing function are summarized in Tables , , respectively.",2023-11-07
0,Scientific Data,41597,10.1038/s41597-023-02843-3,A chromosome-scale assembly of the early-flowering  and comparative genomics of cherries,21,12,2023,,All software used in this study was run according to the official instructions. The version and parameters of the software and the other custom codes used were described in Methods. Anything not specified in Methods was run with default parameters.,2023-12-21
0,Scientific Data,41597,10.1038/s41597-023-02615-z,An ocean front dataset for the Mediterranean sea and southwest Indian ocean,21,10,2023,https://github.com/galuardi/boaR; https://github.com/FlorianeSudre/NOMAD_notebooks,"The code we used to produce thermal gradients with the BOA is available in R on Github thanks to Benjamin Galuardi (available on , accessed on May 19th, 2022). The code to produce the backward FSLEs belongs to Ismael Hernández-Carrasco, was extensively described in Hernández-Carrasco ., and is available upon request. An example jupyter notebook for front visualization is available on a github repository ().",2023-10-21
0,Scientific Data,41597,10.1038/s41597-023-02673-3,Daylong acoustic recordings of grazing and rumination activities in dairy cows,8,11,2023,https://gitlab.com/luciano.mrau/acoustic_dairy_cow_dataset,"The code for automatically adjusting the timesteps of JM labels, computing the JM labels of the audio recordings and for technical validation is available at Gitlab (). All code was written in Python 3.8.10 and distributed under the MIT license. Small changes should be made to the scripts by specifying the path of the audio files of the execution environment.",2023-11-08
0,Scientific Data,41597,10.1038/s41597-023-02889-3,Cytochrome oxidase I DNA barcodes of crocodilians meat selling in Hong Kong,6,1,2024,,There was no custom code used in this study.,2024-01-06
0,Scientific Data,41597,10.1038/s41597-023-02701-2,A Chinese Face Dataset with Dynamic Expressions and Diverse Ages Synthesized by Deep Learning,7,12,2023,https://github.com/TadasBaltrusaitis/OpenFace; https://github.com/omertov/encoder4editing; https://github.com/rosinality/stylegan2-pytorch; https://github.com/yuval-alaluf/SAM,"The code we used to generate the morphed faces is publicly available, including Action Unit(AU) extractor (), StyleGAN2 encoder (), StyleGAN2 generator (), SAM ().",2023-12-07
0,Scientific Data,41597,10.1038/s41597-023-02816-6,"ChillsDB 2.0: Individual Differences in Aesthetic Chills Among 2,900+ Southern California Participants",21,12,2023,https://github.com/ChillsTV/AffectiveStimuliScraper,The code for parsing YouTube and Reddit networks is available under an MIT license at .,2023-12-21
0,Scientific Data,41597,10.1038/s41597-023-02646-6,The smarty4covid dataset and knowledge base as a framework for interpretable physiological audio data analysis,6,11,2023,https://github.com/smarty4covid/smarty4covid.git,"The audio classifier and the algorithm for extracting breathing features are available in a public repository (). Furthermore, the repository includes the weights of the CNNs used by the classifier and a script for generating triples from the available data for the purpose of customizing the smarty4covid OWL knowledge base.",2023-11-06
0,Scientific Data,41597,10.1038/s41597-023-02890-w,Public transport accessibility indicators to urban and regional services in Great Britain,9,1,2024,https://github.com/urbanbigdatacentre/access_uk_open,All the source codes used for producing this dataset are openly available from the following link: . The versions of the relevant software used are stated for each of the key elements in the main body of the paper.,2024-01-09
0,Scientific Data,41597,10.1038/s41597-023-02676-0,Monitoring the West Nile virus outbreaks in Italy using open access data,7,11,2023,https://github.com/fbranda/west-nile,"Refer to the README file accessible at the GitHub repository () for further instructions on how to use the dataset, import it either in R or Python, and carry out some exploratory analysis. The same link also hosts the dynamic version of WNVDB and all source codes to reproduce the results reported in this Data Descriptor.",2023-11-07
0,Scientific Data,41597,10.1038/s41597-023-02703-0,Venom-gland transcriptomics and venom proteomics of the Tibellus oblongus spider,22,11,2023,https://github.com/levitsky/identipy,The script code for toxins cDNA analysis can be accessed as supplementary materials to the article “The mining of toxin-like polypeptides from EST database by single residue distribution analysis”. No special constants were used. Identity search engine is available at .,2023-11-22
0,Scientific Data,41597,10.1038/s41597-023-02704-z,NONAN GaitPrint: An IMU gait database of healthy young adults,5,12,2023,,"The Matlab code GaitPrint.m, template scripts (MATLAB, Python, R), and all associated functions used for post-processing of all raw data are available as part of the database on figshare.",2023-12-05
0,Scientific Data,41597,10.1038/s41597-023-02762-3,EarSet: A Multi-Modal Dataset for Studying the Impact of Head and Facial Movements on In-Ear PPG Signals,1,12,2023,,We provide the raw data files obtained during the data collection structured by a user identifier. We did not implement any specialized code to pre-process the data.,2023-12-01
0,Scientific Data,41597,10.1038/s41597-023-02892-8,Changes in oscillatory patterns of microstate sequence in patients with first-episode psychosis,5,1,2024,https://github.com/zddzxxsmile/Chaos-game-representation-of-EEG-microstate,The code used in our analysis is available on GitHub ().,2024-01-05
0,Scientific Data,41597,10.1038/s41597-023-02705-y,A manually annotated corpus in French for the study of urbanization and the natural risk prevention,22,11,2023,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR32', '#text': '32'}}, 'italic': 'LUPAN_code.zip', 'ext-link': {'@xlink:href': '10.57745/XIVJ65', '@ext-link-type': 'doi', '#text': 'https://doi.org/10.57745/XIVJ65'}, '#text': 'The project framework that has been implemented for evaluating our corpus is stored at the corpus repository in the  file: . It includes all the scripts used for constructing the corpus and the code of the preliminary experiments presented in Technical validation.'}, 'The code has the following structure:', '1. Corpus construction:', {'italic': 'pdf2text.py', '#text': '•  – text extraction from the documents in the PDF format'}, {'italic': 'segment_construction.py', '#text': '•  – segment construction from the annotated documents in the txt format'}, '2. Preliminary experiments:', {'italic': 'data_loader.py', '#text': '•  - load segments from the txt format, split the data into 80%/20% for learning/test'}, {'italic': 'segment_classification.py', '#text': '•  – 4-label classification by the CamemBERT model'}, 'To reconstruct the experiments:', {'italic': 'Corpus_PDF', '#text': '(a) Download and place the PDF documents to the  folder. The links to the original documents are provided in the Text extraction section above.'}, {'italic': ['pdf2text.py', 'Corpus_txt'], '#text': '(b) Extract text from the documents using our  module. The resulting files will be saved to the  directory.'}, {'italic': 'Corpus_Manual_Annotation_Consolidated_Version.zip', '#text': '(c)\xa0Manually pre-process the documents as it is decribed in Methods (annotation details) and Data records (data formats). To skip this part download  from the corpus repository, extract it to the same directory with the code.'}, {'italic': ['segment_construction.py', 'Corpus_Extracted_Segments_Consolidated_Version.zip'], '#text': '(d) Construct label segments from manually annotated documents using  module. To skip this part download  from the corpus repository, extract it to the same directory with the code.'}, {'italic': 'data_loader.py', '#text': '(e) Prepare label segments for learning by using .'}, {'italic': 'segment_classification.py', '#text': '(f) Use  for learning and validation.'}, 'For detailed examples please refer to the README file provided with the code.']",2023-11-22
0,Scientific Data,41597,10.1038/s41597-023-02792-x,American local government elections database,19,12,2023,,"The replication code for the two demonstrations of our data is publicly available on OSF, and can be used under a CC-BY license.",2023-12-19
0,Scientific Data,41597,10.1038/s41597-023-02864-y,Enrichment of lung cancer computed tomography collections with AI-derived annotations,4,1,2024,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR29', '#text': '29'}}, 'ext-link': [{'@xlink:href': 'https://github.com/ImagingDataCommons/nnU-Net-BPR-annotations/releases/tag/v2.0.0', '@ext-link-type': 'uri', '#text': 'https://github.com/ImagingDataCommons/nnU-Net-BPR-annotations/releases/tag/v2.0.0'}, {'@xlink:href': 'https://github.com/ImagingDataCommons/nnU-Net-BPR-annotations/tree/main/usage_notebooks', '@ext-link-type': 'uri', '#text': 'https://github.com/ImagingDataCommons/nnU-Net-BPR-annotations/tree/main/usage_notebooks'}, {'@xlink:href': 'https://learn.canceridc.dev/introduction/getting-started-with-gcp', '@ext-link-type': 'uri', '#text': 'https://learn.canceridc.dev/introduction/getting-started-with-gcp'}], '#text': 'The code for creating the annotations and demonstrating interactions with the data is available as Release v2.0.0, and is also available at . The Colaboratory usage notebook is available here: . In order to run the notebook, users must set up a GCP project by following the instructions here .'}, 'The notebooks used to query, run the analysis, and create the DICOM Segmentation objects and Structured Reports are listed here:', {'ext-link': {'@xlink:href': 'https://github.com/ImagingDataCommons/nnU-Net-BPR-annotations/blob/main/nnunet/notebooks/idc_nsclc_nnunet_and_bpr_infer.ipynb', '@ext-link-type': 'uri', '#text': 'https://github.com/ImagingDataCommons/nnU-Net-BPR-annotations/blob/main/nnunet/notebooks/idc_nsclc_nnunet_and_bpr_infer.ipynb'}, '#text': '1. NSCLC-Radiomics analysis: .'}, {'ext-link': {'@xlink:href': 'https://github.com/ImagingDataCommons/nnU-Net-BPR-annotations/blob/main/nnunet/notebooks/idc_nlst_nnunet_and_bpr_infer.ipynb', '@ext-link-type': 'uri', '#text': 'https://github.com/ImagingDataCommons/nnU-Net-BPR-annotations/blob/main/nnunet/notebooks/idc_nlst_nnunet_and_bpr_infer.ipynb'}, '#text': '2. NLST analysis: .'}, 'The queries that that were used to perform the filtering of the relevant series are here:', {'ext-link': {'@xlink:href': 'https://github.com/ImagingDataCommons/nnU-Net-BPR-annotations/blob/main/common/queries/NSCLC_Radiomics_query.txt', '@ext-link-type': 'uri', '#text': 'https://github.com/ImagingDataCommons/nnU-Net-BPR-annotations/blob/main/common/queries/NSCLC_Radiomics_query.txt'}, '#text': '3. NSCLC-Radiomics: .'}, {'ext-link': {'@xlink:href': 'https://github.com/ImagingDataCommons/nnU-Net-BPR-annotations/blob/main/common/queries/NLST_query.txt', '@ext-link-type': 'uri', '#text': 'https://github.com/ImagingDataCommons/nnU-Net-BPR-annotations/blob/main/common/queries/NLST_query.txt'}, '#text': '4. NLST: .'}]",2024-01-04
0,Scientific Data,41597,10.1038/s41597-023-02706-x,Georeferenced dataset of maritime piracy in the Gulf of Guinea from 2010 to 2021,7,12,2023,https://github.com/ricardomourarpm/Gulf_of_Guinea_Piracy; https://colab.research.google.com/; https://jupyter.org/install.html,"The code used to generate the interactive visualization of the attacks shown in Fig.  is provided in the convert.py (). To run the provided code, it is possible to run it locally using Python in a Jupyter Notebook or even use the Anaconda Distribution, or you can use it directly online using, e.g., the Google Colab (). The Anaconda Distribution () is usually a good choice since it includes Python, the Jupyter Notebook, and other commonly used scientific computing and data science packages.",2023-12-07
0,Scientific Data,41597,10.1038/s41597-023-02793-w,A comprehensive multimodal dataset for contactless lip reading and acoustic analysis,13,12,2023,,"['Matlab and Python scripts are provided in the codes directory of dataset for the users to replicate some of the figures:', {'italic': 'FMCW_Radar_process.m', '#text': '•  This script is used to load the raw signals recorded by the AWR2243 radar. Then it is used to visualise the first and second FFT through distance dimension and angle dimension, respectively. Lastly, by reading the human location’s phase variation, we can get human-related signals, including lip motion. This step can be transferred to PYTHON and other coding methods which supports reading binary files.'}, {'italic': 'UWB_radar_process.m', '#text': '•  This script is used to load the raw signals recorded by the Xethru X4M03 radar and process the data to STFT spectrums. This step can be transferred to PYTHON and other coding methods which supports reading binary files.'}, {'italic': 'plot_InParallel.m', '#text': '•  This script provides a template to plot the spectrums that are shown on paper. First, the preprocessing data is needed to be downloaded.'}, {'italic': 'uwb_cutting.py; mmWave_cutting.py; kinect_cutting.py; laser_cutting.py;', '#text': '•  This script can be utilised to cut the different data sequence in NPY format with given Kinect timestamp, and convert BVH to CSV files. In advance of this step, the radar signals should be processed to spectrums with provided raw data in DAT. This step can be directly used with the radar scripts we provide.'}]",2023-12-13
0,Scientific Data,41597,10.1038/s41597-023-02565-6,Digital elevation models of the sea-ice surface from airborne laser scanning during MOSAiC,20,10,2023,,"[{'xref': {'@rid': 'Fig2', '@ref-type': 'fig', '#text': '2'}, '#text': 'All codes used to process the data sets are publicly available. Here, we list the repositories used for different processing steps following the order of the flow chart (Fig.\xa0).'}, 'Preprocessing and elevation retrieval (Interactive Data Language, IDL):', {'ext-link': {'@xlink:href': 'https://gitlab.awi.de/als-seaice/sea-convert', '@ext-link-type': 'uri', '#text': 'https://gitlab.awi.de/als-seaice/sea-convert'}, '#text': '• Converting POSPac MMS 8 output to separate GNSS and INS files:'}, {'ext-link': {'@xlink:href': 'https://gitlab.awi.de/als-seaice/als_level1b_seaice', '@ext-link-type': 'uri', '#text': 'https://gitlab.awi.de/als-seaice/als_level1b_seaice'}, '#text': '• Retrieving ellipsoidal elevation point clouds from the ALS data:'}, 'Processing gridded data (python3):', {'xref': {'@rid': 'Fig2', '@ref-type': 'fig', '#text': '2b,c'}, 'ext-link': {'@xlink:href': 'https://github.com/awi-als-toolbox/awi-als-toolbox', '@ext-link-type': 'uri', '#text': 'https://github.com/awi-als-toolbox/awi-als-toolbox'}, 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR40', '#text': '40'}}, '#text': '• Gridding of ALS point cloud data to regular grid including all subroutines presented in Fig.\xa0:'}, {'ext-link': {'@xlink:href': 'https://gitlab.awi.de/floenavi-crs/icedrift', '@ext-link-type': 'uri', '#text': 'https://gitlab.awi.de/floenavi-crs/icedrift'}, '#text': '• Ice drift correction:'}, {'ext-link': {'@xlink:href': 'https://gitlab.awi.de/floenavi-crs/floenavi', '@ext-link-type': 'uri', '#text': 'https://gitlab.awi.de/floenavi-crs/floenavi'}, '#text': '• Retrieve position of Polarstern:'}, 'General processing scripts (bash, python3):', {'ext-link': {'@xlink:href': 'https://gitlab.awi.de/als-seaice/mosaic-als-proc', '@ext-link-type': 'uri', '#text': 'https://gitlab.awi.de/als-seaice/mosaic-als-proc'}, '#text': '• Batch processing scripts and config files with flight-specific parameters'}]",2023-10-20
0,Scientific Data,41597,10.1038/s41597-023-02621-1,"IRIDIA-AF, a large paroxysmal atrial fibrillation long-term electrocardiogram monitoring database",18,10,2023,https://github.com/cedricgilon/iridia-af,The code described in the usage notes is available on GitHub (). It includes  tool and example code to start using IRIDIA-AF database.,2023-10-18
0,Scientific Data,41597,10.1038/s41597-023-02650-w,A Large Finer-grained Affective Computing EEG Dataset,25,10,2023,https://doi.org/10.7303/syn50614194,All the codes used for the data pre-processing and the technical validation are publicly available together with the FACED datasets in Synapse (). The codes were developed in Python 3.10. These codes can be executed on Linux and Windows. All required packages are listed in the torch_ubuntu.yml and torch_win.yml files. The README file under the Code file provides a detailed explanation of the procedure to reproduce the validation results using the codes and data.,2023-10-25
0,Scientific Data,41597,10.1038/s41597-023-02751-6,A global dataset on phosphorus in agricultural soils,2,1,2024,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR57', '#text': '57'}}, '#text': 'The scripts of the modelling approach described in this paper was made available at recherche.data.gouv.fr. Our code is available within the GPASOIL_scripts.tgz file along the tgz file that contain the GPASOIL-v1.0 and GPASOIL-v1.1 soil P pools.'}, 'GPASOIL_scripts.tgz contains the whole file tree and associated scripts that were used to generate GPASOIL-v1_output.tgz. The directory GPASOIL/ contains few sub-directories:', '- GrabData_and_PrepDriver/ that contains all scripts (bash and python) used to generate the input of the soil P dynamic model. These input correspond to the different drivers described in the current manuscript. The input files generated (in netcdf format) are within the directory output_prepDriver/ and are here provided. The procedures to download the original dataset (if this dataset is available on the web) used to generate the input files and information are given for each driver in specific README.txt files.', '- main/ contains the soil P dynamic model. Different version are provided: (model\u2009=\u2009v0), (model\u2009=\u2009v1.0) and (model\u2009=\u2009v1.1). The 2nd one (i.e. (model\u2009=\u2009v1.0)) was used to generate GPASOILv1.0 while the latter (i.e. (model\u2009=\u2009v1.1)) was used to generate GPASOILv1.1', '- evaluation/ contains the script to compare the model output to regional databases. The databases are not provided. General information about the procedure to get the regional databases are provided in a specific README.txt file.', '- output_main/ is the directory that receives the different soil P maps simulated.', {'ext-link': {'@xlink:href': 'http://www.python.org', '@ext-link-type': 'uri', '#text': 'http://www.python.org'}, '#text': 'A container generated with Singularity 3 was also provided and allows users to run the different scripts on a server or local computer without issue of incompatibility about Ubuntu distribution or Python packages. Modeling and analysis were performed in using Python (Python Software Foundation. Python Language Reference, version 3.6.10., available at , last access: January 2020).'}]",2024-01-02
0,Scientific Data,41597,10.1038/s41597-023-02737-4,T1DiabetesGranada: a longitudinal multi-modal dataset of type 1 diabetes mellitus,20,12,2023,,"The data described in this manuscript was generated using some custom code located in  of the Zenodo repository T1DiabetesGranada: a longitudinal multi-modal dataset of type 1 diabetes mellitus. The code is provided as Jupyter Notebooks created with Python v. 3.8. The code was used to conduct the tasks described in sections Data preparation and Data Records, such as data curation and transformation, and variables extraction.",2023-12-20
0,Scientific Data,41597,10.1038/s41597-023-02766-z,Real-time speech MRI datasets with corresponding articulator ground-truth segmentations,2,12,2023,https://github.com/BartsMRIPhysics/Speech_MRI_2D_UNet,"The code that accompanies this article is publicly available in the following GitHub repository:  (software licence: Apache version 2.0). The repository contains already trained versions of a state-of-the-art speech MR image segmentation method that are ready to use immediately. These versions were trained using the datasets described in this article. The repository also contains instructions and Python code to train and evaluate new versions of the method using the datasets described in this article. The code is designed to allow users to choose several important training parameters such as the training and validation dataset split, the number of epochs of training, the learning rate and the mini-batch size. In addition, the code is designed to be compatible with any dataset as long as it is organised and named in a specific way. The repository contains Python code to check that the datasets are not corrupted and are organised and named in the specific way required by the segmentation method, as well as Python code to perform the image pre-processing required by the method, namely normalising the images and saving the normalised images as MAT files.",2023-12-02
0,Scientific Data,41597,10.1038/s41597-023-02822-8,Long-term gridded land evapotranspiration reconstruction using Deep Forest with high generalizability,18,12,2023,https://github.com/FQMei/HG-Land-ET.git,"The Python code for dataset generation, validation, and visualization is available at .",2023-12-18
0,Scientific Data,41597,10.1038/s41597-023-02652-8,FAIR EVA: Bringing institutional multidisciplinary repositories into the FAIR picture,4,11,2023,https://github.com/IFCA-Advanced-Computing/FAIR_eva; https://doi.org/10.20350/digitalCSIC/14559,"The FAIR EVA source code developed in Python is fully open access (), a running instance can be found in fair.csic.es and in full detail at DIGITAL.CSIC [].",2023-11-04
0,Scientific Data,41597,10.1038/s41597-023-02681-3,"MMLKG: Knowledge Graph for Mathematical Definitions, Statements and Proofs",10,11,2023,,"['The code to reproduce results, documentation, and tutorials are available in the GitHub repositories:', {'monospace': 'metadata_gen', 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR31', '#text': '31'}}, '#text': '1.0.0 – a tool for generating a property graph metadata,'}, {'monospace': 'csvrelgen', 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR30', '#text': '30'}}, '#text': '1.0.0 – a tool for creating complex relationships between concepts,'}, {'monospace': 'mizgra', 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR27', '#text': '27'}}, '#text': '1.0.0 – a main tool for generating a property graph serialization,'}, {'monospace': 'esx_verify', 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR35', '#text': '35'}}, '#text': '1.0.1 – a tool for checking semantics correctness.'}, 'All software is provided with open-source licenses and has a Docker version. A sample usage contains:', '1. In the first step, users should use ESX files.', {'monospace': 'xmllint', '#text': '2. In the second step, users can optionally check syntactical correctness via  or other XML Schema validators.'}, {'monospace': 'esx_verify.', '#text': '3. In the third step, users can optionally check semantic correctness via'}, {'monospace': 'csvrelgen', '#text': '4. In the fourth step, users may use  to generate relationships.'}, {'monospace': 'metadata_gen', '#text': '5. In the fifth step, users can use  to generate metadata (in XML) for the dataset.'}, '6. In the sixth step, users may prepare RDF resources, e.g. via the SPARQL endpoint.', '7. In the seventh step, users can use ESX (see item 1), CSV (see item 4), XML metadata (see item 5), and RDF (see item 6).', {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR37', '#text': '37'}}, '#text': '8. In the last step, users can load the GraphML data to the graph database, e.g. Neo4j.'}]",2023-11-10
0,Scientific Data,41597,10.1038/s41597-023-02738-3,Coming in handy:  — A comprehensive database of kinematic hand movements across the lifespan,25,11,2023,,"[{'xref': {'@ref-type': 'supplementary-material', '@rid': 'MOESM1', '#text': 'S1'}, 'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR46', '#text': '46'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR42', '#text': '42'}}], 'monospace': 'Code', 'italic': 'CeTI-Age-Kinematic-Hand', '#text': 'The preprocessing of the kinematic data (removal of unused columns in.csv files (see Table\xa0, checking for corrupt files, removal of sensitive data, naming and sorting of files into folders) as well as the normalization of the kinematic data was performed using custom Python code, and can be found in the folder called  within the  database.'}, {'sup': [{'xref': [{'@ref-type': 'bibr', '@rid': 'CR46', '#text': '46'}, {'@ref-type': 'bibr', '@rid': 'CR47', '#text': '47'}, {'@ref-type': 'bibr', '@rid': 'CR56', '#text': '56'}], '#text': ',,'}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR42', '#text': '42'}}], 'monospace': 'Code', 'italic': 'CeTI-Age-Kinematic-Hand', '#text': 'The custom Python scripts used for the descriptive analyses and ML-based movement classifications reported in the Technical Validation section can also be found in the  folder of .'}]",2023-11-25
0,Scientific Data,41597,10.1038/s41597-023-02767-y,A full-body motion capture gait dataset of 138 able-bodied adults across the life span and 50 stroke survivors,1,12,2023,,See usage notes for information on code availability to compute or process the available data.,2023-12-01
0,Scientific Data,41597,10.1038/s41597-023-02653-7,DeepPatent2: A Large-Scale Benchmarking Corpus for Technical Drawing Understanding,7,11,2023,https://github.com/lamps-lab/Patent-figure-segmentor; https://github.com/GoFigure-LANL/figure-segmentation; https://github.com/lamps-lab/Visual-Descriptor,"The code used for preprocessing and segmenting figures is publicly available on GitHub:  and . Similarly, the software used for extracting semantic information, including object names and viewpoints from patent captions, is publicly available at .",2023-11-07
0,Scientific Data,41597,10.1038/s41597-023-02682-2,Photos and rendered images of LEGO bricks,18,11,2023,https://github.com/LegoSorter,"Custom tools used to take photos, generate renders, annotate photos, and extract annotated bricks from the complete scene, including the trained neural networks, are publicly available through the Lego Sorter project and its repositories available at .",2023-11-18
0,Scientific Data,41597,10.1038/s41597-023-02768-x,Surface oxygen concentration on the Qinghai-Tibet Plateau (2017–2022),15,12,2023,https://github.com/MysteriousBuddha/Surface-oxygen-concentration-on-the-Qinghai-Tibet-Plateau-2017-2022.git,"The code in this study for constructing the oxygen concentration estimation model and estimating the oxygen concentration distribution data on the QTP were based on Python 3.9.2, and the key packages were  and . The code can be found on GitHub ().",2023-12-15
0,Scientific Data,41597,10.1038/s41597-023-02654-6,"SM2RAIN-Climate, a monthly global long-term rainfall dataset for climatological studies",31,10,2023,https://github.com/IRPIhydrology/sm2rain,"SM2RAIN algorithm code is available in python, R, and Matlab on GitHub ().",2023-10-31
0,Scientific Data,41597,10.1038/s41597-023-02869-7,Constructing a finer-grained representation of clinical trial results from ClinicalTrials.gov,6,1,2024,https://github.com/xuanyshi/Finer-Grained-Clinical-Trial-Results,"The source codes of data collection, processing and analysis are stored at: ().",2024-01-06
0,Scientific Data,41597,10.1038/s41597-023-02683-1,A brain MRI dataset and baseline evaluations for tumor recurrence prediction after Gamma Knife radiotherapy,8,11,2023,https://github.com/siolmsstate/brain_mri,"The repository of brain tumor recurrence prediction data can be found on our GitHub (). Pydicom version 2.3.0 and SimpleITK version 2.1.0 have been used in data preprocessing. The baseline model framework is generated using TensorFlow version 2.8.0. We release sample codes for users to get started with raw data, guiding through loading the data and all preprocessing steps. Fundamental data visualization is also available.",2023-11-08
0,Scientific Data,41597,10.1038/s41597-023-02710-1,Interaction networks of  replication proteins under different bacterial growth conditions,10,11,2023,,"[{'ext-link': {'@xlink:href': 'https://github.com/MaximeCarlier12/Interactome_proteins_ecoli', '@ext-link-type': 'uri', '#text': 'https://github.com/MaximeCarlier12/Interactome_proteins_ecoli'}, '#text': 'The code used to assess significance of the interactions was deposited under the following link: .'}, {'ext-link': {'@xlink:href': 'https://github.com/MaximeCarlier12/Interactome_proteins_ecoli/blob/master/LICENSE', '@ext-link-type': 'uri', '#text': 'https://github.com/MaximeCarlier12/Interactome_proteins_ecoli/blob/master/LICENSE'}, '#text': 'The MIT license is available at: .'}]",2023-11-10
0,Scientific Data,41597,10.1038/s41597-023-02798-5,A global land cover training dataset from 1984 to 2020,7,12,2023,https://github.com/parevalo/measures_collector; https://github.com/ma-friedl/GlanceFiltering; https://measures-glance.github.io/glance-grids/params; https://glance.earthengine.app/view/fulltstools; https://github.com/repository-preservation/lcmap-pyccd,"We used open-source tools to ensure transparency and reproducibility of our research, including R (4.3.0), Python 3.6.7, and Google Earth Engine. Time series tools for training data collection are available on GitHub () as is the repository for filtering training data (). Custom continental definitions can be found at this repository: . Continuous Change Detection and Classification (CCDC) tools and applications can be found on Google Earth Engine () and python ().",2023-12-07
0,Scientific Data,41597,10.1038/s41597-023-02684-0,3D surgical instrument collection for computer vision and extended reality,11,11,2023,,"The proprietary software of Artec Leo and Autoscan Inspec was used for processing the scans. For the analysis of the dimensions of instruments, a Python script was written using the Trimesh, Numpy and Pandas library. Another Python script is provided for rescaling and smoothing the original models into a multifold of models, using deformations and affine transformations. All code is available in the data repository.",2023-11-11
0,Scientific Data,41597,10.1038/s41597-023-02826-4,Gap-free 16-year (2005–2020) sub-diurnal surface meteorological observations across Florida,16,12,2023,,"No custom software was used to process the data described in this paper. The open-source software used to conduct this study was Python version 3.7.6. The packages and libraries used included Numpy (V 1.18.1), Pandas (V 1.3.0), Matplotlib (V 3.1.3), and Scipy (V 1.4.1). Specific functions for the statistical analysis including the T-test, F-test, and Kolmogorov-Smirnov functions were conducted using the Scipy stats module.",2023-12-16
0,Scientific Data,41597,10.1038/s41597-023-02627-9,Subjective data models in bioinformatics and how wet lab and computational biologists conceptualise data,2,11,2023,https://github.com/yochannah/subjective-data-models-analysis,Analysis code is deposited on Zenodo and on GitHub at . Code is shared under a permissive MIT licence.,2023-11-02
0,Scientific Data,41597,10.1038/s41597-023-02656-4,LESO: A ten-year ensemble of satellite-derived intercontinental hourly surface ozone concentrations,25,10,2023,https://github.com/soonyenju/LESO,"The scripts for processing and reading the LESO datasets are accessible on Github () under the MIT license. The tools and libraries, including Python v3.9, Numpy v1.20.3, Xarray v0.19.0, Pandas v1.3.3, Deep Forest v2021.2.1 (DF21), scigeo v0.0.13, and sciml v0.0.5, were used to build the LESO framework for generating datasets of surface O concentrations. The validation of LESO datasets was processed using scitbx v0.0.42 and scikit-learn v0.24.2.",2023-10-25
0,Scientific Data,41597,10.1038/s41597-023-02741-8,Structured dataset of human-machine interactions enabling adaptive user interfaces,25,11,2023,,"[{'italic': ['replicate the experiment', 'ui.json', 'renderJSON'], 'ext-link': {'@xlink:href': 'https://github.com/mu-sse/adaptiveUIs-project/tree/main/app-mixing-machine', '@ext-link-type': 'uri', '#text': 'https://github.com/mu-sse/adaptiveUIs-project/tree/main/app-mixing-machine'}, '#text': 'The code to  is available online. The frontend of the experiment was developed using Next.js and Chakra UI. Next.js is a React framework that enables server-side rendering, automatic code splitting, and other useful features for building web applications. Chakra UI is a component library that provides a set of customizable and accessible UI components to build user interfaces quickly and easily. The UI layout is described in the file  and rendered on the app through the function . The backend was implemented using Node.js and Express, with a PostgreSQL database used to store data. The frontend and backend communicate with each other through a RESTful API, providing a secure and efficient way for data to be transmitted between the two. Researchers interested in replicating the experiment can access the code and customize it to their needs, allowing for greater flexibility and control over the experimental setup. More information can be found in the Github repository ().'}, {'italic': 'technical validation', 'ext-link': [{'@xlink:href': 'https://www.ibm.com/account/reg/us-en/signup?formid=urx-50307', '@ext-link-type': 'uri', '#text': 'https://www.ibm.com/account/reg/us-en/signup?formid=urx-50307'}, {'@xlink:href': 'https://gist.github.com/aicarrera/e6f99ea7f857de4c949afd2dfe1ff9be', '@ext-link-type': 'uri', '#text': 'https://gist.github.com/aicarrera/e6f99ea7f857de4c949afd2dfe1ff9be'}], '#text': 'The code for conducting  has been developed using Python and is accessible through the GitHub repository provided in this document. To make use of the IBM API data quality for AI, a free sign-up process is necessary, which can be initiated at the following URL: . This sign-up process will provide access to the required API keys, as detailed in the accompanying documentation. For additional information and access to the code, please refer to the GitHub repository available online ().'}, {'italic': 'illustrating dataset usage', 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR21', '#text': '21'}}, 'ext-link': {'@xlink:href': 'https://github.com/mu-sse/adaptiveUIs-project/tree/main/HMI-sequence-recommender', '@ext-link-type': 'uri', '#text': 'https://github.com/mu-sse/adaptiveUIs-project/tree/main/HMI-sequence-recommender'}, '#text': 'Code  in the context of sequential recommendation (next-step prediction) through Markov chains has been developed using Python, building upon prior research. More information is available online in the Github repository ().'}]",2023-11-25
0,Scientific Data,41597,10.1038/s41597-023-02628-8,Solar active region magnetogram image dataset for studies of space weather,24,11,2023,,"All code used to generate and manipulate the dataset, as well as code used in the Technical Validation is available at the GitHub repository. Further details and documentation regarding code usage are included therein.",2023-11-24
0,Scientific Data,41597,10.1038/s41597-023-02742-7,A DNA barcode library for woody plants in tropical and subtropical China,22,11,2023,,The code used to check species names can be found in the R package ‘plantlist’ version 0.7.2.,2023-11-22
0,Scientific Data,41597,10.1038/s41597-023-02687-x,"CELLULAR, A Cell Autophagy Imaging Dataset",16,11,2023,https://github.com/simula/cellular,The code and models used to perform the experiments are available online at the following link: .,2023-11-16
0,Scientific Data,41597,10.1038/s41597-023-02743-6,Dataset of user interactions across four large pilots on the use of augmented reality in learning experiences,24,11,2023,,"[{'ext-link': {'@xlink:href': 'https://github.com/Stocastico/xapi_analysis', '@ext-link-type': 'uri', '#text': 'https://github.com/Stocastico/xapi_analysis'}, '#text': 'The authors provide an open-source Python package () that simplifies the data analysis of datasets like the ones described in this manuscript. The library requires Python ≥3.9, Pandas and Seaborn, and it has been created using the nbdev v2 environment.'}, {'ext-link': [{'@xlink:href': 'https://vicomtech.box.com/s/ehswkw3j5xils4lhtjoqlcy4s3zbojjx', '@ext-link-type': 'uri', '#text': 'English Literacy video'}, {'@xlink:href': 'https://vicomtech.box.com/s/x51yubfh4pu1tm2x1h1ofi0h7ir8qjn7', '@ext-link-type': 'uri', '#text': 'STEM video'}, {'@xlink:href': 'https://gitlab.com/aretewp5/PBISAR-App', '@ext-link-type': 'uri', '#text': 'https://gitlab.com/aretewp5/PBISAR-App'}, {'@xlink:href': 'https://github.com/WEKIT-ECS/MIRAGE-XR/', '@ext-link-type': 'uri', '#text': 'https://github.com/WEKIT-ECS/MIRAGE-XR/'}], '#text': 'The code for the applications used in English Literacy and STEM pilots will not be released as open source since the authors are planning to further develop and exploit them. Instead, two videos (, ) are shared so that the reader can see how the applications were and how the students could interact with them. The code used for PBIS has been released under the EUPL license and the repository () is available on Gitlab. And finally, the LXD pilot has been released with an MIT license and the repository () is available on GitHub.'}]",2023-11-24
0,Scientific Data,41597,10.1038/s41597-023-02873-x,"A multilayered urban tree dataset of point clouds, quantitative structure and graph models",4,1,2024,https://github.com/hadi-yazdi/TreeML-Data,"The TreeML-SM script “TreeML-SM.py”, transformation script “point_transformation.py” for location transformation from project coordinate system to global coordinate system, and pre-trained point cloud segmentation model are published in the GitHub repository (). Please refer to the Readme file in the Github repository for further information.",2024-01-04
0,Scientific Data,41597,10.1038/s41597-023-02316-7,Restructuring and serving web-accessible streamflow data from the NOAA National Water Model historic simulations,20,10,2023,https://www.hydroshare.org/resource/84c2b029f97343a59d0739115d4087f1/,"All code for data download and reformatting can be found in the appropriate USGS repository. The  R package is available on GitHub and the dataset is documented and published via HydroShare. All the data are currently open, and publicly available at this URL: .",2023-10-20
0,Scientific Data,41597,10.1038/s41597-023-02659-1,Grammars Across Time Analyzed (GATA): a dataset of 52 languages,28,11,2023,https://doi.org/10.5281/zenodo.8250217; https://github.com/cldf-datasets/gata,"The dataset is stored on Zenodo () and curated on Github (). The current release of the repository is Version 1.0.0 and was peer-reviewed in 2023. All data is available under a CC-BY 4.0 license. All scripts that have been used during the pre-processing of the data are made available within the repository. Specifically, Python-scripts were used after the manual annotation of the data for the standardization of all annotations, as well as for the aggregation of the individual spreadsheets. The script that was used for the conversion into CLDF is also part of this repository.",2023-11-28
0,Scientific Data,41597,10.1038/s41597-023-02801-z,Towards understanding policy design through text-as-data approaches: The policy design annotations (POLIANNA) dataset,13,12,2023,https://github.com/kueddelmaier/POLIANNA,"Accompanying scripts are made available at . The repository contains scripts to split the raw policy text retrieved from EUR-Lex into articles, to process new data labeled with Inception, and to generate summary statistics.",2023-12-13
0,Scientific Data,41597,10.1038/s41597-023-02846-0,An electricity smart meter dataset of Spanish households: insights into consumption patterns,10,1,2024,https://github.com/DeustoTech/GoiEner-dataset,The code files used to process the dataset provided by GoiEner are publicly available on GitHub () and are licensed under the GPL-3.0. The repository includes a comprehensive  file with detailed information and instructions on how to run the code. The code is written in R version 4.2.2 and is compatible with both Windows and Linux operating systems.,2024-01-10
0,Scientific Data,41597,10.1038/s41597-023-02902-9,"Enhanced U-Pb detrital zircon, Lu-Hf zircon, δO zircon, and Sm-Nd whole rock global databases",9,1,2024,,None of the code or equations are new. All methods are from previously published research. The code used to validate the data fields are standard Excel functions.,2024-01-09
0,Scientific Data,41597,10.1038/s41597-023-02717-8,Birds of a feather flock together: a dataset for  and  genes from migration genetics studies,9,11,2023,https://github.com/MichaelBoireau/Scopus2CitNet; https://github.com/LSLeClercq/ABCal,The custom R code used to convert data retrieved from Scopus to the appropriate format for visualisation in CitNetExplorer is available from GitHub (). The custom PYTHON script used for plotting the scientometric aspects of the included literature is also available from GitHub ().,2023-11-09
0,Scientific Data,41597,10.1038/s41597-023-02802-y,A  heart optical coherence microscopy dataset for automatic video segmentation,9,12,2023,https://github.com/mattfishman/Drosophila-Heart-OCM,"All the models were trained using Python 3.9 and Tensorflow 2.10. Models were trained locally on a 3090 GPU with 24 GB of memory. A GitHub repository which contains the Jupyter notebook for training is available at . To run the code, first download the data and clone the repository. There is a requirements.txt file within the repository that contains all the dependencies. Next, use the utilities to generate a pickle file by changing the path inside create_pickle.py to point to the parent directory that contains all the flies. Insert the pickle file path into the training notebook and run all cells begin training. Our trained model has been provided as a starting place for training, but users may opt to train without this initialization. This notebook will output the model as a.h5 file. The path to the trained model can then be inserted into processing_utils.py to use this model to predict the segmentation on new images. Additional details can be found in the README file within the repository for how to run additional code to produce cardiac parameters.",2023-12-09
0,Scientific Data,41597,10.1038/s41597-023-02576-3,A Satellite Imagery Dataset for Long-Term Sustainable Development in United States Cities,4,12,2023,https://github.com/axin1301/Satellite-imagery-dataset,"The Python codes to collect, process, and plot the dataset as well as the supplementary files for this study are publicly available through the GitHub repository (). Detailed instruction for the running environment, file structure, and codes is available in the repository.",2023-12-04
0,Scientific Data,41597,10.1038/s41597-023-02818-4,A dataset of digital holograms of normal and thalassemic cells,2,1,2024,,"The technical details of the reconstruction algorithm used to generate phase images are described in detail in the Methods section, providing the necessary information for reproducibility of the phase reconstruction of the presented dataset.",2024-01-02
0,Scientific Data,41597,10.1038/s41597-023-02690-2,MultiXC-QM9: Large dataset of molecular and reaction energies from multi-level quantum chemical methods,8,11,2023,https://github.com/chemsurajit/largeDFTdata,The energy calculations with the 76 different post-SCF functionals were performed using the SCM software package. The GFN2-xTB energies were computed using the XTB version 6.3.3 software package. The G4MP2 energies were obtained from a previous paper by Kim .. The workflow of the calculations and collection of data are build using the Python3.7.10 and BASH scripts. The atomistic simulation environment (ASE) was used to create the database file in SQLite3 format. The csv files were created using pandas. The plots were generated using the matplotlib library. All scripts are available on GitHub under the MIT license agreement ().,2023-11-08
0,Scientific Data,41597,10.1038/s41597-023-02747-2,A new database of the analysis of the physiological needs in amateur female basketball during official matches,1,12,2023,,"['A Python source code is provided to help future dataset users. We provide an API for accessing the data and some examples of the type of studies that can be done by using this dataset.', {'italic': 'phyafb_api.py', '#text': 'The API ( file) is composed of the following functions:'}, {'italic': 'read_hr_file', '#text': '• : This function is designed to read an HR file for a specific game and player. It returns a list containing the HR values recorded. If there is no file available for the given game and player (indicating that the player did not participate in that particular game), the function returns an empty list.'}, {'italic': 'read_playing_file', '#text': '• : The purpose of this function is to read a PLAYING file for a specific game and player. It returns a list consisting of binary values (1 or 0). A value of 1 signifies that the player was on the court at that particular moment, while a value of 0 indicates that the player was on the bench. If there is no file available for the given game and player, the function returns an empty list.'}, {'italic': 'read_game_file', '#text': '• : This function facilitates access to specific fields within the GAME files. Along with the game and player parameters, the field name needs to be specified as an additional parameter. The function then returns a list containing the values of the specified field. If there is no file available for the given game and player, the function returns an empty list.'}, {'italic': 'phyafb_demo.py', '#text': 'We also provide a Python script  to obtain the figures shown in the previous section.'}]",2023-12-01
0,Scientific Data,41597,10.1038/s41597-023-02776-x,transcriptome assembly of hyperaccumulating  for gene discovery,1,12,2023,https://github.com/matevzl533/Noccaea_praecox_transcriptome,The specific codes for analyses of RNA-seq data are available at .,2023-12-01
0,Scientific Data,41597,10.1038/s41597-023-02662-6,DOES - A multimodal dataset for supervised and unsupervised analysis of steel scrap,8,11,2023,https://github.com/micschaefer/does-utils,The dataset is freely available as described in Data Records. The custom code to generate or process these data can be found in the following GitHub repository: . The rights to the source code of the validation model belong to Saarstahl AG and unfortunately cannot be published.,2023-11-08
0,Scientific Data,41597,10.1038/s41597-023-02748-1,A proteomic meta-analysis refinement of plasma extracellular vesicles,28,11,2023,,The R and Python scripts written to generate Fig. 5 and Figure S2 are available in GitHub.,2023-11-28
0,Scientific Data,41597,10.1038/s41597-023-02777-w,High-resolution grids of daily air temperature for Peru - the new PISCOt v1.2 dataset,1,12,2023,https://github.com/adrHuerta/PISCOt_v1-2,The construction of the gridded dataset PISCOt v1.2 was performed using the R (v3.6.3) and Python (v3.8.5) programming languages. The entire code used is freely available at figshare and GitHub () under the GNU General Public License v3.0.,2023-12-01
0,Scientific Data,41597,10.1038/s41597-023-02804-w,A database of hourly wind speed and modeled generation for US wind plants based on three meteorological models,8,12,2023,https://github.com/AmosRAncell/PLUSWIND,"Custom scripts were developed in R and python to process, manage, and clean the data. These scripts are available publicly at the repository . Additionally, users may contact the corresponding author with questions about these scripts or about our source data.",2023-12-08
0,Scientific Data,41597,10.1038/s41597-023-02549-6,High-resolution (1 km) Köppen-Geiger maps for 1901–2099 based on constrained CMIP6 projections,23,10,2023,https://github.com/hylken/Koppen-Geiger_maps,The new Köppen-Geiger classifications have been produced using Python version 3.10. The code can be accessed at  and is licensed under the GNU General Public License v3.0.,2023-10-23
0,Scientific Data,41597,10.1038/s41597-023-02634-w,Land cover and forest health indicator datasets for central India using very-high resolution satellite data,25,10,2023,https://doi.org/10.57760/sciencedb.10422/,The code classifying land cover from PlanetScope imagery and deriving the BGI was written in Google Earth Engine. The JavaScript language to classify land covers from Planetscope imagery and derive the BGI from the land cover is available as the ‘Code’ text file from Science Data Bank at .,2023-10-25
0,Scientific Data,41597,10.1038/s41597-023-02749-0,District-scale surface temperatures generated from high-resolution longitudinal thermal infrared images,2,12,2023,https://github.com/aloisklink/flirextractor; https://github.com/wkentaro/labelme,"Python 3 was used for data processing and analysis. The simple code demonstration suggested in this work can be found in the GitHub repository stated in the Data Record section. The code demonstrates the methodology for extraction and subsequent processing of the thermal images. Few images were selected from the dataset for illustration purposes. The code uses two specific GitHub packages: Flirextracter and Labelme. Flirextractor is an efficient Python package for extracting temperature data from thermal images and converting it into an array, then saving it as a CSV file for further access. The link to the GitHub package describing the usage of Flirextractor in detail is as follows: . Labelme is a graphical image annotation tool written in Python. It allows users to demarcate the desired area of any shape with simple mouse clicks. Detailed instructions for Labelme can be found at the following link: . The GitHub code repository exhibits A meticulously structured hierarchy designed to enhance navigation and understandability. It incorporates three primary directories: ‘data’, ‘notebook’, and ‘src’. The ‘data’ directory segregates original, converted, and processed files, whereas the ‘notebook’ directory encompasses Jupyter notebooks detailing file conversion, data visualization, and data analysis procedures. The ‘src’ directory hosts a Python script dedicated to image conversion. Furthermore, the ‘data’ folder manifests a systematic organization structured to efficiently manage different stages of data processing. It consists of three primary subdirectories: ‘original’, ‘convert’, and ‘labelme’. The ‘original’ and ‘processed’ subdirectories mirror each other, containing dated folders representing different acquisition dates, with each date folder further divided into ‘view_1’, ‘view_2’, and ‘view_3’ subdirectories. These subdirectories contain the respective images captured from each view named “smap-YYYY-MM-DDTHH-MM-SS.MS.jpeg”. During the data processing phase, corresponding ‘json’ files for each image are generated in the ‘labelme’ directory. In addition, for each image, a dedicated directory is established within the ‘convert’ subdirectory, encapsulating original, processed, and labeled images and a text file enumerating the detected labels. Detailed file structure tree plots, included within the respective README files, furnish comprehensive visual representations of the file and folder organization, thus facilitating an understanding of the hierarchical structure and interrelationships among different components of the codebase. Moreover, an example involving a representative image has been conducted to illustrate the efficacy of the employed image processing techniques; the results highlight the segmentation, further processing, and final presentation of the image augmented with enhanced colors and informative labels. Please contact the corresponding author directly if you have any particular requirements.",2023-12-02
0,Scientific Data,41597,10.1038/s41597-023-02778-9,Annotating Macromolecular Complexes in the Protein Data Bank: Improving the FAIRness of Structure Data,1,12,2023,https://github.com/PDBe-KB/process-complex-data; https://github.com/PDBe-KB/pdbe-complex-analysis-demo,"The source code for our data pipeline is publicly available on GitHub at . This repository contains a Python package that aggregates data for macromolecular complexes from the PDBe graph database, assigns unique identifiers, and generates human-readable names. Additionally, we have created a demo repository that allows developers to test and benchmark the complex identifier-generating process. This repository is available on GitHub at .",2023-12-01
0,Scientific Data,41597,10.1038/s41597-023-02805-9,Deep learning downscaled high-resolution daily near surface meteorological datasets over East Asia,12,12,2023,,"[{'ext-link': [{'@xlink:href': 'https://github.com/cran/MBC', '@ext-link-type': 'uri', '#text': 'https://github.com/cran/MBC'}, {'@xlink:href': 'https://github.com/tensorflow/tensorflow', '@ext-link-type': 'uri', '#text': 'https://github.com/tensorflow/tensorflow'}], '#text': 'QDM approach in this study is carried out using the R-packages of the Multivariate Bias Correction of Climate Model Outputs (MBC) project and it is available through the following Github link: . The UNet downscaling approach is carried out using the python-packages of the tensorflow2 and it is available through the following Github link: .'}, {'ext-link': {'@xlink:href': 'https://github.com/LinHai-debug/CLIMEA-BCUD-code', '@ext-link-type': 'uri', '#text': 'https://github.com/LinHai-debug/CLIMEA-BCUD-code'}, '#text': 'All code used in this study can be available through the following Github link: .'}]",2023-12-12
0,Scientific Data,41597,10.1038/s41597-023-02720-z,Antarctic daily mesoscale air temperature dataset derived from MODIS land and ice surface temperature,27,11,2023,https://github.com/evabendix/AntAir-ICE,Python 3.8 was used for conversion of the MODIS products from HDF files to raster and all data handling and processing was thereafter done in R version 4.0.0. All data processing and modelling procedures are available as R scripts on a public Github repository: . Using this code it is possible to download new available MODIS LST and IST scenes and apply the model to continue the near-surface air temperature dataset.,2023-11-27
0,Scientific Data,41597,10.1038/s41597-023-02779-8,MultiPro: DDA-PASEF and diaPASEF acquired cell line proteomic datasets with deliberate batch effects,2,12,2023,https://github.com/kaipengl/batcheffectsdataset,The R scripts for reproducing the main figures are available through the GitHub repository at .,2023-12-02
0,Scientific Data,41597,10.1038/s41597-023-02636-8,Integrating databases for spatial analysis of parasite-host associations and the novel Brazilian dataset,2,11,2023,,"For data processing and generation of data products, we relied on open-source software packages. Specific software versions, when relevant, have been duly specified. The R code used to retrieve data from NCBI Nucleotide, GBIF, EID2, and NHM datasets is available at figshare. Codes for cross-referencing Nucleotide and PubMed, as well as meta-network analysis, are housed in the same repository. GBIF and NCBI Nucleotide data are within the *.RData file, which can be loaded into R using the ‘load’ statement.",2023-11-02
0,Scientific Data,41597,10.1038/s41597-023-02694-y,Harmonized geospatial data to support infrastructure siting feasibility planning for energy system transitions,9,11,2023,https://github.com/IMMM-SFA/vernon-etal_2023_scidata; https://github.com/IMMM-SFA/cerf,The code used to build the data described in the GRIDCERF package can be accessed on GitHub here: . They are in a Jupyter Notebook format to allow for interactive visualization and description throughout. The CERF modeling software is also available at  and is detailed in Vernon ..,2023-11-09
0,Scientific Data,41597,10.1038/s41597-023-02721-y,3DSC - a dataset of superconductors including crystal structures,21,11,2023,https://github.com/aimat-lab/3DSC,"Code and data are available free of charge. The code is provided in our Github repository . For reproducibility, the SHA of the final commit for this publication is 2471dd51a298a854cb4f365ebd39e72c7cbf3634. The data is available on figshare.",2023-11-21
0,Scientific Data,41597,10.1038/s41597-023-02408-4,Two excited-state datasets for quantum chemical UV-vis spectra of organic molecules,21,8,2023,https://github.com/ORNL/Analysis-of-Large-Scale-Molecular-Datasets-with-Python,The code for calculating the electronic excitation energies and statistical analysis of the dataset is open-source and available at the ORNL-GitHub repository .,2023-08-21
0,Scientific Data,41597,10.1038/s41597-023-02437-z,A natural language fMRI dataset for voxelwise encoding models,23,8,2023,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR25', '#text': '25'}}, '#text': 'All code used for encoding model fitting is publicly available and can be found on github and zenodo.'}, {'ext-link': {'@xlink:href': 'https://github.com/alexhuth/sensimetrics_filter', '@ext-link-type': 'uri', '#text': 'https://github.com/alexhuth/sensimetrics_filter'}, '#text': 'The code used for filtering the audio for each story to correct for frequency response and phase errors induced by the headphones using calibration data provided by Sensimetrics and custom Python code. This code can be found at .'}]",2023-08-23
0,Scientific Data,41597,10.1038/s41597-023-02495-3,A framework for FAIR robotic datasets,13,9,2023,https://github.com/CorradoMotta/FAIR-Data-in-Marine-Robotics,"Scripts, notebooks and modules to generate metadata in several formats following FAIR principles for marine robotic data is available on GitHub (), under the GNU General Public License v3.0. The dedicated GitHub page of the project supports the understanding and usage of the codes.",2023-09-13
0,Scientific Data,41597,10.1038/s41597-023-02551-y,Multi-view emotional expressions dataset using 2D pose estimation,22,9,2023,https://doi.org/10.5281/zenodo.8185369,The MATLAB code for parsing the JSON file and processing the coordinates can be found at .,2023-09-22
0,Scientific Data,41597,10.1038/s41597-023-02608-y,Tryp: a dataset of microscopy images of unstained thick blood smears for trypanosome detection,18,10,2023,https://github.com/esla/trypanosome_parasite_detection,The code and detailed documentation on how to use it to reproduce the results presented in this study is publicly available at  under the permissive Berkeley Software Distribution (BSD) 3-Clause license.,2023-10-18
0,Scientific Data,41597,10.1038/s41597-023-02409-3,DIPS-Plus: The enhanced database of interacting protein structures for interface prediction,3,8,2023,Zenodo; GitHub; GitHub; DOI,"Preprocessed data for DIPS-Plus as well as its associated source code and instructions for data processing and reproducibility can be found on  and , respectively. The GitHub instructions illustrate how users can install the Python programming language and build an Anaconda virtual environment containing the software dependencies required to preprocess and analyze DIPS-Plus using the provided Python scripts. Lastly, the GitHub instructions show users how to run such scripts and the order in which to do so to successfully rebuild DIPS-Plus from scratch, to featurize a given PDB file, or to train new machine learning models (e.g., NeiA) for protein interface prediction. For provenance, the original DIPS dataset’s source code can also be found on , along with a corresponding .",2023-08-03
0,Scientific Data,41597,10.1038/s41597-023-02382-x,Human alterations of the global floodplains 1992–2019,28,7,2023,,"['The global floodplain alteration dataset was derived entirely through ArcGIS 10.5 and ENVI 5.1 geospatial analysis platforms. To assist in reuse and application of the dataset, we developed additional Python codes aggregated as three web-based tools:', {'ext-link': {'@xlink:href': 'https://colab.research.google.com/drive/1xQlARZXKPexmDInYV-EMoJ-HZxmFL-eW?usp=sharing', '@ext-link-type': 'uri', '#text': 'https://colab.research.google.com/drive/1xQlARZXKPexmDInYV-EMoJ-HZxmFL-eW?usp=sharing'}, '#text': 'Floodplain Mapping Tool: .'}, {'ext-link': {'@xlink:href': 'https://colab.research.google.com/drive/1vmIaUCkL66CoTv4rNRIWpJXYXp4TlAKd?usp=sharing', '@ext-link-type': 'uri', '#text': 'https://colab.research.google.com/drive/1vmIaUCkL66CoTv4rNRIWpJXYXp4TlAKd?usp=sharing'}, '#text': 'Land Use Change Tool: .'}, {'ext-link': {'@xlink:href': 'https://colab.research.google.com/drive/1r2zNJNpd3aWSuDV2Kc792qSEjvDbFtBy?usp=share_link', '@ext-link-type': 'uri', '#text': 'https://colab.research.google.com/drive/1r2zNJNpd3aWSuDV2Kc792qSEjvDbFtBy?usp=share_link'}, '#text': 'Human Alteration Tool: .'}, 'See Usage Notes section for details.']",2023-07-28
0,Scientific Data,41597,10.1038/s41597-023-02524-1,Open access dataset integrating EEG and fNIRS during Stroop tasks,12,9,2023,https://github.com/Yaaaaaaaaabby/fNIRS-data-pre-processing-from-Zemeng-Chen.git; https://gitee.com/chen-zemeng/f-nirs-data-pre-processing-from-zemeng-chen.git,Scripts to import the fNIRS raw data (.tdms file format) into MATLAB and fNIRS data processing code used above are available at  or . A user guide describing the basic situation and usage of the dataset is uploaded together with the code. There are two files in the zip file. The MATLAB code file named “process_fNIRS_EEG_Stroop” is used to pre-process the fNIRS data of a subject. The folder used for MATLAB to load the .tdms file format is named “Matlab_read_tdms_file”. Please add the folder to the MATLAB search path before loading the .tdms file format.,2023-09-12
0,Scientific Data,41597,10.1038/s41597-023-02553-w,Homologous Pairs of Low and High Temperature Originating Proteins Spanning the Known Prokaryotic Universe,7,10,2023,,All of the products of this manuscript as well as the parameterized code pipelines are free and openly available. Please see “Usage Notes.”,2023-10-07
0,Scientific Data,41597,10.1038/s41597-023-02298-6,FAIR for AI: An interdisciplinary and international community building perspective,26,7,2023,,,2023-07-26
0,Scientific Data,41597,10.1038/s41597-023-02410-w,ChoCo: a Chord Corpus and a Data Transformation Workflow for Musical Harmony Knowledge Graphs,20,9,2023,,"[{'xref': {'@rid': 'Tab2', '@ref-type': 'table', '#text': '2'}, '#text': 'The ChoCo dataset and Knowledge Graph, together with the ontological ecosystem and code, are publicly available from several repositories (c.f. Table\xa0). As detailed in Methods, ChoCo is currently released in 2 modalities:'}, {'monospace': ['type', 'Sandbox'], '#text': '• As a JAMS dataset, where audio and score annotations are distinguished by the  attribute in their ; and temporal/metrical information is expressed in seconds (for audio) and measure:beat (for scores) ;'}, {'xref': {'@rid': 'Tab2', '@ref-type': 'table', '#text': '2'}, '#text': '• As a Knowledge Graph, based on our JAMS ontology to model music annotations, and on the Chord and Roman ontologies to semantically describe chords; Table\xa0 also provides links to a live SPARQL endpoint.'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR21', '#text': '21'}}, 'ext-link': {'@xlink:href': 'https://github.com/smashub/choco', '@ext-link-type': 'uri', '#text': 'https://github.com/smashub/choco'}, '#text': 'We have implemented a number of actions to ensure that these outputs are in compliance with the FAIR Guiding Principles for scientific data management and stewardship. A GitHub repository hosts data, code, and instructions (), to fully reproduce the corpus creation from the original collections. To improve reproducibility, the repository also provides a Docker image for the project (platform agnostic). To improve data consistency, both the latest versions of ChoCo (JAMS file and RDF triples) are available on Zenodo, in synchronisation with GitHub releases.'}, 'Via GitHub and Zenodo, the ChoCo project has a unique and persistent identifier and is registered in a searchable source. Additionally, via our integration framework, ChoCo contains fine-grained provenance descriptions that allow to keep track of the original source of each harmonic annotation – both in terms of annotators (the person who contributed the harmonic analysis) and data curator (the maintainer of the original collection).', {'italic': ['Creative Commons Attribution 4.0', 'Creative Commons Attribution-NonCommercial-ShareAlike 4.0'], 'monospace': ['CC-BY 4.0', 'CC-BY-NC-SA 4.0', 'CC-BY 4.0'], 'xref': {'@rid': 'Tab8', '@ref-type': 'table', '#text': '8'}, '#text': 'Finally, to comply with the original collections, all data and code in ChoCo is released under the  licence (), with the exception of the JAAH, CASD, and Mozart Piano Sonata subsets – which follow the  international licence (). This required an in-depth analysis of the licensing policies of the integrated collections (see Table\xa0). Indeed, for 7 collections, we could not find any specific licensing information from related scientific articles, technical reports, online resources, repositories, dataset metadata, and so forth. For these cases, the authors of these collections were contacted and confirmed whether the use of the  licence – on our derivative integration work – was compatible with their original releasing strategies.'}]",2023-09-20
0,Scientific Data,41597,10.1038/s41597-023-02469-5,DiaTrend: A dataset from advanced diabetes technology to enable development of novel analytic solutions,23,8,2023,https://github.com/Augmented-Health-Lab/Diatrend,Python was used for all data processing described in this paper. The Python code used to generate all figures in this paper is available on the Augmented Health Lab’s Github: .,2023-08-23
0,Scientific Data,41597,10.1038/s41597-023-02525-0,"DISCOVER-EEG: an open, fully automated EEG pipeline for biomarker discovery in clinical neuroscience",11,9,2023,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR30', '#text': '30'}}, '#text': 'The EEG pipeline code is available at GitHub under the CC-BY 4.0 license, and it is co-deposited in Zenodo, and referenced with a unique DOI.'}, {'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR24', '#text': '24'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR25', '#text': '25'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR45', '#text': '45'}}], '#text': 'The pipeline was created and tested in Matlab 2020b (The Mathworks, Inc.) on Ubuntu 18.04.5 LTS with the Signal Processing and Statistical and Machine Learning Toolboxes installed. EEGLab (v2022.0) with the plugins bids-matlab-tools (v6.1), bva-io (v1.7), firfilt, (v2.4), cleanLine (v2.0), ICLabel (v1.3), clean_rawdata (v2.6) and dipfilt (v4.3) were installed and used for preprocessing. FieldTrip (revision ee916f5e5) was used for source reconstruction and EEG feature extraction, and the Brain Connectivity Toolbox (version 03 2019) was used for network analysis.'}]",2023-09-11
0,Scientific Data,41597,10.1038/s41597-023-02554-9,Multimodal video and IMU kinematic dataset on daily life activities using affordable devices,22,9,2023,https://doi.org/10.5281/zenodo.7681316; https://doi.org/10.5281/zenodo.7693096; https://github.com/twyncoder/vidimu-tools,"The VIDIMU dataset () was built using the free tools  (v0.8) and  (v4.4). The VIDIMU-TOOLS code contains the Jupyter notebooks and Python scripts used for data conversion, data synchronization and checking the contents of the dataset to ensure its integrity. A first release of the VIDIMU-TOOLS project is accessible in Zenodo () and the latest version of the code can be found in GitHub ().",2023-09-22
0,Scientific Data,41597,10.1038/s41597-023-02583-4,The R package for DICOM to brain imaging data structure conversion,4,10,2023,https://github.com/bidsconvertr/bidsconvertr; https://bidsconvertr.github.io,"All code is hosted freely and open-source on a GitHub repository, accessible via . The documentation is hosted on the GitHub page: .",2023-10-04
0,Scientific Data,41597,10.1038/s41597-023-02411-9,Dataset of mechanical properties and electrical conductivity of copper-based alloys,29,7,2023,,"Data processing, validation and plotting were performed using Excel and Jupyter notebooks in a Python 3 environment. No custom code has been used.",2023-07-29
0,Scientific Data,41597,10.1038/s41597-023-02499-z,An RFI-suppressed SMOS L-band multi-angular brightness temperature dataset spanning over a decade (since 2010),8,9,2023,https://doi.org/10.11888/Terre.tpdc.300406; https://cstr.cn/18406.11.Terre.tpdc.300406; https://github.com/thimpeng/RFI-Suppressed_SMOS_L-band_multi-angular_TB_Refinement,The software and codes for processing the collected data and for plotting the figures are conducted in Python 3.7 and included in the dataset at  or . And they are also available on GitHub: .,2023-09-08
0,Scientific Data,41597,10.1038/s41597-023-02555-8,CherryChèvre: A fine-grained dataset for goat detection in natural environments,11,10,2023,https://doi.org/10.57745/QEZBNA,"The Python 3.10 scripts used for converting the VGG VIA csv format to YOLO format, as well as other scripts used for generating statistics presented in the article, are available at .",2023-10-11
0,Scientific Data,41597,10.1038/s41597-023-02640-y,One-year dataset of hourly air quality parameters from 100 air purifiers used in China residential buildings,18,10,2023,https://github.com/weijiaze/Scientific-data/blob/master/imputefile.ipynb; https://github.com/weijiaze/Scientific-data,"Each CSV file has a large amount of data. Programming languages (such as Python) for data visualization and manipulation are recommended for data exploration and analysis. The data preprocessing codes are written in Python on Jupyter Notebook, which can be found at “. The code runs on a Windows computer using Python 3. The Python 3 codes to reproduce the examples provided in this article can be found at .",2023-10-18
0,Scientific Data,41597,10.1038/s41597-023-02327-4,A comprehensive multi-domain dataset for mitotic figure detection,25,7,2023,https://github.com/DeepMicroscopy/MIDOGpp,We provide the code that we used to run all baseline experiments and all data in our GitHub repository ().,2023-07-25
0,Scientific Data,41597,10.1038/s41597-023-02470-y,Applying FAIR4RS principles to develop an integrated modeling environment for the magnetic confinement fusion,7,9,2023,https://doi.org/10.5281/zenodo.8098117,"We use the ZENODO service to keep a persistent archive of FyDev code, with .",2023-09-07
0,Scientific Data,41597,10.1038/s41597-023-02585-2,A Spitzoid Tumor dataset with clinical metadata and Whole Slide Images for Deep Learning models,16,10,2023,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR23', '#text': '23'}}, '#text': 'The file code is in the figshare repository as “paper_dataset.zip”. They are mainly two folders. The folder “First_step” has the file “Train_Patch_level_model.py”. This file contains the necessary statements to perform patch-level classification for regions of interest and non-interest.'}, 'In the folder “Second_step,” three files are located. The file “WSI_prediction_MIL” contains the main functionalities for training an algorithm based on Multiple Instance Learning (MIL). This script calls several functions stored in the “DataGenerator” and “utils_1.” files. The file “DataGenerator” holds the necessary functions for loading data in the form of bags required for a Multiple Instance Learning algorithm. In the “utils_1” folder, all the necessary functions for extracting metrics, calculating loss, and more can be accessed. These functions are utilized in the “WSI_prediction_MIL” file.']",2023-10-16
0,Scientific Data,41597,10.1038/s41597-023-02328-3,Home-to-school pedestrian mobility GPS data from a citizen science experiment in the Barcelona area,4,7,2023,https://github.com/ferranlarroyaub/Beepath-Schools,"The  repository holds the Python code and scripts to process the input data and to replicate the statistical analysis and the figures. The 3.8 Python version is used to build the code with the main libraries:  and  to plot the trajectories on OpenStreet maps.  and  to process, clean, and analyze the data in Data-Frame format and perform the basic statistic calculations.  for more advanced calculations such as fitting models to the empirical data and  for plotting purposes. The Python code is built in different Jupyter notebook files which contain a detailed description of the study and the code documentation.",2023-07-04
0,Scientific Data,41597,10.1038/s41597-023-02442-2,"Development of an integrated and inferenceable RDF database of glycan, pathogen and disease resources",6,9,2023,https://github.com/glycoinfo/GlycanBind/releases/tag/v1.0.2; https://zenodo.org/record/8072786,All code to generate the SugarBind RDF resources and the generated RDF data files are available from the GitHub repository as a v1.0.2 release at . The entire Github repository for the v1.0.2 release is archived on Zenodo: .,2023-09-06
0,Scientific Data,41597,10.1038/s41597-023-02586-1,Measurements of aerosol microphysical and chemical properties in the central Arctic atmosphere during MOSAiC,11,10,2023,,"[{'italic': 'et al', 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR80', '#text': '80'}}, 'ext-link': {'@xlink:href': '10.5281/zenodo.5761101', '@ext-link-type': 'doi', '#text': 'https://doi.org/10.5281/zenodo.5761101'}, '#text': 'The pollution detection algorithm described in Beck . to identify and flag periods of primary polluted data is available on Zenodo ().'}, {'ext-link': {'@xlink:href': 'https://cires1.colorado.edu/jimenez-group/wiki/index.php/High_Resolution_ToF-AMS_Analysis_Guide', '@ext-link-type': 'uri', '#text': 'https://cires1.colorado.edu/jimenez-group/wiki/index.php/High_Resolution_ToF-AMS_Analysis_Guide'}, '#text': 'High Resolution ToF-AMS Analysis guide from J. L. Jimenez research group’s wiki (CIRES, University of Colorado at Boulder, USA):  (last accessed: 03/03/2022).'}]",2023-10-11
0,Scientific Data,41597,10.1038/s41597-023-02358-x,A dataset on energy efficiency grade of white goods in mainland China at regional and household levels,12,7,2023,https://github.com/CEEGDataset/CEEG-Dataset.git,Examples of the code that we used to produce the datasets presented in this paper (mainly for establishing REPM and HEPM) are provided in GitHub ().,2023-07-12
0,Scientific Data,41597,10.1038/s41597-023-02443-1,A quantum chemical interaction energy dataset for accurately modeling protein-ligand interactions,12,9,2023,,"Script merge_monomers.py, used to create the Psi4 input files that were used for SAPT0 energy calculations, is available as part of the figshare repository containing the Splinter dataset.",2023-09-12
0,Scientific Data,41597,10.1038/s41597-023-02529-w,MUSE-RASA captures human dimension in climate-energy-economic models via global geoAI-ML agent datasets,12,10,2023,,"The algorithms and formulas used in this study have been previously provided. This research used three programmatic free and open-source platforms: (1) R Statistical Software and Programming Language; (2) Quantum GIS (QGIS) software; and (3) Python software. A range of R Packages for geospatial big data analytics used in this research are presented in Bivand. QGIS is used for data exploration purposes because of its features of viewing, editing, and analysing geospatial data. Python is the development programmatic environment for the MUSE model. The MUSE-RASA model has been built from the integration of the R-based geospatial RASA model with a Python-based MUSE model to end with the MUSE-RASA model. The R code used to create the shape files in the RASA model is available upon request with proper justification from the corresponding author. The MUSE model is an open source code available in Giarola, .. Due to sponsorship agreements, the authors are not allowed to make the RASA code publicly available.",2023-10-12
0,Scientific Data,41597,10.1038/s41597-023-02415-5,Continuous observations of the surface energy budget and meteorology over the Arctic sea ice during MOSAiC,4,8,2023,https://github.com/MOSAiC-flux/data-processing,"The code and associated libraries used to create Level 1, Level 2, and Level 3 processed files are based in Python with the following dependencies: Python > = 3.6; netCDF4 > = 1.3.0, NumPy > = 1.13.0, Scipy > = 1.1.0, Pandas > = 0.20, XArray > = 0.11; PVLib > = 0.8.1. Files uploaded to ADC have the following versions: Level 1 are v1.5 (1/8/2020) and Levels 2 and 3 v4.1 (2/1/2023). Code is archived on GitHub, .",2023-08-04
0,Scientific Data,41597,10.1038/s41597-023-02473-9,Monitoring of carbon-water fluxes at Eurasian meteorological stations using random forest and remote sensing,7,9,2023,https://doi.org/10.6084/m9.figshare.21510183.v2,The code to generate the carbon-water flux datasets is available at figshare ().,2023-09-07
0,Scientific Data,41597,10.1038/s41597-023-02559-4,The floodplain inundation history of the Murray-Darling Basin through two-monthly maximum water depth maps,23,9,2023,https://github.com/csiro-hydroinformatics/water-depth-estimation,"Version 1.0 of the water-depth-estimation code used for calculating FwDET is available under GPLv3 licensing at . The repository also contains a Jupyter notebook (notebooks/example_water_depth.ipynb), which is useful for exploring the water depth outputs.",2023-09-23
0,Scientific Data,41597,10.1038/s41597-023-02389-4,"AtOM, an ontology model to standardize use of brain atlases in tools, workflows, and data infrastructures",26,7,2023,https://github.com/tgbugs/pyontutils/tree/master/nifstd/nifstd_tools/parcellation,Python code for generating parcellations for the NIF-Ontology is publicly available via GitHub: . Archives of release are available via Zenodo.,2023-07-26
0,Scientific Data,41597,10.1038/s41597-023-02416-4,"MedalCare-XL: 16,900 healthy and pathological synthetic 12 lead ECGs from electrophysiological simulations",8,8,2023,,"Code for solving the Eikonal equation and the forward problem of electrocardiography using the boundary element method as used for the atrial simulations is openly available (Stenroos ., Schuler .). The electrophysiology of the ventricular-torso model was simulated using the proprietary CARPentry-Pro software (NumeriCor, Graz, Austria). Similar simulations can also be carried out with the publicly available openCARP simulation framework. Python code for synthesizing single beat P waves and QRST complexes to a 10 s time series using multi-variate normal distributions for amplitude scaling and interval selection is publicly available.",2023-08-08
0,Scientific Data,41597,10.1038/s41597-023-02445-z,A large EEG database with users’ profile information for motor imagery brain-computer interface research,5,9,2023,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR32', '#text': '32'}}, 'ext-link': {'@xlink:href': 'http://openvibe.inria.fr/', '@ext-link-type': 'uri', '#text': 'http://openvibe.inria.fr/'}, '#text': 'As described above, all the code of the experiments in available free and open-source. The OpenViBE scenarios are shared with the database, while the free and open-source OpenViBE BCI platform itself can be downloaded there: ().'}, {'ext-link': [{'@xlink:href': 'http://openvibe.inria.fr/', '@ext-link-type': 'uri', '#text': 'http://openvibe.inria.fr/'}, {'@xlink:href': 'http://openvibe.inria.fr/pub/src/', '@ext-link-type': 'uri', '#text': 'http://openvibe.inria.fr/pub/src/'}, {'@xlink:href': 'https://gitlab.inria.fr/openvibe/meta', '@ext-link-type': 'uri', '#text': 'https://gitlab.inria.fr/openvibe/meta'}], '#text': 'To access and download the free and open source OpenViBE BCI platform, go to: . The source code for all versions of OpenViBE is available at , for each version you will find a README with build instructions in the archive you have downloaded. You can also find the sources at . To install the corresponding versions, which will allow you to reuse the scenarios of the experiments corresponding to this manuscript, you need to download older versions of OpenViBE.'}, {'ext-link': [{'@xlink:href': 'http://openvibe.inria.fr/pub/bin/win32/', '@ext-link-type': 'uri', '#text': 'http://openvibe.inria.fr/pub/bin/win32/'}, {'@xlink:href': 'http://openvibe.inria.fr/pub/bin/win64/', '@ext-link-type': 'uri', '#text': 'http://openvibe.inria.fr/pub/bin/win64/'}], 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR32', '#text': '32'}}, '#text': 'OpenViBE 2.1.0 (for dataset A) is available at  and OpenViBE 2.2.0 (for dataset B) is available at . You will then need to open all the scenarios provided in zenodo and you will be able to run them.'}]",2023-09-05
0,Scientific Data,41597,10.1038/s41597-023-02474-8,CA-discharge: Geo-Located Discharge Time Series for Mountainous Rivers in Central Asia,4,9,2023,,The data set is available from Zenodo. R scripts used to pre-process the data is available from the same Zenodo repository. The GEE code for the extraction of snow cover faction is available from Zenodo.,2023-09-04
0,Scientific Data,41597,10.1038/s41597-023-02501-8,Shared metadata for data-centric materials science,14,9,2023,,,2023-09-14
0,Scientific Data,41597,10.1038/s41597-023-02589-y,City-scale Vehicle Trajectory Data from Traffic Camera Videos,17,10,2023,,"The codes for how we generate the trajectory dataset based on visual embedded traffic camera records, evaluate the vehicle Re-ID and trajectory recovery metrics, and report statistical characteristics are available in our GitHub repository. There are also tips for installing Python requirements.",2023-10-17
0,Scientific Data,41597,10.1038/s41597-023-02417-3,Geometrical digital twins of the as-built microstructure of three-leaf stone masonry walls with laser scanning,10,8,2023,https://ibois-epfl.github.io/Cockroach-documentation/; https://github.com/ibois-epfl/collide; https://doi.org/10.5281/zenodo.7093710,"The pipeline is based on the use of Rhinoceros3D and the Cockroach plugin which is available at . A tutorial for the implementation of the pipeline is freely available online. In this work, we used Rhinoceros3D Version 7 SR29. The collision analysis algorithm is available at: . The Python script used for the analysis of the collision data is available at: .",2023-08-10
0,Scientific Data,41597,10.1038/s41597-023-02475-7,Development of global monthly dataset of CMIP6 climate variables for estimating evapotranspiration,26,8,2023,https://pyeto.readthedocs.io/en/latest/,"The code to produce the data was written using Python, PyCharm 2022.2.2. The code is available in pyeto ().",2023-08-26
0,Scientific Data,41597,10.1038/s41597-023-02531-2,AmeriFlux BASE data pipeline to support network growth and data sharing,11,9,2023,https://github.com/AMF-FLX/AMF-BASE-QAQC; https://doi.org/10.5281/zenodo.8250754,The core Python-based BASE data-processing pipeline code is available under a modified BSD license at . The R-based code for generating the article’s figures is available at .,2023-09-11
0,Scientific Data,41597,10.1038/s41597-023-02503-6,EWELD: A Large-Scale Industrial and Commercial Load Dataset in Extreme Weather Events,11,9,2023,https://github.com/Judy0718/EWELD,The code implementation was done using Python. Source codes that were used to develop and analyze the data are publicly available in the GitHub repository ().,2023-09-11
0,Scientific Data,41597,10.1038/s41597-023-02590-5,A comprehensive spectral assay library to quantify the  NRC-1 proteome by DIA/SWATH-MS,13,10,2023,https://github.com/alanlorenzetti/protDynContGenExp_v2,"Code to perform PCA, generate heat maps and volcano plots, and carry out the transcriptome differential expression analysis is publicly available under repository .",2023-10-13
0,Scientific Data,41597,10.1038/s41597-023-02391-w,DUO-GAIT: A gait dataset for walking under dual-task and fatigue conditions with inertial measurement units,21,8,2023,,"[{'ext-link': {'@xlink:href': 'https://github.com/HPI-CH/fatigue-dual-task-data', '@ext-link-type': 'uri', '#text': 'https://github.com/HPI-CH/fatigue-dual-task-data'}, '#text': 'All data processing procedures described in this paper were performed using Python 3.7. The code repository and more detailed usage instructions can be found at . The main functionalities of the code are as follows:'}, '• Segment the IMU recordings into walking sessions and fatigue exercise', '• Calculate spatio-temporal gait parameters from the IMU signals', '• Summarize gait parameters and other study-related information']",2023-08-21
0,Scientific Data,41597,10.1038/s41597-023-02477-5,A Quantum-Chemical Bonding Database for Solid-State Materials,11,9,2023,https://github.com/naik-aakash/lobster-database-paper-analysis-scripts; https://doi.org/10.5281/zenodo.8172527,"The following program versions have been used in this study:  2022.11.7,  2023.3.10,  1.0.3,  4.1.0, and  5.4.4 for  and  computations using the workflow. For data validation and processing, we have used  2023.6.23 and  0.2.9. All the scripts used in this study, from starting the workflow, generating data records, reproducing technical validation plots, and ML model evaluations, can be accessed here:  ().",2023-09-11
0,Scientific Data,41597,10.1038/s41597-023-02392-9,NeuMa - the absolute Neuromarketing dataset en route to an holistic understanding of consumer behaviour,3,8,2023,https://github.com/NeuroMkt/NeuMa_Dataset_Processing/tree/main/NeuMa_Raw_Usage_Code; https://github.com/NeuroMkt/NeuMa_Dataset_Processing/tree/main/NeuMa_PreProcessed_Usage_Code,"The Matlab code developed for data loading, data parsing, segmentation and preprocessing required for the use of both datasets (provided via Figshare) alongside with two Matlab Live Scripts that demonstrate the data handling process for each dataset are provided in our Github repository (NeuMa Raw Dataset, ; NeuMa Pre-processed Dataset, ).",2023-08-03
0,Scientific Data,41597,10.1038/s41597-023-02364-z,The benefits and struggles of FAIR data: the case of reusing plant phenotyping data,13,7,2023,,"All associated code is available on our Github repository and deposited on Zenodo, including Jupyter notebooks to transform data, scripts to run the FDP and the triple store.",2023-07-13
0,Scientific Data,41597,10.1038/s41597-023-02393-8,EUSEDcollab: a network of data from European catchments to monitor net soil erosion by water,4,8,2023,https://github.com/matfran/EUSEDcollab.git,"All code can be found at: . We include the R language code to perform the quality control procedure on each time series entry to produce the JSON time series evaluation files for each record. Additionally, a Python language Jupyter notebook is included to demonstrate simple operations that can be undertaken using the database, such as reading and filtering the database, calculating metadata statistics and importing specific time series for analysis.",2023-08-04
0,Scientific Data,41597,10.1038/s41597-023-02535-y,A daily high-resolution (1 km) human thermal index collection over the North China Plain from 2003 to 2020,18,9,2023,https://github.com/CSLixiang/HiTIC-NCP.git,"The HiTIC-NCP dataset generation codes are available on GitHub (), and operational under Python 3.8 or JavaScript. In the GitHub repository, we uploaded three code scripts, i.e., “Data preprocessing code.py”, “HiTIC-NCP Code.py” and “Figures code.py”. Additionally, the data samples were uploaded to the “Data Samples” folder.",2023-09-18
0,Scientific Data,41597,10.1038/s41597-023-02366-x,"Atomic structures, conformers and thermodynamic properties of 32k atmospheric molecules",12,7,2023,https://github.com/Supervitux/COSMO_on_Merlin; http://geckoa.lisa.u-pec.fr/; https://www.3ds.com/; https://merlin.readthedocs.io/en/latest/index.html#,Custom code written for data generation mainly consists of scripts for pre- and postprocessing steps linking together the software mentioned below. These scripts are executed through a Merlin workflow. All these scripts are publicly available in a GitHub repository: . GECKO-A is available at their website . COSMO 4.3 and COSMO 2021 and their licenses were purchased from Dassault Systemes (). We provide our custom COSMO jobtemplate (( in the repository. Merlin version 1.7.5 is freely available from .,2023-07-12
0,Scientific Data,41597,10.1038/s41597-023-02451-1,A Data Set of Signals from an Antenna for Detection of Partial Discharges in Overhead Insulated Power Line,21,8,2023,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR5', '#text': '5'}}, 'ext-link': {'@xlink:href': 'https://github.com/Lukykl1/dataset_pd_vsb', '@ext-link-type': 'uri', '#text': 'https://github.com/Lukykl1/dataset_pd_vsb'}, '#text': 'The code for loading the data set, testing, and utility functions is available in a Github repository, which can be found at the following .'}, 'The repository includes code for loading the data in npy format and the metadata in csv format, as well as utility functions for processing and analyzing the data. The code is documented and includes comments to explain each step of the process.', 'To run the Python scripts, you will need to have the necessary libraries installed, including NumPy, pandas, Matplotlib, and any other required dependencies. You can install these libraries using a package manager such as pip or conda.', 'We hope that this code will be useful to researchers and data scientists who are working with these data and looking for an efficient and flexible way to load and process them.']",2023-08-21
0,Scientific Data,41597,10.1038/s41597-023-02567-4,Inertial measurement data from loose clothing worn on the lower body during everyday activities,17,10,2023,,"Data and MATLAB scripts are available in figshare. Further, CSV files of the MATLAB variables and Python scripts to read the MAT files directly are also available in figshare",2023-10-17
0,Scientific Data,41597,10.1038/s41597-023-02339-0,Three-dimensional topology dataset of folded radar stratigraphy in northern Greenland,7,8,2023,https://gitlab.com/openpolarradar/opr; https://gitlab.com/openpolarradar/opr/-/wikis/home,"The CReSIS toolbox used to process the MCoRDS RES data is available at , and the main documentation can be found at .",2023-08-07
0,Scientific Data,41597,10.1038/s41597-023-02424-4,GRAPE: A multi-modal dataset of longitudinal follow-up visual field and fundus images for glaucoma management,5,8,2023,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR36', '#text': '36'}}, '#text': 'The GRAPE dataset can be downloaded at the Figshare as mentioned above. The codes of drawing annotations on ROI images are saved as the python file “draw.py”.'}, 'The DL models applied in “Technical Validation” are not the as a part of the GRAPE dataset. We uploaded these models at the Figshare as two separated parts, “Baseline model validation for VF estimation” and “Baseline model validation for VF progression prediction”, that correspond to the 2 chapters. The parameters tuning is detailed in the article.']",2023-08-05
0,Scientific Data,41597,10.1038/s41597-023-02482-8,Label-free tumor cells classification using deep learning and high-content imaging,26,8,2023,https://github.com/cmb-chula/CancerCellVision-CCA,All code used in this experiment was written in Python3 and could be publicly accessed at . The code is based on PyTorch and MMDetection.,2023-08-26
0,Scientific Data,41597,10.1038/s41597-023-02369-8,Data scheme and data format for transferable force fields for molecular simulation,27,7,2023,,The code used for converting the data format files and building the SQL-based format are publicly available in an online repository.,2023-07-27
0,Scientific Data,41597,10.1038/s41597-023-02398-3,Open data for COVID-19 policy analysis and mapping,27,7,2023,,"[{'ext-link': {'@xlink:href': 'https://api.covidamp.org/docs', '@ext-link-type': 'uri', '#text': 'https://api.covidamp.org/docs'}, '#text': 'COVID AMP data are available via an application programming interface (API) and are licensed under the Creative Commons Attribution CC BY Standard at: .'}, {'ext-link': [{'@xlink:href': 'https://covidamp.org/', '@ext-link-type': 'uri', '#text': 'https://covidamp.org/'}, {'@xlink:href': 'https://covidamp.org/data?type=policy', '@ext-link-type': 'uri', '#text': 'https://covidamp.org/data?type=policy'}, {'@xlink:href': 'https://covidamp.org/about/doc', '@ext-link-type': 'uri', '#text': 'https://covidamp.org/about/doc'}], '#text': 'We provide a public, interactive web interface for visual exploration of the dataset at: . Within the site, data is available at: . This page allows for download of the full dataset or filtered subsets of the data. Additionally, documentation of the methods, including a data dictionary and glossary, are available at: .'}, {'ext-link': {'@xlink:href': '10.5281/zenodo.8087600', '@ext-link-type': 'doi', '#text': 'https://doi.org/10.5281/zenodo.8087600'}, '#text': 'In addition to this manuscript, a static version of the COVID AMP dataset itself can be cited directly as Zenodo .'}, 'All policies and directives coded within the dataset have been reviewed and technically validated. We hope that the dataset will support research efforts aimed at improving pandemic response strategies and inform future outbreak policy analysis.']",2023-07-27
0,Scientific Data,41597,10.1038/s41597-023-02425-3,Hyperlocal environmental data with a mobile platform in urban environments,5,8,2023,https://github.com/MIT-Senseable-City-Lab/OSCS/tree/main,"Other than air quality data stamped with time and location, we also provide a compilation of land use GIS layers that are used in our and NYCCAS’ LUR models for convenient reproduction of the results in our Github repository (). These GIS layers are published by NYC and New York State governments and processed by the authors for modeling, with 2021 as the base year. The audience is encouraged to explore the repository, regarding the details about how we design, build, calibrate, and make use of the CS platform. Python code is available for automatic land use feature extraction, LUR training, and performance evaluation.",2023-08-05
0,Scientific Data,41597,10.1038/s41597-023-02483-7,Standardised images of novel objects created with generative adversarial networks,2,9,2023,https://artbreeder.com,"Software to generate novel objects is available at . Code to perform data collection (i.e., run the online experiment) was created using jsPsych (version 7.2.1), and a modified version of the  extension (all available at). Code to extract the objective properties of each object, and to compile the subjective ratings from our online study, was written in Python, MATLAB and Julia respectively (available at).",2023-09-02
0,Scientific Data,41597,10.1038/s41597-023-02510-7,SC2EGSet: StarCraft II Esport Replay and Game-state Dataset,8,9,2023,,"[{'sup': {'xref': [{'@ref-type': 'bibr', '@rid': 'CR25', '#text': '25'}, {'@ref-type': 'bibr', '@rid': 'CR26', '#text': '26'}, {'@ref-type': 'bibr', '@rid': 'CR27', '#text': '27'}], '#text': ', –'}, '#text': 'Our dataset was created by using open-source tools that were published with separate digital object identifiers (doi) minted for each of the repositories. These tools are indexed on Zenodo.'}, {'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR73', '#text': '73'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR74', '#text': '74'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR31', '#text': '31'}}], 'ext-link': {'@xlink:href': 'https://github.com/Kaszanas/SC2_Datasets', '@ext-link-type': 'uri', '#text': 'https://github.com/Kaszanas/SC2_Datasets'}, '#text': 'We have made available a PyTorch and PyTorch Lightning API published to PyPI for accessing our dataset and performing various analyses. Additionally, Our API is accessible in the form of a GitHub repository - , which is available on Zenodo with a separate doi. All of the instructions for accessing the data and specific field documentation are published there.'}, {'ext-link': {'@xlink:href': 'https://github.com/Kaszanas/SC2EGSet_article_experiments', '@ext-link-type': 'uri', '#text': 'https://github.com/Kaszanas/SC2EGSet_article_experiments'}, '#text': 'The code used for technical validation experiments is available for preview in a GitHub repository: .'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR74', '#text': '74'}}, '#text': 'In the process of preparing this article, PyTorch Lightning has changed its name into Lightning. We have decided to use the old form of the name, following the citation template provided by the Lightning project on GitHub.'}, 'GitHub Links to the tooling used in the dataset preparation:', {'ext-link': {'@xlink:href': 'https://github.com/Kaszanas/SC2InfoExtractorGo', '@ext-link-type': 'uri', '#text': 'https://github.com/Kaszanas/SC2InfoExtractorGo'}, '#text': '• ,'}, {'ext-link': {'@xlink:href': 'https://github.com/Kaszanas/SC2DatasetPreparator', '@ext-link-type': 'uri', '#text': 'https://github.com/Kaszanas/SC2DatasetPreparator'}, '#text': '• ,'}, {'ext-link': {'@xlink:href': 'https://github.com/Kaszanas/SC2MapLocaleExtractor', '@ext-link-type': 'uri', '#text': 'https://github.com/Kaszanas/SC2MapLocaleExtractor'}, '#text': '• ,'}, {'ext-link': {'@xlink:href': 'https://github.com/Kaszanas/SC2_Datasets', '@ext-link-type': 'uri', '#text': 'https://github.com/Kaszanas/SC2_Datasets'}, '#text': 'The official dataset API is available at the following repository: .'}, {'ext-link': {'@xlink:href': 'https://github.com/Kaszanas/SC2AnonServerPy', '@ext-link-type': 'uri', '#text': 'https://github.com/Kaszanas/SC2AnonServerPy'}, '#text': 'Additional tooling for potential anonymization tasks with data from private collections is available at: .'}]",2023-09-08
0,Scientific Data,41597,10.1038/s41597-023-02340-7,"A pipeline to further enhance quality, integrity and reusability of the NCCID clinical data",27,7,2023,https://gitlab.developers.cam.ac.uk/maths/cia/covid-19-projects/nccidxclean,"Our extended cleaning pipeline  is publicly available and can be accessed on GitLab:  with accompanying documentation available on the project website (package version at the time of publication: v1.0). The package will automatically download the necessary packages and requirements during installation, including the original NCCID cleaning pipeline upon which our pipeline builds. The python package is independent of the operating system and allows replication of our results using the command line interface, python scripts, or the included Jupyter notebooks. For full analysis, the  parameter can be set to ‘all_with_original’, returning all possible data features. Additional code is provided to allow for the original NHSx pipeline to be run using a single command in the command line. The data from the NHSx pipeline may be then used in the analysis subpackage to generate all figures and numerical results found in this work. An additional subpackage  is included for exploratory data analysis of the final cleaned data. Full step-by-step guidance and further details of the package are provided on the project’s website. .",2023-07-27
0,Scientific Data,41597,10.1038/s41597-023-02569-2,A synthetic data set to benchmark anti-money laundering methods,28,9,2023,https://sdv.dev; https://sdv.dev/SDV/user_guides/relational/hma1; https://scikit-learn.org; https://lightgbm.readthedocs.io/en/stable,"All our simulations are made using version 0.14.1 of the SDV library (). We specifically employ the HMA1 model class using two tables as inputs: a primary table with alerts (see Table ) and a secondary table with transactions (see Table ). A demonstration by the SDV developers is available online (; using data different from ours). Due to confidentiality, we do not share our code that (i) transforms the raw data so that it can be fed to the HMA1 model class and (ii) re-transforms and adds noise to the simulated data. The data-providing bank felt that providing this code would reveal sensitive information about its internal setup and the real data. All our transformations are, however, described in detail in our subsection “Implementation: Pre- and Postprocessing.” Our machine learning experiments were conducted with version 1.1.3 of the Scikit-learn library () and version 3.3.3 of the LightGBM library ().",2023-09-28
0,Scientific Data,41597,10.1038/s41597-023-02399-2,A multi-predator trophic database for the California Current Large Marine Ecosystem,27,7,2023,,"No custom code was created in the generation or processing of data sets. R version 3.6.0, Notepad version 7.7.1, and Microsoft Excel 2016 were used to process each submitted data set and to build the eight previously described data tables. SQL Server Management Studio 18.12.1 was used to establish primary and foreign key relationships between these data tables to create the CCTD.",2023-07-27
0,Scientific Data,41597,10.1038/s41597-023-02455-x,Machine learning-ready remote sensing data for Maya archaeology,23,8,2023,,"[{'ext-link': {'@xlink:href': 'https://github.com/EarthObservation/RVT_py', '@ext-link-type': 'uri', '#text': 'https://github.com/EarthObservation/RVT_py'}, '#text': 'ALS visualisations were calculated with Relief Visualization Toolbox (version 2.2.1), available at .'}, {'ext-link': {'@xlink:href': 'https://github.com/EarthObservation/Sentinel-S1-S2-ML-patches-workflow', '@ext-link-type': 'uri', '#text': 'https://github.com/EarthObservation/Sentinel-S1-S2-ML-patches-workflow'}, '#text': 'The code for creating the satellite data records from Sentinel-1 and Sentinel-2 is available at .'}]",2023-08-23
0,Scientific Data,41597,10.1038/s41597-023-02484-6,"2DeteCT - A large 2D expandable, trainable, experimental Computed Tomography dataset for machine learning",4,9,2023,https://github.com/mbkiss/2DeteCTcodes; www.astra-toolbox.com; www.astra-toolbox.com,"Python scripts for loading, pre-processing and reconstructing the projection data in the way described above are published on GitHub: . They make use of the ASTRA toolbox, which is openly available on () or accessible as a conda package (). ASTRA is currently only fully supported for Windows and Linux. Installing it on Mac OS is possible but in the current state very involved and version-dependent. All reference reconstructions provided have been computed with the Python scripts. Furthermore, while the scripts allow for angular sub-sampling the projections and the reference reconstructions were computed with all projections as mentioned in the subsection “Reconstruction production” above.",2023-09-04
0,Scientific Data,41597,10.1038/s41597-023-02511-6,Automated Construction of a Photocatalysis Dataset for Water-Splitting Applications,22,9,2023,https://github.com/CambridgeMolecularEngineering/chemdataextractor2,"ChemDataExtractor 2.2 is available at , and the automatically generated dependency parser, and the files used to specify the knowledge representation have been made open source and are available on Figshare.",2023-09-22
0,Scientific Data,41597,10.1038/s41597-023-02540-1,: Cell cycle phenotype annotations of label-free time-lapse imaging data from cultured human cells,4,10,2023,,"[{'sc': ['lements', 'iewer', 'uto', 'cale', 'iji', 'mage'], 'ext-link': [{'@xlink:href': 'http://www.microscope.healthcare.nikon.com', '@ext-link-type': 'uri', '#text': 'www.microscope.healthcare.nikon.com'}, {'@xlink:href': 'https://fiji.sc', '@ext-link-type': 'uri', '#text': 'https://fiji.sc'}, {'@xlink:href': 'https://imagej.nih.gov/ij', '@ext-link-type': 'uri', '#text': 'https://imagej.nih.gov/ij'}], 'italic': 'ALFI', 'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR22', '#text': '22'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR23', '#text': '23'}}], '#text': 'The NIS-E V software (v. 4.11.0, ) was used to automatically equalize the videos (AS LUTs tool) and extract individual frames (see  dataset), that were converted to the PNG format using the F () distribution of IJ ().'}, {'sc': ['ilastik', 'ixel', 'lassification'], 'ext-link': {'@xlink:href': 'http://www.ilastik.org', '@ext-link-type': 'uri', '#text': 'www.ilastik.org'}, 'italic': ['ALFI', 'Task 1', 'mitosis', 'interphase', 'background'], 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR26', '#text': '26'}}, '#text': 'The  tool (v. 1.3.3, ) was used to produce initial semantic segmentations of interphase nuclei and mitotic cells for sequences MI01-MI08 (see  annotations for ) using the P C workflow. The workflow performed the following steps: (1) load the selected input image sequence; (2) define three user-driven classes: , , ; (3) define pixel filters, based on color, intensity, and texture, as image features at different scales; (4) learn examples for each class interactively by user-defined brushstrokes with classes colors on one or more selected input images; (5) classify image pixels for the whole sequence using output filters, and a Random Forest classifier; (6) correct pixel classification interactively repeating steps 4-5); (7) save the output.'}, {'sc': ['atlab', 'ideo', 'abeler'], 'ext-link': {'@xlink:href': 'http://www.mathworks.com', '@ext-link-type': 'uri', '#text': 'www.mathworks.com'}, 'italic': ['ALFI', 'Task 1', 'ALFI', 'Task 1', 'ALFI', 'Task 1', 'Task 2'], 'monospace': 'regionprops', '#text': 'M (v. R2020a, ) was used to (1) refine the initial masks (see  annotations for ) using the V L App; (2) extract the bounding boxes from the masks (see  annotations for ) using the  function; (3) associate the bounding boxes of the same object in adjacent frames based on motion, estimated by a Kalman filter (see  annotations for  and ); and (4) produce the use examples (see Usage Notes).'}, {'monospace': 'makesense.ai', 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR25', '#text': '25'}}, 'ext-link': {'@xlink:href': 'https://github.com/SkalskiP/make-sense', '@ext-link-type': 'uri', '#text': 'https://github.com/SkalskiP/make-sense'}, '#text': 'The bounding box annotations have been produced using the  online tool ().'}]",2023-10-04
0,Scientific Data,41597,10.1038/s41597-023-02341-6,Database of lower limb kinematics and electromyography during gait-related activities in able-bodied subjects,14,7,2023,https://github.com/Rvs94/MyPredict,The scripts that facilitate re-use of the data can be found in the GitHub repository . These scripts were developed and written in Python 3.9. All required software packages are open-source and available online.,2023-07-14
0,Scientific Data,41597,10.1038/s41597-023-02599-w,Global database of cement production assets and upstream suppliers,13,10,2023,Spatial macrolocalisation model; Cement production types and capacity estimation models; Sentinel-2 cement assets deployment models,The deployment phase code notebooks are available from the GitHub Spatial Finance repository: [1] ; [2] ; [3] .,2023-10-13
0,Scientific Data,41597,10.1038/s41597-023-02570-9,Multi-year belowground data of minirhizotron facilities in Selhausen,3,10,2023,,"[{'italic': 'et al', 'ext-link': {'@xlink:href': '10.34731/pbn7-8g89', '@ext-link-type': 'doi', '#text': 'https://doi.org/10.34731/pbn7-8g89'}, '#text': 'Custom code was used to process the data. For the GPR Data we used MATLAB version: 9.13. 0 (R2022b) to run the codes. The root image processing and soil sensor data is run with Python 3.10.10. Processing codes for the roots images can be found in the Supporting Material for Bauer . at\xa0. The soil water content data measured with the FDR device was processed using R version 4.0.2.'}, 'The custom codes can not be made publicly accessable due to copyright issues, but are available upon request, by contacting the corresponding or senior author.']",2023-10-03
0,Scientific Data,41597,10.1038/s41597-023-02428-0,"A database with frailty, functional and inertial gait metrics for the research of fall causes in older adults",25,8,2023,,"We provide the gait analysis software used to obtain the parameters provided in the register. The Python codes, which are detailed in the Usage Notes section, can be found in the folder named “Gait_analysis_w_Python”. To generate the calibrated signals from the IMU raw measurements, we use a calibration function similar to the one provided in the  code. For a correct functioning, the distribution of folders must remain as organized in Zenodo or the file paths have to be updated to their new locations.",2023-08-25
0,Scientific Data,41597,10.1038/s41597-023-02457-9,A large public dataset of annotated clinical MRIs and metadata of patients with acute stroke,22,8,2023,,No custom code was used.,2023-08-22
0,Scientific Data,41597,10.1038/s41597-023-02258-0,The Translational Data Catalog - discoverable biomedical datasets,20,7,2023,https://datacatalog.elixir-luxembourg.org/,"[{'ext-link': {'@xlink:href': 'https://github.com/FAIRplus/imi-data-catalogue', '@ext-link-type': 'uri', '#text': 'https://github.com/FAIRplus/imi-data-catalogue'}, '#text': 'All Data Catalog code is available in a dedicated repository of the FAIRplus Github organisation, at . The repository includes full documentation on how to deploy a stand-alone version of the Data Catalog.'}, {'ext-link': {'@xlink:href': 'https://github.com/datatagsuite/schema', '@ext-link-type': 'uri', '#text': 'https://github.com/datatagsuite/schema'}, '#text': 'The DATS model is available on Github at .'}]",2023-07-20
0,Scientific Data,41597,10.1038/s41597-023-02372-z,Multi-campaign ship and aircraft observations of marine cloud condensation nuclei and droplet concentrations,20,7,2023,,"The python code is available for transparency and use through Dryad (Data Citation 1); however, due to the large number of data files needed for input and the need to account for variable file formats there are many custom aspects to the code that are included for specific datasets making the code complicated. The coding environment dependencies and versions are included in a .yml file.",2023-07-20
0,Scientific Data,41597,10.1038/s41597-023-02429-z,Structure prediction of novel isoforms from uveal melanoma by AlphaFold,4,8,2023,https://github.com/epi2me-labs/pychopper; https://github.com/nanoporetech/pinfish; https://github.com/BrooksLabUCSC/flair; https://github.com/ZhangNestor/magic,"Here we list the details of the software used for data analysis. Pychopper (), version 2, was used to identify, orient and trim FL Nanopore cDNA reads. LoRDEC, version 1.4.1, was used for correcting errors of long-read RNA-seq data based on short-read RNA-seq data. Pinfish (), version 0.1.0, which is a collection of tools helping to make sense of long-read RNA-seq data. Flair (), version 1.5.0, was used for isoform definition with long-read RNA-seq data. Cuffcompare, version 2.2.1, was used to identify novel isoform based on gene annotation information. Samtools, version 1.9, was used to extract sequence according to the coordinates of novel isoforms. ORFfinder, version 0.4.3, was used to predict ORFs based on nucleotide sequences. AlphaFold Multimer (an extension of AlphaFold2, version 2.2.0) was used to predict the 3D structures of novel isoforms. Then, we also use some in-house scripts to filter and prepare the input and output files, which have been deposited in github ().",2023-08-04
0,Scientific Data,41597,10.1038/s41597-023-02487-3,Aci-bench: a Novel Ambient Clinical Intelligence Dataset for Benchmarking Automatic Visit Note Generation,6,9,2023,https://github.com/wyim/aci-bench,"All code used to run data statistics, baseline models, and evaluation to analyze the  corpus is freely available at .",2023-09-06
0,Scientific Data,41597,10.1038/s41597-023-02514-3,Ontology for the Avida digital evolution platform,9,9,2023,https://graphdb.fortunalab.org,"[{'ext-link': [{'@xlink:href': 'https://gitlab.com/fortunalab/ontoavida', '@ext-link-type': 'uri', '#text': 'https://gitlab.com/fortunalab/ontoavida'}, {'@xlink:href': 'https://gitlab.com/fortunalab/ontoavida/-/blob/master/ontoavida.obo', '@ext-link-type': 'uri', '#text': 'https://gitlab.com/fortunalab/ontoavida/-/blob/master/ontoavida.obo'}, {'@xlink:href': 'https://gitlab.com/fortunalab/ontoavida/-/blob/master/ontoavida.owl', '@ext-link-type': 'uri', '#text': 'https://gitlab.com/fortunalab/ontoavida/-/blob/master/ontoavida.owl'}, {'@xlink:href': 'http://www.obofoundry.org/ontology/ontoavida.html', '@ext-link-type': 'uri', '#text': 'http://www.obofoundry.org/ontology/ontoavida.html'}, {'@xlink:href': 'http://creativecommons.org/licenses/by/4.0/', '@ext-link-type': 'uri', '#text': 'http://creativecommons.org/licenses/by/4.0/'}], '#text': 'The ontology is available in both OBO and OWL format from the GitLab repository () and can be found at  and . OntoAvida OBO and OWL files are also available from the OBO Foundry (). All files are available under the Creative Commons Attribution 4.0 International License () which allows for the copying, redistribution and adaption of the ontology for any purpose.'}, {'ext-link': [{'@xlink:href': 'https://github.com/rdflib/pyLODE', '@ext-link-type': 'uri', '#text': 'https://github.com/rdflib/pyLODE'}, {'@xlink:href': 'https://gitlab.com/fortunalab/pyLODE', '@ext-link-type': 'uri', '#text': 'https://gitlab.com/fortunalab/pyLODE'}, {'@xlink:href': 'https://owl.fortunalab.org/ontoavida', '@ext-link-type': 'uri', '#text': 'https://owl.fortunalab.org/ontoavida'}], 'xref': {'@rid': 'Fig4', '@ref-type': 'fig', '#text': '4'}, '#text': 'pyLODE () was used to obtain a user-friendly visualization of the ontology. pyLODE is based on the OWL Documentation Environment tool (LODE), implemented in Python, and used to generate human-readable HTML documents for OWL and RDF ontologies. We have customized the original pyLODE templates () to convert a scheme of OntoAvida, in a HTML file so that its classes, object properties, and datatype properties can be easily visualized (Fig.\xa0). The pyLODE file of OntoAvida is available at .'}]",2023-09-09
0,Scientific Data,41597,10.1038/s41597-023-02315-8,HeiPorSPECTRAL - the Heidelberg Porcine HyperSPECTRAL Imaging Dataset of 20 Physiological Organs,24,6,2023,https://github.com/MIC-Surgery-Heidelberg; https://github.com/IMSY-DKFZ/htc,Data acquisition was performed with the TIVITA® Suite (version 1.6.0.1. The polygon annotations were created with a software developed in-house which is available on GitHub: . All data analyses and visualizations in this manuscript were performed using Python and the corresponding code is available at .,2023-06-24
0,Scientific Data,41597,10.1038/s41597-023-02373-y,Single-nucleus chromatin landscapes during zebrafish early embryogenesis,19,7,2023,https://figshare.com/articles/dataset/Code/22121171,The codes used to analyze the data in this study were available online ().,2023-07-19
0,Scientific Data,41597,10.1038/s41597-023-02230-y,Signing data citations enables data verification and citation persistence,27,6,2023,https://github.com/bio-guoda/preston; https://doi.org/10.5281/zenodo.1410543; https://doi.org/10.5281/zenodo.7005141,The source code for the Preston software is available in GitHub at  and in Zenodo at . Preston version 4.4 has been assigned the . The MD5 content signature for the zip archive of the source code is .,2023-06-27
0,Scientific Data,41597,10.1038/s41597-023-02544-x,Points for energy renovation (PointER): A point cloud dataset of a million buildings linked to energy features,20,9,2023,https://github.com/kdmayer/PointER,"The code used for generating building point clouds is available at . The repository includes a detailed description of software and python packages used, as well as their versions.",2023-09-20
0,Scientific Data,41597,10.1038/s41597-023-02573-6,A twenty-year dataset of high-resolution maize distribution in China,26,9,2023,https://github.com/Pengqy97/TWDTW_codes,"The classification of maize for each province in this study was performed on the local computer. The codes used is written in Python, Fortran, and Julia which are available from .",2023-09-26
0,Scientific Data,41597,10.1038/s41597-023-02489-1,A pseudoproxy emulation of the PAGES 2k database using a hierarchy of proxy system models,14,9,2023,https://doi.org/10.5281/zenodo.7652533; https://github.com/fzhu2e/paper-pseudoPAGES2k,The Jupyter notebooks illustrating the usage of the pseudoPAGES2k dataset can be accessed at  or .,2023-09-14
0,Scientific Data,41597,10.1038/s41597-023-02601-5,Chromosome-level genome assembly of an important wolfberry fruit fly ( Becker),4,10,2023,,"The data analyses were performed according to the manuals and protocols by the developers of corresponding bioinformatics tools and all software, and codes used in this work are publicly available, with corresponding versions indicated in Methods.",2023-10-04
0,Scientific Data,41597,10.1038/s41597-023-02630-0,A dataset of skin lesion images collected in Argentina for the evaluation of AI tools in this population,18,10,2023,https://github.com/piashiba/HIBASkinLesionsDataset,"Python scripts for exploratory data analysis and dataset comparison, as well as supplementary data, are publicly available at .",2023-10-18
0,Scientific Data,41597,10.1038/s41597-023-02375-w,KINECAL: A Dataset for Falls-Risk Assessment and Balance Impairment Analysis,18,9,2023,,"The code used to process the files, along with some example scripts are available from a physionet repository.",2023-09-18
0,Scientific Data,41597,10.1038/s41597-023-02290-0,Ensemble of CMIP6 derived reference and potential evapotranspiration with radiative and advective components,27,6,2023,https://github.com/nelsbjarke/PET,"The Python scripts used to calculate ET, ET, ET components, and VPD can be found within the repository alongside the data described herein. Accompanying the code is a small subset of GCM data that can be used to test run the script. Python scripts utilize a small subset of libraries in the Python3 base and the xarray (v2022.11.0) library to handle calculations of the gridded datasets. The python code used to generate the dataset described above can be found in the GitHub repository using the following link: .",2023-06-27
0,Scientific Data,41597,10.1038/s41597-023-02318-5,Helicopter-borne RGB orthomosaics and photogrammetric digital elevation models from the MOSAiC Expedition,3,7,2023,https://gitlab.com/mosaic12/orthomosaics,All processing developed within this study is wrapped in a python environment and is available at . For calculating image footprint locations we used the  python package. Note that part of the code is based on the commercial Agisoft Metashape software requiring licensing.,2023-07-03
0,Scientific Data,41597,10.1038/s41597-023-02432-4,"FracAtlas: A Dataset for Fracture Classification, Localization and Segmentation of Musculoskeletal Radiographs",5,8,2023,https://doi.org/10.6084/m9.figshare.22363012; https://github.com/XLR8-07/FracAtlas,"The conversion of DICOM to JPEG image format was done using proprietary software of the X-ray machines from brands like Fujifilm and Philips hence they could not be made available. The mask annotations for segmentation were done using an open-source web tool named makedsense.ai. It was also used for generating VGG annotations from COCO format. As explained in the Methods section, the annotation conversion procedures from COCO to YOLO and YOLO to PASCAL VOC were performed using Python 3.10.1 on a Windows 11 operating system using ‘coco2yolo.ipynb’ and ‘yolo2voc.ipynb’. Both the Jupyter notebooks can be found inside the ‘Utility’ folder along with the dataset at Figshare (). The code used for technical validation can be accessed from (). There are 2 notebooks inside ‘notebooks’ under the root folder called ‘Train_8s.ipynb’ and ‘Prediction_8s.ipynb’. The ‘Train_8s.ipynb’ is used to train 2 models of ‘YOLO8s_seg’ and ‘YOLO8s’ variants targeted toward segmentation and localization tasks respectively. ‘Prediction_8s.ipynb’ is used to generate predictions out of the 2 aforementioned models and view the results.",2023-08-05
0,Scientific Data,41597,10.1038/s41597-023-02348-z,Single cell transcriptome sequencing of stimulated and frozen human peripheral blood mononuclear cells,6,7,2023,https://github.com/erbon7/sc_pbmc,"All custom R and python scripts for quality control, data integration, figures and analysis are available on our GitHub repository ().",2023-07-06
0,Scientific Data,41597,10.1038/s41597-023-02404-8,Annotated dataset for deep-learning-based bacterial colony detection,28,7,2023,,"As mentioned above, the correct position of the annotations was verified by drawing the corresponding bounding boxes on the images using Detectron2. The Python script used for this is in the file bbox_placement_test.py. The input annotation file for this run is a COCO JSON one. This was also generated from the tab-delimited annotation file using a Python script provided in TSV_to_COCO.py. Both script files are available in the Figshare repository.",2023-07-28
0,Scientific Data,41597,10.1038/s41597-023-02433-3,ThoughtSource: A central hub for large language model reasoning data,8,8,2023,https://github.com/OpenBioLink/ThoughtSource; https://doi.org/10.5281/zenodo.8199390; https://doi.org/10.5281/zenodo.8199538,"All code, data and tools are openly available at , a snapshot of the GitHub repository is archived at , and a snapshot of dataset contents is archived at . Our code and data are licensed under an MIT license, while data adapted from existing datasets are available under the licenses of their respective sources.",2023-08-08
0,Scientific Data,41597,10.1038/s41597-023-02519-y,TIHM: An open dataset for remote healthcare monitoring in dementia,9,9,2023,https://github.com/PBarnaghi/TIHM-Dataset,"The TIHM dataset is available in the corresponding Zenodo repository and consists of five separate tables (Activity, Sleep, Physiology, Labels, and Demographics). For further information on the data records, please refer to the README file. The code for the experiments presented in the manuscript is available on the Github repository (). The libraries and their versions and dependencies that are used in the code are also provided as a separate configuration file in JSON/YAML format.",2023-09-09
0,Scientific Data,41597,10.1038/s41597-023-02405-7,ReCANVo: A database of real-world communicative and affective nonverbal vocalizations,5,8,2023,https://doi.org/10.5281/zenodo.5786859,We used the Python programming language for the data processing described above. Volume segmentation was implemented using the  libary. The label assignment algorithm is summarized in Fig. . The code is available as part of our dataset in Zenodo: .,2023-08-05
0,Scientific Data,41597,10.1038/s41597-023-02434-2,High-quality wild barley genome assemblies and annotation with Nanopore long reads and Hi-C sequencing data,10,8,2023,,No specific code or script was used in this work. All commands used in the processing were executed according to the manual and protocols of the corresponding bioinformatics software.,2023-08-10
0,Scientific Data,41597,10.1038/s41597-023-02463-x,Making Biomedical Research Software FAIR: Actionable Step-by-step Guidelines with a User-support Tool,23,8,2023,https://github.com/fairdataihub/FAIRshare; https://github.com/fairdataihub/FAIRshare-Docs,"The code associated with this manuscript consists of a “main.ipynb” Jupyter notebook, the source code of FAIRshare, and the source code of the FAIRshare documentation. The “main.ipynb” Jupyter notebook contains the code used to analyze the findings from the review and to conduct other analyses presented in this manuscript (e.g., generate Fig. ). This notebook is available in a GitHub repository called “Code” (also maintained in the FAIR-BioRS GitHub organization). The dataset associated with this notebook was made FAIR according to the FAIR-BioRS guidelines using FAIRshare v2.1.0, and shared under the permissible MIT license. The latest version associated with this manuscript (v3.0.0) is archived on Zenodo. The source code for FAIRshare is hosted on GitHub (). The current version of FAIRshare (v2.1.0) discussed in this manuscript was made FAIR using FAIRshare itself, and shared on Zenodo under the permissible MIT license. The source code for the FAIRshare documentation is maintained on GitHub as well () and the current version (5.0.0) was shared under the permissible MIT license on Zenodo.",2023-08-23
0,Scientific Data,41597,10.1038/s41597-023-02578-1,A Global Feature-Rich Network Dataset of Cities and Dashboard for Comprehensive Urban Analyses,30,9,2023,https://github.com/winstonyym/urbanity; https://github.com/winstonyym/urbdash,Urbanity Python package source code is hosted under an open source MIT license (). Urbanity dashboard is generated with Dash version 2.7.1. with open source code ().,2023-09-30
0,Scientific Data,41597,10.1038/s41597-023-02321-w,A curated gene and biological system annotation of adverse outcome pathways related to human health,24,6,2023,https://doi.org/10.5281/zenodo.7980953,Custom code and data used in the NLP-based prioritisation of the gene set annotations is available in the data repository on Zenodo at  (file aop_mapping_nlp.tar.gz).,2023-06-24
0,Scientific Data,41597,10.1038/s41597-023-02465-9,AIMD-Chig: Exploring the conformational space of a 166-atom protein  with  molecular dynamics,22,8,2023,,We employed ORCA 4.2.1 to run  MD simulations and perform calculation accuracy comparisons between PM3 and HF approaches as well as DFTB + 22.2 for DFTB approach. We used the Amber20 sander to run REMD simulations and perform calculation accuracy comparisons on MM. We also employed Amber20 pmemd.cuda for conventional MD simulations. We used mdtraj 1.9.7 and MSMBuilder 3.8.0 for trajectory analysis and anchor selection. We applied pytorch 1.13 and torch-geometric 2.0.4 for the training of VisNet. The time course and distribution analysis were drawn by seaborn 0.11.2. The free energy surfaces were generated via MATLAB R2019a.,2023-08-22
0,Scientific Data,41597,10.1038/s41597-023-02494-4,SECURES-Met: A European meteorological data set suitable for electricity modelling applications,7,9,2023,https://doi.org/10.5281/zenodo.8108927,The data are stored as ASCII text (csv) and no specific software is necessary to access the data. The production of the data was done with Python. These scripts are mainly data manipulation routines and do not contribute in processing the data further. To assure repeatability all scripts are available at Zenodo ().,2023-09-07
0,Scientific Data,41597,10.1038/s41597-023-02322-9,An open dataset for intelligent recognition and classification of abnormal condition in longwall mining,27,6,2023,https://github.com/heartexlabs/labelImg; https://github.com/jozhang97/deta; https://github.com/czczup/vit-adapter; https://github.com/wongkinyiu/yolov7; https://github.com/RapidAI/YOLO2COCO,"DsLMF+ datasets are publicly available at the figshare data repository, and the code for automatically filtering is also published alongside the dataset, archived as “DsLMF.7z”. Furthermore, the annotation tool Labelimg can be accessed and downloaded through the official website link , the specific usage can refer to the corresponding README file. The codes used for training and validation of the DsLMF+ datasets in this work adopts DETA, ViT-Adapter-L and YOLOv7 official published open source scripts, and the code of the above three deep learning network for dataset verification can be accessed via the following website link (), (), and (). Table  presents the required site-packages and their corresponding versions for the above three different networks. The software packages can be downloaded according to README files under the corresponding links on different networks, and can be installed with the python package installer (pip). Researchers can complete the label format conversion from YOLO format to COCO format, by visiting the following link (), the link provides the label format conversion code and the README file that can be used as a reference.",2023-06-27
0,Scientific Data,41597,10.1038/s41597-023-02238-4,An Observation-Based Dataset of Global Sub-Daily Precipitation Indices (GSDR-I),22,6,2023,https://doi.org/10.5281/zenodo.7492877,The Python code for indices calculation based on gauge records and subsequent gridding is available in the code repository here: .,2023-06-22
0,Scientific Data,41597,10.1038/s41597-023-02267-z,Genome-resolved carbon processing potential of tropical peat microbiomes from an oil palm plantation,8,6,2023,,Open-source software packages were used to process data and generate data products. Software versions and non-default parameters are specified where required.,2023-06-08
0,Scientific Data,41597,10.1038/s41597-023-02153-8,", a comprehensive electrocardiographic feature dataset",13,5,2023,,The ECG features directly correspond to the outputs of the respective algorithms up to minor harmonization. We provide code to apply the predefined SNOMED CT mappings to the labels in the dataset ( released as part of the data repository). Links to code samples facilitating the usage of the dataset are described under Usage Notes and are released in a dedicated code repository.,2023-05-13
0,Scientific Data,41597,10.1038/s41597-023-02239-3,Rootstock–scion interaction affects  transcriptome profiles in response to cadmium,23,5,2023,,"['PCA analysis: R prcomp in software (3.1.1).', 'Differential analysis: R package DESeq 2 1.6.3.', {'ext-link': {'@xlink:href': 'https://jvenn.toulouse.inrae.fr/app/example.html', '@ext-link-type': 'uri', '#text': 'https://jvenn.toulouse.inrae.fr/app/example.html'}, '#text': 'Veen: .'}]",2023-05-23
0,Scientific Data,41597,10.1038/s41597-023-01984-9,SARS-CoV2 billion-compound docking,28,3,2023,,"[{'ext-link': [{'@xlink:href': 'https://github.com/jvermaas/autodock-gpu', '@ext-link-type': 'uri', '#text': 'https://github.com/jvermaas/autodock-gpu'}, {'@xlink:href': 'https://github.com/ccsb-scripps/AutoDock-GPU', '@ext-link-type': 'uri', '#text': 'https://github.com/ccsb-scripps/AutoDock-GPU'}, {'@xlink:href': 'https://github.com/ccsb-scripps/AutoDock-Vina', '@ext-link-type': 'uri', '#text': 'https://github.com/ccsb-scripps/AutoDock-Vina'}], 'xref': [{'@rid': 'Fig4', '@ref-type': 'fig', '#text': '4'}, {'@rid': 'Fig4', '@ref-type': 'fig', '#text': '4'}], 'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR15', '#text': '15'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR52', '#text': '52'}}], '#text': 'The primary version of AutoDock used to generate the primary dataset is available from . As noted in Fig.\xa0, this is not recommended for new docking calculations. Instead, new projects should use current versions of AutoDock-GPU such as 1.5.3, available from . To generate the Vina data for Fig.\xa0, we used Vina 1.2.3 from .'}, 'As described in the ‘Data Generation Protocol’ section, several custom software packages were developed and used in this project, including', {'ext-link': {'@xlink:href': 'https://code.ornl.gov/99R/launchad/-/tags/v1.2', '@ext-link-type': 'uri', '#text': 'https://code.ornl.gov/99R/launchad/-/tags/v1.2'}, '#text': '1.'}, {'ext-link': {'@xlink:href': 'https://code.ornl.gov/99R/pymapreduce', '@ext-link-type': 'uri', '#text': 'https://code.ornl.gov/99R/pymapreduce'}, '#text': '2.'}, {'ext-link': {'@xlink:href': 'https://code.ornl.gov/99R/pmake', '@ext-link-type': 'uri', '#text': 'https://code.ornl.gov/99R/pmake'}, 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR45', '#text': '45'}}, '#text': '3.'}, {'ext-link': {'@xlink:href': 'https://github.com/frobnitzem/mpi_list', '@ext-link-type': 'uri', '#text': 'https://github.com/frobnitzem/mpi_list'}, '#text': '4.'}, {'ext-link': {'@xlink:href': 'https://github.com/frobnitzem/sars_docking', '@ext-link-type': 'uri', '#text': 'https://github.com/frobnitzem/sars_docking'}, 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR47', '#text': '47'}}, '#text': '5.'}]",2023-03-28
0,Scientific Data,41597,10.1038/s41597-023-02040-2,EUBUCCO v0.1: European building stock characteristics in a common and open database for 200+ million individual buildings,20,3,2023,https://github.com/ai4up/eubucco/releases/tag/v0.1,All the code used in this study is available on Github as a release: . It is free to re-use and modify with attribution under the .,2023-03-20
0,Scientific Data,41597,10.1038/s41597-023-02269-x,PANGAEA - Data Publisher for Earth & Environmental Science,2,6,2023,https://github.com/pangaea-data-publisher,"The code supporting the users with data retrieval and submission is freely available at . PANGAEA as a repository does not generate, test, or process data and metadata, therefore no custom code has been used.",2023-06-02
0,Scientific Data,41597,10.1038/s41597-023-02127-w,Dataset of theoretical multinary perovskite oxides,28,4,2023,https://www.unf.edu/~michael.lufaso/spuds/; https://github.com/zaba1157/PySPuDS; https://github.com/rymo1354/crystal_motifs,"We used the pymatgen python package, which is open-source software under the Massachusetts Institute of Technology License, for materials analysis as well as the generation of VASP inputs and CIF files. The VASP DFT code used is accessible under a paid license, copyrighted by the University of Vienna, Austria. Initial structures were generated with SPuDS DOS version >2.20.08.06 () using a custom high-throughput python wrapper available on GitHub (). Perovskite/non-perovskite classifications were performed using a custom python package that is available on GitHub () and based on the pymatgen and NetworkX graph network python packages.",2023-04-28
0,Scientific Data,41597,10.1038/s41597-023-02071-9,Late-season corn stalk nitrate measurements across the US Midwest from 2006 to 2018,7,4,2023,,"[{'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR23', '#text': '23'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR24', '#text': '24'}}], '#text': 'The dataset is easy to access by Microsoft Excel or other software like R or Python.'}, {'ext-link': {'@xlink:href': 'https://github.com/AnabelleLaurent/onfant.dataset', '@ext-link-type': 'uri', '#text': 'https://github.com/AnabelleLaurent/onfant.dataset'}, '#text': 'The complete dataset can be obtained from GitHub ().'}]",2023-04-07
0,Scientific Data,41597,10.1038/s41597-023-02128-9,A distributable German clinical corpus containing cardiovascular clinical routine doctor’s letters,14,4,2023,,"For information on software packages and versions used for pre-processing, see methods section. No additional custom code was implemented for CARDIO:DE.",2023-04-14
0,Scientific Data,41597,10.1038/s41597-023-02213-z,FOPPA: an open database of French public procurement award notices from 2010–2020,19,5,2023,https://github.com/CompNet/FoppaInit,"Our Python source code is publicly available online as a GitHub repository (). It is designed to be applied to the raw TED tables, and leverages the Hexaposte and SIRENE databases mentioned in the  section. It performs the integrality of the processing described in this section, and produces the FOPPA database. When performed in parallel on 10 NVIDIA GeForce RTX 2080 Ti GPUs, this processing requires approximately 6 days.",2023-05-19
0,Scientific Data,41597,10.1038/s41597-023-02242-8,How to establish and maintain a multimodal animal research dataset using DataLad,5,6,2023,https://doi.org/10.12751/g-node.3yl5qi,DataLad and GIN are freely available. The manuscript contains all code to reproduce the workflow.,2023-06-05
0,Scientific Data,41597,10.1038/s41597-023-02271-3,A synthetic dataset of Danish residential electricity prosumers,8,6,2023,,The real data used as the input of CTGAN is unavailable due to regulations around consumers’ privacy. Others wishing to repeat the work or perform studies with the raw data should approach Watts A/S. The code for data validation and analysis is available in the public repository of Figshare.,2023-06-08
0,Scientific Data,41597,10.1038/s41597-023-02043-z,Comprehensive exploration of graphically defined reaction spaces,20,3,2023,,"The version of YARP used to generate the data is freely available through GitHub under the GNU GPL-3.0 License. The code used to load the reaction dataset, parse the reaction features and DFT energies, and reproduce Figs. ,  is also freely available. Further details on how to use these scripts are given in the Usage Notes.",2023-03-20
0,Scientific Data,41597,10.1038/s41597-023-02214-y,Responses of pyramidal cell somata and apical dendrites in mouse visual cortex over multiple days,17,5,2023,https://github.com/colleenjg/OpenScope_CA_Analysis; https://github.com/AllenInstitute/AllenSDK; https://github.com/colleenjg/cred_assign_stimuli; https://github.com/schnitzer-lab/EXTRACT-public; http://www.mackenziemathislab.org/deeplabcut; https://github.com/AllenInstitute/ophys_nway_matching; https://github.com/jeromelecoq/QC_2P/blob/master/Example%20use%20of%20QC_2P.ipynb,"Data pre-processing was performed in Python 3.6 with custom scripts that are freely available on GitHub () and were developed using the following packages: NumPy, SciPy, Pandas, Matplotlib, Scikit-learn 0.21.1, and the AllenSDK 1.6.0. (). Stimuli were generated by Python 2.7 custom scripts based on PsychoPy 1.82.01 and CamStim 0.2.4. The code is freely available (along with instructions to reproduce the stimuli, and example videos) on GitHub (). Dendritic segmentation was run in Matlab 2019a using a robust estimation algorithm (). Pupil tracking was performed using DeepLabCut 2.0.5 (). ROIs were matched across sessions using a custom-modified version of the n-way cell matching package developed by the Allen Institute (). Code for estimating photon conversion statistics on the raw imaging stacks is available on GitHub ().",2023-05-17
0,Scientific Data,41597,10.1038/s41597-023-02272-2,A physiological signal database of children with different special needs for stress recognition,14,6,2023,https://github.com/hiddenslate/aktives-dataset-2022,"The codes include preprocessing of physiological signals, annotation synchronization, facial expression detection, and technical validation available at the Repository for the AKTIVES Dataset 2022 GitHub repository . The Python 3.9 version has been utilized for the development of algorithms. In the requirements.txt file, all necessary packages are mentioned. Uploaded codes can be helpful guidelines to preprocess and analyze the AKTIVES dataset.",2023-06-14
0,Scientific Data,41597,10.1038/s41597-023-02073-7,Cholec80-CVS: An open dataset with an evaluation of Strasberg’s critical view of safety for AI,8,4,2023,https://github.com/ManuelRios18/CHOLEC80-CVS-PUBLIC,"We provide scripts to transform our annotations to the frame-wise labels and also the source code of some baseline models that use standard deep learning techniques to detect CVS criteria using our database for interested users. All these scripts were coded using Python 3.8.11 and Pytorch as the machine learning framework. All scripts were tested on Linux Machines. The repository README file contains detailed instructions to ease the use of the repository and brief descriptions of all files. The code is publicly available at , licensed under MIT OpenSource license. Therefore, permission is granted free of charge to copy and use this software and its associated files.",2023-04-08
0,Scientific Data,41597,10.1038/s41597-023-02100-7,VinDr-Mammo: A large-scale benchmark dataset for computer-aided diagnosis in full-field digital mammography,12,5,2023,https://www.python.org/; https://pydicom.github.io/; https://docs.python.org/3/library/hashlib.html; https://github.com/vinbigdata-medical/vindr-mammo,The codes used in this study were made publicly available. The scripts used for loading and processing DICOM images are based on the following open-source repositories: Python 3.8.0 (); Pydicom 1.2.0 (); and Python hashlib (). The code for data pseudonymization and stratification was made publicly available at .,2023-05-12
0,Scientific Data,41597,10.1038/s41597-023-02101-6,Unified access to up-to-date residue-level annotations from UniProtKB and other biological databases for PDB data,12,4,2023,https://colab.research.google.com/github/PDBe-KB/sifts_data_analysis/blob/main/sifts.ipynb; https://github.com/PDBe-KB/sifts_data_analysis,"To assist users in utilising the updated PDBx/mmCIF files and SIFTS annotations, a Google Colab notebook is available at  or via GitHub at . This notebook provides information on how to parse, extract and filter SIFTS annotations from the updated PDBx/mmCIF files. Additionally, the notebook demonstrates how users can compare various numbering schemes of a given residue across different PDB structures of the same protein.",2023-04-12
0,Scientific Data,41597,10.1038/s41597-023-02017-1,A multi-sensor dataset with annotated activities of daily living recorded in a residential setting,23,3,2023,https://github.com/IRC-SPHERE/sphere-challenge-sdata/,"A  repository is publicly available at . In this repository a number of scripts for visualisation, bench marking and data processing are available. (All subsequent sensor images were generated using these scripts).",2023-03-23
0,Scientific Data,41597,10.1038/s41597-023-02046-w,High-resolution calibrated and validated Synthetic Aperture Radar Ocean surface wind data around Australia,23,3,2023,https://github.com/aodn/imos-user-code-library/blob/master/Python/notebooks/SAR_winds/SAR_winds_getting_started_jupyter_notebook/ausar_winds_getting_started_notebook.ipynb,A Python Jupyter notebook for getting started with reading the data and comparing them with other reanalyses datasets at matching times (as outlined in the Usage Notes Section) is available at the AODN GitHub repository ().,2023-03-23
0,Scientific Data,41597,10.1038/s41597-023-02075-5,"GazeBaseVR, a large-scale, longitudinal, binocular eye-tracking dataset collected in virtual reality",30,3,2023,,"During data collection, raw CSV files were generated from the data stream accessed with SMI’s provided ET API within Unity. These raw CSV files were later converted to the format described in Table  using custom Python code. The code used to convert the raw files to the final format, along with the code used to generate Figs. – and the data for Tables , , is available on figshare. This code was developed using Python 3.7.11 with the following main packages: numpy 1.21.6, pandas 1.3.5, openpyxl 3.0.9, and matplotlib 3.2.2. This repository also contains other supplementary material including a manual for the ET-HMD, a pamphlet with manufacturer-provided technical specifications for the ET-HMD, and the video clips used for the VID task.",2023-03-30
0,Scientific Data,41597,10.1038/s41597-023-02102-5,"PediCXR: An open, large-scale chest radiograph dataset for interpretation of common thoracic diseases in children",27,4,2023,https://www.python.org/; https://pydicom.github.io/; https://pypi.org/project/opencv-python/; https://docs.python.org/3/library/hashlib.html; https://github.com/vinbigdata-medical/vindr-cxr; https://github.com/vinbigdata-medical/DICOM-Imaging-Router; https://vindr.ai/vindr-lab,This study used the following open-source repositories to load and process DICOM scans: Python 3.7.0 (); Pydicom 1.2.0 (); OpenCV-Python 4.2.0.34 (); and Python hashlib (). The code for data de-identification was made publicly available at . The code to train CNN classifier for the out-of-distribution task was made publicly available at . The VinDr Lab is an open source software and can be found at .,2023-04-27
0,Scientific Data,41597,10.1038/s41597-023-02160-9,A dataset of direct observations of sea ice drift and waves in ice,3,5,2023,,"[{'ext-link': [{'@xlink:href': 'https://github.com/jerabaul29/LoggerWavesInIce_InSituWithIridium', '@ext-link-type': 'uri', '#text': 'https://github.com/jerabaul29/LoggerWavesInIce_InSituWithIridium'}, {'@xlink:href': 'https://github.com/jerabaul29/OpenMetBuoy-v2021a', '@ext-link-type': 'uri', '#text': 'https://github.com/jerabaul29/OpenMetBuoy-v2021a'}, {'@xlink:href': 'https://github.com/jerabaul29/data_release_sea_ice_drift_waves_in_ice_marginal_ice_zone_2022', '@ext-link-type': 'uri', '#text': 'https://github.com/jerabaul29/data_release_sea_ice_drift_waves_in_ice_marginal_ice_zone_2022'}, {'@xlink:href': '10.21343/AZKY-0X44', '@ext-link-type': 'doi', '#text': 'https://doi.org/10.21343/AZKY-0X44'}], '#text': 'The firmwares of the instruments v2018 and v2021, as well as the binary data decoder scripts, are fully available on the corresponding github repositories: , . The scripts to plot the data from the netCDF files are available on the main Github repository: , together with the raw data. All code is developed in modern python (version 3.8 or higher), or C++, or Matlab, unless specified otherwise. The netCDF datafiles are following the netCDF4 standard, with CF attributes conventions. We also provide a mirror of the netCDF data files on the THREDDS server of the Norwegian Meteorological Institute in the context of the Arctic Data Center repository, at the following address: .'}, {'ext-link': {'@xlink:href': 'https://github.com/jerabaul29/data_release_sea_ice_drift_waves_in_ice_marginal_ice_zone_2022', '@ext-link-type': 'uri', '#text': 'https://github.com/jerabaul29/data_release_sea_ice_drift_waves_in_ice_marginal_ice_zone_2022'}, '#text': 'We will offer reasonable support regarding the data and its use through the Issues tracker of the data repository at , and we invite readers in need of specific help to contact us there. In addition, we plan on releasing extensions to this dataset periodically as more data are collected. We invite scientists who own similar data and are willing to release these as open source materials to contact us so that they can get involved in the next data release we will perform.'}, {'sup': [{'xref': [{'@ref-type': 'bibr', '@rid': 'CR75', '#text': '75'}, {'@ref-type': 'bibr', '@rid': 'CR76', '#text': '76'}, {'@ref-type': 'bibr', '@rid': 'CR77', '#text': '77'}], '#text': ', –'}, {'xref': [{'@ref-type': 'bibr', '@rid': 'CR78', '#text': '78'}, {'@ref-type': 'bibr', '@rid': 'CR79', '#text': '79'}], '#text': ','}], 'ext-link': {'@xlink:href': 'https://github.com/jerabaul29/meta_overview_sea_ice_available_data', '@ext-link-type': 'uri', '#text': 'https://github.com/jerabaul29/meta_overview_sea_ice_available_data'}, 'italic': 'in situ', '#text': 'In addition, we discovered, in the context of the present work, that there are already some openly available data about sea ice drift and waves in ice (for example,, though there may be more such data available that we do not know of); however, these are scattered across the internet, and may be difficult to find. Therefore, in addition to the data release intrinsic to this dataset, we have started to maintain an index of similar open data at . We invite the reader aware of additional open datasets to notify us so that these can be added to our index, which we will keep extending in the future. We hope that these data, together with a variety of datasets that have been recently gathered, will be a significant contribution towards building large, well sampled datasets of  observations of the MIZ and sea ice dynamics.'}]",2023-05-03
0,Scientific Data,41597,10.1038/s41597-023-02076-4,A collection of read depth profiles at structural variant breakpoints,6,4,2023,https://github.com/latur/SWaveform,"A software suite accompanying the resource is available on . The repository contains scripts for a) database and GUI deployment on the SQLite platform and b) a toolkit for DOC profile and SV data processing and management. The toolkit contains scripts for generation of DOC profiles corresponding to breakpoint loci from alignment files (SAM, BAM or CRAM format) and annotated VCF files, as well as DOC profile conversion into BCOV format. In addition, we provide tools for profile clustering, motif discovery and a script for subsequent motif detection in DOC profiles.",2023-04-06
0,Scientific Data,41597,10.1038/s41597-023-02103-4,An open database of computed bulk ternary transition metal dichalcogenides,30,5,2023,,Scripts used to collate the ASE database and perform structural analyses are located alongside the dataset. All scripts are written in Python (v3.9) and are described in the accompanying README.md.,2023-05-30
0,Scientific Data,41597,10.1038/s41597-023-02247-3,Chromosome-level genome assembly and annotation of the prickly nightshade  Dunal,1,6,2023,,"No specific custom codes were developed in this study. All commands and pipelines used for data analyses were conducted according to the manuals or protocols provided by the corresponding software development team, which are described in detail in the Methods section. Default parameters were employed if no detailed parameters were mentioned for the software used in this study. Supplementary Table  lists the versions, settings, and parameters of the relevant software used in this study.",2023-06-01
0,Scientific Data,41597,10.1038/s41597-023-02048-8,A dataset of rodent cerebrovasculature from  multiphoton fluorescence microscopy imaging,17,3,2023,https://github.com/ctpn/minivess,"We provide the Python code to separate multichannel and time series 2PFM image volumes into single volumes, which are easier to manipulate. Multichannel XY, XYZ, XYT, and XYZT images are supported. For multichannel images, the user will be asked to select the channel of interest to export. For images with multi-T volumes (XYT and XYZT), the user has the option of exporting each T-stack separately, or as a single file. We also provide sample code for the image pre-processing tools described above. All code can be accessed at the MiniVess Github repository .",2023-03-17
0,Scientific Data,41597,10.1038/s41597-023-02104-3,Network Analysis of Academic Medical Center Websites in the United States,28,4,2023,https://github.com/davidchen0420/Academic-Medical-Center-Topology,"The code used to calculate the node-specific metrics, network-wide metrics, as well as static and interactive visualizations of each of the 40 AMC websites can be found at . The Jupyter notebook AMC_Topology_Metrics.ipynb describes the steps used to calculate the metrics as comments. To run the Jupyter notebook, installation of the Anaconda distribution of Python 3.8.0+ and required scientific packages listed in the notebook is needed. Example input data and expected output results are provided in example_data.zip in the GitHub. The example input data is a subset of 3 AMC website nodes and internal edges that can also be found in the Figshare repository (see Data Records).",2023-04-28
0,Scientific Data,41597,10.1038/s41597-023-02277-x,Identifying and sharing per-and polyfluoroalkyl substances hot-spot areas and exposures in drinking water,16,6,2023,,"['The MIAGIS Python package is available on GitHub and the Python Package Index with comprehensive user documentation on GitHub.io:', {'ext-link': {'@xlink:href': 'https://github.com/MoseleyBioinformaticsLab/miagis', '@ext-link-type': 'uri', '#text': 'https://github.com/MoseleyBioinformaticsLab/miagis'}}, {'ext-link': {'@xlink:href': 'https://pypi.org/project/miagis/', '@ext-link-type': 'uri', '#text': 'https://pypi.org/project/miagis/'}}, {'ext-link': {'@xlink:href': 'https://moseleybioinformaticslab.github.io/miagis/', '@ext-link-type': 'uri', '#text': 'https://moseleybioinformaticslab.github.io/miagis/'}}]",2023-06-16
0,Scientific Data,41597,10.1038/s41597-023-02049-7,Characterizing uncertainty in Community Land Model version 5 hydrological applications in the United States,6,4,2023,https://doi.org/10.57931/1922953; https://doi.org/10.5281/zenodo.6653704; https://github.com/ESCOMP/CTSM/tree/branch_tags/PPE.n11_ctsm5.1.dev030; https://doi.org/10.5281/zenodo.7039118; https://github.com/UW-Hydro/MetSim,"The CLM5 hydrological datasets are available to the public at  in comma-separated value (.csv) and netcdf (.nc) formats. This experiment used a modified version of CLM5 designed to allow easier parameterization and support machine-specific compilation. The modified source code is available at , forked from . Source codes that were used to develop and analyze the data are available at . The MetSim disaggregation code is available at .",2023-04-06
0,Scientific Data,41597,10.1038/s41597-023-02105-2,A new kinematic dataset of lower limbs action for balance testing,14,4,2023,,"['The data processing procedures that we perform on the dataset are those mentioned above.', {'ext-link': {'@xlink:href': '10.6084/m9.figshare.20579541.v1', '@ext-link-type': 'doi', '#text': 'https://doi.org/10.6084/m9.figshare.20579541.v1'}, 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR27', '#text': '27'}}, '#text': '1. The Python code used to extract the frame number of CSV files to be found at \xa0(Csv and Bvh are the raw data and angle is the corresponding parsed data part. The csv file initials have been updated to the participant experiment sequence number.)'}, {'ext-link': {'@xlink:href': 'https://physionet.org/content/kinematic-actors-emotions/2.1.0/', '@ext-link-type': 'uri', '#text': 'https://physionet.org/content/kinematic-actors-emotions/2.1.0/'}, '#text': '2. Web-based download: the BVH files to calculate durations can be found at .'}]",2023-04-14
0,Scientific Data,41597,10.1038/s41597-023-02134-x,OpCitance: Citation contexts identified from the PubMed Central open access articles,28,4,2023,https://doi.org/10.13012/B2IDB-4353270_V2,The code of our XML parser is provided in the Supplementary_File_1.zip on our data repository: .,2023-04-28
0,Scientific Data,41597,10.1038/s41597-023-02163-6,A spatio-temporal dataset on food flows for four West African cities,10,5,2023,,"['Initially, the data were organised in a relational PostgreSQL database with separated tables for metadata, calibration lists, and spatial characteristics. The final dataset was generated in pgAdmin 4 using PostgreSQL 12 including all relevant information from these previously related tables. This included the following steps:', '1. Metadata and spatial characteristics of locations joined to original data, secondary data for Ouagadougou added to the dataset, missing information on Loumbila checkpoint added (lean season 2014); duplicates removed.', '2. Unit volume, product density, and specific product-unit combinations joined to the dataset.', '3. Selected products extracted and livestock quantities adjusted (from number of animals to kg).', '4. Missing quantity computed and adjusted quantities calculated.', '5. Distance calculated and geometry added.', {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR21', '#text': '21'}}, '#text': 'The database structure as well as the auxiliary tables are available on GitHub, however, they are not needed for reusing the data.'}]",2023-05-10
0,Scientific Data,41597,10.1038/s41597-023-01993-8,"Specimen, biological structure, and spatial ontologies in support of a Human Reference Atlas",27,3,2023,,"[{'ext-link': {'@xlink:href': 'https://github.com/hubmapconsortium/ccf-ontology', '@ext-link-type': 'uri', '#text': 'https://github.com/hubmapconsortium/ccf-ontology'}, '#text': 'All data and standard operating procedures are released under Creative Commons Attribution 4.0 International (CC BY 4.0). All code was released under the MIT License and can be accessed in GitHub at .'}, 'ASCT+B APIs:', {'ext-link': {'@xlink:href': 'https://mmpyikxkcp.us-east-2.awsapprunner.com', '@ext-link-type': 'uri', '#text': 'https://mmpyikxkcp.us-east-2.awsapprunner.com'}, '#text': 'API Endpoint (includes interactive documentation):'}, {'ext-link': {'@xlink:href': 'https://hubmapconsortium.github.io/ccf-asct-reporter/docs/api', '@ext-link-type': 'uri', '#text': 'https://hubmapconsortium.github.io/ccf-asct-reporter/docs/api'}, '#text': 'API Documentation:'}, {'ext-link': {'@xlink:href': 'https://mmpyikxkcp.us-east-2.awsapprunner.com/asctb-api-spec.yaml', '@ext-link-type': 'uri', '#text': 'https://mmpyikxkcp.us-east-2.awsapprunner.com/asctb-api-spec.yaml'}, '#text': 'OpenAPI specification:'}, 'CCF-API:', {'ext-link': {'@xlink:href': 'https://ccf-api.hubmapconsortium.org', '@ext-link-type': 'uri', '#text': 'https://ccf-api.hubmapconsortium.org'}, '#text': 'API Endpoint (includes interactive documentation):'}, {'ext-link': {'@xlink:href': 'https://ccf-api.hubmapconsortium.org', '@ext-link-type': 'uri', '#text': 'https://ccf-api.hubmapconsortium.org'}, '#text': 'API Documentation and OpenAPI specification:'}, {'ext-link': {'@xlink:href': 'https://github.com/rdfjs/N3.js', '@ext-link-type': 'uri', '#text': 'https://github.com/rdfjs/N3.js'}, '#text': 'API Database backend is N3.js:'}, {'ext-link': {'@xlink:href': 'https://github.com/hubmapconsortium/ccf-ui/tree/main/projects/ccf-database', '@ext-link-type': 'uri', '#text': 'https://github.com/hubmapconsortium/ccf-ui/tree/main/projects/ccf-database'}, '#text': 'Code to instantiate/use CCF Database:'}, {'ext-link': {'@xlink:href': 'https://github.com/hubmapconsortium/ccf-ui', '@ext-link-type': 'uri', '#text': 'https://github.com/hubmapconsortium/ccf-ui'}, '#text': 'CCF-User Interfaces:'}, {'ext-link': {'@xlink:href': 'https://github.com/hubmapconsortium/ccf-validation-tools', '@ext-link-type': 'uri', '#text': 'https://github.com/hubmapconsortium/ccf-validation-tools'}, '#text': 'Validation Tools:'}]",2023-03-27
0,Scientific Data,41597,10.1038/s41597-023-02079-1,Resonant inelastic x-ray scattering data for Ruddlesden-Popper and reduced Ruddlesden-Popper nickelates,29,3,2023,,The data reported here were generated via synchrotron experiments and did not require any processing of datasets beyond trivial binning of the two-dimensional data into one-dimensional spectra and calibration of the energy loss.,2023-03-29
0,Scientific Data,41597,10.1038/s41597-023-02050-0,High-resolution livestock seasonal distribution data on the Qinghai-Tibet Plateau in 2020,18,3,2023,https://github.com/NingZhan1978/High-resolution-livestock-seasonal-distribution-data-on-the-Qinghai-Tibet-Plateau-in-2020.git,"The code in this study is fully operational under Python 3.8.8, and the key packages were contained in the  and the  toolkit in Python 3.8.8. The code can be found on GitHub ().",2023-03-18
0,Scientific Data,41597,10.1038/s41597-023-02107-0,The Materials Provenance Store,6,4,2023,https://github.com/modelyst/dbgen; https://github.com/modelyst/dbgen; https://github.com/modelyst/mps-client,"The MPS database was generated using DBgen (v1.0.0a7) (), an open-source framework for building scientific databases and pipelines available at . A python API, a command-line interface (CLI), and a Jupyter notebook with example queries are available in the Materials Provenance Store Client repository ().",2023-04-06
0,Scientific Data,41597,10.1038/s41597-023-02166-3,The FAIR Cookbook - the essential resource for and by FAIR doers,19,5,2023,https://github.com/FAIRplus/the-fair-cookbook,The code is open source and available in a dedicated public repository on GitHub: .,2023-05-19
0,Scientific Data,41597,10.1038/s41597-023-02195-y,iOBPdb A Database for Experimentally Determined Functional Characterization of Insect Odorant Binding Proteins,19,5,2023,https://github.com/sshuklz/iobpdb_app,iOBPdb GitHub source code can be accessed online here: .,2023-05-19
0,Scientific Data,41597,10.1038/s41597-023-02280-2,A large expert-curated cryo-EM image dataset for machine learning protein particle picking,22,6,2023,https://github.com/BioinfoMachineLearning/cryoppp,"The data analysis methods, software and associated parameters used in this study are described in the section of Methods. All the scripts associated with various steps of data curation are available at the GitHub repository: , which includes the instructions about how to download the data.",2023-06-22
0,Scientific Data,41597,10.1038/s41597-023-02138-7,Complete Global Total Electron Content Map Dataset based on a Video Imputation Algorithm VISTA,25,4,2023,https://vista-tec.shinyapps.io/VISTA-Dashboard/,"Details about codes that generate the dataset as well as the usage notes on accessing, downloading and pre-processing the datasets are made available on the homepage of the dataset on the Deep Blue Data system of University of Michigan. Future updates of the codes and dataset will be made available on this website as well. Please contact the corresponding author for data request and questions. Additionally, our users can explore our interactive database dashboard () for more technical details and run the VISTA algorithm live.",2023-04-25
0,Scientific Data,41597,10.1038/s41597-023-02196-x,Database covering the prayer movements which were not available previously,12,5,2023,https://biomechlab.iyte.edu.tr/en/homepage/38,"The code written for the development of the database is available upon request from the authors, but it is not open to the external users through the website to protect the database. The desired main functions of the database were created in Python and Django framework. Django was used for creating the model for the backend and Javascript for filtering functions. For the front-end, HTML, CSS, and JQuery were used. The database is available through website  and the public repository Database covering the previously excluded daily life activities | Aperta (ulakbim.gov.tr).",2023-05-12
0,Scientific Data,41597,10.1038/s41597-023-02223-x,Forearm sEMG data from young healthy humans during the execution of hand movements,20,5,2023,,"A Matlab script ( available in ) and a Python script ( available in ) are provided to demonstrate how the dataset can be accessed and how to visualize an sEMG signal. The provided code has a menu where the user can select a specific signal to be visualized. Another code ( available in ) is provided to access the dataset and obtain the FFT of the dataset. This code can be used to generate an input file to implement a classification algorithm using the Neural Net Pattern Recognition toolbox in Matlab, as explained in the first example of the Technical Validation Section. Also, the code ( available in ) is provided, this code help to the user to implement an identification algorithm with the signals in the dataset.",2023-05-20
0,Scientific Data,41597,10.1038/s41597-023-02281-1,A proposed FAIR approach for disseminating geospatial information system maps,16,6,2023,,"['The miagis Python package is available on GitHub and the Python Package Index with comprehensive user documentation on GitHub.io:', {'ext-link': {'@xlink:href': 'https://github.com/MoseleyBioinformaticsLab/miagis', '@ext-link-type': 'uri', '#text': 'https://github.com/MoseleyBioinformaticsLab/miagis'}}, {'ext-link': {'@xlink:href': 'https://pypi.org/project/miagis/', '@ext-link-type': 'uri', '#text': 'https://pypi.org/project/miagis/'}}, {'ext-link': {'@xlink:href': 'https://moseleybioinformaticslab.github.io/miagis/', '@ext-link-type': 'uri', '#text': 'https://moseleybioinformaticslab.github.io/miagis/'}}]",2023-06-16
0,Scientific Data,41597,10.1038/s41597-023-02053-x,A Long-term Consistent Artificial Intelligence and Remote Sensing-based Soil Moisture Dataset,22,3,2023,https://github.com/os2328/CASM-dataset,"All code is written in Python, the analysis is conducted using Columbia University high performance computing clusters (Ginsburg), and is available at .",2023-03-22
0,Scientific Data,41597,10.1038/s41597-023-02082-6,Three-Dimensional Motion Capture Data of a Movement Screen from 183 Athletes,24,4,2023,https://github.com/Graham-Lab1/3D_MoCap_Data_of_a_Movement_Screen,Python and Matlab scripts used to de-identify the .c3d and .mat files and validate the selected joint angles are available on Github: . No custom code was used in addition to the Visual3D software to process the dataset.,2023-04-24
0,Scientific Data,41597,10.1038/s41597-023-02197-w,A labeled dataset for building HVAC systems operating in faulted and fault-free states,1,6,2023,,"[{'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR39', '#text': '39'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR40', '#text': '40'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR34', '#text': '34'}}], '#text': 'The Modelica Buildings Library and EnergyPlus are freely available for download,. EnergyPlus runs on Windows, Mac OSX, and Linux operating systems. A Windows or Linux-based computer and Dymola solver are required to run Modelica, and Dymola can be licensed from Modelica Buildings Library. HVACSIM\u2009+\u2009is also freely available, upon request from NIST, and has no operating system requirements. The Modelon air conditioning library that was used to model the faults in the RTU refrigerant side, was accessed from Modelon’s library suite. The Modelica-based library is used to design, analyze and optimize air conditioning systems.'}, {'italic': 'Method of Brick schema model development', '#text': 'A custom Python-based script was developed to create Brick model .ttl files for each system in the dataset, following the process described in . The .ttl files are included in the data repository.'}]",2023-06-01
0,Scientific Data,41597,10.1038/s41597-023-02282-0,"Population, land use and economic exposure estimates for Europe at 100 m resolution from 1870 to 2020",8,6,2023,https://doi.org/10.5281/zenodo.7556953; https://doi.org/10.5281/zenodo.6783023; https://doi.org/10.4121/collection:HANZE,"The source code of HANZE v2.0 (implemented in Python 3.9) presented in the paper is archived at . All necessary input data are archived at . The flood impact data shown in Usage Notes, with a description of sources of the data, are available in the HANZE v1.0 repository, .",2023-06-08
0,Scientific Data,41597,10.1038/s41597-023-02198-9,SciSciNet: A large-scale open data lake for the science of science research,1,6,2023,https://github.com/kellogg-cssi/SciSciNet,"The source code for data selection and curation, data linkage, and metrics calculation is available at .",2023-06-01
0,Scientific Data,41597,10.1038/s41597-023-02225-9,SRAM-Based PUF Readouts,27,5,2023,https://github.com/servinagrero/SRAMPlatform; https://servinagrero.github.io/SRAMPlatform; https://www.postgresql.org/; https://www.rabbitmq.com/,"The source code of the platform and the STM32 devices are available under the GPL-2.0 license at . Online documentation on the platform and guidance on custom station set up can be found at . The full list of python dependencies is available in the  file in GitHub repository. PostgreSQL () is needed to store data, a message broker is necessary to communicate with the station, RabbitMQ ()in our case, and Grafana is used to monitory metrics and sensors in a dashboard.",2023-05-27
0,Scientific Data,41597,10.1038/s41597-023-02310-z,Simulated Inherent Optical Properties of Aquatic Particles using The Equivalent Algal Populations (EAP) model,24,6,2023,,"[{'ext-link': {'@xlink:href': 'https://github.com/lisllain/EAP-model', '@ext-link-type': 'uri', '#text': 'https://github.com/lisllain/EAP-model'}, '#text': 'A Jupyter notebook of the EAP model code is available on Github: .'}, 'It is shared under a GNU General Public License. Appropriate acknowledgement should be made when the model is used in publications.', {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR2', '#text': '2'}}, '#text': 'Matlab code for writing IOP input files for the Hydrolight 4-component user-defined IOP model is also available, as are discretised EAP phase function files for Hydrolight. These will be added to the Github in due course but are available on request in the interim.'}]",2023-06-24
0,Scientific Data,41597,10.1038/s41597-023-02055-9,An intracochlear electrocochleography dataset - from raw data to objective analysis using deep learning,22,3,2023,,The code used to create and process the presented data is provided in or is part of open-source repositories.,2023-03-22
0,Scientific Data,41597,10.1038/s41597-023-02111-4,Genome-wide chromatin accessibility and gene expression profiling during flatfish metamorphosis,8,4,2023,https://github.com/GuerreroP/FISHRECAP-ATAC-RNA,We relied on open source tools to perform data analysis. Custom code performed in R used in this analysis have been published in the following repository: .,2023-04-08
0,Scientific Data,41597,10.1038/s41597-023-02140-z,Thermal Bridges on Building Rooftops,10,5,2023,,"[{'ext-link': [{'@xlink:href': 'https://github.com/Helmholtz-AI-Energy/TBBRDet', '@ext-link-type': 'uri', '#text': 'https://github.com/Helmholtz-AI-Energy/TBBRDet'}, {'@xlink:href': 'https://opensource.org/licenses/BSD-3-Clause', '@ext-link-type': 'uri', '#text': 'https://opensource.org/licenses/BSD-3-Clause'}], 'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR43', '#text': '43'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR44', '#text': '44'}}], '#text': 'Processing code is publicly available and can be found at . The software is licensed under the Revised Berkley Software Distribution (BSD-3) license (). All scripts are implemented with the Python (v.3.6.8) programming language and utilize the PyTorch (v.1.10.2) machine learning framework.'}, 'Conceptually, the software provides the following functionalities:', {'bold': 'VGG annotation to COCO JSON converter', '#text': 'implementing fully automatic conversion from the annotation format generated during the manual labeling process into the COCO JSON format archived on Zenodo.'}, {'bold': 'Dataset mappers', 'monospace': ['Detectron2', 'MMDetection'], '#text': 'for the  and  libraries implementing random-access collections to individual images and corresponding annotations. These are necessary for enabling the loading of five-channel images in each library. Data may be augmented by arbitrary transformations during the loading procedure.'}, {'bold': 'Model configuration', 'monospace': ['Detectron2', 'MMDetection'], '#text': 'for all  and  experiments performed in related works.'}, {'bold': 'Training/evaluation scripts', 'monospace': ['Detectron2', 'MMDetection'], '#text': 'for performing training and evaluation of neural networks for both  and .'}, {'bold': 'Dataset/experiment utilities', 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR45', '#text': '45'}}, '#text': 'for exploring the dataset, calculating image normalization coefficients, combining model scores, and calculating SLURM workload manager system statistics (consumed energy, runtime, etc.).'}, {'ext-link': {'@xlink:href': 'https://github.com/kit-data-manager/FAIR-DO-Lab', '@ext-link-type': 'uri', '#text': 'https://github.com/kit-data-manager/FAIR-DO-Lab'}, '#text': 'For creating, updating, and validating the FAIR DOs, the Typed PID Maker was used. This is a component of the FAIR DO Lab for working on FAIR DO tasks, which is found at .'}]",2023-05-10
0,Scientific Data,41597,10.1038/s41597-023-02199-8,KG: A FAIR Knowledge Graph of Graffiti,25,5,2023,https://github.com/dice-group/Ingrid; https://www.gnu.org/licenses/gpl-3.0,Our source code to generate the new versions of KG is publicly available at () and will be maintained in parallel with the knowledge graph. We provide our source code under the software license of GPL 3.0 ().,2023-05-25
0,Scientific Data,41597,10.1038/s41597-023-02284-y,Carbon Monitor Europe near-real-time daily CO emissions for 27 EU countries and the United Kingdom,8,6,2023,https://github.com/kepiyu/Carbon-Monitor-Europe/blob/main/CM_EU_v2.py,Python code for producing data for 27 EU countries and the United Kingdom in the dataset is provided at .,2023-06-08
0,Scientific Data,41597,10.1038/s41597-023-02311-y,Sea ice drift tracks from autonomous buoys in the MOSAiC Distributed Network,23,6,2023,,The Interactive Data Language (IDL) code and Python and shell scripts used to produce the processed DN buoy drift tracks are archived and available for download at the Arctic Data Center.,2023-06-23
0,Scientific Data,41597,10.1038/s41597-023-02227-7,A high spatial resolution dataset of China’s biomass resource potential,15,6,2023,https://github.com/Rui-W-A/biomass-resource-China,"The code used for calculating agricultural, forestry residues, and energy crops is written in Python and available from .",2023-06-15
0,Scientific Data,41597,10.1038/s41597-023-02256-2,A guide to sharing open healthcare data under the General Data Protection Regulation,24,6,2023,https://doi.org/10.6084/m9.figshare.22643419,No code was written or used for this paper.,2023-06-24
0,Scientific Data,41597,10.1038/s41597-023-02086-2,Global monthly sectoral water use for 2010–2100 at 0.5° resolution across alternative futures,11,4,2023,,"Table  provides links to all models, data, versions and DOI’s used to generate this dataset.",2023-04-11
0,Scientific Data,41597,10.1038/s41597-023-02058-6,A 100-member ensemble simulations of global historical (1951–2010) wave heights,6,6,2023,,"[{'bold': 'Sub-daily data generation code', 'italic': 'H', 'sub': {'italic': 's'}, 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR16', '#text': '16'}}, 'ext-link': {'@xlink:href': '10.18164/d68361d0-8141-48b9-a25e-a9bc98d71438', '@ext-link-type': 'doi', '#text': 'https://doi.org/10.18164/d68361d0-8141-48b9-a25e-a9bc98d71438'}, '#text': 'The technical details of the statistical model used to generate the 6-hourly  from SLP predictors are included in the corresponding reference paper which allow for the reproducibility of the presented dataset. Additionally, the corresponding Fortran and R codes are publicly available in the Government of Canada Open Data Portal, together with the d4PDF-WaveHs dataset (DOI ).'}, {'bold': ['Computation of statistics/indices:', {'italic': 'getStat.f'}, {'italic': 'getHsEx.f'}], 'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR8', '#text': '8'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR25', '#text': '25'}}], 'italic': ['H', 'getStat.f', 'getHsEx.f'], 'sub': {'italic': 's'}, 'ext-link': {'@xlink:href': 'https://cowclip.org/data-access', '@ext-link-type': 'uri', '#text': 'https://cowclip.org/data-access'}, '#text': ', To be consistent with COWCLIP2.0, the  statistics and indices were computed with  and , after a slight modification to account for missing data (see Methods Section). The original Fortran code was developed as part of the COWCLIP community framework and can accessed via the COWCLIP website (). The code can be compiled with a Fortran compiler, with netCDF4 and HDF5 libraries. Additional attribute information to account for CF conventions and ACCD standards, was added to this Fortran code output with standard NetCDF operators (NCOs) for file manipulation, such as the “ncatted” command.'}]",2023-06-06
0,Scientific Data,41597,10.1038/s41597-023-02087-1,Big Field of View MRI T1w and FLAIR Template - NMRI225,14,4,2023,https://github.com/barbrakr/NMRI225.git,"We make our code available at  as NMRI225_run.m, NMRI225_run.py and nmri_functions, under a CC BY license. We used MATLAB 2018b to run NMRI225_run.m and Python 3.8 for running NMRI225_run.py. We have summarized the packages of the conda repository in Supplementary Materials.",2023-04-14
0,Scientific Data,41597,10.1038/s41597-023-02143-w,"Product, building, and infrastructure material stocks dataset for 337 Chinese cities between 1978 and 2020",20,4,2023,,"The data gap were filled in Python 3.8, using the following libraries: pandas 1.3.4, numpy 1.18.5, statsmodels 0.13.2, sklearn 1.0.2, scipy 1.7.3. The full custom Python script is provided on the open-access online dataset Figshare [].",2023-04-20
0,Scientific Data,41597,10.1038/s41597-023-02287-9,EEG-based BCI Dataset of Semantic Concepts for Imagination and Perception Tasks,15,6,2023,https://github.com/hWils/Semantics-EEG-Perception-and-Imagination,"The Psychopy files to compile the experiment are stored on the Github repository . Also on this repository are the Python processing and technical validation scripts. Users can directly use the Python code provided 1) to compute preprocessing as described in this paper, and 2) to reproduce the experimental results presented in the technical validation section.",2023-06-15
0,Scientific Data,41597,10.1038/s41597-023-02144-9,Very High Resolution Projections over Italy under different CMIP5 IPCC scenarios,26,4,2023,,,2023-04-26
0,Scientific Data,41597,10.1038/s41597-023-01974-x,Evaluating explainability for graph neural networks,18,3,2023,https://zitniklab.hms.harvard.edu/projects/GraphXAI; https://github.com/mims-harvard/GraphXAI,"Project website for GXAI is at . The code to reproduce results, documentation, and tutorials are available in GXAI ‘s Github repository at . The repository contains Python scripts to generate and evaluate explanations using performance metrics and also visualize explanationa. In addition, the repository contains information and Python scripts to build new versions of GXAI as the underlying primary resources get updated and new data become available.",2023-03-18
0,Scientific Data,41597,10.1038/s41597-023-02030-4,A synthetic population for agent-based modelling in Canada,21,3,2023,https://pypi.org/,"The python scripts (python 3.10) developed for the generation and validation of the synthetic dataset are publicly and freely accessible on Zenodo. The scripts use the following python packages: pandas (1.4.4), numpy (1.23.2), pyreadstat (1.19), scipy (1.9.1) for the Pearson’s correlation coefficient computation, scikit-learn (1.1.2) for the RMSE computation, and matplotlib (3.5.3) to generate the charts. All these python packages are available from the Python Package Index: . The humanleague package (2.1.10) providing the QISI and IPF implementations is available from the Python Package Index and on Zenodo.",2023-03-21
0,Scientific Data,41597,10.1038/s41597-023-02259-z,"Full scale, microscopically resolved tomographies of sandstone and carbonate rocks augmented by experimental porosity and permeability values",7,6,2023,,"[{'ext-link': {'@xlink:href': 'https://github.com/IBM/microCT-Dataset', '@ext-link-type': 'uri', '#text': 'https://github.com/IBM/microCT-Dataset'}, '#text': 'The algorithms used for processing and segmenting the raw grayscale images are available as Python code at: .'}, 'The code repository contains Jupyter Notebooks for simplifying data processing and visualization along with usage guidance.']",2023-06-07
0,Scientific Data,41597,10.1038/s41597-023-02089-z,A corpus of CO electrocatalytic reduction process extracted from the scientific literature,29,3,2023,https://github.com/kg4sci/electrocatalytic_db; chemdataextractor.org; radimrehurek.com; https://github.com/pymupdf/PyMuPDFPyMuPDF; www.pytorch.org; scikit-learn.org,"The scripts utilized to parse articles and extract entities are home-written codes which are publicly available at the github repository . The underlying machine-learning libraries used in this project are all open-source: ChemDataExtractor (), gensim (), PyMuPDF(), Pytorch () and scikit-learn ().",2023-03-29
0,Scientific Data,41597,10.1038/s41597-023-02116-z,CRAFTED: An exploratory database of simulated adsorption isotherms of metal-organic frameworks,20,4,2023,https://github.com/st4sd/nanopore-adsorption-experiment,"The Jupyter notebooks providing the panel visualisation of the isotherm curves, enthalpy of adsorption data, IAST-based multicomponent mixture isotherm, and the t-SNE + DBSCAN analysis of the chemical and geometric properties of MOFs are distributed alongside the database in the Zenodo repository. A fully automated workflow that is capable of recreating the dataset was made available as an open-source project (v1.0.0) on GitHub ().",2023-04-20
0,Scientific Data,41597,10.1038/s41597-023-02174-3,M100 ExaData: a data collection campaign on the CINECA’s Marconi100 Tier-0 supercomputer,18,5,2023,,"[{'ext-link': {'@xlink:href': 'https://github.com/EEESlab/examon', '@ext-link-type': 'uri', '#text': 'https://github.com/EEESlab/examon'}, '#text': 'The code we have used to generate this dataset has multiple sources, albeit all being publicly available. As defined previously, the data is collected through a monitoring infrastructure called ExaMon, which we have developed and deployed on the CINECA infrastructure. The code is publicly available and can be used in different supercomputing facilities ().'}, 'The libraries and software modules used to process the data streams and obtain the dataset presented here are the following:', '• examon-client', '• NumPy (1.23.1)', '• Pandas (1.5.1)', '• PyArrow (9.0.0)', {'ext-link': {'@xlink:href': 'https://gitlab.com/ecs-lab/exadata', '@ext-link-type': 'uri', '#text': 'https://gitlab.com/ecs-lab/exadata'}, '#text': 'In addition, in the source code repository () it is possible to recover the list of software modules used to access the data and to perform the analysis described in Sec. “Technical Validation”.'}]",2023-05-18
0,Scientific Data,41597,10.1038/s41597-023-01946-1,Data and Tools Integration in the Canadian Open Neuroscience Platform,6,4,2023,https://github.com/CONP-PCNO/conp-portal,The code used for the portal is available on  and a version of the code is available on Zenodo.,2023-04-06
0,Scientific Data,41597,10.1038/s41597-023-02060-y,"Healthy Cities, A comprehensive dataset for environmental determinants of health in England cities",25,3,2023,https://github.com/0oshowero0/HealthyCities,"The Python codes to generate the dataset are publicly available through the GitHub repository (). Detailed instruction for software environment preparation, folder structure and commands to run the provided codes is available in the repository.",2023-03-25
0,Scientific Data,41597,10.1038/s41597-023-02146-7,High throughput calculations for a dataset of bilayer materials,21,4,2023,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR43', '#text': '43'}}, 'ext-link': {'@xlink:href': 'https://github.com/rkb12/BMDB-databaseBMDB-database', '@ext-link-type': 'uri', '#text': 'https://github.com/rkb12/BMDB-databaseBMDB-database'}, '#text': 'In addition to the CSV file, several components to work with the reported bilayer materials can be found in figshare and in GitHub ().'}, {'xref': {'@rid': 'Fig2', '@ref-type': 'fig', '#text': '2'}, '#text': 'In the stacking-pattern-PYTHON-code-for-each-class zipped directory, a code that obtains all possible stacking patterns (following the naming notation in Fig.\xa0) are given. Inside each subdirectory, a prototype example of monolayer structure and the corresponding stacking code are given. Executing the code in the same directory will create the stacking pattens for the respective class of bilayer.'}, 'In the zipped soc-bandstructure-code, an in-house code generating the band structure with SOC is provided. The code reads a VASP output file and generates a graphical representation for the band structure.', {'italic': ['σ', 'τ', 'σ', 'τ'], 'sup': '2', '#text': 'In the zipped transport-cal-code, an in-house code generating the transport properties such as conductivity (/), Seebeck coefficient (S), and power factor (S/) as function of chemical potential at different temperature for each bilayer material. The code reads VASP and BoltzTraP output files and generates a graphical representation for the transport properties.'}, 'In the zipped soc-bandstructure-figures and transport-figures a graphical representation for the energy band structure (with and without SOC) and transport properties for the ground state stacking compositions of all bilayer materials are also given.']",2023-04-21
0,Scientific Data,41597,10.1038/s41597-023-02202-2,"Race and ethnicity data for first, middle, and surnames",19,5,2023,,"['The underlying voter files are proprietary and sourced from L2, Inc. The data can be directly purchased from L2, Inc. Otherwise, the count files generated herein can be recreated by acquiring voter files from each state. Among the six states we have considered, data from North Carolina is the most straightforward to obtain, as the voter file can be downloaded for free from the State Board of Elections website.', {'italic': 'Data Processing', 'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR13', '#text': '13'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR22', '#text': '22'}}], '#text': 'Data processing code is straightforward and involves iterating through each voter file and tallying names by race. Initial processing code was written in Python (version 3.6), while a small amount of post-processing was done in R (version 4.1.2). These steps are described in the  subsection. Sample data processing code are available alongside the count files in the Harvard Dataverse. Code to recreate each of the figures can be found in a separate repository in the Harvard Dataverse.'}]",2023-05-19
0,Scientific Data,41597,10.1038/s41597-023-02061-x,HIDSAG: Hyperspectral Image Database for Supervised Analysis in Geometallurgy,23,3,2023,,"[{'ext-link': {'@xlink:href': 'https://github.com/alges/hidsag', '@ext-link-type': 'uri', '#text': 'https://github.com/alges/hidsag'}, '#text': 'The data records can be used straightforwardly as single files or as a set. A step-by-step example to extract the numerical data from each HDF5 file, along with a library developed for this purpose is available on GitHub: .'}, 'Along with the library, a Python notebook with sample usage is included containing:', '1. The import of a data record from a path', '2. The selection of a single sample', '3. Slicing of a specific band and position for numerical data extraction', '4. Visualization of the resulting slice', '5. The list of each sample on the data record, its associated variables and metadata']",2023-03-23
0,Scientific Data,41597,10.1038/s41597-023-02147-6,Insider trading,26,4,2023,,"The code used for the data normalization and merging steps was created and run in Stata/MP 17.0 and is made available in the data repository. The acquisition and processing scripts are not shared publicly because EDGAR servers may block the simultaneous use of the same acquisition script based on the user agent in request headers. However, downloading regulatory filings via HTTP follows a standard procedure, and parsing XML files in Python using the lxml library is also well-documented.",2023-04-26
0,Scientific Data,41597,10.1038/s41597-023-02176-1,Crowd cluster data in the USA for analysis of human response to COVID-19 events and policies,10,5,2023,https://www.mpi-forum.org,The codes were written in C++ and Python 3. They rely on Python DBSCAN package and on MPI () for parallelization. The code is available from the data site.,2023-05-10
0,Scientific Data,41597,10.1038/s41597-023-02203-1,CHQ- SocioEmo: Identifying Social and Emotional Support Needs in Consumer-Health Questions,27,5,2023,https://github.com/Ashwag1/CHQ-SocioEmo-,"The code used to prepare the CHQ-SocioEmo dataset is provided at , and the source code for the benchmarked experiments can be found with the dataset.",2023-05-27
0,Scientific Data,41597,10.1038/s41597-023-02261-5,A tiled multi-city urban objects dataset for city-scale building energy simulation,2,6,2023,,"[{'ext-link': {'@xlink:href': 'https://github.com/ruirzma/UPTO', '@ext-link-type': 'uri', '#text': 'https://github.com/ruirzma/UPTO'}, '#text': 'The shared dataset is prepared based on the default setting of the UrbanPatch container and D-radius. If users want to customizable this dataset with different settings, they can use the shared UrbanPatch generation package (). There are four files included in the package:'}, '• “ConPatchForTile.py”: construct UrbanPatch individuals for UrbanTile objects when changing the receptive radius.', '• “ConPatchForBuilding.py”: construct UrbanPatch individuals for Building objects for a given receptive radius.', '• “GenMicroclimate.py”: generate the UrbanTile-scale microclimate.', '• “GenIDF.py”: generate UrbanTile-scale EnergyPlus IDF file.']",2023-06-02
0,Scientific Data,41597,10.1038/s41597-023-02062-w,Large scale crowdsourced radiotherapy segmentations across a variety of cancer anatomic sites,22,3,2023,https://github.com/kwahid/C3RO_analysis,"Segmentations were performed using the commercially-available ProKnow (Elekta AB, Stockholm, Sweden) software. The code for NIfTI file conversion of DICOM CT images and corresponding DICOM RTS segmentations, along with code for consensus segmentation generation, was developed using in-house Python scripts and is made publicly available through GitHub: .",2023-03-22
0,Scientific Data,41597,10.1038/s41597-023-02091-5,A Large-Scale Dataset of Three-Dimensional Solar Magnetic Fields Extrapolated by Nonlinear Force-Free Method,30,3,2023,,"[{'ext-link': [{'@xlink:href': 'https://github.com/deepsolar/pynlfff', '@ext-link-type': 'uri', '#text': 'https://github.com/deepsolar/pynlfff'}, {'@xlink:href': 'https://github.com/mbobra/SHARPs', '@ext-link-type': 'uri', '#text': 'https://github.com/mbobra/SHARPs'}], 'monospace': 'pip install pynlfff', 'xref': {'@rid': 'Fig12', '@ref-type': 'fig', '#text': '12'}, '#text': 'In order for this dataset to be fully reproducible and expandable in the future, we have open-sourced all the Python code used to generate and validate the resource in the following code repository () and can be downloaded directly via pip as . The code can be divided into three parts, dataset generation code, label generation code and dataset Toolkit code. The dataset generation code is for generating the dataset, label generation code is for labeling flare information to nlfff data list, and dataset Toolkit code is for manipulating the data. The whole process of code usage is shown in Fig.\xa0 to explain this usage more clearly. The tools and examples for getting original Bp, Bt and Br fits can be found at ().'}, {'bold': 'Dataset generation code'}, 'The code of dataset generation mainly consists of three different components. The first component contains the preparing boundary conditions programs. This utility uses Bp.fits, Bt.fits and Br.fits of “hmi.sharp_cea_720s” to generate “boundary.ini”, “mask.dat”, “grid.ini” and “allboundaries.dat” for the next step. This code is multi-threaded for computing efficiency, allowing the users set the number of threads. Note that if the raw data file is corrupted or with the quality problem, the boundary conditions file may not be generated properly. The corrupted raw files may report an error when operating them, e.g., the quality problem of raw data may cause generated “mask.dat” file with “NaN”.', {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR22', '#text': '22'}}, '#text': 'The second component is for magnetic field calculation, consisting of Python code for computing flow control and magnetic field extrapolation module provided by Wiegelmann’s team. The Python code is responsible for scheduling and controlling core computing, specifying the number of running processes, binding tasks and cores, adaptively assigning cores according to the task, maximizing the use of computing resources, quality control, and logging, etc.'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR22', '#text': '22'}}, '#text': 'The third component is magnetic field calculation written by C language program. It is not included in our published “pynlfff” package since its copyright is owned by Wiegelmann’s team. The C code needs to be compiled beforehand, and “pynlfff” provides bash scripts to automatically compile and generate single-process and multi-process programs. In addition, we have rewritten multi-grid bash scripts to perform magnetic field extrapolation for each layer separately. Python and C should be implemented together, using single-process C programs for small tasks and multi-process C programs for large tasks. In addition, we allocate computing cores according to the task size and employ core binding technique to maximize the use of computing resources.'}, {'bold': 'Dataset Toolkit Code'}, 'After getting the dataset file, you can implement your own program to read the product file “Bout.bin” based on the storage structure of the product file “Bout.bin” which has been described in subsection NLFFF Data Format, and we provide a toolkit for python implementation to help you with the reading operation.', {'bold': 'Flare label generation code'}, {'xref': {'@rid': 'Fig4', '@ref-type': 'fig', '#text': '4'}, 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR39', '#text': '39'}}, '#text': 'As shown in Fig.\xa0, pynlfff already implements these processes and has updated the label information in the project website, if there is any other information that needs to be customized, it can be done through pynlfff or by modifying the pynlfff code.'}]",2023-03-30
0,Scientific Data,41597,10.1038/s41597-023-02233-9,Mechanosensitive changes in the expression of genes in colorectal cancer-associated fibroblasts,2,6,2023,,"['The following software and versions were used for quality control and data analysis:', {'ext-link': {'@xlink:href': 'http://www.bioinformatics.babraham', '@ext-link-type': 'uri', '#text': 'http://www.bioinformatics.babraham'}, '#text': '1. FastQC, version 0.11.8 and MultiQC, version 1.7 were used for quality analysis of raw FASTQ sequencing data: . ac.uk/projects/fastqc/'}, {'ext-link': {'@xlink:href': 'http://www.ccb.jhu.edu/software/hisat/index.shtml', '@ext-link-type': 'uri', '#text': 'http://www.ccb.jhu.edu/software/hisat/index.shtml'}, '#text': '2. HISAT2 was used for mapping of sequence reads to the human GRCh38.13 genome assembly:'}, {'ext-link': {'@xlink:href': 'http://bioinf.wehi.edu.au/featureCounts/', '@ext-link-type': 'uri', '#text': 'http://bioinf.wehi.edu.au/featureCounts/'}, '#text': '3. HTSeq, version 0.9.1 was used for calculating the gene counts:'}, {'ext-link': {'@xlink:href': 'https://bioconductor.org/packages/release/bioc/html/edgeR.html', '@ext-link-type': 'uri', '#text': 'https://bioconductor.org/packages/release/bioc/html/edgeR.html'}, '#text': '4. edgeR, version 3.34.1 was used for normalization and visualization of differential gene expression analysis output:'}, 'Software and codes are open source and readily available.']",2023-06-02
0,Scientific Data,41597,10.1038/s41597-023-02262-4,Simulated sulfur K-edge X-ray absorption spectroscopy database of lithium thiophosphate solid electrolytes,2,6,2023,https://github.com/atomisticnet/xas-tools/releases/tag/v0.1.0,"Short scripts used for extracting useful information from the VASP output files, such as the XAS and energies, are provided with the database. The workflow is available on GitHub ().",2023-06-02
0,Scientific Data,41597,10.1038/s41597-023-02092-4,A materials data framework and dataset for elastomeric foam impact mitigating materials,5,6,2023,,"A snapshot version of our custom analysis tools at the time of initial data preparation is included in the “” directories within each experiment type. These include μCT (Python), DIC (Matlab), and DMA (Python and Excel) analyses, with documentation. Settings used for μCT analysis are summarized in Table  and settings for DIC analysis are in Table . All first-party data and code referenced here are available free and open source per the included NIST license file; however, certain commercial formats and analysis software are also used. Example code for access, visualization, and filtering of data are provided free and open source (MIT License) as a Git repository, as described in the  section.",2023-06-05
0,Scientific Data,41597,10.1038/s41597-023-02064-8,ChillsDB: A Gold Standard for Aesthetic Chills Stimuli,20,5,2023,https://github.com/ChillsTV/AffectiveStimuliScraper,The code for parsing YouTube and Reddit networks is available under an MIT license at .,2023-05-20
0,Scientific Data,41597,10.1038/s41597-023-02093-3,A dataset on corporate sustainability disclosure,31,3,2023,,The codes used for calculation and analysis in this study are available in .,2023-03-31
0,Scientific Data,41597,10.1038/s41597-023-02235-7,The first high-quality chromosome-level genome of the Sipuncula  using HiFi and Hi-C data,25,5,2023,,No specific code or script was used in this work. Commands used for data processing were all executed according to the manuals and protocols of the corresponding software.,2023-05-25
0,Scientific Data,41597,10.1038/s41597-023-02065-7,Brightfield vs Fluorescent Staining Dataset–A Test Bed Image Set for Machine Learning based Virtual Staining,22,3,2023,,"The source code of the baseline, as well as a direct link to the VirtualStaining Dataset, is available in the GitLab repository. The installation of Python and Jupyter using the virtual environment is recommended, with the necessary technical instruction supplied in the “ReadMe.md” inside the repository.",2023-03-22
0,Scientific Data,41597,10.1038/s41597-023-02094-2,CarbonMonitor-Power near-real-time monitoring of global power generation on hourly to daily scales,17,4,2023,https://github.com/KowComical/CM_Power_Data; https://power.carbonmonitor.org,"The generated datasets and the codes for producing the datasets are available from  and . The most up-to-date, continuously updated data can be visualized and uploaded from . Codes are available upon reasonable requests.",2023-04-17
0,Scientific Data,41597,10.1038/s41597-023-02150-x,Fatigue database of additively manufactured alloys,2,5,2023,ChemDataExtractor 2.0; table extractor; Simple Transformer; https://simpletransformers.ai/; https://github.com/xuzpgroup/ZianZhang/tree/main/FatigueData-AM2022,"The scripts utilized to extract information from figures, tables, and text are mainly based on open-source codes such as , , and  (), respectively. The in-house scripts for data extraction and analysis are publicly released at the GitHub repository (), which can be used by acknowledging the current article and under the MIT license. These scripts include a detailed, step-by-step tutorial for loading and analyzing the dataset in the repository.",2023-05-02
0,Scientific Data,41597,10.1038/s41597-023-02207-x,67 million natural product-like compound database generated via molecular language processing,19,5,2023,https://github.com/SIBERanalytics/Natural-Product-Generator,Code used to train the molecular language model as well as the trained model used for natural product-like molecule generation is available from GitHub at .,2023-05-19
0,Scientific Data,41597,10.1038/s41597-023-02066-6,HIT-UAV: A high-altitude infrared thermal dataset for Unmanned Aerial Vehicle-based object detection,20,4,2023,https://pegasus.ac.cn,"The data processing code is available in the  folder of . The code is written in Python. The functions of the tools are as follows: (1) The  is to convert oriented bounding boxes to standard bounding boxes and generate the dataset, (2) The  is to visualize images with bounding boxes, (3) The  is to generate the label files with the YOLO format to help users train the YOLO, which is the representative object detection algorithm.",2023-04-20
0,Scientific Data,41597,10.1038/s41597-023-02095-1,A century and a half precipitation oxygen isoscape for China generated using data fusion and bias correction,6,4,2023,https://doi.org/10.5281/zenodo.7306199,"The codes for two bias correction methods (LS and DT) and three neural network data fusion methods (BP, LSTM and CNN) are available at . The codes were programmed using MATLAB version 2022a and Python 3.8.",2023-04-06
0,Scientific Data,41597,10.1038/s41597-023-02208-w,CORE: A Global Aggregation Service for Open Access Papers,7,6,2023,https://github.com/oacore/,"CORE consists of multiple services. Most of our source code is open source and available in our public repository on GitHub (). As of today, we are unfortunately not yet able to provide the source code to our data ingestion module. However, as we want to be as transparent as possible with our community, we have documented in this paper the key algorithms and processes which we apply using pseudocode.",2023-06-07
0,Scientific Data,41597,10.1038/s41597-023-02096-0,A standardized catalogue of spectral indices to advance the use of remote sensing in Earth system research,8,4,2023,https://github.com/awesome-spectral-indices/awesome-spectral-indices; https://github.com/awesome-spectral-indices/spyndex; https://pypi.org/project/spyndex/; https://anaconda.org/conda-forge/spyndex; https://share.streamlit.io/davemlz/espectro/main/espectro.py; https://github.com/awesome-spectral-indices/espectro,"The ASI Catalogue code is open-source and can be found at  and Zenodo. The  Python package is open-source and can be found at . It is also available through PyPI () and conda-forge (). The catalogue in CSV format can also be downloaded from the  Streamlit web app, which is available at . The  Streamlit web app code is open-source and can be found at .",2023-04-08
0,Scientific Data,41597,10.1038/s41597-023-02123-0,A comprehensive dataset of annotated brain metastasis MR images with clinical and radiomic data,14,4,2023,https://github.com/ysuter/OpenBTAI-radiomics,"We provide the code used to extract the features with PyRadiomics at . For reproducibility and convenience in case any user wants to customize the extraction, all the.py files needed and a “readme” file are available.",2023-04-14
0,Scientific Data,41597,10.1038/s41597-023-02152-9,A continent-wide detailed geological map dataset of Antarctica,18,5,2023,https://gis.gns.cri.nz/server/rest/services/SCAR_GeoMAP/ATA_SCAR_GeoMAP_Geology/MapServer; https://data.gns.cri.nz/ata_geomap/index.html; https://geomap.readthedocs.io/en/latest/; https://github.com/selkind/GeoMap,"GeoMAP v.2022-08 has been generated for ArcGIS (10.8.1) and QGIS (3.4) as geodatabase and geopackage material, using a GCS WGS 1984 geographic coordinate reference and WGS 1984 Antarctic Polar Stereographic projection. Data were developed manually, then stored in a GIS database developed, web-delivered and maintained by GNS Science in New Zealand. Software ArcGIS® has been used to create the GIS database, but data can be exported in a variety of formats and compatible with most other GIS software. ArcGIS data are available from the PANGAEA data archive, an ArcGIS REST service (), or viewed through a webmap (). A series of QGIS and Google Earth KMZ files, exported from the ArcGIS geodatabase layers, are also available from the archive. The original data have been segmented into ten regions to keep KMZ files at a reasonable (<25 Mb) and useable size. GeoMAP documentation () has been generated using code deposited on GitHub ().",2023-05-18
0,Scientific Data,41597,10.1038/s41597-023-02181-4,"M4Raw: A multi-contrast, multi-repetition, multi-channel MRI k-space dataset for low-field MRI research",10,5,2023,https://github.com/mylyu/M4Raw,"To facilitate users of this dataset, we have released the following Github repository: . The repository contains Python examples for data reading and deep learning model training, and the trained model weights to reproduce the results in Figs. –.",2023-05-10
0,Scientific Data,41597,10.1038/s41597-022-01862-w,": Dataset of foraging for visual information, gaze typing and empathy assessment",3,12,2022,,"The .csv files composing the dataset were extracted directly from Tobii Pro X3–120 Eye Tracker, as well as images such as heatmaps. The original raw-data files and the python code used to generate the .csv files, figures and statistics here are available at .",2022-12-03
0,Scientific Data,41597,10.1038/s41597-023-01954-1,Assessing ternary materials for fluoride-ion batteries,11,2,2023,https://github.com/donmctaggart15/ternary_f_cathodes,"All code used is open source and available at . The datasets are provided on the same repository. We recommend reading the Simmate, Materials Project API, and pymatgen documentation to follow filtering syntax.",2023-02-11
0,Scientific Data,41597,10.1038/s41597-022-01892-4,"A Brazilian classified data set for prognosis of tuberculosis, between January 2001 and April 2020",15,12,2022,https://github.com/dotlab-brazil/tuberculosis_preprocessing,The code used to pre-process the data set is publicly available on GitHub and is accessible through the link: .,2022-12-15
0,Scientific Data,41597,10.1038/s41597-023-01955-0,All-hazards dataset mined from the US National Incident Management System 1999–2020,24,2,2023,,"The source code used to create the ICS-209-PLUS dataset, the spatiotemporal linkage, and the ICS-FIRED linked database are publicly accessible on GitHub.",2023-02-24
0,Scientific Data,41597,10.1038/s41597-023-01985-8,A large-scale dataset for end-to-end table recognition in the wild,23,2,2023,https://github.com/MaxKinny/TabRecSet,"A link to the dataset, along with Python codes that are used to create the dataset, statistical analysis and plots, is released and publicly available at .",2023-02-23
0,Scientific Data,41597,10.1038/s41597-022-01807-3,Unifying the identification of biomedical entities with the Bioregistry,19,11,2022,https://github.com/biopragmatics/bioregistry,The source code for the Bioregistry is available at  under the MIT License. The source code specific to the version of Bioregistry used in this article (v0.5.132) is archived on Zenodo.,2022-11-19
0,Scientific Data,41597,10.1038/s41597-022-01836-y,Multi-day dataset of forearm and wrist electromyogram for hand gesture recognition and biometrics,30,11,2022,,"[{'sup': {'xref': [{'@ref-type': 'bibr', '@rid': 'CR20', '#text': '20'}, {'@ref-type': 'bibr', '@rid': 'CR21', '#text': '21'}], '#text': ','}, '#text': 'The custom codes used for reading the signals of the database was created in MATLAB R2017b and is freely accessible at Physionet and IEEE Dataport. To implement the codes, the users will need a MATLAB License'}, {'monospace': '(readme.txt)', '#text': '• A readme file  with instructions about how to run the code in a 2017b or higher MATLAB version.'}, {'monospace': ['(fileread.m)', '.mat'], '#text': '• A Matlab script with a simple example about how to read WFDB files and convert them to format.'}, {'monospace': '(MotionSequence.txt)', '#text': '• A text filewhich provides the gesture sequence, and thus can be used to assign class labels to input data.'}, {'monospace': ['(feature_extraction.m)', 'featiDFTI.m', 'segmentEMG.m'], '#text': '• A Matlab scriptallows a simple example to extract frequency features using  and functions.'}, {'monospace': '(featiDFTI.m)', '#text': '• A Matlab functionfor generating frequency division technqiue features from sEMG Data.'}, {'monospace': '(segmentEMG.m)', '#text': '• A Matlab scriptfor implementing windowing of sEMG Data.'}]",2022-11-30
0,Scientific Data,41597,10.1038/s41597-022-01894-2,A Python library to check the level of anonymity of a dataset,26,12,2022,https://github.com/IFCA/pycanon; https://pycanon.readthedocs.org,Code for the  library and each compliance test is available in . The library documentation can be found in .,2022-12-26
0,Scientific Data,41597,10.1038/s41597-022-01780-x,So2Sat POP - A Curated Benchmark Data Set for Population Estimation from Space on a Continental Scale,19,11,2022,https://github.com/zhu-xlab/So2Sat-POP,Python is used for all the analyses and implementations. The code to create the features for each city and to run the baseline experiments is available on our GitHub project ().,2022-11-19
0,Scientific Data,41597,10.1038/s41597-023-02013-5,Policy relevant health related liveability indicator datasets for addresses in Australia’s 21 largest cities,25,2,2023,,"[{'ext-link': {'@xlink:href': 'https://github.com/healthy-liveable-cities/australian-national-liveability-study', '@ext-link-type': 'uri', '#text': 'https://github.com/healthy-liveable-cities/australian-national-liveability-study'}, '#text': 'Code is available on GitHub at . The project was conducted between 2018 and 2020 using Python 2.7 with PostgreSQL 9.6, PostGIS 2.4, the ArcGIS 10.6 arcpy python library and network analyst extension. The code also draws heavily on the psycopg2, sqlalchemy, pandas and osmnx libraries. The code was developed across the duration of the project to meet evolving stakeholder needs for data and indicators. Unfortunately, across this period, software versions also evolved, and when a newer version of ArcGIS was installed in 2020 following expiry and renewal of institutional licences this required the use of Python 3. While this initially provided impetus to re-factor and update the code, project priorities within our research group changed and it became apparent this code would not be used in future projects, and there was not funding or scope to complete final code re-factoring. The exception to this was the ‘highlife’ project branch which contains code developed to create built environment measures targeting 2019 for the separate High Life study; this was the branch with the most recent and complete development efforts, and was therefore set as the default branch for the repository. An incomplete re-factoring for Python 3 is located on the ‘python3_2020 branch’; and the final main working branch of the overall project is the one titled ‘main’.'}, {'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR8', '#text': '8'}}, {'xref': [{'@ref-type': 'bibr', '@rid': 'CR4', '#text': '4'}, {'@ref-type': 'bibr', '@rid': 'CR66', '#text': '66'}, {'@ref-type': 'bibr', '@rid': 'CR67', '#text': '67'}], '#text': ',,'}], '#text': 'Many lessons were learnt about managing large code projects through the course of the study. The code for this project would ideally be re-factored but no team members had capacity to do so for this completed project. Project experiences meant that the team had broad desire to move towards more open source software solutions, for which the methods developed for this study were adapted and applied in other projects.'}]",2023-02-25
0,Scientific Data,41597,10.1038/s41597-022-01906-1,Image dataset for benchmarking automated fish detection and classification algorithms,3,1,2023,https://github.com/tzutalin/labelImg,The developed Python code for tagging and labelling the images is available through the Zenodo repository. Another device that can be used for tagging fishes is the public Label Image tool ().,2023-01-03
0,Scientific Data,41597,10.1038/s41597-022-01850-0,City-scale holographic traffic flow data based on vehicular trajectory resampling,25,1,2023,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR25', '#text': '25'}}, 'xref': {'@rid': 'Fig1', '@ref-type': 'fig', '#text': '1'}, '#text': 'To further describe the details of data processing in our method, we also provide code and instructions for reproducing the presented results. In general, files that end with “.py” are supporting python module files, other files with “.ipynb” are written as Jupyter Notebook instruction, and the files under the folder “measurement” are the source code of the resampling software. The instruction files demonstrate the whole data processing workflow in Fig.\xa0, including trip measurement, trajectory reconstruction, virtual traffic flow detection, and data validation. These files can be used to better understand the modeling and validation steps.'}, 'This study proposes a resampling method of vehicular trajectories using the LPR data. A city-scale holographic unbiased trajectories dataset is reconstructed. Then it is validated by the consistency with other data sources on travel time results and demonstrated with the macroscopic characteristics of the fundamental diagram. The correlative coefficient of travel time is about 0.688 to 0.749. Moreover, with the anonymous interactive measurement, users can acquire multiple traffic data from the individual level without the risk of personal information abuse. This dataset and the tool could support relative research goals such as data fusion, patterns of mobility recognition, and sensor network optimization.']",2023-01-25
0,Scientific Data,41597,10.1038/s41597-023-01929-2,A 1.2 Billion Pixel Human-Labeled Dataset for Data-Driven Classification of Coastal Environments,20,1,2023,https://github.com/CoastTrain/CoastTrainMetaPlots; https://github.com/kvos/CoastSat; https://github.com/giswqs/geemap; https://coasttrain.github.io/CoastTrain/,"All the figures presented in this manuscript may be generated using computational notebooks provided (). Utilities for npz file variable extraction and class remapping are provided in the Doodler and Segmentation Gym software packages. All labels were created with Doodler. Imagery was downloaded using CoastSat () and Geemap () functionality. For more information, please see the Coast Train project website ().",2023-01-20
0,Scientific Data,41597,10.1038/s41597-023-01958-x,Annotated Datasets of Oil Palm Fruit Bunch Piles for Ripeness Grading Using Deep Learning,4,2,2023,,"[{'ext-link': {'@xlink:href': 'https://www.videolan.org/index.id.html', '@ext-link-type': 'uri', '#text': 'https://www.videolan.org/index.id.html'}, '#text': 'The software used to process the dataset consists of software for converting video data into a collection of images using VLC ().'}, {'ext-link': {'@xlink:href': 'https://github.com/darkpgmr/DarkLabel', '@ext-link-type': 'uri', '#text': 'https://github.com/darkpgmr/DarkLabel'}, '#text': 'Software used for labelling and making image bounding boxes using Darklabel is provided by .'}, {'ext-link': {'@xlink:href': 'https://roboflow.com/', '@ext-link-type': 'uri', '#text': 'https://roboflow.com/'}, '#text': 'Software used for converting labelled data into data that is ready for input in processed modelling with a deep learning is the Roboflow () and the software used for data validation is the Python program.'}]",2023-02-04
0,Scientific Data,41597,10.1038/s41597-022-01867-5,DeepLontar dataset for handwritten Balinese character detection and syllable recognition on Lontar manuscript,10,12,2022,,The images data are available at Figshare repository and data augmentation code are available using OpenCV library. Data annotation tool using LabelImg is available online.,2022-12-10
0,Scientific Data,41597,10.1038/s41597-023-01959-w,Bioclimatic atlas of the terrestrial Arctic,19,1,2023,https://github.com/fmidev/resiclim-climateatlas,The Python codes needed to reproduce the dataset are available from Github: .,2023-01-19
0,Scientific Data,41597,10.1038/s41597-023-01988-5,High-throughput computation of Raman spectra from first principles,8,2,2023,,"VASP used in all DFT calculations is a proprietary software. For the database, dimensionality analysis, and web app, we used Atomic Simulation Environment (ASE) and Atomic Simulation Recipes (ASR), both released under GNU Lesser General Public License (LGPL). Phonopy used in calculating the eigenvectors and performing symmetry analysis is released under New Berkeley Software Distribution (BSD) License. The workflow is defined as a part of Atomate code package with FireWorks for defining, managing, and executing jobs which both are released under a modified BSD license and free to the public. Pymatgen (Python Materials Genomics) used for producing inputs parameters and custodian for performing error checking are both open-source packages under Massachusetts Institute of Technology (MIT) license. To store results and task parameters, MongoDB NoSQL database was used with the Server Side Public License (SSPL). All the information for prescreening and phonon calculation extracted from Phonon Database and from Materials project are both released under Creative Commons Attribution 4.0 International License. Fitting analysis of the experimental spectra was performed by Least-Squares Minimization fitting (LMfit) python package released under New Berkeley Software Distribution (BSD) license.",2023-02-08
0,Scientific Data,41597,10.1038/s41597-022-01908-z,A large dataset of scientific text reuse in Open-Access publications,26,1,2023,https://github.com/webis-de/scidata22-stereo-scientific-text-reuse,"The complete source code used for candidate retrieval and text alignment is openly accessible and permanently available on GitHub (). The data processing pipeline is written in Python 3.7, utilizing the pyspark framework. The compute cluster on which we carried out the data processing and our experiments run Spark Version 2.4.8. The text alignment component is written in Go 1.16 and can be used as a standalone application. Detailed documentation about each pipeline component, recommendations for compute resources, and suggestions for parameter choices are distributed alongside the code to facilitate code reuse.",2023-01-26
0,Scientific Data,41597,10.1038/s41597-023-02044-y,"LocalView, a database of public meetings for the study of local politics and policy-making in the United States",15,3,2023,https://doi.org/10.7910/DVN/NJTBEM; https://doi.org/10.7910/DVN/KHUXIN; https://localview.net,"The  dataset is publicly available at . Code to replicate the main and supplementary analyses in this paper is available at . More information, including a codebook and related research, is linked on our companion website at .",2023-03-15
0,Scientific Data,41597,10.1038/s41597-023-01930-9,Mobility recorded by wearable devices and gold standards: the Mobilise-D procedure for data standardization,19,1,2023,https://doi.org/10.5281/zenodo.7185429,"[{'ext-link': {'@xlink:href': 'https://github.com/luca-palmerini/Procedure-wearable-data-standardization-Mobilise-D', '@ext-link-type': 'uri', '#text': 'https://github.com/luca-palmerini/Procedure-wearable-data-standardization-Mobilise-D'}, '#text': 'The presented data structure and the related example subjects were created in MATLAB R2021b. Example of the data structures in Matlab are available (see Data Availability Section). To illustrate how to standardize raw data, we provide the Matlab code to standardize the raw data of the example subject from the ICICLE dataset (see Data Availability). The code is in the “MATLAB code for standardization” folder in the following GitHub repository (release version v1.0.0 upon article submission): .'}, {'italic': ['data.mat', 'infoforalgo.mat'], '#text': 'The code was used to obtain the standardized  and  of the corresponding example subject. In the same repository, in the “Python code for access” and in the “R code for access”, we provide code to access the standardized data with Python and R programming languages, respectively.'}]",2023-01-19
0,Scientific Data,41597,10.1038/s41597-023-02016-2,Annotated computed tomography coronary angiogram images and associated data of normal and diseased arteries,10,3,2023,https://github.com/Ramtingh/ASOCADataDescription,"The code for creation of this dataset, usage examples and evaluation code used in the challenge is available on GitHub (). Figures – were created with data included in the dataset. A copy of the raw data used is included in the repository under the corresponding folder to maker recreating these figures easier. 3D Slicer (version 4.3) was used in the preparation of the dataset and Figs.  and . Vascular Modelling Tool Kit (version 1.4) was used to calculate centerlines and generate Fig. .",2023-03-10
0,Scientific Data,41597,10.1038/s41597-022-01909-y,Observation based climatology Martian atmospheric waves perturbation Datasets,3,1,2023,,,2023-01-03
0,Scientific Data,41597,10.1038/s41597-023-02045-x,IceLines – A new data set of Antarctic ice shelf front positions,15,3,2023,https://github.com/khdlr/HED-UNet; https://download.geoservice.dlr.de/icelines/files/icelines_auxiliary_v1.zip,"The processing of IceLines has been carried out in the Calvalus system and GPU-cluster available at DLR’s Earth Observation Center by means of proprietary software and dedicated Python (v3.6 and v3.7) scripts. Given the use of proprietary tools, the implemented processing pipeline cannot be openly released to the public. However, all processing steps can be accessed and reproduced as follows: The pre-processing of the Sentinel-1 imagery can be replicated with the open source ESA SNAP Toolbox 8.0 and the processing steps described in Fig. . The code of the HED-Unet based on Pytorch (v1.7) is available at  and the final post-processing script (Python v3.7) is available at . Additionally, this folder includes data used for the accuracy assessment, a Python script for bulk download (bulk-download-icelines.py) and an exemplary script (‘display-icelines-gee.js’) to display IceLines data with the corresponding Sentinel-1 scenes in Google Earth Engine.",2023-03-15
0,Scientific Data,41597,10.1038/s41597-023-01960-3,Building a knowledge graph to enable precision medicine,2,2,2023,https://zitniklab.hms.harvard.edu/projects/PrimeKG; https://github.com/mims-harvard/PrimeKG; Harvard Dataverse; https://doi.org/10.7910/DVN/IXA7BM,"The PrimeKG’s project website is at . The code to reproduce results, together with documentation and tutorials, is available in PrimeKG’s GitHub repository at . In addition, the repository contains information and Python scripts to build new versions of PrimeKG as the underlying primary resources get updated and new data become available. PrimeKG data resource is hosted on  under a persistent identifier . We have deposited the knowledge graph and all relevant intermediate files at this repository.",2023-02-02
0,Scientific Data,41597,10.1038/s41597-022-01784-7,The Health Gym: synthetic health-related datasets for the development of reinforcement learning algorithms,11,11,2022,,"[{'ext-link': {'@xlink:href': 'https://github.com/Nic5472K/ScientificData2021_HealthGym', '@ext-link-type': 'uri', '#text': 'https://github.com/Nic5472K/ScientificData2021_HealthGym'}, '#text': 'The software code related to the Health Gym project is publicly available at .'}, {'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR90', '#text': '90'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR91', '#text': '91'}}, {'xref': [{'@ref-type': 'bibr', '@rid': 'CR21', '#text': '21'}, {'@ref-type': 'bibr', '@rid': 'CR22', '#text': '22'}], '#text': ','}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR23', '#text': '23'}}], 'ext-link': [{'@xlink:href': 'https://physionet.org/content/mimiciii/1.4/', '@ext-link-type': 'uri', '#text': 'https://physionet.org/content/mimiciii/1.4/'}, {'@xlink:href': 'http://engine.euresist.org/database/', '@ext-link-type': 'uri', '#text': 'http://engine.euresist.org/database/'}], '#text': 'Our code is mainly written in Python using the PyTorch package for deep learning. In order to replicate our results, users will need access to the MIMIC-III and EuResist databases. MIMIC-III is a restricted-access resource; and users must complete the data use agreements on PhysioNet (see: ). The EuResist Integrated DataBase (EIDB) can be accessed for scientific studies once a proposal for analysis has been approved by EuResist’s Scientific Board (see: ).'}, {'italic': 'et al', 'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR10', '#text': '10'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR92', '#text': '92'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR93', '#text': '93'}}, {'xref': [{'@ref-type': 'bibr', '@rid': 'CR94', '#text': '94'}, {'@ref-type': 'bibr', '@rid': 'CR95', '#text': '95'}, {'@ref-type': 'bibr', '@rid': 'CR96', '#text': '96'}, {'@ref-type': 'bibr', '@rid': 'CR97', '#text': '97'}], '#text': ', , –'}], 'ext-link': {'@xlink:href': 'https://gitlab.doc.ic.ac.uk/AIClinician/AIClinician/-/tree/master/', '@ext-link-type': 'uri', '#text': 'https://gitlab.doc.ic.ac.uk/AIClinician/AIClinician/-/tree/master/'}, '#text': 'Additional code for the data pre-processing for acute hypotension and sepsis can be found in the repository of Komorowski . at . This includes a combination of code in SQL, Matlab, Python, and their extension packages. For more details, see explanations and usages in the Supplementary Materials.'}]",2022-11-11
0,Scientific Data,41597,10.1038/s41597-023-01932-7,A Non-Laboratory Gait Dataset of Full Body Kinematics and Egocentric Vision,12,1,2023,https://github.com/abs711/The-way-of-the-future,"We provide an example python script for loading the processed motion capture and vision data, named ‘main.py’ in the directory ‘data_loading_example’ on the Github repository. In addition we provide scripts for synchronization and frame-dropping, and examples of loading into pytorch machine learning pipeline. All code is available on ().",2023-01-12
0,Scientific Data,41597,10.1038/s41597-022-01756-x,Longitudinal data collection to follow social network and language development dynamics at preschool,22,12,2022,,"The program codes for data cleaning and temporal network reconstruction are shared along the dataset in an open repository. The codes have been developed in Python language using only standard or open licensed packages, and they are shared as iPython notebooks at.",2022-12-22
0,Scientific Data,41597,10.1038/s41597-022-01841-1,Construction motion data library: an integrated motion dataset for on-site activity recognition,26,11,2022,,"[{'ext-link': {'@xlink:href': 'https://github.com/lawrennd/mocap', '@ext-link-type': 'uri', '#text': 'https://github.com/lawrennd/mocap'}, '#text': 'This study utilized Mathwork Matlab 2020a to parse and export the ASF/AMC and BVH files. The open-source code used for parsing these files can be obtained from .'}, {'ext-link': [{'@xlink:href': 'https://github.com/HW140701/VideoTo3dPoseAndBvh', '@ext-link-type': 'uri', '#text': 'https://github.com/HW140701/VideoTo3dPoseAndBvh'}, {'@xlink:href': 'https://github.com/YUANYUAN2222/GIT_json_to_BVH', '@ext-link-type': 'uri', '#text': 'https://github.com/YUANYUAN2222/GIT_json_to_BVH'}, {'@xlink:href': 'https://github.com/YUANYUAN2222/Integrated-public-3D-skeleton-form-CML-library', '@ext-link-type': 'uri', '#text': 'https://github.com/YUANYUAN2222/Integrated-public-3D-skeleton-form-CML-library'}], '#text': 'This study utilized Python 3.7.6 and extended a 17-joint BVH conversion package, video-to-pose3D (), to generate BVH files. The newly developed package can transform 15 or 20-joint models’ JSON files into BVH files. The developed code can be accessed with the following URL: . Meanwhile, the code could be used to retag and process different datasets (i.e., Resampling and Skeletal structure alignment) is made public on the GitHub (), which allow all readers and potential users to process the source dataset by themselves.'}]",2022-11-26
0,Scientific Data,41597,10.1038/s41597-022-01870-w,Transition1x - a dataset for building generalizable reactive machine learning potentials,24,12,2022,,"[{'ext-link': [{'@xlink:href': 'https://gitlab.com/matschreiner/Transition1x', '@ext-link-type': 'uri', '#text': 'https://gitlab.com/matschreiner/Transition1x'}, {'@xlink:href': 'https://gitlab.com/matschreiner/QM9x', '@ext-link-type': 'uri', '#text': 'https://gitlab.com/matschreiner/QM9x'}], '#text': 'There are download scripts and data loaders available in the repositories  and . See the repositories and README for examples and explanations of how to use the scripts and datasets.'}, {'monospace': ['scripts/neb.py', 'scripts/combine_dbs.py'], '#text': 'All electronic structure calculations were computed with the ORCA electronic structure packages, version 5.0.2. All NEB calculations were computed with ASE version 3.22.1. Scripts for calculating, gathering, and filtering data can be found in the Transition1x repository.  takes reactant, product, output directory, and various optional arguments and runs NEB on the reaction while saving all intermediate calculations in an ASE database in the specified output directory.  takes an output path for the HDF5 file and a JSON\xa0list of all output directories produced by running the previous script and combines them in the HDF5 file as described in the paper. See the repository for how to install, specific commands, options, and further documentation.'}]",2022-12-24
0,Scientific Data,41597,10.1038/s41597-023-01991-w,A 21-year dataset (2000–2020) of gap-free global daily surface soil moisture at 1-km grid resolution,15,3,2023,https://github.com/zhengchaolei/GlobalSSMGapfillDownscaling.git,The codes used in this study will be available at  after this work is accepted.,2023-03-15
0,Scientific Data,41597,10.1038/s41597-022-01813-5,COVID-19 in Switzerland real-time epidemiological analyses powered by EpiGraphHub,17,11,2022,github.com/thegraphnetwork/COVID-CH-dashboard; github.com/thegraphnetwork/epigraphhub_py; epigraphhub-libraries.readthedocs.io,"The source code for all the analyses presented in this paper, and also the web dashboard, can be found on these GitHub repositories:  and . The first one for the web dashboard, and the second for the all the data collection and analyses. The documentation can be found on this link: .",2022-11-17
0,Scientific Data,41597,10.1038/s41597-022-01842-0,"HeliantHOME, a public and centralized database of phenotypic sunflower data",30,11,2022,https://github.com/grimmlab/HeliantHome; http://www.helianthome.org,All code for the web server backend and frontend are publicly available for download on GitHub: .The web-application can be accessed via: .,2022-11-30
0,Scientific Data,41597,10.1038/s41597-023-01934-5,Aerodynamic characterisation of porous fairings: pressure drop and Laser Doppler Velocimetry measurements,19,1,2023,,"In order to read the “*.bin” files, a python code is provided in the zenodo deposit. The program “PSD_estimation.py” gives some routines to read and calculate the PSD of a given velocity component at a given point. Python required version is written as a comment in the python code.",2023-01-19
0,Scientific Data,41597,10.1038/s41597-023-01963-0,Near-real-time global gridded daily CO2 emissions 2021,2,2,2023,https://github.com/xinyudou820/GRACED2021,"Python code for producing, reading and plotting data in the dataset is provided at .",2023-02-02
0,Scientific Data,41597,10.1038/s41597-023-01992-9,Harmonized and Open Energy Dataset for Modeling a Highly Renewable Brazilian Power System,22,2,2023,,"[{'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR55', '#text': '55'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR17', '#text': '17'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR17', '#text': '17'}}], '#text': 'Direct use of our provided datasets is available on Zenodo The source code used for data collection, processing and analysis is also\xa0on Gitlab. The data processing is performed using Python 3.9 and the necessary toolboxes, such as Pandas and Geopandas. The data collection process is fully described in the paper. By open-sourcing the code,\xa0we aimi to\xa0provide the most relevant information for integrating the dataset\xa0into energy system models. Although step-by-step tutorials could also be helpful for this purpose. However, we think such information is best conveyed through the source codes.'}, 'We regret that we cannot provide scripts for the vRES potential data. The data of vRES potential is created by the EnDAT framework, which is in the process of being open-sourced and only available within DLR. For those data for which the license is “citation”, we have been permitted to redistribute the data after modifying it for this paper. We do not, however, have permission to publish their original data.', 'We will continue to update this dataset and apply this dataset to further energy system studies. We encourage readers to contribute to fill in the gaps and improve the hypotheses of this dataset mentioned in the paper.']",2023-02-22
0,Scientific Data,41597,10.1038/s41597-022-01872-8,"A massive dataset of the NeuroCognitive Performance Test, a web-based cognitive assessment",8,12,2022,https://github.com/pauljaffe/lumos-ncpt-tools/tree/v1.1.0,All of the code used to generate the figures and perform the analyses are included with the public lumos-ncpt-tools repository described above: . See the README file for instructions on how to reproduce the figures and analyses. The software underlying the cognitive tasks themselves is proprietary and consequently cannot be shared at this time.,2022-12-08
0,Scientific Data,41597,10.1038/s41597-023-01964-z,Real-world measurements of ground reaction forces of normal gait of young adults wearing various footwear,30,1,2023,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR22', '#text': '22'}}, '#text': 'The source code of our custom programs in the Matlab and Python languages is publicly available at figshare, under the link to the dataset. These programs allow users to read data from files, to visualize measurements and patterns, to extract samples related to the stance phase of gait, and to perform exploratory analyses of data. The source code has been designed so as to form a programming library, which could be used to develop more advanced applications related to signal processing and pattern recognition.'}, 'In particular, the Matlab programs are as follows:', {'monospace': 'runMeasurementDemo.m', '#text': '1. :'}, 'Demonstrates how to use the remaining programs to access and validate raw measurements and participant data.', {'monospace': 'runPatternDemo.m', '#text': '2. :'}, 'Demonstrates how to use the remaining programs to access and validate processed data and participant data.', {'monospace': 'loadParticipants.m', '#text': '3. :'}, 'Loads participant data from a CSV file into a structure of arrays. Each array describes one feature of all participants.', {'monospace': 'summarizeParticipants.m', '#text': '4. :'}, 'Shows histograms and statistics of distributions of ages, weights, and heights of participants.', {'monospace': 'loadMeasurements.m', '#text': '5. :'}, 'Loads raw data of gait measurements from CSV files into an array of structures of vectors. Each structure describes all signals of one measurement.', {'monospace': 'showMeasurement.m', '#text': '6. :'}, {'xref': {'@rid': 'Fig2', '@ref-type': 'fig', '#text': '2'}, '#text': 'Plots all signals of a given measurement, pointing out their fragments related to the stance phase of gait, between the foot-strike and foot-off events. Creates windows like that in Fig.\xa0.'}, {'monospace': 'findStance.m', '#text': '7. :'}, 'Determines initial estimates of indexes of raw samples related to the foot-strike and foot-off events. Estimation is based on analyzing the temporal slope of the vertical GRF.', {'monospace': 'cutStance.m', '#text': '8. :'}, 'Extracts the stance-related fragment (wave) of a signal of the vertical GRF. Can use extrapolation to refine indexes and values of samples related to the foot-strike and foot-off events, so as to smooth the endings of the fragment.', {'monospace': 'extractStancesFromMeasurements.m', '#text': '9. :'}, 'Cuts out samples between the foot-strike and foot-off events from raw GRF signals.', {'monospace': 'summarizeStances.m', '#text': '10. :'}, 'Shows histograms, maximums, and minimums of distributions of indexes and values of boundary samples of stance-related fragments of GRF signals. Allows one to identify trends and potential outliers among measurements.', {'monospace': 'showStances.m', '#text': '11. :'}, 'Plots stance-related fragments of GRF signals.', {'monospace': 'checkLeftLegFirst.m', '#text': '12. :'}, 'Checks whether the first of two series of stance-related samples of the medio-lateral GRF is related to the left leg, while the second is related to the right leg.', {'monospace': 'convertStancesIntoPatterns.m', '#text': '13. :'}, 'Time-normalizes stance-related fragments of GRF signals and assigns them to legs. Forms processed data so that they can be directly used as patterns in experiments in recognizing persons and shoes.', {'monospace': 'savePatterns.m', '#text': '14. :'}, 'Saves processed data into CSV files.', {'monospace': 'loadPatterns.m', '#text': '15. :'}, 'Loads processed data from CSV files into a structure of arrays. Each array describes values of one feature of all files.', {'monospace': 'showPatterns.m', '#text': '16. :'}, 'Plots processed data of one or many measurements. Allows one to detect potential outliers among measurements.', {'monospace': 'summarizePatterns.m', '#text': '17. :'}, 'Shows histograms, maximums, and minimums related to processed data. Allows one to identify trends and potential outliers.', 'Each file begins with extensive comments that explain its purpose and way of usage.', {'monospace': 'gait.py', '#text': 'The Python programs have the same names and functionalities as the Matlab ones, and are as well commented, so it would make little sense to list and describe them herein. Only two remarks on them seem to be necessary. Firstly, the programs have been combined into a single module, the  file, so users that would like to develop applications based on our library can import all functions by writing one, simple statement. Secondly, some of our programs show plots in windows, so users should have the Tk GUI back-end installed and associated with the matplotlib library.'}]",2023-01-30
0,Scientific Data,41597,10.1038/s41597-022-01844-y,A database of water chemistry in eastern Siberian rivers,30,11,2022,,"[{'ext-link': [{'@xlink:href': 'http://www.cru.uea.ac.uk/', '@ext-link-type': 'uri', '#text': 'http://www.cru.uea.ac.uk/'}, {'@xlink:href': 'https://climatedataguide.ucar.edu/climate-data/gpcc-global-precipitation-climatology-centre', '@ext-link-type': 'uri', '#text': 'https://climatedataguide.ucar.edu/climate-data/gpcc-global-precipitation-climatology-centre'}], 'bold': {'italic': 'code'}, '#text': 'Within the repository, we also provide code for extracting climate data of each subbasin from the Climatic Research Unit at the University of East Anglia () and the Global Precipitation Climatology Centre () in the  folder.'}, {'bold': [{'italic': 'shp'}, {'italic': 'shp'}], '#text': '♦ The  folder contains 218 subbasin boundary  files.'}, {'bold': [{'italic': 'nc'}, {'italic': 'yearmean_cru_ts4.04.1901.2019.tmp.dat.nc'}, {'italic': 'yearmean_GPCC_1901–2019_05.nc'}, {'italic': 'yearmean_cru_ts4.04.1901.2019.pet.dat.nc'}], '#text': '♦ The downloaded input data are stored in 3  files with annual average temperature, annual precipitation, and annual potential evaporation data (1901–2019) in , , and , respectively.'}, {'bold': [{'italic': 'extract_tmp-nc_to_xlsx.py'}, {'italic': 'extract_precip-nc_to_xlsx.py'}, 'and', {'italic': 'extract_pet-nc_to_xlsx.py'}], '#text': '♦ The code for extracting data from the .nc file to the .xlsx file was written in Python, and , ,  were used to extract temperature, precipitation, and evaporation data, respectively.'}, {'bold': [{'italic': 'multi_yr_tmp_subbasins6_1901–2019.xlsx'}, {'italic': 'multi_yr_precip_subbasins6_1901–2019.xlsx'}, {'italic': 'multi_yr_pet_subbasins6_1901–2019.xlsx'}, {'italic': 'tmp_clip_sub6'}, {'italic': 'precip_clip_sub6'}, {'italic': 'PET_clip_sub6'}], '#text': '♦ The output data will be stored in .xlsx format (, ,  for temperature, precipitation, and potential evaporation data, respectively) in folders ,  and  after running the code.'}]",2022-11-30
0,Scientific Data,41597,10.1038/s41597-022-01900-7,MOSAiC-ACA and AFLUX - Arctic airborne campaigns characterizing the exit area of MOSAiC,29,12,2022,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR114', '#text': '114'}}, '#text': 'Each instrument is controlled either by code developed by the institution operating it or by code developed by the manufacturer and therefore often closed source or not even freely available and bundled with the instrument. Code used in the post-processing of the data has been developed by each institution, compiled in a package, and made available.'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR104', '#text': '104'}}, '#text': 'For the basic acquisition system of the Polar 5 aircraft and the KT-19, Werum Software & Systems AG has developed the software to communicate with the instruments and store the data. MiRAC-A radar, MiRAC-P, and HATPRO have been operated with software of the manufacturer Radiometer Physics GmbH. A LabView program by AWI controls AMALi and Nikon. The cloud particle probes CAS, CDP, CIP, and PIP are operated by a software from the manufacturer Droplet Measurement Technologies (DMT), where as for the 2DS it is Spec. Inc. and a LabView based program for the Polar Nephelometer. The spectral imager data acquisition software was developed by the manufacturer Specim, Spectral Imaging Ltd. Data evaluation was performed using the ENVI image analysis software. SMART is controlled by a LabView based software developed by Enviscope GmbH. The dropsonde system AVAPS has been post-processed with the Atmospheric Sounding Processing ENvironment (ASPEN, Version 3.4.4), which is publicly available.'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR23', '#text': '23'}}, '#text': 'The ac3airborne package and tools developed within the project are written in python, open source, and publicly available on github.'}]",2022-12-29
0,Scientific Data,41597,10.1038/s41597-022-01730-7,A resource for automated search and collation of geochemical datasets from journal supplements,25,11,2022,,"[{'ext-link': {'@xlink:href': 'https://github.com/erinlmartin/figshare_geoscrape.git', '@ext-link-type': 'uri', '#text': 'https://github.com/erinlmartin/figshare_geoscrape.git'}, '#text': 'The web scraping and data-basing code used in this paper are available from . Instructions for use are available with the code.'}, {'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR22', '#text': '22'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR35', '#text': '35'}}], 'ext-link': [{'@xlink:href': '10.25625/FWQ7DT', '@ext-link-type': 'doi', '#text': 'https://doi.org/10.25625/FWQ7DT'}, {'@xlink:href': '10.6084/m9.figshare.16870603.v3', '@ext-link-type': 'doi', '#text': 'https://doi.org/10.6084/m9.figshare.16870603.v3'}], '#text': 'All data presented in this study are previously published and available on the Figshare repository. DOIs for the data are included within the GEOSCRAPE database hosted with GEOROC at: . This database will be updated by ELM until 2027. Table S1, SQLite template files and metadata table available from Figshare: .'}]",2022-11-25
0,Scientific Data,41597,10.1038/s41597-023-01936-3,Conductivity experiments for electrolyte formulations and their automated analysis,19,1,2023,,"[{'italic': ['MADAP', 'MADAP'], 'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR15', '#text': '15'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR15', '#text': '15'}}], 'ext-link': [{'@xlink:href': 'https://github.com/fuzhanrahmanian/MADAP', '@ext-link-type': 'uri', '#text': 'https://github.com/fuzhanrahmanian/MADAP'}, {'@xlink:href': 'https://fuzhanrahmanian.github.io/MADAP/', '@ext-link-type': 'uri', '#text': 'https://fuzhanrahmanian.github.io/MADAP/'}], 'monospace': 'pip install madap', '#text': 'The code of the  package is publicly available on  and the documentation can be found in . A stand-alone windows executable can be downloaded from the GitHub repository as well. Furthermore,  can be installed by running .'}, {'italic': ['MADAP', 'MADAP'], 'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR15', '#text': '15'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR15', '#text': '15'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR39', '#text': '39'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR40', '#text': '40'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR38', '#text': '38'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR22', '#text': '22'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR26', '#text': '26'}}], '#text': 'The analysis results presented in this article are generated using  version 1.0. Contributions are welcome, but should follow the common guidelines for group software development, which can be found in the CONTRIBUTION section of the  the repository. The code is developed for the Python version 3.9 and above and should use the following packages and versions: attrs >\u2009=\u200921.4.0, matplotlib >\u2009=\u20093.5.3, numpy >\u2009=\u20091.22.4, pandas >\u2009=\u20091.4.2, pytest >\u2009=\u20097.1.2, scikit_learn >\u2009=\u20091.1.2, and impedance >\u2009=\u20091.4.1. For running the GUI, PySimpleGUI >\u2009=\u20094.60.3 is required additionally.'}]",2023-01-19
0,Scientific Data,41597,10.1038/s41597-023-01965-y,An open database on global coal and metal mine production,24,1,2023,www.github.com/fineprint-global/compilation_mining_data,"The code used to derive the final data product from the raw input data file is available under the licence GNU General Public License v3.0 (GPL-v3) from the GitHub repository . All processing scripts were written in R, and geoprocessing was conducted with the R package sf.",2023-01-24
0,Scientific Data,41597,10.1038/s41597-023-01994-7,Trait biases in microbial reference genomes,9,2,2023,,"All software used in this paper have been described in the Methods and are freely available online. A copy of our workflow (bash, python, R code files) is also available at Figshare.",2023-02-09
0,Scientific Data,41597,10.1038/s41597-022-01914-1,High resolution synthetic residential energy use profiles for the United States,6,2,2023,,"Programming languages such as Python 3 and Java 8 are used for modeling, analyzing, and developing the framework. The code is deposited in the repository alongwith the dataset.",2023-02-06
0,Scientific Data,41597,10.1038/s41597-023-01966-x,"Eco-ISEA3H, a machine learning ready spatial database for ecometric and species distribution modeling",7,2,2023,https://github.com/mechenich/eco-isea3h,"R and Python code developed for the Eco-ISEA3H database was committed to a public GitHub repository, and may be accessed via the following URL: .",2023-02-07
0,Scientific Data,41597,10.1038/s41597-023-02022-4,A Global Database of Soil Plant Available Phosphorus,7,3,2023,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR27', '#text': '27'}}, '#text': 'The following code and outputs are available on-line:'}, '• The R code and outputs of the efficacy and performance of the models employed to estimate the global Olsen phosphorus concentration from the predictor variables.', '• Python code describing the filtering and post-processing steps involved in the use and analysis of the raw data and predictor variables to estimate global soil Olsen phosphorus concentration values and stocks.']",2023-03-07
0,Scientific Data,41597,10.1038/s41597-022-01875-5,ISLES 2022: A multi-center magnetic resonance imaging stroke lesion segmentation dataset,10,12,2022,https://github.com/ezequieldlrosa/isles22; https://github.com/ezequieldlrosa/isles22_docker_evaluation,"In order to facilitate future users of this dataset to get familiarized with the images, we have released the following ISLES 2022 Github repository: . The repository contains scripts to read the images, visualize them, and to quantify the algorithmic results performance with the same metrics used in the challenge to rank participants. Besides, we have also released the image container that is used during the challenge in order to evaluate the participants’ algorithm submissions ().",2022-12-10
0,Scientific Data,41597,10.1038/s41597-023-01967-w,An RNA-seq time series of the medaka pituitary gland during sexual maturation,31,1,2023,,R code used in the analyses is included in the NMBU Open Research Data record.,2023-01-31
0,Scientific Data,41597,10.1038/s41597-023-01996-5,Genome-wide hydroxymethylation profiles in liver of female Nile tilapia with distinct growth performance,1,3,2023,https://github.com/IoannisKonstantinidis/RRHP_Code,Supplementary files 5 and 6 were deposited in GitHub on 2022/12/20. They can be found at the URL: .,2023-03-01
0,Scientific Data,41597,10.1038/s41597-022-01889-z,Satellite-derived multivariate world-wide lake physical variable timeseries for climate studies,14,1,2023,https://climate.esa.int/en/explore/analyse-climate-data/,An option to visualise the dataset is to use the software Climate Analysis Toolbox (Cate) see ).,2023-01-14
0,Scientific Data,41597,10.1038/s41597-023-02023-3,"PLEIAData: consumption, HVAC, temperature, weather and motion sensor data for smart buildings applications",3,3,2023,,The code implementation was done in Python3 (3.8.3) using Jupyter notebook. The script that describes the step by step process followed is available at.,2023-03-03
0,Scientific Data,41597,10.1038/s41597-022-01916-z,Citizen science helps in the study of fungal diversity in New Jersey,4,1,2023,www.gbif.org/ipt,"Figures were prepared using R Statistical Software (v4.1.2; R Core Team 2021), packages  and. The dataset was shared via GBIF.org using Integrated Publishing Toolkit (IPT, ). No original code was created to generate the dataset.",2023-01-04
0,Scientific Data,41597,10.1038/s41597-022-01818-0,Building the European Social Innovation Database with Natural Language Processing and Machine Learning,12,11,2022,https://github.com/EuropeanSocialInnovationDatabase/ESID_V2,All the code is freely available in Github at .,2022-11-12
0,Scientific Data,41597,10.1038/s41597-022-01762-z,Benchmarking second and third-generation sequencing platforms for microbial metagenomics,11,11,2022,https://forgemia.inra.fr/metagenopolis/benchmark_mock,"All reference genomes and scripts for mapping, assembly, genome coverage estimation, subsampling and correlation calculations associated with tables and figures are available at .",2022-11-11
0,Scientific Data,41597,10.1038/s41597-023-01939-0,Dataset on child vaccination in Brazil from 1996 to 2021,11,1,2023,https://doi.org/10.7303/syn26453964,"We automated all data processing and curation in the free and open software R (4.2.1, current version for windows). The data resources described in this paper, including R codes, can be accessed with no restrictions on the Synapse repository at . Anyone can browse the content on the Synapse website, but you must register for an account using your email address to download the files and datasets. Please see Table  and references for details and links to the data resources.",2023-01-11
0,Scientific Data,41597,10.1038/s41597-023-01968-9,Developing a standardized but extendable framework to increase the findability of infectious disease datasets,23,2,2023,https://github.com/Hughes-Lab/niaid-schema-publication; https://doi.org/10.5281/zenodo.6816052; https://zenodo.org/record/681605292; https://crawler.biothings.io/; https://github.com/biothings/biothings.crawler; https://metadataplus.biothings.io/; https://github.com/biothings/metadataplus; https://github.com/biothings/discovery-app; https://github.com/Hughes-Lab/niaid-schema-publication/blob/main/figures-code/Table%203%20-%20Datasets%20by%20Grant.qmd,"Code used to analyze the prevalence of Schema.org properties and create figures is available at GitHub () and archived on Zenodo (): . Code used to harvest metadata via Metadata Crawler (, ) and Metadata Plus (, ) are available on GitHub, and the code to create the Data Discovery Engine, including the NIAID SysBio Dataset and ComputationalTool registration guides, is available on GitHub (). Code to dynamically generate File 5 is available at .",2023-02-23
0,Scientific Data,41597,10.1038/s41597-023-02024-2,A global synthesis of high-resolution stable isotope data from benthic foraminifera of the last deglaciation,10,3,2023,,All code used to generate the figures and analysis of this paper is available in the Zenodo repository.,2023-03-10
0,Scientific Data,41597,10.1038/s41597-022-01819-z,"Evolving collaboration, dependencies, and use in the Rust Open Source Software ecosystem",16,11,2022,https://github.com/wschuell/repo_datasets,Code to recreate the database is included in our Figshare upload and can also be found in a dedicated repository . The software is written in the Python programming language. The database can be created as either PostgreSQL or SQLite database. Version requirements are recorded in the project’s Readme file.,2022-11-16
0,Scientific Data,41597,10.1038/s41597-022-01848-8,China’s Gridded Manufacturing Dataset,2,12,2022,https://doi.org/10.6084/m9.figshare.19808407,The python manufacturing classification code and machine learning samples are available at .,2022-12-02
0,Scientific Data,41597,10.1038/s41597-022-01877-3,The MetroPT dataset for predictive maintenance,13,12,2022,,The raw data collected and stored in a MySQL database was then converted to CSV formats using custom scripts in Python 3.8 with the library pandas for data manipulation. All plots supporting the fault descriptions were performed using Plotly library.,2022-12-13
0,Scientific Data,41597,10.1038/s41597-023-01998-3,"WS22 database, Wigner Sampling and geometry interpolation for configurationally diverse molecular datasets﻿",15,2,2023,https://github.com/virtualzx-nad/geodesic-interpolate; https://github.com/maxjr82/QCDP; https://github.com/maxjr82/PCA-for-WS22,"All the data annotations with the chemical properties were obtained from density functional theory calculations performed with the Gaussian 09 program. The set of molecular geometries used as input for these calculations was generated using the Newton-X CS (version 2.2-B08) package for Wigner sampling together with a Python code for geometry interpolations [] as described in the Methods section. Both programs are open access. A custom Python script was written to extract the relevant information from the Gaussian 09 output files, and it is publicly available in the GitHub repository . Finally, the Python script to perform the dimension reduction of the molecular geometries with the PCA method is available to download from .",2023-02-15
0,Scientific Data,41597,10.1038/s41597-022-01918-x,A benchmark dataset for binary segmentation and quantification of dust emissions from unsealed roads,5,1,2023,https://github.com/RajithaRanasinghe/Automatic_Thresholding,All the Python scripts used to generate the secondary data (binary images by Otsu’s thresholding) are provided at .,2023-01-05
0,Scientific Data,41597,10.1038/s41597-022-01878-2,"CloudSEN12, a global dataset for semantic understanding of cloud and cloud shadow in Sentinel-2",24,12,2022,https://github.com/cloudsen12/,"The code to (1) create the raw CloudSEN12 imagery dataset, (2) download assets associated with each ROI, (3) create the manual annotations, (4) build and deploy cloudApp, (5) generate automatic cloud masking, (6) reproduce all the figures, (7) replicate the technical validation, (8) modify  Python package, and (9) train DL models are available in our GitHub organization .",2022-12-24
0,Scientific Data,41597,10.1038/s41597-022-01719-2,The Dresden Surgical Anatomy Dataset for Abdominal Organ Segmentation in Surgical Data Science,12,1,2023,https://gitlab.com/nct_tso_public/dsad; https://zenodo.org/record/6958337#.YvIsP3ZBxaQ,"The scripts for frame extraction, annotation merging, and statistical analysis, as well as the results of the statistical analysis are made public on  and via . All code is written in  and freely accessible.",2023-01-12
0,Scientific Data,41597,10.1038/s41597-023-01940-7,Chinese diabetes datasets for data-driven machine learning,19,1,2023,,"The code for the analysis of the datasets and the generation of the figures and tables can be accessed in the Figshare repository, which is a JUPYTER notebook named “data_analysis.ipynb”. The script can be executed with Python 3.6 and allows for reproducibility and code reuse.",2023-01-19
0,Scientific Data,41597,10.1038/s41597-022-01919-w,The global historical climate database HCLIM,19,1,2023,,"[{'ext-link': {'@xlink:href': 'https://github.com/elinlun/Hclim', '@ext-link-type': 'uri', '#text': 'https://github.com/elinlun/Hclim'}, '#text': 'R code used for formatting, quality control, removing duplicates, and breakpoint detection are publicly available under .'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR15', '#text': '15'}}, '#text': 'The data are available at PANGAEA.'}]",2023-01-19
0,Scientific Data,41597,10.1038/s41597-022-01879-1,An open time-series simulated dataset covering various accidents for nuclear power plants,13,12,2022,https://github.com/thu-inet/NuclearPowerPlantAccidentData/tree/main/Simulator/; https://github.com/thu-inet/NuclearPowerPlantAccidentData,These simulations were conducted using PCTRAN-PWR3LP (). The data processing step was performed using scripts written in the Python 3.10 programming language. More about this dataset can be found on the dataset’s GitHub page ().,2022-12-13
0,Scientific Data,41597,10.1038/s41597-023-01970-1,A high-resolution gridded grazing dataset of grassland ecosystem on the Qinghai–Tibet Plateau in 1982–2015,2,2,2023,https://github.com/nanmeng123456/Grazing-spatilization.git,"The code is fully operational under Python 3.6, and the Python scripts used to implement the gridded grazing dataset can be obtained from . Further questions can be directed to Nan Meng (nanmeng_st@rcees.ac.cn).",2023-02-02
0,Scientific Data,41597,10.1038/s41597-023-01942-5,A longitudinal microstructural MRI dataset in healthy C57Bl/6 mice at 9.4 Tesla,14,2,2023,https://doi.org/10.20383/103.0594; https://gitlab.com/cfmm/pipelines/mouse_dmri_MT_dicomTOscalarMaps; https://osf.io/5eusw/,"As mentioned previously, all code required to process dicoms to the final scalar maps is available: . The code is also available publicly through GitLab: . This includes a Snakemake pipeline, which includes FSL, MRtrix3, and ANTs commands, and MATLAB functions. The custom dMRI pulse sequences used in this work are available as binary methods: , and the source code is available upon reasonable request.",2023-02-14
0,Scientific Data,41597,10.1038/s41597-022-01893-3,A dataset to assess mobility changes in Chile following local quarantines,3,1,2023,https://raw.githubusercontent.com/MinCiencia/Datos-COVID19/master/output/producto33/IndiceDeMovilidad.csv; https://github.com/MinCiencia/Datos-COVID19/blob/master/output/producto29/Cuarentenas-Activas.csv,"The up-to-date data are available from the general repository of the Ministry of Science of Chile at:  (IM indeces), and  (quarantines). The code to download the up-to-date data automatically and to reproduce the analysis in our paper is available at.",2023-01-03
0,Scientific Data,41597,10.1038/s41597-022-01721-8,MedMNIST v2 - A large-scale lightweight benchmark for 2D and 3D biomedical image classification,19,1,2023,https://github.com/MedMNIST/MedMNIST; https://github.com/MedMNIST/experiments,The data API and evaluation script in Python is available at . The reproducible experiment codebase is available at .,2023-01-19
0,Scientific Data,41597,10.1038/s41597-023-02028-y,FAIRification of health-related data using semantic web technologies in the Swiss Personalized Health Network,10,3,2023,https://www.biomedit.ch/rdf/sphn-ontology/sphn/2022/2; https://git.dcc.sib.swiss/sphn-semantic-framework/sphn-ontology/-/tree/master/ontology; https://doi.org/10.5281/zenodo.7390281,No custom code was used to generate this work.,2023-03-10
0,Scientific Data,41597,10.1038/s41597-023-01972-z,Data Archive for the BRAIN Initiative (DABI),9,2,2023,https://doi.org/10.18120/sr2n-gz34; https://doi.org/10.18120/x7mj-am06,"['Open-source software was used for analysis, including the open-source H2O library for machine learning frameworks, Jupyter notebooks for python analysis, and R Analysis and Visualization of intracranial EEG (RAVE).', 'This project is supported by the NIH/NINDS under award number R24MH114796. Data used in the sample analyses presented were supported by the NIH/NINDS under award numbers UH3NS100553, R01NS119520 and U01NS098961 and the Michael J. Fox Foundation under grant number 15098.']",2023-02-09
0,Scientific Data,41597,10.1038/s41597-022-01881-7,The LUMIERE dataset: Longitudinal Glioblastoma MRI with expert RANO evaluation,15,12,2022,https://github.com/ysuter/gbm-data-longitudinal,The code used for processing this dataset is publicly available in our GitHub repository (). The Python and Bash scripts are available to reproduce and customize the extraction of radiomics features.,2022-12-15
0,Scientific Data,41597,10.1038/s41597-022-01824-2,LivWell: a sub-national Dataset on the Living Conditions of Women and their Well-being for 52 Countries,22,11,2022,https://gitlab.pik-potsdam.de/belmin/livwelldata-paper; https://gitlab.pik-potsdam.de/belmin/livwelldata,"The processing steps to obtain the dataset were carried out in  and  and are reproducible (except for one step of the harmonization of DHS regions and variables that had to be done manually). All the code is available on the git repository of this article: . The source code for the companion  package livwelldata is available on the git repository of the package: . The following  packages were central to the development of LivWell: , , ,  and .",2022-11-22
0,Scientific Data,41597,10.1038/s41597-023-01975-w,Caravan - A global community dataset for large-sample hydrology,31,1,2023,https://github.com/kratzert/Caravan/,The code that was used to produce the Caravan dataset is available at .,2023-01-31
0,Scientific Data,41597,10.1038/s41597-023-02002-8,Neonatal EEG graded for severity of background abnormalities in hypoxic-ischaemic encephalopathy,10,3,2023,,"['Custom code was not used to generate the data. EEG files were exported from proprietary format to EDF files using the associated EEG reviewing software for the NicoletOne and Neurofax EEG machines. Details on how to view the EEG data and import it into programming environments is described in the Usage Notes section.', {'xref': {'@rid': 'Fig5', '@ref-type': 'fig', '#text': '5e'}, 'ext-link': {'@xlink:href': 'https://github.com/otoolej/downsample_open_eeg', '@ext-link-type': 'uri', '#text': 'https://github.com/otoolej/downsample_open_eeg'}, '#text': 'To assist with computer-based analysis of the EEG, we provide freely-available code to downsample the EEG to a lower and uniform sampling rate. For quantitative or machine-learning analysis, the neonatal EEG is often downsampled to a lower sampling rate, as the majority of the power is typically below 10 to 20\u2009Hz. For example, Fig.\xa0 shows that 95% of spectral power is below 25\u2009Hz. The processing routines include an anti-aliasing filter before downsampling. Both Matlab and Python versions of the code are included at  (commit: 22e92db).'}]",2023-03-10
0,Scientific Data,41597,10.1038/s41597-022-01799-0,"High-resolution estimates of social distancing feasibility, mapped for urban areas in sub-Saharan Africa",18,11,2022,https://www.ecopiatech.com/; https://github.com/heatherchamberlain/SocDistIndex,"The code for data processing and analysis was written in Python (version 3.6.9), using ArcPy in an ArcGIS Notebook with ArcGIS Pro (version 2.5.1). The DigitizeAfrica building footprints used in creating the output index for urban areas in sub-Saharan Africa are available for humanitarian purposes on request from Ecopia (). Similar datasets for some countries are openly-available, such as Microsoft building footprints or Google Africa Open Buildings. The code used to create the spatial units and calculate the index values described in this paper is available to download from GitHub, in the following repository: .",2022-11-18
0,Scientific Data,41597,10.1038/s41597-022-01826-0,The PhanSST global database of Phanerozoic sea surface temperature proxy data,6,12,2022,https://github.com/EJJudd/SciDataSupplement,"Figures – were produced in Matlab. Example code and auxiliary functions to (1) reproduce Figs. – and (2) run the automated QC checks on the database are available on GitHub (). The paleocoordinates used to produce Figs. , were estimated using the plate model of Scotese and Wright, implemented in G-Plates (Version 2.2.0).",2022-12-06
0,Scientific Data,41597,10.1038/s41597-022-01855-9,A speech corpus of Quechua Collao for automatic dimensional emotion recognition,24,12,2022,https://github.com/qccData/qccCorpus,"Code and data splits for baseline algorithms are available at Github, in .",2022-12-24
0,Scientific Data,41597,10.1038/s41597-022-01884-4,The China plant trait database version 2,15,12,2022,https://github.com/lpice/code-CPTDv2-.git; https://bitbucket.org/labprentice/splash,"The R code for estimating photosynthetic capacities, calculating the timing and seasonality of precipitation, and extracting soil and vegetation information are available in the open GitHuB repository () The SPLASH code, in four programming languages (C++, FOR- TRAN, Python, and R), is available on an online repository under the GNU Lesser General Public License ()",2022-12-15
0,Scientific Data,41597,10.1038/s41597-022-01712-9,FAIR principles for AI models with a practical application for accelerated high energy diffraction microscopy,10,11,2022,,"The three AI models introduced in this article, along with the Jupyter notebook and scientific software needed to reproduce our results have been released through DLHub. Scientific software to train these models may be found in an open source GitHub repository.",2022-11-10
0,Scientific Data,41597,10.1038/s41597-022-01827-z,A comprehensive suite of earthquake catalogues for the 2016-2017 Central Italy seismic sequence,18,11,2022,,"[{'sup': [{'xref': [{'@ref-type': 'bibr', '@rid': 'CR31', '#text': '31'}, {'@ref-type': 'bibr', '@rid': 'CR32', '#text': '32'}], '#text': ','}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR40', '#text': '40'}}, {'xref': [{'@ref-type': 'bibr', '@rid': 'CR41', '#text': '41'}, {'@ref-type': 'bibr', '@rid': 'CR42', '#text': '42'}], '#text': ','}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR35', '#text': '35'}}, {'xref': [{'@ref-type': 'bibr', '@rid': 'CR36', '#text': '36'}, {'@ref-type': 'bibr', '@rid': 'CR38', '#text': '38'}], '#text': ','}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR14', '#text': '14'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR48', '#text': '48'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR49', '#text': '49'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR50', '#text': '50'}}], '#text': 'For generating the catalogues, the IpoP code, the Complete Automatic Seismic Processor (CASP) and RSNI-Picker2 are available upon request. All of the other codes are all open access: NonLinLoc software used for CAT1 and CAT3; HypoDD for CAT2, CAT4 and CAT5; PhaseNet picker, (REAL) package, Velest code and HypoInverse software used for generating the dataset of CAT5.'}, {'xref': [{'@rid': 'Tab1', '@ref-type': 'table', '#text': '1'}, {'@rid': 'Fig3', '@ref-type': 'fig', '#text': '3'}, {'@rid': 'Fig4', '@ref-type': 'fig', '#text': '4'}, {'@rid': 'Fig6', '@ref-type': 'fig', '#text': '6'}, {'@rid': 'Fig1', '@ref-type': 'fig', '#text': '1'}, {'@rid': 'Fig2', '@ref-type': 'fig', '#text': '2'}, {'@rid': 'Fig5', '@ref-type': 'fig', '#text': '5'}, {'@rid': 'Fig6', '@ref-type': 'fig', '#text': '6'}], 'sub': 'c', 'sup': ['Lilliefors', {'xref': {'@ref-type': 'bibr', '@rid': 'CR57', '#text': '57'}}], 'italic': ['Generic Mapping Tools', 'plotly'], 'ext-link': [{'@xlink:href': 'http://www.soest.hawaii.edu/gmt', '@ext-link-type': 'uri', '#text': 'www.soest.hawaii.edu/gmt'}, {'@xlink:href': 'http://www.plotly.com/python', '@ext-link-type': 'uri', '#text': 'www.plotly.com/python'}, {'@xlink:href': 'http://www.mathworks.com', '@ext-link-type': 'uri', '#text': 'www.mathworks.com'}], '#text': 'The performed processing (Table\xa0, Figs.\xa0, , and ) are common statistical representations of the data and do not require custom codes; M was calculated with the Python class of Herrmann and Marzocchi. The  () were used for creating Fig.\xa0, the Python graphing library  () for creating Figs.\xa0–, and Matlab () for creating Fig.\xa0.'}]",2022-11-18
0,Scientific Data,41597,10.1038/s41597-022-01885-3,"IDSEM, an invoices database of the Spanish electricity market",26,12,2022,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR24', '#text': '24'}}, 'monospace': ['random, time, math, os', 'string', 'json'], '#text': 'The code used to generate the dataset is released under the BSD license and is available at Zenodo. The repository has a directory with invoice templates in DOCX format and another directory with the dictionaries described above. The code of the pipeline was implemented in Python 3. For the first step, we used standard functions for simulating the data, based on the , and  libraries. We also used the  library for working with JSON files.'}, {'monospace': ['docxtpl', 'python-docx', 'jinja2', 'DocxTemplate', 'docx2pdf', 'python3 main.py'], '#text': 'We used the  library to replace fields in the DOCX templates with the JSON data in the second step. Internally, this library relies on two packages:  for reading, writing, and creating documents; and  for managing tags in the template. The  function allows replacing the text between braces with its corresponding value in the JSON file. The result is written in a DOCX file. Finally, this file is converted to PDF using the  library. The program can be executed from the command line as .'}, 'In order to generate the final version of the database, the program was executed in an Intel Core i9 CPU with 14 cores and 32.0 GB RAM, under Windows 10. The size of the database is approximately 30 GB in the disk. Each invoice may have between 2 and 4 pages, and the size of each PDF file on disk is between 160 KB and 1.68 MB. The size of JSON files is between 3 KB and 4 KB. The database contains 135 000 files with six and nine directories in the training and test sets, respectively.', 'The test set has 45 000 invoices in PDF format and the training set has 90 000 files: 30 000 invoices in PDF format; 30 000 annotated invoices in PDF format; and 30 000 JSON files with the value of labels.', 'In the non-distributable database, we have also included annotated invoices and labels for the test set, which amounts to 45 000 PDF and 45 000 JSON files, respectively. Therefore, the total number of files in this version is 225 000 files, which occupy 49.7 GB of disk.', {'xref': {'@rid': 'Fig6', '@ref-type': 'fig', '#text': '6'}, '#text': 'Regarding the execution time for the generation of the database, we calculated the average time of each step in the pipeline for each invoice; see Fig.\xa0. This average time was calculated after eight runs with 500 invoices. The results in seconds per bill are as follows:'}, '• Simulation process: 0.036\u2009s', '• Generation of the invoice file in DOCX format: 0.544\u2009s', '• Conversion from DOCX to PDF: 2.053\u2009s', 'Therefore, the average time for creating one invoice was 2.633\u2009seconds. The slowest step was the generation of PDF files (77.98% of the total time), followed by the creation of invoices (20.67%), and, finally, the simulation process (1.35%). The total run time was approximately 197 475\u2009seconds, or 54.85\u2009hours. The process was executed in several batches.']",2022-12-26
0,Scientific Data,41597,10.1038/s41597-023-01977-8,Reaction profiles for quantum chemistry-computed [3 + 2] cycloaddition reactions,1,2,2023,https://github.com/coleygroup/dipolar_cycloaddition_dataset,The code described in the previous section is freely available in GitHub under the MIT license (). Further details on how to use it is provided in the associated README.md file.,2023-02-01
0,Scientific Data,41597,10.1038/s41597-023-02004-6,Deep learning based atomic defect detection framework for two-dimensional materials,14,2,2023,https://github.com/MeatYuan/MOS2.We,All the code to produce the results of this paper is accessible at:  all use Python and jupyter notebook.,2023-02-14
0,Scientific Data,41597,10.1038/s41597-023-02033-1,First assessment of underwater sound levels in the Northern Adriatic Sea at the basin scale,15,3,2023,,"The Jupyter Notebook interactive document for data post-processing is freely available in ROHub, the Research object management platform.",2023-03-15
0,Scientific Data,41597,10.1038/s41597-022-01926-x,A database of synthetic inelastic neutron scattering spectra from molecules and crystals,24,1,2023,,"Gaussian 09, Phonopy, and OCLIMAX are used to generate the datasets. Gaussian 09 is commercial software that requires the users to purchase a license. Phonopy and OCLIMAX are freely available to the public. All parameters used in the calculations are provided in the database as input files.",2023-01-24
0,Scientific Data,41597,10.1038/s41597-023-01978-7,Datasets for learning of unknown characteristics of dynamical systems,7,2,2023,,All the shared and previously described datasets were generated by the MATLAB programs. MATLAB Version: 9.9.0.1467703 (R2020b) was used. All MATLAB codes were published through the Gitlab repository. There is no restriction to accessing this public repository of the source code.,2023-02-07
0,Scientific Data,41597,10.1038/s41597-023-02005-5,Mapping the terraces on the Loess Plateau based on a deep learning-based model at 1.89 m resolution,2,3,2023,https://github.com/LYHTTUCAS1/code,"The source code used the Python language. The source code contains five sections: data_loader5_shanxitezhengqu_LP.py, unet_2d.py, data_preprocess.py, train_shanxitezhengqu_LP.py, Config_shanxitezhengqu_LP.py. The source code can be downloaded at .",2023-03-02
0,Scientific Data,41597,10.1038/s41597-022-01887-1,Comparative transcriptome profiles of four sexually size dimorphic fish,17,12,2022,,"[{'ext-link': {'@xlink:href': 'https://www.bioinformatics.babraham.ac.uk/projects/fastqc/', '@ext-link-type': 'uri', '#text': 'https://www.bioinformatics.babraham.ac.uk/projects/fastqc/'}, '#text': 'All software used in this study were executed according to the manual and protocols of the published bioinformatic tools. FastQC, version 0.11.3, was used for the quality check of the raw FASTQ sequencing files. . The versions and parameters of the transcriptome assembly and expression analysis software described in the methods section are as follows:'}, 'Trimmomatic, version 0.36, LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:50.', 'Trinity, version 2.4.0, --seqType fq--SS_lib_type RF.', 'hisat2, version 2.2.1.0, --rna-strandness rf –fr.', 'cufflinks, version 2.2.1, --library-type fr-firststrand.', 'htseq-count, version 0.9.1, -s reverse', 'DESeq, version 1.18.0, pvalue\u2009<\u20090.05, |log2FoldChange| >1']",2022-12-17
0,Scientific Data,41597,10.1038/s41597-022-01715-6,The ImSURE phantoms: a digital dataset for radiomic software benchmarking and investigation,12,11,2022,,The MATLAB code used to design the two phantoms presented in this work is available in the Figshare “” repository. The MATLAB version 2020a and the Image Processing Toolbox are required to run the code.,2022-11-12
0,Scientific Data,41597,10.1038/s41597-022-01888-0,A data set of global river networks and corresponding water resources zones divisions v2,15,12,2022,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR33', '#text': '33'}}, '#text': 'The codes to construct the GRNWRZ V2.0 data set are available in Figshare. There are two files and a folder:'}, {'italic': '01_Data Prepare.tbx', '#text': '– Prepare hydrological data, including flow direction and flow accumulation.'}, {'italic': '02_RNWRZ', '#text': '– Contain Fortran files, which are used to generate the RN and WRZ at levels from 3 to 7.'}, {'italic': '03_extract_RNWRZ.py', '#text': '– Generate codes for the RN and WRZ.'}, 'The ArcGIS Model(.tbx) and Python(.py) scripts are run in ArcGIS Pro (v2.4 or above vision). The remaining folder contains all Fortran files. It was compiled by Intel Parallel Studio XE 2018, installed in Visual Studio 2013. All software needs to be installed in the Windows 10 OS.']",2022-12-15
0,Scientific Data,41597,10.1038/s41597-022-01830-4,"COVID-19 SeroHub, an online repository of SARS-CoV-2 seroprevalence studies in the United States",26,11,2022,,"['COVID-19 SeroHub uses custom code to store manually extracted data without processing. As described above, these data are downloadable by users via spreadsheet or API. Tools for visualizing extracted data were produced using HTML 5.0 and ECMAScript/Javascript and tested with Chrome Browser 90.x, Safari 14.x, and Firefox 88.x. The Apache eCharts 5.1.1 library was used to produce the Studies Map, Interactive Seroprevalence Tool, and Individual Study Page data visualizations. API code was produced using the Python version 3.8 on AWS Lambda, AWS API Gateway, and Elasticsearch 7.9.', 'All code and cloud resources are secured in Federal Information Security Modernization Act (FISMA) compliant environments.']",2022-11-26
0,Scientific Data,41597,10.1038/s41597-023-01951-4,A crowdsourced dataset of aerial images with annotated solar photovoltaic arrays and installation metadata,28,1,2023,https://github.com/gabrielkasmi/bdappv,"Our public repository accessible at this URL  contains the code to generate the masks, filter the metadata and analyze our results. Interested users can clone this repository to replicate our results or conduct analyses.",2023-01-28
0,Scientific Data,41597,10.1038/s41597-022-01860-y,Particle filtration efficiency measured using sodium chloride and polystyrene latex sphere test methods,7,12,2022,,"Beyond the raw data files, we have also included code in Python, JavaScript (via the Node.js framework), and Matlab to read the JSON file into a native format (e.g., a Python dictionary).",2022-12-07
0,Scientific Data,41597,10.1038/s41597-023-02008-2,"Global Dam Tracker: A database of more than 35,000 dams with location, catchment, and attribute information",23,2,2023,https://doi.org/10.5281/zenodo.6784716; https://earthengine.google.com,"Code for replicating results in this article is publicly available on Zenodo (). We use Python (versions 3.6 and 3.7), Stata MP (version 15.1), Google Earth Engine () to obtain dam catchment data and conduct analysis.",2023-02-23
0,Scientific Data,41597,10.1038/s41597-023-02037-x,WAVES – The Lucile Packard Children’s Hospital Pediatric Physiological Waveforms Dataset,7,3,2023,https://bitbucket.org/surfstanfordmedicine/waves-utilities/src/main/; https://pypi.org/project/waves-utilities/,"Redivis provides a visual drag-and-drop filtering user interface that allows the user to select columns of interest, filter on properties of interest, and limit output parameters before creating a downloadable CSV file. Sample scripts for working with data downloaded from Redivis and plotting sample waveforms are available in open-source repositories:  and .",2023-03-07
0,Scientific Data,41597,10.1038/s41597-022-01901-6,COVID-Dynamic: A large-scale longitudinal study of socioemotional and behavioral change across the pandemic,3,2,2023,,"[{'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR83', '#text': '83'}}, {'xref': [{'@ref-type': 'bibr', '@rid': 'CR84', '#text': '84'}, {'@ref-type': 'bibr', '@rid': 'CR85', '#text': '85'}], '#text': ','}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR63', '#text': '63'}}], 'ext-link': {'@xlink:href': 'https://github.com/adolphslab/CVD-DYN_datarelease', '@ext-link-type': 'uri', '#text': 'https://github.com/adolphslab/CVD-DYN_datarelease'}, '#text': 'All questionnaires were implemented in Qualtrics, tasks were implemented using the JsPsych toolbox (Version 6.3.1 and 6.0.5). Data processing, analyses and visualizations were conducted in Python 3.7 and R. All Qualtrics qsf and pdf files and task code, as well as preprocessing and visualization code is publicly available via github ().'}, 'Additional Public Resources', 'a) Web-based data explorer:', {'ext-link': {'@xlink:href': 'http://coviddynamicdash.caltech.edu/shiny/coviddash/', '@ext-link-type': 'uri', '#text': 'http://coviddynamicdash.caltech.edu/shiny/coviddash/'}}, 'b) Summary of COVID-19 Psychological Studies:', {'ext-link': {'@xlink:href': 'https://coviddynamic.caltech.edu/resources/other-covid-studies', '@ext-link-type': 'uri', '#text': 'https://coviddynamic.caltech.edu/resources/other-covid-studies'}}, 'c) 2020 Timeline:', {'ext-link': {'@xlink:href': 'https://coviddynamic.caltech.edu/resources/timeline-2020-world-events', '@ext-link-type': 'uri', '#text': 'https://coviddynamic.caltech.edu/resources/timeline-2020-world-events'}, '#text': '.'}]",2023-02-03
0,Scientific Data,41597,10.1038/s41597-022-01832-2,"RedDB, a computational database of electroactive molecules for aqueous redox flow batteries",28,11,2022,https://github.com/ergroup/RedDB,"All classical and quantum chemical calculations have been performed by using the SMSS, which is a proprietary software package. The solubility predictions have been made by using the AqSolPred, which is a freely accessible tool. In addition, the in-house developed Python scripts that have been used to parse the calculation outputs and to convert them into relational database formats, are openly accessible at .",2022-11-28
0,Scientific Data,41597,10.1038/s41597-022-01777-6,"Effects of heat stress on 16S rDNA, metagenome and metabolome in Holstein cows at different growth stages",22,10,2022,,"Serum enzyme activity, antioxidant capacity, and immune parameter were statistically processed and analyzed using R version 4.0.3, default parameters or parameters recommended by the developer were used.",2022-10-22
0,Scientific Data,41597,10.1038/s41597-022-01578-x,A dataset of mentorship in bioscience with semantic and demographic estimations,2,8,2022,https://github.com/sciosci/AFT-MAG,"All the code for generating the dataset and figures is published as IPython notebooks on Github, . All the coding was completed using Python.",2022-08-02
0,Scientific Data,41597,10.1038/s41597-022-01692-w,Pan-tumor CAnine cuTaneous Cancer Histology (CATCH) dataset,27,9,2022,https://github.com/DeepPathology/CanineCutaneousTumors,"Code examples for training the segmentation and classification architectures can be found in the form of Jupyter notebooks in our GitHub repository (). Furthermore, we provide exported fastai learners to reproduce the results stated in this work. The  file lists the train, validation, and test split on slide level. For network inference, we provide two Jupyter notebooks for patch-level results ( and ) and one notebook for slide-level results. This  notebook produces segmentation and classification outputs as compressed numpy arrays. After inference, these prediction masks can be visualized as overlays on top of the original images using our custom SlideRunner plugins  and . To integrate these plugins into their local SlideRunner installation, users have to copy the respective plugin from our GitHub repository into their SlideRunner  directory. Additionally, the  notebook provides methods to compute confusion matrices from network predictions and calculate class-wise Jaccard coefficients and the tumor classification recall. As mentioned previously, we provide six python modules to convert annotations back and forth between MS COCO and EXACT, MS COCO and SQLite, and EXACT and SQLite formats. This enables users to extend the annotations by custom classes or polygons in their preferred annotation format. These modules can be found in the  directory of our GitHub repository.",2022-09-27
0,Scientific Data,41597,10.1038/s41597-022-01493-1,Version 3 of the Global Aridity Index and Potential Evapotranspiration Database,15,7,2022,https://doi.org/10.6084/m9.figshare.20005589,"Geospatial processing and analysis were done using ESRI ArcGIS Pro (version 2.9), ArcMap (version 10.8), Python (versions 2.7 & 3.6) programming language, and Microsoft Excel for further data analysis, graphic and presentation. The Python programming code used to run the calculation of ET and AI is provided and available online at: .",2022-07-15
0,Scientific Data,41597,10.1038/s41597-022-01520-1,A long-term reconstructed TROPOMI solar-induced fluorescence dataset using machine learning algorithms,20,7,2022,https://github.com/chen-xingan/Reconstruct-TROPOMI-SIF.git,The code for generating the RTSIF is available at .,2022-07-20
0,Scientific Data,41597,10.1038/s41597-022-01805-5,pISA-tree - a data management framework for life science research projects using a standardised directory tree,10,11,2022,http://github.com/NIB-SI/pISA-tree; https://github.com/NIB-SI/pisar; https://github.com/NIB-SI/seekr,The code and documentation for pISA-tree is available at . R packages and documentation are available from  and . The code is openly available under MIT license terms.,2022-11-10
0,Scientific Data,41597,10.1038/s41597-022-01579-w,Fundamental social motives measured across forty-two cultures in two waves,16,8,2022,,"All code used to process and visualize the data, including information on software packages used, is freely available in the OSF project.",2022-08-16
0,Scientific Data,41597,10.1038/s41597-022-01664-0,A Dataset of 3D Structural and Simulated Transport Properties of Complex Porous Media,3,10,2022,,"[{'ext-link': {'@xlink:href': 'https://github.com/je-santos/Large-simulation-dataset', '@ext-link-type': 'uri', '#text': 'https://github.com/je-santos/Large-simulation-dataset'}, 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR71', '#text': '71'}}, '#text': 'The code used for the geometry curation and standarization, and plotting can be found at . The fluid flow simulations were performed using.'}, 'For all studies using custom code in the generation or processing of datasets, a statement must be included under the heading “Code availability“, indicating whether and how the code can be accessed, including any restrictions to access. This section should also include information on the versions of any software used, if relevant, and any specific variables or parameters used to generate, test, or process the current dataset.']",2022-10-03
0,Scientific Data,41597,10.1038/s41597-022-01720-9,Toward practical transparent verifiable and long-term reproducible research using Guix,4,10,2022,https://gitlab.com/nivall/guixreprodsci; https://archive.softwareheritage.org/swh:1:rev:707f00afef8f6ef1f29a7a4c961dd714f82833f5,All source codes are available on our Gitlab repository:  (Tag: v1.0-pre2) and the version at submission time is archived on Software Heritage at .,2022-10-04
0,Scientific Data,41597,10.1038/s41597-022-01550-9,The near-global ocean mesoscale eddy atmospheric-oceanic-biological interaction observational dataset,22,7,2022,https://doi.org/10.6084/m9.figshare.19802062.v1; http://www.mathworks.com,"The mesoscale eddy automatic detection code is available to the public, and can be downloaded from . It should also be noted that the vector geometry eddy detection algorithm has regional differences, with results deteriorating in coastal zones and near islands, but is still highly accurate for the open ocean. All data processing and calculations in the process of constructing the dataset are completed by MATLAB R2016b (). Considering user-friendliness, the eddy automatic detection code is currently being converted into a Python format, but due to the huge workload, the code conversion work is still in progress, and the dataset will be uploaded and updated in the future.",2022-07-22
0,Scientific Data,41597,10.1038/s41597-022-01806-4,Pre- and post-surgery brain tumor multimodal magnetic resonance imaging data optimized for large scale computational modelling,5,11,2022,,"[{'ext-link': {'@xlink:href': 'https://github.com/haerts/The-Virtual-Brain-Tumor-Patient', '@ext-link-type': 'uri', '#text': 'https://github.com/haerts/The-Virtual-Brain-Tumor-Patient'}, '#text': 'Code for the TVB Brain Tumor pipeline is available at .'}, {'ext-link': {'@xlink:href': 'https://github.com/bids-apps/rsHRF', '@ext-link-type': 'uri', '#text': 'https://github.com/bids-apps/rsHRF'}, '#text': 'Code for retrieving the voxelwise resting state hemodynamic response function is available at .'}, {'ext-link': {'@xlink:href': 'https://github.com/AmoghJohri/TVB-Tests', '@ext-link-type': 'uri', '#text': 'https://github.com/AmoghJohri/TVB-Tests'}, '#text': 'Proof-of-concept notebooks for the introduction of region- and subject-specific HRF in TVB are presented at .'}, {'ext-link': [{'@xlink:href': 'https://github.com/BrainModes/TVB-empirical-data-pipeline', '@ext-link-type': 'uri', '#text': 'https://github.com/BrainModes/TVB-empirical-data-pipeline'}, {'@xlink:href': 'https://search.kg.ebrains.eu/instances/Software/71265c9f-5fe3-40e3-a7e4-b2bb45b5ea6e', '@ext-link-type': 'uri', '#text': 'https://search.kg.ebrains.eu/instances/Software/71265c9f-5fe3-40e3-a7e4-b2bb45b5ea6e'}], '#text': 'The TVB processing pipeline is available at  and  for cloud computing.'}]",2022-11-05
0,Scientific Data,41597,10.1038/s41597-022-01607-9,"A test-retest resting, and cognitive state EEG dataset during multiple subject-driven states",13,9,2022,https://www.fieldtriptoolbox.org/workshop/madrid2019/bids_sedation/; http://bids.neuroimaging.io,"The code used to convert the test-retest resting, and cognitive state EEG dataset for sharing with OpenNeuro was referred to the conversion of the EEG sedation dataset for sharing in BIDS, which is public available at . The BIDS background is explained on , details on the specification can be found on the paper titled “EEG-BIDS, an extension to the brain imaging data structure for electroencephalography”, which was published in Scientific Data.",2022-09-13
0,Scientific Data,41597,10.1038/s41597-022-01636-4,TILES-2019: A longitudinal physiologic and behavioral data set of medical residents in an intensive care unit,1,9,2022,https://github.com/usc-sail/tiles-2019-dataset/,The code is available at .,2022-09-01
0,Scientific Data,41597,10.1038/s41597-022-01665-z,A georeferenced rRNA amplicon database of aquatic microbiomes from South America,13,9,2022,https://github.com/microsudaqua/usudaquadb,"The workflow included several custom-made R and python scripts, which are accessible GitHub ().",2022-09-13
0,Scientific Data,41597,10.1038/s41597-022-01522-z,Mapping 20 years of irrigated croplands in China using MODIS and statistics and existing irrigation products,15,7,2022,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR60', '#text': '60'}}, '#text': 'Python code used to generate the irrigation maps is available from the figshare repository.'}, 'The software used in this work include:', '∙ ArcGIS 10.2', '∙ Python 2.7, numpy 1.16.6, pandas 0.19.0, scipy 1.2.3, scikit-learn 0.20.3, matplotlib 1.1.1']",2022-07-15
0,Scientific Data,41597,10.1038/s41597-022-01580-3,A Multi-Modal Gait Database of Natural Everyday-Walk in an Urban Environment,3,8,2022,https://github.com/HRI-EU/multi_modal_gait_database,"To streamline the processing of the data, we provide various tools and scripts that are accessible at . In particular, a Python script is available to join the CSV files into one single pandas data frame, which also supports filtering for specific tasks, participants, and data columns. Furthermore, we provide a visualization tool that jointly displays all three sensor modalities as illustrated by Fig. . The tool allows the adjustment of current labels and the creation of custom labels or tags, enabling the generation of additional machine learning tasks.",2022-08-03
0,Scientific Data,41597,10.1038/s41597-022-01608-8,"BRAX, Brazilian labeled chest x-ray dataset",10,8,2022,,"[{'ext-link': {'@xlink:href': 'https://github.com/edreisMD/BRAX-labeler', '@ext-link-type': 'uri', '#text': 'https://github.com/edreisMD/BRAX-labeler'}, '#text': 'The BRAX Labeler code used for the extraction of labels from Brazilian-Portuguese radiology reports is available on Github ().'}, 'To prevent the risk of patient re-identification, the anonymization code is not provided.']",2022-08-10
0,Scientific Data,41597,10.1038/s41597-022-01695-7,An analysis-ready and quality controlled resource for pediatric brain white-matter research,12,10,2022,,"[{'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR108', '#text': '108'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR109', '#text': '109'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR110', '#text': '110'}}, {'xref': [{'@ref-type': 'bibr', '@rid': 'CR111', '#text': '111'}, {'@ref-type': 'bibr', '@rid': 'CR112', '#text': '112'}], '#text': ','}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR113', '#text': '113'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR114', '#text': '114'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR115', '#text': '115'}}], 'ext-link': {'@xlink:href': 'https://github.com/richford/hbn-pod2-qc', '@ext-link-type': 'uri', '#text': 'https://github.com/richford/hbn-pod2-qc'}, 'monospace': ['make', 'make help', 'make build'], '#text': 'To facilitate replicability, Jupyter notebooks and Dockerfiles necessary to reproduce the methods described herein are provided in the HBN-POD2 GitHub repository at . The specific version of the repository used in this study is documented in. Most of the code in this repository uses Pandas, Numpy, Matplotlib, and Seaborn. The  or  commands will list the available commands and  will build the requisite Docker images to analyze HBN-POD2 QC data.'}, {'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR116', '#text': '116'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR117', '#text': '117'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR118', '#text': '118'}}], 'monospace': ['make data', 'make niftis', 'make tfrecs', 'make'], '#text': 'In order to separate data from analysis code, we provide intermediate data necessary to analyze the QC results in an OSF project, the contents of which can be downloaded using the  command in the root of the HBN-POD2 GitHub repository. The NIFTI-1 files and TFRecord files provided as input to the CNN models may be separately downloaded using the  and  commands, respectively. The remaining  commands and Jupyter notebooks follow the major steps of the methods section:'}, {'italic': ['cloudknot', 'QSIPrep', 'QSIPrep'], 'monospace': 'preprocess-remaining-hbn-curated.ipynb', '#text': '1. The  preprocessing function used to execute  workflows on curated data was a thin wrapper around ’s command line interface and is provided in the “notebooks” directory of the HBN-POD2 GitHub repository in a Jupyter notebook with the suffix .'}, {'monospace': 'make expert-qc', '#text': '2. The expert rating analysis can be replicated using the  command in the HBN-POD2 GitHub repository.'}, {'italic': ['Fibr', 'Fibr'], 'ext-link': {'@xlink:href': 'https://github.com/richford/fibr', '@ext-link-type': 'uri', '#text': 'https://github.com/richford/fibr'}, '#text': '3. The  community science web application is based on the SwipesForScience framework (swipesforscience.org), which generates a web application for community science given an open repository of images to be labelled and a configuration file. The source code for the  web application is available at .'}, {'italic': ['Fibr', 'DIPY', 'cloudknot', 'Fibr', 'Fibr'], 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR45', '#text': '45'}}, 'monospace': 'TensorModel', 'xref': {'@rid': 'Tab3', '@ref-type': 'table', '#text': '3'}, '#text': '4. The images that the  raters saw were generated using a  in a -enabled Jupyter notebook that is available in the “notebooks” directory of the  GitHub repository.  saves each community rating to its Google Firebase backend, the contents of which have been archived to the HBN-POD2 OSF project as specified in Table\xa0.'}, {'monospace': ['make community-qc', 'make data'], '#text': '5. The community ratings analysis can be replicated using the  command in the HBN-POD2 GitHub repository. Saved model checkpoints for each of the XGB models are available in the HBN-POD2 OSF project and are automatically downloaded with the  command.'}, {'italic': ['DIPY', 'cloudknot', 'Nobrainer'], 'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR45', '#text': '45'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR46', '#text': '46'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR119', '#text': '119'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR107', '#text': '107'}}], 'monospace': ['save-b0-tensorfa-nifti.ipynb', 'save-tfrecs.ipynb'], '#text': '6. The input multichannel volumes for the CNN models were generated using  and  and saved as NIfTI-1 files. These NIfTI files were then converted to the Tensorflow TFRecord format using the  deep learning framework. The Jupyter notebooks used to create these NIfTI and TFRecord files are available in the “notebooks” directory of the HBN-POD2 GitHub repository, with suffixes  and , respectively.'}, {'monospace': ['make dl-train', 'make dl-predict', 'README_GCP.md', 'make deep-learning-figures'], '#text': '7. We trained the CNN models using the Google Cloud AI Platform Training service; the HBN-POD2 GitHub repository contains Docker services to launch training (with ) and prediction (with ) jobs on Google Cloud, if the user has provided the appropriate credentials in an environment file and placed the TFRecord files on Google Cloud Storage. Further details on how to organize these files and write an environment file are available in the HBN-POD2 GitHub repository’s  file. To generate the figures depicting the deep learning QC pipeline and results, use the  command.'}, {'monospace': ['make dl-integrated-gradients', 'README_GCP.md'], '#text': '8. We provide a Docker service to compute integrated gradient attribution maps on Google Cloud, which can be invoked using the  command. This step also requires the setup steps described in .'}, {'monospace': ['make dl-site-generalization', 'README_GCP.md', 'make site-generalization'], '#text': '9. We provide a Docker service to conduct the CNN-i site generalization experiments on Google Cloud, which can be invoked using the  command, which, again, requires the setup steps described in . Similarly, theThe XGB-q site generalization experiments can be replicated locally using the  command, which will also plot the results of the CNN-i experiments.'}, {'italic': 'cloudknot', 'monospace': 'afq-hbn-curated.ipynb', 'ext-link': {'@xlink:href': 'https://yeatmanlab.github.io/pyAFQ/auto_examples/cloudknot_example.html', '@ext-link-type': 'uri', '#text': 'https://yeatmanlab.github.io/pyAFQ/auto_examples/cloudknot_example.html'}, '#text': '10. The tractometry pipeline was executed using pyAFQ and  in a Jupyter notebook provided in the “notebooks” directory of the HBN-POD2 GitHub repository with the with suffix afq-hbn-curated.ipynb. with suffix , provided in the HBN-POD2 GitHub repository in the “notebooks” directory. The pyAFQ documentation contains a more pedagogical example of using\xa0pyAFQ with cloudknot to analyze a large openly available dataset\xa0().'}, {'monospace': 'inference', '#text': '11. The bundle profile and age prediction analyses can be replicated using the make bundle-profiles and make  commands, respectively.'}]",2022-10-12
0,Scientific Data,41597,10.1038/s41597-022-01722-7,A multi-camera and multimodal dataset for posture and gait analysis,6,10,2022,,"[{'italic': 'PhysioNet', 'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR32', '#text': '32'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR34', '#text': '34'}}], '#text': 'This database is accompanied by a folder with all the scripts used to process, handle, visualize, and evaluate the data described (available in  and GitHub). All scripts are based on the Python programming language and, thus, open source. The code contains a permissive MIT license for unrestricted usage.'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR35', '#text': '35'}}, '#text': 'The dataset has also been used on a related publication, to develop and evaluate deep learning based algorithms for patient pose estimation using the robotic walker. The authors hope it can further contribute to the development and evaluation of classic or data-driven vision-based pose estimation algorithms, applications in human detection, joint tracking, and movement forecasting, and gait/posture metrics analysis targeting solutions for motor rehabilitation.'}]",2022-10-06
0,Scientific Data,41597,10.1038/s41597-022-01751-2,Dataset of parent-child hyperscanning functional near-infrared spectroscopy recordings,15,10,2022,,The dataset presented in this manuscript includes raw data that have not been processed by any script or software-based procedure.,2022-10-15
0,Scientific Data,41597,10.1038/s41597-022-01609-7,Simplifying complex fault data for systems-level analysis: Earthquake geology inputs for U.S. NSHM 2023,18,8,2022,https://doi.org/10.5066/P9E3B8AG,The code utilized to generate the individual fault maps in the visual verification and quality assurance of the database is written in Python 3.0 and is available at  as a Jupyter Notebook. This notebook is intended to share the plotting processes for how faults were visualized and can be manipulated by a user to prepare map images of specific faults or regions of choice.,2022-08-18
0,Scientific Data,41597,10.1038/s41597-022-01638-2,A multi-sensor human gait dataset captured through an optical system and inertial measurement units,7,9,2022,https://github.com/geisekss/motion_capture_analysis; https://www.jyu.fi/hytk/fi/laitokset/mutku/en/research/materials/mocaptoolbox,The developed Matlab and Python codes to process the data are freely available on the first author’s github repository (). The MoCap Toolbox is freely available and extensively documented on the University of Jyväskylä website ().,2022-09-07
0,Scientific Data,41597,10.1038/s41597-022-01696-6,Solar and wind power data from the Chinese State Grid Renewable Energy Generation Forecasting Competition,21,9,2022,https://github.com/Bob05757/Renewable-energy-generation-input-feature-variables-analysis,"All the code and processing scripts used to produce the results of this paper were written in Python, Jupyter lab. Links to scripts and data for analysis can be found in the GitHub repository ().",2022-09-21
0,Scientific Data,41597,10.1038/s41597-022-01752-1,A thermoelectric materials database auto-generated from the scientific literature using ChemDataExtractor,22,10,2022,https://github.com/odysie/thermoelectricsdb,"The code used to automatically generate the database is available at , along with examples, code for cleaning and aggregating the database, and supplementary information about the database and the data extraction process.",2022-10-22
0,Scientific Data,41597,10.1038/s41597-022-01524-x,EPIC: Annotated epileptic EEG independent components for artifact reduction,20,8,2022,https://doi.org/10.5281/zenodo.6620655,The custom code used to create this dataset is available on .,2022-08-20
0,Scientific Data,41597,10.1038/s41597-022-01639-1,"QDataSet, quantum datasets for machine learning",23,9,2022,https://github.com/eperrier/QDataSet,"The datasets are stored in an online repository and are accessible via links on the site. The largest of the datasets is over 500GB (compressed), the smallest being around 1.4GB (compressed). The QDataSet is provided subject to open-access MIT/CC licensing for researchers globally. The code used to generate the QDataSet is contained in the associated repository (see below), together with instructions for reproduction of the dataset. The QDataSet code requires Tensorflow > 2.0 along with a current Anaconda installation of Python 3. The code used to simulate the QDataSet is available via the Github repository (). A Jupyter notebook containing the code used for technical validation and verification of the datasets is available on this QDataSet Github repository.",2022-09-23
0,Scientific Data,41597,10.1038/s41597-022-01782-9,Benchmarking emergency department prediction models with machine learning and public electronic health records,27,10,2022,https://github.com/nliulab/mimic4ed-benchmark,The code used to analyze the data in the current study is available at: .,2022-10-27
0,Scientific Data,41597,10.1038/s41597-022-01498-w,VinDr-CXR: An open dataset of chest X-rays with radiologist’s annotations,20,7,2022,https://www.python.org/; https://pydicom.github.io/; https://pypi.org/project/opencv-python/; https://docs.python.org/3/library/hashlib.html; https://github.com/vinbigdata-medical/vindr-cxr,The code used for loading and processing DICOM images is based on the following open-source repositories: Python 3.7.0 (); Pydicom 1.2.0 (); OpenCV-Python 4.2.0.34 (); and Python hashlib (). The code for data de-identification and outlier detection was made publicly available at .,2022-07-20
0,Scientific Data,41597,10.1038/s41597-022-01754-z,Density-of-states similarity descriptor for unsupervised learning from materials data,22,10,2022,https://cmrdb.fysik.dtu.dk/c2db/; https://github.com/kubanmar/dos-fingerprints-data,"[{'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR23', '#text': '23'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR29', '#text': '29'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR1', '#text': '1'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR30', '#text': '30'}}], 'ext-link': [{'@xlink:href': 'https://github.com/kubanmar/dos-fingerprints', '@ext-link-type': 'uri', '#text': 'https://github.com/kubanmar/dos-fingerprints'}, {'@xlink:href': 'https://nomad-lab.eu/aitoolkit/tutorial-dos-similarity', '@ext-link-type': 'uri', '#text': 'https://nomad-lab.eu/aitoolkit/tutorial-dos-similarity'}], '#text': 'Our implementation of the DOS fingerprint is part of the NOMAD project and can be obtained as a stand-alone Python package hosted at Github (). Additionally, we provide a tutorial that explains how to use the DOS descriptor to find in the NOMAD Repository the most similar materials to a given reference. This tutorial is implemented in the NOMAD AI Toolkit and can be accessed online ().'}, {'italic': ['ε', 'ε', 'N', 'ε', 'ε', 'ε', 'W', 'W', 'N', 'ρ', 'N', 'ρ', 'ρ', 'N', 'ρ', 'ρ'], 'sub': [{'italic': 'min'}, {'italic': 'max'}, {'italic': 'max'}, {'italic': 'min'}, 'ref', {'italic': 'H'}, {'italic': 'ρ'}, {'italic': 'min'}, {'italic': 'ρ'}, {'italic': 'min'}, {'italic': 'max'}, {'italic': 'H'}, {'italic': 'max'}, {'italic': 'min'}], 'xref': [{'@rid': 'Equ2', '@ref-type': 'disp-formula', '#text': '2'}, {'@rid': 'Equ4', '@ref-type': 'disp-formula', '#text': '4'}], 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR31', '#text': '31'}}, 'ext-link': {'@xlink:href': 'https://github.com/kubanmar/similarity_threshold_clusterer', '@ext-link-type': 'uri', '#text': 'https://github.com/kubanmar/similarity_threshold_clusterer'}, '#text': 'In this work, we choose to focus our DOS descriptor on an energy region around the Fermi energy. To this end, we set the parameters Δ\u2009=\u20090.05\u2009eV, Δ\u2009=\u20091.05 eV, \u2009=\u2009Δ/Δ\u2009=\u200921, \u2009=\u20090 eV, and \u2009=\u20094 eV (see Eq.\xa0). \u2009=\u20094 eV, \u2009=\u2009512, \u2009=\u2009Δ\u2009=\u20090.25, \u2009=\u20092.75, and \u2009=\u2009/\u2009=\u200911 (see Eq.\xa0). An implementation of our clustering algorithm is available at Github ().'}]",2022-10-22
0,Scientific Data,41597,10.1038/s41597-022-01555-4,Expert tumor annotations and radiomics for locally advanced breast cancer in DCE-MRI for ACRIN 6657/I-SPY1,23,7,2022,https://github.com/CBICA/CaPTk; https://github.com/CBICA/GaNDLF,"In favor of transparency and reproducibility, but also in line with the scientific data principles of Findability, Accessibility, Interoperability, and Reusability (FAIR), we have made the tools used to generate the data for this study publicly available. Specifically, the CaPTk platform, version 1.8.1, was used for all the preprocessing steps. CaPTk’s source code and binary executables are publicly available for multiple operative systems through its official GitHub repository (). The implementation and configuration of the U-Net with residual connections, used in this study, can be found in the GitHub page of the Generally Nuanced Deep Learning Framework (GaNDLF), version 0.0.14 (). Finally, ITK-SNAP, was used for all the manual annotation refinements.",2022-07-23
0,Scientific Data,41597,10.1038/s41597-022-01699-3,Optical emissivity dataset of multi-material heterogeneous designs generated with automated figure extraction,29,9,2022,https://github.com/ViktoriiaBaib/curvedataextraction,"The source code (implemented in Python) for performing all the described figure analysis steps and generating the data entries is available at . The axis and legend detection step uses the TensorFlow2 Object Detection API and provides a fine-tuned CNN model. File “object_detection_axes_legend.py” performs object detection of legend, x-axis, and y-axis objects and generates PNG and JSON records for these objects. File “color_decomposition.py” performs clustering by color and produces PNG of color-isolated image, palette, as well as PNG and JSON records of separate color clusters in pixel coordinates. It uses methods from “posterization.py”. File “final-record.py” performs axes scale parsing and applies it to all the clusters, producing cluster records in units of measurement. It utilizes methods from “final_record_func.py”.",2022-09-29
0,Scientific Data,41597,10.1038/s41597-022-01527-8,A quantum chemical molecular dynamics repository of solvated ions,21,7,2022,https://dftbplus.org/download/dftb-stable; https://github.com/dftbplus/dftbplus; https://dftb.org/parameters/download,All data contained in the IonSolvR database was generated with the DFTB+ program (v. 19.1). This code is freely available via the DFTB+ website () and GitHub () via the GNU Lesser General Public Licence (LGPL-3). The 3ob-3-1 parameter set can be freely downloaded from the dftb.org website ().,2022-07-21
0,Scientific Data,41597,10.1038/s41597-022-01585-y,Dataset of identified scholars mentioned in acknowledgement statements,1,8,2022,,"To identify individuals from acknowledgement statements, the NLP software CoreNLP was used. Script files were created using Python programming with version 3.9, to build a dataset that is available in Zenodo.",2022-08-01
0,Scientific Data,41597,10.1038/s41597-022-01670-2,A dataset of hourly sea surface temperature from drifting buoys,14,9,2022,https://github.com/selipot/sst-drift.git,"A Matlab software associated with this manuscript is licensed under MIT and published on GitHub at  and archived on Zenodo. This software allows the user to fit model () to temperature observations and derive the resulting SST estimates and their uncertainties. Input arguments to the model fitting function include an arbitrary order for the background non-diurnal SST model and arbitrary frequencies for the diurnal oscillatory model. A sample of Level-1 data from drifter AOML ID 55366 is provided in order to test the routines and produce figures similar to Figs.  and . Alternatively, the main code can also generate stochastic data for testing purposes.",2022-09-14
0,Scientific Data,41597,10.1038/s41597-022-01727-2,Large-scale audio dataset for emergency vehicle sirens and road noises,4,10,2022,https://github.com/tabarkarajab/Large-Scale-Audio-dataset,"Code and the script files used to convert the sounds files into meaningful format are published in (-). We developed this code using Python and Pycharm Community software (Version 2021.3). The large-Scale Audio Dataset relies on the following dependencies: os, logging, traceback, shlex, and subprocess.",2022-10-04
0,Scientific Data,41597,10.1038/s41597-022-01557-2,"A multimodal psychological, physiological and behavioural dataset for human emotions in driving tasks",6,8,2022,,"The data pre-processing methods and procedures of validation mentioned in the technical validation section were carried out in Jupyter Notebook. Python version 3.5.8 was used throughout. The correlation analysis and distribution display are conducted using seaborn, sciki-learn and pandas packages. The codes and a brief description(readme.md) have been uploaded.",2022-08-06
0,Scientific Data,41597,10.1038/s41597-022-01728-1,"A detailed behavioral, videographic, and neural dataset on object recognition in mice",13,10,2022,,"['We used the following packages to generate this dataset:', {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR25', '#text': '25'}}, '#text': '• Open Ephys acquisition GUI, version 0.4'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR27', '#text': '27'}}, '#text': '• Kilosort, version 1'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR28', '#text': '28'}}, '#text': '• Phy, version 2.0a1'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR12', '#text': '12'}}, '#text': '• pynwb, version 2.0.0'}, {'ext-link': {'@xlink:href': 'https://github.com/rly/ndx-pose', '@ext-link-type': 'uri', '#text': 'https://github.com/rly/ndx-pose'}, '#text': '• ndx_pose,'}, {'sup': {'xref': [{'@ref-type': 'bibr', '@rid': 'CR17', '#text': '17'}, {'@ref-type': 'bibr', '@rid': 'CR18', '#text': '18'}], '#text': ','}, 'ext-link': [{'@xlink:href': 'https://github.com/cxrodgers/PoseTF', '@ext-link-type': 'uri', '#text': 'https://github.com/cxrodgers/PoseTF'}, {'@xlink:href': 'https://github.com/eldar/pose-tensorflow', '@ext-link-type': 'uri', '#text': 'https://github.com/eldar/pose-tensorflow'}], '#text': '• pose-tensorflow, , lightly modified fork of'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR16', '#text': '16'}}, 'ext-link': [{'@xlink:href': 'https://github.com/cxrodgers/whisk', '@ext-link-type': 'uri', '#text': 'https://github.com/cxrodgers/whisk'}, {'@xlink:href': 'https://github.com/nclack/whisk', '@ext-link-type': 'uri', '#text': 'https://github.com/nclack/whisk'}], '#text': '• whisk, , lightly modified fork of'}, {'ext-link': {'@xlink:href': 'https://github.com/kkroening/ffmpeg-python', '@ext-link-type': 'uri', '#text': 'https://github.com/kkroening/ffmpeg-python'}, '#text': '• ffmpeg-python,'}, '• ffmpeg, version 4.0.2', {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR29', '#text': '29'}}, '#text': '• ipython, version 7.22.0'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR30', '#text': '30'}}, '#text': '• pandas, version 1.2.4'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR31', '#text': '31'}}, '#text': '• numpy, version 1.19.2'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR32', '#text': '32'}}, '#text': '• scipy, version 1.6.2'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR33', '#text': '33'}}, '#text': '• scikit-learn, version 0.24.1'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR34', '#text': '34'}}, '#text': '• scikit-image, version 0.18.1'}, '• statsmodels, version 0.12.2', {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR35', '#text': '35'}}, 'ext-link': {'@xlink:href': 'https://github.com/glm-tools/pyglmnet', '@ext-link-type': 'uri', '#text': 'https://github.com/glm-tools/pyglmnet'}, '#text': '• pyglmnet,'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR36', '#text': '36'}}, '#text': '• matplotlib, version 3.3.4'}]",2022-10-13
0,Scientific Data,41597,10.1038/s41597-022-01786-5,An all-Africa dataset of energy model “supply regions” for solar photovoltaic and wind power,31,10,2022,https://github.com/bhussain89/Model-Supply-Regions-MSR-Toolset,"The Python code used to generate the MSRs along with all their metadata, including hourly profiles, as well as the code to perform screening and clustering, is openly available on .",2022-10-31
0,Scientific Data,41597,10.1038/s41597-022-01529-6,"High accuracy barrier heights, enthalpies, and rate coefficients for chemical reactions",18,7,2022,,"The code used to generate this data is freely available on GitHub under the MIT license. Details on how to use the scripts to generate the data are provided in the Usage Notes. Some of the scripts utilize helpful components of the Reaction Mechanism Generator, such as RMG-Py, RMG-database, and the Automatic Rate Calculator (ARC). All related software is open-source under the MIT license and freely accessible on GitHub. For RMG-Py, checkout the  branch, and for RMG-database, checkout . The GitHub version commit string was  for ARC on the main branch.",2022-07-18
0,Scientific Data,41597,10.1038/s41597-022-01587-w,Muscle and adipose tissue segmentations at the third cervical vertebral level in patients with head and neck cancer,2,8,2022,https://github.com/kwahid/C3_sarcopenia_data_descriptor; https://github.com/RJain12/matlab-tag-reader,"Segmentation was performed using the commercially-available tool sliceOmatic v. 5.0 (Tomovision, Magog, Canada). The code for NIfTI file conversion of DICOM CT images and corresponding .tag format muscle/adipose tissue segmentations was developed using in-house Python scripts and is made publicly available through GitHub: . Alternative code for converting .tag files to Matlab readable format can be located at: .",2022-08-02
0,Scientific Data,41597,10.1038/s41597-022-01643-5,A multidevice and multimodal dataset for human energy expenditure estimation using wearable devices,1,9,2022,,We provide the raw  data files obtained during the data collection structured by user and device identifier. We did not implement any custom code to generate or process the data.,2022-09-01
0,Scientific Data,41597,10.1038/s41597-022-01729-0,A ten-year (2009–2018) database of cancer mortality rates in Italy,21,10,2022,,"Data used for the production of the Italian SMR database are available from ISTAT (see the paragraph ). The elaborations have followed the procedure described in the Methods section and can be implemented in whatever numerical computing environment (e.g., R, Matlab, Python). In our case, the algorithms used were created in Python 3.7 and are available on Zenodo.",2022-10-21
0,Scientific Data,41597,10.1038/s41597-022-01787-4,Multiorder hydrologic Position for Europe — a Set of Features for Machine Learning and Analysis in Hydrology,29,10,2022,https://doi.org/10.4211/hs.8ea376970c904c6698fc8cfe392689de; https://github.com/MxNl/macro_mohp_feature/releases/tag/v013.1.1.0; https://github.com/MxNl/macro_mohp_feature,"As stated previously, all processing steps including the generation of the dataset, most of the figures and the manuscript are script based. All required source code can be found on Hydroshare () as a static code repository. Due to the procedure of the reviewing process, this static code repository only contains the status of the code before the last reviewing iteration. The final code used for submitting the reviewed manuscript can be found in this separate code release on Github (). The actively developed code can be also found in the same repository on Github (). We encourage interested users of this dataset to report errors in the code or to give hints on further methodological or programming improvements through opening an issue in the Github repository or contacting the corresponding author via E-mail.",2022-10-29
0,Scientific Data,41597,10.1038/s41597-022-01615-9,"ASL-BIDS, the brain imaging data structure extension for arterial spin labeling",6,9,2022,https://github.com/bids-standard/bids-validator,"The BIDS validator code is available in the BIDS Validator repository on GitHub, .",2022-09-06
0,Scientific Data,41597,10.1038/s41597-022-01474-4,Agricultural SandboxNL: A national-scale database of parcel-level processed Sentinel-1 SAR data,13,7,2022,https://github.com/ManuelHuber-Github/Agricultural-SandboxNL,"Python code to access, query, visualize and analyze the Agricultural SandboxNL database is distributed, with the dataset and accompanying documentation. GitHub repository to share all GEE/python scripts and information used to create Agricultural SandboxNL database. .",2022-07-13
0,Scientific Data,41597,10.1038/s41597-022-01530-z,The draft genome sequence of the Brahminy blindsnake,15,7,2022,,"['All analyses were conducted on Linux systems. The version and code and parameters of the main software tools are described below.', '(1) LongQC, version 1.2.0c, parameters used: default.', '(2) Filtlong, version 0.2.1, parameters used: min_length 1000, keep_percent 90, split 100, mean_q_weight 10.', '(3) KMC, version 3.1.1, parameters used: k21, ci1, cs10000.', '(4) GenomeScope, version 2.0, parameters used: ploidy 3, kmer_length 21.', '(5) MaSuRCA, version 4.0.5, parameters used: LIMIT_JUMP_COVERAGE\u2009=\u2009300, CA_PARAMETERS\u2009=\u2009cgwErrorRate\u2009=\u20090.15, FLYE_ASSEMBLY\u2009=\u20090.', '(6) HaploMerger2, version 20180603, parameters used: default; hm.batchA and hm.batchB.', '(7) QUAST, version 5.0.2, parameters used: default.', '(8) BUSCO, version 5.2.2, parameters used: lineage_dataset sauropsida_odb10.', '(9) RepeatMasker, version 4.1.1, parameters used: engine ncbi, xsmall, Database: Dfam with RBRM.', '(10) RepeatModeler, version 2.0.1, parameters used: default, Database: The scaffolds assembled with MaSuRCA and HaploMerger2.', '(11) Augustus, version 3.4.0, parameters used: species\u2009=\u2009Database trained with BUSCO, alternatives-from-evidence\u2009=\u2009true, hintsfile\u2009=\u2009Output of RepeatMasker.', '(12) Diamond, version 2.0.13, parameters used: more-sensitive, max-target-seqs. 1, evalue 1e-5.']",2022-07-15
0,Scientific Data,41597,10.1038/s41597-022-01701-y,A 10 m resolution urban green space map for major Latin American cities from Sentinel-2 remote sensing images and OpenStreetMap,24,9,2022,https://github.com/yangju-90/urban_greenspace_classification,"We used Google Earth Engine via Python to query Sentinel-2 images and to extract spectral indices and texture from the images. We performed all other steps, including image downloading, PCA, sample collection and filtering, and image classification in Python. Code is available at .",2022-09-24
0,Scientific Data,41597,10.1038/s41597-022-01675-x,Projecting 1 km-grid population distributions from 2020 to 2100 globally under shared socioeconomic pathways,12,9,2022,https://doi.org/10.6084/m9.figshare.19609356.v3,"The global gridded population dataset was created using python 3.9.7 as well as ArcGIS 10.6 software platform, and the code of key steps can be available at Figshare. The code can be downloaded at Figshare ().",2022-09-12
0,Scientific Data,41597,10.1038/s41597-022-01702-x,TumorMet: A repository of tumor metabolic networks derived from context-specific Genome-Scale Metabolic Models,7,10,2022,,"[{'ext-link': {'@xlink:href': 'https://github.com/cds-group/Met2Graph', '@ext-link-type': 'uri', '#text': 'https://github.com/cds-group/Met2Graph'}, '#text': 'The R package Met2Graph developed and used to generate the TumorMet datasets is publicly available at the Met2Graph Github repository (). The package has a detailed tutorial to generate the networks. Met2Graph implements a flexible process flow to build graphs starting from a GSM and can be easily integrated with user-customized functions. It allows the creation of the three different types of graphs described, based on the selection of nodes, edges, and attributes: Metabolites-, Enzymes- and Reactions-based graphs. It allows integrating gene expression data into Metabolites-based graphs. It provides several options and parameters to customize the resulting graphs. To name a few: to create multiple or simplified edges (simplification is possible using three different methods), to remove recurring metabolites, to consider the double direction in case of reversible reactions, to generate graphs as directed or not, and to plot the networks. All the details and the different arguments are described in the package manual and “help” section of the related functions.'}, {'ext-link': {'@xlink:href': 'https://github.com/cds-group/GraphDistances', '@ext-link-type': 'uri', '#text': 'https://github.com/cds-group/GraphDistances'}, '#text': 'The code to compute the distribution based distance measures and to obtain the simplified networks is also available at the GraphDistances Github repository ().'}]",2022-10-07
0,Scientific Data,41597,10.1038/s41597-022-01561-6,Utility-driven assessment of anonymized data via clustering,30,7,2022,https://github.com/Farmerinpt/clustering-anonymization-utility,The source code of the anonymization tasks is publicly available as open source software. The clustering and cluster validity methods were based on the open source scikit-learn Python library implementation. The clustering & validity pipeline source code is publicly available at .,2022-07-30
0,Scientific Data,41597,10.1038/s41597-022-01590-1,"CRITTERBASE, a science-driven data warehouse for marine biota",6,8,2022,https://critterbase.awi.de; https://marine-data.org,"[{'ext-link': {'@xlink:href': 'https://critterbase.awi.de/code', '@ext-link-type': 'uri', '#text': 'https://critterbase.awi.de/code'}, '#text': 'The source code of the web service and portal (CRITTERBASE Version 1.0.0) - for accessing quality-checked data sets - is released under the BSD 3 license and is publicly available via the following stable link: . The software libraries and versions used are referenced in the source code.'}, {'ext-link': {'@xlink:href': '10.5281/zenodo.5724021', '@ext-link-type': 'doi', '#text': 'https://doi.org/10.5281/zenodo.5724021'}, 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR49', '#text': '49'}}, '#text': 'The Collector App for CRITTERBASE - the software for data quality checking and storage - is available as a free download under the open-source BSD 3 license (). The software libraries and versions used are referenced in the README.MD file.'}]",2022-08-06
0,Scientific Data,41597,10.1038/s41597-022-01647-1,A large EEG dataset for studying cross-session variability in motor imagery brain-computer interface,1,9,2022,,A script containing all the algorithms in this paper stored in ‘code.zip’ is provided with the dataset. All code is implemented in python (version python3.7 on Windows).,2022-09-01
0,Scientific Data,41597,10.1038/s41597-022-01732-5,Integrated database for economic complexity,15,10,2022,https://efcdata.cref.it/,The repository  contains the Jupyter notebooks written in Python 3 necessary to reproduce the reconstruction and the Fitness and Complexity algorithm.,2022-10-15
0,Scientific Data,41597,10.1038/s41597-022-01761-0,High-resolution crop yield and water productivity dataset generated using random forest and remote sensing,21,10,2022,https://doi.org/10.5281/zenodo.6444614,"The codes we developed for crop yield computation and crop yield dataset generation are available at . The code was programmed using Python 3.9. In this code, we used the sklearn library for calling machine learning algorithm and GDAL library for raster data reading and processing. Moreover, the band calculation tool of ArcGIS 10.4 was used for crop water productivity dataset generation.",2022-10-21
0,Scientific Data,41597,10.1038/s41597-022-01504-1,"Atmospheric temperature, water vapour and liquid water path from two microwave radiometers during MOSAiC",1,9,2022,,"Almost all parts of this study have been coded with Python (version 3.8.10) using the following libraries: tensorflow (2.5.0), keras (2.5.0), numpy (1.17.4 and 1.19.5 (latter for NN retrieval)), sklearn (0.24.2), netCDF4 (1.5.3 and 1.5.7 (latter for NN retrieval)), matplotlib (3.4.3), and xarray (0.18.2). The codes of the NN retrieval, the visualization scripts for the Technical Validation and Usage Notes are openly accessible and listed in Table . The scripts for HATPRO retrievals and processing of TB data of both instruments, written in the programming language IDL, are also available on Github and Zenodo.",2022-09-01
0,Scientific Data,41597,10.1038/s41597-022-01533-w,"Automatic question answering for multiple stakeholders, the epidemic question answering dataset",21,7,2022,https://github.com/h4ste/epic_qa,"The code used to prepare the EPIC-QA dataset is provided at , and a Python script for computing the evaluation metrics reported in the technical validation section of this manuscript is provided with the dataset.",2022-07-21
0,Scientific Data,41597,10.1038/s41597-022-01562-5,Standard metadata for 3D microscopy,27,7,2022,https://github.com/Defining-Our-Research-Methodology-DORy/3D-Microscopy-Metadata-Standards-3D-MMS,"All custom software, including the python code to transfer csv-formatted data to JSON and to validate JSON formatted data, are freely available at .",2022-07-27
0,Scientific Data,41597,10.1038/s41597-022-01591-0,Gridded land use data for the conterminous United States 1940–2015,13,8,2022,https://www.safe.com/; https://github.com/johannesuhl/ztrax2sqlite2csv,The ZTRAX dataset was stored in relational databases using Safe Software Feature Manipulation Engine (FME) (). Code for this pipeline is available at .,2022-08-13
0,Scientific Data,41597,10.1038/s41597-022-01733-4,"High-content, arrayed compound screens with rhinovirus, influenza A virus and herpes simplex virus infections",8,10,2022,,"[{'italic': ['AntiVir_batchprocessing.m', 'idr0130/3-Screen/Analysis', 'AntiVir_batchprocessing.m', 'idr130/3-Screen/Analysis', 'Plaque2/matlab', 'AntiVir_batchprocessing.m'], 'ext-link': [{'@xlink:href': 'https://github.com/plaque2/matlab', '@ext-link-type': 'uri', '#text': 'https://github.com/plaque2/matlab'}, {'@xlink:href': 'https://github.com/plaque2/matlab/tree/antivir', '@ext-link-type': 'uri', '#text': 'https://github.com/plaque2/matlab/tree/antivir'}], '#text': 'Plaque2.0 batch image analysis for infection scoring was performed by MATLAB (version R2016b, The MathWorks, Natick, USA) script  used by UZH for image analysis is provided at IDR under . It is based on the Plaque2.0 software available on GitHub:  under GPLv3 open source license. To batch analyze the virus screening data by Plaque2.0, fork or download the Plaque2.0 AntiVir code from GitHub: . Place the  file from  into the  folder and follow the instructions in . A MATLAB license is required.'}, {'italic': ['AntiVir_hitfiltering', 'idr130/3-Screen/Analysis'], '#text': 'Hit filtering using R was performed in R (version 4.0.2.) script  used by UZH for data processing and hit filtering is provided at IDR under .'}]",2022-10-08
0,Scientific Data,41597,10.1038/s41597-022-01649-z,A comprehensive data set of physical and human-dimensional attributes for China’s lake basins,25,8,2022,,"Two core tools applied in the study were ‘Spatial join’ and ‘Zonal Statistics’ provided by ESRI’s. ArcGIS 10.7 software package. In addition, the customized batch steps of reprocessing data, including lake area extraction and raster attribute extraction, were programmed using Python 2.7 scripts which were provided in our data set named ‘Lake_area_extraction.py’ and ‘Raster_attribute_ extraction.py’, respectively.",2022-08-25
0,Scientific Data,41597,10.1038/s41597-022-01764-x,", a 4-month multi-modal dataset capturing unobtrusive snapshots of our lives in the wild",31,10,2022,https://github.com/Datalab-AUTH/LifeSnaps-Anonymization; https://github.com/Datalab-AUTH/LifeSnaps-EDA,"All code for dataset anonymization is available at . All code for reading, processing, and exploring the data is made openly available at . Information about setup, code dependencies, and package requirements are available in the same GitHub repository.",2022-10-31
0,Scientific Data,41597,10.1038/s41597-022-01507-y,The RETA Benchmark for Retinal Vascular Tree Analysis,11,7,2022,,"['We would like to release the following software, algorithms and trained vessel segmentation model that used in the construction process of RETA benchmark. They can be found from the “codes”, “models”, “software” and “supports” folders.', {'bold': 'CARL software', '#text': '• . It could help users to investigate annotation details of vascular structures in a zoomed and magnified view. Users are also able to visualize and evaluate segmentation results of retinal anatomical structures and lesions using CARL software. It can work as a model auditing tool and is available in the “software” folder.'}, {'bold': 'Image transformation and enhancement', '#text': '• . The “codes” folder contains essential MATLAB source code of our image preprocessing algorithm. Users can use “IDRiD_cropImage.m” to crop original colour images of the IDRiD challenge. “IDRiD_restoreImage.m” is used to upsample labelled vessel masks. A script file named as “CARL_image2mat.m” can transform users’ private images into our custom mat files.'}, {'bold': 'Vessel segmentation model', '#text': '• . A pretrained vessel segmentation model and corresponding model inference code are in the “models” folder. Please read usage documentation before executing model inference. It may take some time to set up the running environment.'}, {'bold': 'Vessel centreline extraction', '#text': '• . We borrowed a Python implementation of the skeletonization method from “scikit-image” Python package to extract vessel centrelines. The parameter “method” of the used skeletonize function is “lee” in our experiment.'}, {'bold': 'Data augmentation', '#text': '• . We utilised basic image rotation and flipping to create a large-scale image set for model training. A multi-thread image augmentation code implemented in Python programming language is saved in the “supports” folder.'}]",2022-07-11
0,Scientific Data,41597,10.1038/s41597-022-01594-x,A high-resolution Orbitrap Mass spectral library for trace volatile compounds in fruit wines,13,8,2022,,"The Processing setup, Quan browser and Qual browser (Thermo Fisher Scientific, Les Ulis, France) in Xcalibur version 4.1 and Thermo Scientific TraceFinder (version 4.1) were used for collecting the HRMS library of volatile compounds. The structures of the volatile compounds were drawn using ChemDraw Professional 17.0 (Cambridgesoft, USA). High-resolution mass spectrums are plotted using Python (version 3.7).",2022-08-13
0,Scientific Data,41597,10.1038/s41597-022-01707-6,Machine actionable metadata models,30,9,2022,https://github.com/FAIRsharing/mircat,"['All the code produced for the present study is available from the following GitHub repositories:', {'ext-link': {'@xlink:href': 'http://github.com/fairsharing/jsonldschema', '@ext-link-type': 'uri', '#text': 'http://github.com/fairsharing/jsonldschema'}, '#text': '-'}, {'ext-link': {'@xlink:href': 'https://github.com/fairsharing/jsonschema-documenter', '@ext-link-type': 'uri', '#text': 'https://github.com/fairsharing/jsonschema-documenter'}, '#text': '-'}, {'ext-link': {'@xlink:href': 'https://github.com/FAIRsharing/JSONschema-compare-and-view', '@ext-link-type': 'uri', '#text': 'https://github.com/FAIRsharing/JSONschema-compare-and-view'}, '#text': '-'}, {'ext-link': {'@xlink:href': 'https://jsonldschema.readthedocs.io', '@ext-link-type': 'uri', '#text': 'https://jsonldschema.readthedocs.io'}, '#text': 'Supporting documentation is available from .'}]",2022-09-30
0,Scientific Data,41597,10.1038/s41597-022-01622-w,An Electroencephalography-based Database for studying the Effects of Acoustic Therapies for Tinnitus Treatment,17,8,2022,,"The computer program to estimate the IAF can be found in, and is described in.",2022-08-17
0,Scientific Data,41597,10.1038/s41597-022-01737-0,"OSeMOSYS Global, an open-source, open data global electricity system model generator",14,10,2022,https://github.com/OSeMOSYS/osemosys_global; https://osemosys-global.readthedocs.io,"All code for OSeMOSYS Global is provided in a public GitHub repository () under a MIT license. Instructions on how to install and run OSeMOSYS Global can be found in the repository or on the OSeMOSYS Global website (). While a basic understanding of Python and OSeMOSYS will be beneficial, it is not required to run the workflow. All users and readers are invited to contribute to OSeMOSYS Global following the contribution guidelines outlined on the repository.",2022-10-14
0,Scientific Data,41597,10.1038/s41597-022-01509-w,EEG Dataset for RSVP and P300 Speller Brain-Computer Interfaces,8,7,2022,https://github.com/KyunghoWon-GIST/EEG-dataset-for-RSVP-P300-speller,"Project name: EEG dataset for RSVP and P300 Speller Brain-Computer Interfaces. Project home page: . Operating system(s): Windows, MAC. Programming language: MATLAB, Python. Other requirements: MATLAB r2020a or higher, Python 3.6 or higher. License: MIT License. We note that the results of the article were produced using MATLAB. We provide MATLAB and Python scripts, and users can use Python to extract features and evaluate P300 speller performance as well, but the result may differ slightly from MATLAB.",2022-07-08
0,Scientific Data,41597,10.1038/s41597-022-01538-5,A resource for assessing dynamic binary choices in the adult brain using EEG and mouse-tracking,16,7,2022,https://github.com/andlab-um/MT-EEG-dataset,"The code used to preprocess the data and plot results is openly available on GitHub (). For more details about code usage, please refer to the GitHub repository.",2022-07-16
0,Scientific Data,41597,10.1038/s41597-022-01623-9,"The NIMH intramural healthy volunteer dataset: A comprehensive MEG, MRI, and behavioral resource",25,8,2022,https://github.com/nih-megcore/hv_protocol,"MEG task paradigms, and scripts used for DICOM to BIDS format conversion and de-identification of structural MRI scans are available in the study git repository: .",2022-08-25
0,Scientific Data,41597,10.1038/s41597-022-01796-3,Near-real-time daily estimates of fossil fuel CO emissions from major high-emission cities in China,10,11,2022,https://github.com/dh107/Carbon-Monitor-Cities/,"Python code for producing, reading and plotting data in the dataset is provided at .",2022-11-10
0,Scientific Data,41597,10.1038/s41597-022-01624-8,A multi-scale probabilistic atlas of the human connectome,23,8,2022,https://github.com/connectomicslab/probconnatlas,The custom code used to apply the atlas to new subjects is implemented in Python 3.8 and is available at the github repository . This code needs the multi-scale probabilistic atlas files stored on the Zenodo repository. The used and the current version of the software is 1.0. All the parameters employed to process the datasets are provided in the atlas files.,2022-08-23
0,Scientific Data,41597,10.1038/s41597-022-01510-3,The heterogeneous pharmacological medical biochemical network PharMeBINet,11,7,2022,https://github.com/ckoenigs/PharMeBINet; https://pharmebi.net,"The script to generate PharMeBINet is available from Zenodo and from GitHub . The repository contains python (3.8.5) programs and bash (4.4.20) scripts to execute all python programs, download databases, and execute the Neo4j Cypher tool. The website is available at . The download of PharMeBINet is provided on the website.",2022-07-11
0,Scientific Data,41597,10.1038/s41597-022-01739-y,Implementation of FAIR principles in the IPCC: the WGI AR6 Atlas repository,15,10,2022,https://github.com/IPCC-WG1/Atlas; https://github.com/SantanderMetGroup/2022_Iturbide_FAIRprinciplesIPCC,"All code and notebooks in the Atlas repository are available at  and on Zenodo under the Creative Commons Attribution license, CC-BY 4.0. The new notebook reusing the Atlas repository dataset to produce GWL plots (Fig. ) is available at  and also on Zenodo.",2022-10-15
0,Scientific Data,41597,10.1038/s41597-022-01598-7,SeEn: Sequential enriched datasets for sequence-aware recommendations,4,8,2022,https://github.com/lasigeBioTM/SeEn,The code used for creating the datasets is available at: .,2022-08-04
0,Scientific Data,41597,10.1038/s41597-022-01710-x,Introducing the FAIR Principles for research software,14,10,2022,,No code is shared in this paper.,2022-10-14
0,Scientific Data,41597,10.1038/s41597-022-01711-w,Lake volume variation in the endorheic basin of the Tibetan Plateau from 1989 to 2019,8,10,2022,,The annual area and RLV of lakes larger than 1 km from 1989 to 2019 were produced using GEE API and Python. The code developed for this work are openly shared with the scientific community at Zenodo repository. GEE should be used to access and edit the code.,2022-10-08
0,Scientific Data,41597,10.1038/s41597-022-01512-1,Proteomic characterisation of triple negative breast cancer cells following CDK4/6 inhibition,11,7,2022,https://artyomovlab.wustl.edu/phantasus/,"Statistics (one sample t-test) was performed using the Perseus software version 1.6.14.0. PCA was performed and plotted using the python bioinfokit package. Pearson correlation, hierarchical clustering analysis and subsequent similarity matrix were performed and created using the Phantasus software version 1.11.0 (). Volcano plots were created using VolcaNoseR (mirror R2). Network analysis was performed using the String application on Cytoscape version 3.9.0. Heatmap and bar charts were created using GraphPad Prism version 9.2.0. Images were stored on Perkin Elmer Columbus server version 2.9.1. No custom code was used to process the data in this manuscript.",2022-07-11
0,Scientific Data,41597,10.1038/s41597-022-01570-5,A high spatial resolution land surface phenology dataset for AmeriFlux and NEON sites,27,7,2022,https://github.com/BU-LCSC/PLSP,Python and R source code to download and process the PlanetScope imagery and generate the product can be obtained through a public repository at . R source code for generating the figures in the Technical Validation section is also available on the same repository.,2022-07-27
0,Scientific Data,41597,10.1038/s41597-022-01656-0,Five years calibrated observations from the University of Bonn X-band weather radar (BoXPol),8,9,2022,https://github.com/vlouf/cluttercal; https://github.com/vlouf/cluttercal; https://github.com/vlouf/gpmmatch,"The described  calibration and correction procedures are available in the python packages wradlib, Py-ART, cluttercal () and gpmmatch () and demonstration scripts for data visualization, processing and absolute calibration are provided as part of the data repository and the published relative calibration codes are also available on github (cluttercal).",2022-09-08
0,Scientific Data,41597,10.1038/s41597-022-01542-9,Dataset of Speech Production in intracranial Electroencephalography,22,7,2022,https://github.com/neuralinterfacinglab/SingleWordProductionDutch,"All Python code to re-run the technical validation described in this report can be found on our Github: . The code relies on the numpy, scipy, pynwb, scikit-learn and pandas packages.",2022-07-22
0,Scientific Data,41597,10.1038/s41597-022-01571-4,qMRI-BIDS: An extension to the brain imaging data structure for quantitative magnetic resonance imaging data,24,8,2022,,"Source code to generate quantitative maps in the example dataset are provided by qMRLab, hMRI-Toolbox and pymp2rage. Each software provides extensive user documentation, which were followed to create derivative datasets.",2022-08-24
0,Scientific Data,41597,10.1038/s41597-022-01657-z,Carbon Monitor Cities near-real-time daily estimates of CO emissions from 1500 cities worldwide,1,9,2022,https://github.com/dh107/Carbon-Monitor-Cities/,"Python code for producing, reading and plotting data for any city in the dataset is provided at .",2022-09-01
0,Scientific Data,41597,10.1038/s41597-022-01572-3,Country-level fire perimeter datasets (2001–2021),30,7,2022,https://github.com/earthlab/firedpy; https://hub.docker.com/repository/docker/earthlab/firedpy,"All code is open source and available at . The data we produced is available at the Earth Lab Data collection at CU Scholar. Links to each dataset are provided on the front page of the aforementioned github repository, and are provided here in Table . The docker container with a custom software environment for running firedpy is at .",2022-07-30
0,Scientific Data,41597,10.1038/s41597-022-01743-2,Elaboration of a new framework for fine-grained epidemiological annotation,26,10,2022,https://doi.org/10.18167/DVN1/YGAKNB; https://scikit-learn.org/,The labelled corpus in our experiments are publicly available in a Dataverse repository: . The whole classification and evaluation pipeline was performed using the scikit-learn library (Python): .,2022-10-26
0,Scientific Data,41597,10.1038/s41597-022-01573-2,", a multimodal activity recognition dataset acquired from radio frequency and vision-based sensors",3,8,2022,,"[{'monospace': 'codes', '#text': 'Some Matlab and Python scripts have been made available in the  directory for the users to replicate some of the figures in this Data Descriptor:'}, {'monospace': '• plot_wificsi.m', 'xref': {'@rid': 'Fig5', '@ref-type': 'fig', '#text': '5(a)'}, '#text': ': This script is used to load the complex WiFi CSI data recorded by each NUC device and visualize the amplitude variations over time, as illustrated in Fig.\xa0. The user can specify the start and stop timestamps and visualize the CSI stream in that time segment (for a given transmit antenna, receive antenna, and subcarrier index). Furthermore, for comparison purposes, the generated plots consist of the raw (unfiltered) CSI data and those which have been denoised using Discrete Wavelet Transform (DWT).'}, {'monospace': '• plot_uwb.m', 'xref': [{'@rid': 'Fig5', '@ref-type': 'fig', '#text': '5(b)'}, {'@rid': 'Fig7', '@ref-type': 'fig', '#text': '7'}], '#text': ': This script is used to load the complex CIR data recorded by each passive UWB system, convert it into CFR using FFT and visualize the amplitude variations over a given time segment (between a given pair of UWB nodes), as illustrated in Fig.\xa0. Furthermore, this script allows the users to plot the accumulated and aligned CIR measurements, as shown in Fig.\xa0.'}, {'monospace': ['• plot_uwb_fppow_crowdcount.m', 'exp028'], 'xref': {'@rid': 'Fig6', '@ref-type': 'fig', '#text': '6'}, '#text': ': This script is used to load the UWB data for the crowd counting experiment () and plot the first path power level (in dBm) over time for each UWB system (between a given pair of UWB nodes), as illustrated in Fig.\xa0.'}, {'monospace': ['• plot_PWR_demonstration.m', 'plot_pwr_spectrogram.py'], 'xref': {'@rid': 'Fig5', '@ref-type': 'fig', '#text': '5(d)'}, '#text': 'and : These scripts allow the users to visualize the PWR spectrograms from the three surveillance channels: “rx2” (as illustrated in Fig.\xa0), “rx3” and “rx4”, as a function of time and Doppler.'}, {'monospace': '• plot_kinect_data.m', 'xref': [{'@rid': 'Fig5', '@ref-type': 'fig', '#text': '5(c)'}, {'@rid': 'Fig12', '@ref-type': 'fig', '#text': '12(a,b)'}], '#text': ': This script allows the user to plot the motion capture data (as a function of velocity versus time) from one of the two Kinect systems, as illustrated in Fig.\xa0. Furthermore, the users can visualize the stick (skeletal) representation of the kinect motion capture data as an animation over the specified time segment. For example, two frames of the stick representations when a person is standing and sitting are illustrated in Fig.\xa0, respectively.'}, {'monospace': '• oddet.py', 'ext-link': {'@xlink:href': 'https://github.com/RogetK/ODDET', '@ext-link-type': 'uri', '#text': 'https://github.com/RogetK/ODDET'}, '#text': ': This python script allows the user to extract only the modalities and features needed rather than having to load the entire files and then stripping out unused features. With this python script, one can select the modality, experiment number and features needed through the command line interface. Additionally if a specific set of features are required, one can also specify all the columns needed through YAML configurations which will allow the user to curate the dataset to the format that more closely suits the usage. This python script is available at the following GitHub repository: .'}]",2022-08-03
0,Scientific Data,41597,10.1038/s41597-022-01659-x,World carbon pricing database: sources and methods,17,9,2022,https://github.com/g-dolphin/WorldCarbonPricingDatabase,"All code is written in the Python 3 programming language. All files, including Python files, necessary to the compilation of the dataset are available on the following GitHub repository: .",2022-09-17
0,Scientific Data,41597,10.1038/s41597-022-01773-w,Comparative sensitivity of social media data and their acceptable use in research,22,10,2022,https://github.com/casmlab/personal-data-survey; https://doi.org/10.5281/zenodo.6807258,Code used to perform the data preparation and analysis for this paper is available on GitHub at  and through Zenodo with the identifier .,2022-10-22
0,Scientific Data,41597,10.1038/s41597-022-01545-6,A large collection of real-world pediatric sleep studies,19,7,2022,https://github.com/liboyue/sleep_study,"The code that was used to analyze patient data, read EDF files, run baseline sleep stage classifier, and generate figures and tables in this paper is published at .",2022-07-19
0,Scientific Data,41597,10.1038/s41597-022-01630-w,ADTC-InSAR: a tropospheric correction database for Andean volcanoes,27,8,2022,,ADTC-InSAR has been developed in MathWorks MATLAB version R2019a using TRAIN MATLAB software. It is possible to generate tropospheric corrections for the previously mentioned volcanoes using the Corr__ADTC_InSAR.m and Corr__ADTC_InSAR.py scripts.,2022-08-27
0,Scientific Data,41597,10.1038/s41597-022-01716-5,Niche deconvolution of the glioblastoma proteome reveals a distinct infiltrative phenotype within the proneural transcriptomic subgroup,1,10,2022,www.coxdocs.org; www.r-project.org; https://orange.biolab.si/; https://www.gsea-msigdb.org/gsea/index.jsp; http://bioconductor.org/packages/release/bioc/html/ComplexHeatmap.html,"All analyses were generated with existing packages, and no original code was created. The packages associated with this analysis can be found below. Data analysis was performed using a variety of biostatistical platforms Perseus 1.6.15.0 (), R () (version 4.0.4), Orange3 python package () (version 3.31.0), and GSEA 4.1.0 (). Gene set enrichment analysis (GSEA) was used to define pathways enriched in each grouping. Heatmaps and clustering were performed using the R package ComplexHeatmap () (version 2.9.3).",2022-10-01
0,Scientific Data,41597,10.1038/s41597-022-01745-0,American election results at the precinct level,3,11,2022,,"A large code-base was required to translate the original raw data into organized and structured datasets. This suite of state-specific files, written variously in Python, R, and Stata, is stored and retained internally. The scripts that we used to clean the data from each state in 2018 and 2020 are all available on a public GitHub repository.",2022-11-03
0,Scientific Data,41597,10.1038/s41597-022-01517-w,The United States COVID-19 Forecast Hub dataset,1,8,2022,https://github.com/reichlab/covid19-forecast-hub-validations; https://github.com/reichlab/covidEnsembles; https://doi.org/10.5281/zenodo.5207940; https://doi.org/10.5281/zenodo.5208224,"All code for forecast data validation and storage associated with the current submission is available in the Forecast Hub GitHub repository, . Ensemble models are built with code in the  R package, . The code for forecast analysis is at  (covidHubUtils R package) and  (covidData R package). Any updates will also be published on Zenodo.",2022-08-01
0,Scientific Data,41597,10.1038/s41597-022-01746-z,A representation-independent electronic charge density database for crystalline materials,28,10,2022,https://github.com/materialsproject/api,Access to the charge density data provided by the Materials Project API () and grid transforms of the charge density is done using the  python package. See the [sec:usage]Usage Notes section for more information. The scripts used to generate the validation data can be access at along with the direct download of the validation dataset,2022-10-28
0,Scientific Data,41597,10.1038/s41597-022-01775-8,Sentinel2GlobalLULC: A Sentinel-2 RGB image tile dataset for global land use/cover mapping with deep learning,9,11,2022,https://doi.org/10.5281/zenodo.5638409,All used scripts to implement or use our dataset and links to the GEE stored assets are available in the following Github repository () repository with guidelines stored in a README file explaining all instructions about their execution.,2022-11-09
0,Scientific Data,41597,10.1038/s41597-022-01547-4,An update on global mining land use,22,7,2022,www.github.com/fineprint-global/app-mining-area-polygonization,"All the code and geoprocessing scripts used to produce the results of this paper are distributed under the GNU General Public License v3.0 (GPL-v3) from the repository . The processing scripts were written in R, Python, and GDAL (Geospatial Data Abstraction Library). The web application to delineate the polygons was written in R Shiny using a PostgreSQL database with PostGIS extension for storage. The full app setup uses Docker containers to facilitate management, portability, and reproducibility.",2022-07-22
0,Scientific Data,41597,10.1038/s41597-022-01690-y,From raw measurements to human pose - a dataset with low-cost and high-end inertial-magnetic sensor data,30,9,2022,,"[{'italic': 'Zenodo', 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR15', '#text': '15'}}, '#text': 'This database is accompanied by a folder with all the scripts used to process and handle the data described. It is openly hosted in .'}, {'ext-link': {'@xlink:href': 'https://github.com/ManuelPalermo/HumanInertialPose.git', '@ext-link-type': 'uri', '#text': 'https://github.com/ManuelPalermo/HumanInertialPose.git'}, '#text': 'Additionally, an extended code repository is available on Github () with updated code to not only process the data described, but also calculate kinematics, visualize and evaluate the resulting motions and offers extended support for general inertial pose estimation pipelines. All scripts are based on the Python programming language and, thus, open source. The code contains a permissive MIT license for unrestricted usage.'}, 'We hope this dataset and associated code can further contribute to the development and evaluation of classic or data-driven inertial human pose estimation solutions, with applications, for example, in human movement understanding and forecasting, ergonomic assessment and gait/posture analysis.']",2022-09-30
0,Scientific Data,41597,10.1038/s41597-022-01491-3,Making marine image data FAIR,15,7,2022,https://gitlab.hzdr.de/datahub/marehub/ag-videosimages/mariqt,The Python library MarIQT is available as open source and under rapid development. Contributions in the form of issue submissions and merge requests are welcome. .,2022-07-15
0,Scientific Data,41597,10.1038/s41597-022-01718-3,A whole-body FDG-PET/CT Dataset with manually annotated Tumor Lesions,4,10,2022,https://github.com/lab-midas/TCIA; https://github.com/lab-midas/autoPET/tree/master/,We provide the code of the data conversion and processing pipeline under  processing. The trained PET/CT lesion segmentation model is publicly available under .,2022-10-04
0,Scientific Data,41597,10.1038/s41597-022-01776-7,A comprehensive ultra-wideband dataset for non-cooperative contextual sensing,22,10,2022,,"['A MATLAB script has been made available in the dataset directory for the users to replicate some of the figures in this Data Descriptor:', {'monospace': ['plot_uwb_signals.m', 'background_CIR', 'target_CIR', '.csv', '.mat'], 'xref': [{'@rid': 'Fig3', '@ref-type': 'fig', '#text': '3'}, {'@rid': 'Fig4', '@ref-type': 'fig', '#text': '4'}, {'@rid': 'Fig5', '@ref-type': 'fig', '#text': '5'}], '#text': '• : This script can be used to load the two files  and  (either in  format or  format) and plot the Channel Frequency Response (CFR) of the UWB data for a desired time segment (between a chosen pair of UWB nodes), as illustrated in Fig.\xa0. Furthermore, this script allows the users to plot the first path power level (in dBm) and ground truth trajectory, as depicted in Fig.\xa0, and the aligned CIR measurements as demonstrated in Fig.\xa0.'}]",2022-10-22
0,Scientific Data,41597,10.1038/s41597-022-01691-x,Reference bioimaging to assess the phenotypic trait diversity of bryophytes within the family Scapaniaceae,4,10,2022,,"Software code and scripts used in this study are available as Open Source in github. Python scripts were tested under Python 3.7 and require the additional modules PIL, pandas, xml, csv, errno, sys, os, argparse, glob, pathlib and re. R scripts were tested in R 4.1.3 and require the additional packages parallel, foreach, and doMC. Shell scripts were tested using Bourne Again Shell (bash) 5.1.16.",2022-10-04
0,Scientific Data,41597,10.1038/s41597-022-01349-8,A high-speed railway network dataset from train operation records and weather data,27,5,2022,,We share our codes for data processing and generation in GitHub. The detailed description of the codes is in the README.md.,2022-05-27
0,Scientific Data,41597,10.1038/s41597-022-01378-3,"A multisource database tracking the impact of the COVID-19 pandemic on the communities of Boston, MA, USA",20,6,2022,https://github.com/BARIBoston,All the codes are published through BARI’s GitHub account (user: @BARIBoston; ).,2022-06-20
0,Scientific Data,41597,10.1038/s41597-022-01405-3,"Wet-Bulb Globe Temperature, Universal Thermal Climate Index, and Other Heat Metrics for US Counties, 2000–2020",17,6,2022,,We developed the  R package to facilitate replication of these methods to other meteorological data sets. The package is available to download via figshare.,2022-06-17
0,Scientific Data,41597,10.1038/s41597-022-01434-y,An 8-year record of phytoplankton productivity and nutrient distributions from surface waters of Saanich Inlet,4,7,2022,https://github.com/bjmcnabb/Saanich_Inlet,"The majority of data processing was done using Microsoft Excel 2010® version 14.0.4734.100. Python v3.8 was used for the calculations of the percentage size fractions of Chl-a, seasonal averaging (. binning values into a monthly average for each sampled depth) and scaling the figure colormaps. The specific code written for this manuscript can be found within the plotting script at the following open source GitHub repository: .",2022-07-04
0,Scientific Data,41597,10.1038/s41597-022-01264-y,"RA-MAP, molecular immunological landscapes in early rheumatoid arthritis and healthy vaccine recipients",9,5,2022,https://github.com/C4TB/RA-MAP,Fully annotated Executable R scripts and R Markdown documents are available in our public RA-MAP GitHub in order to allow complete reproduction of our analysis workflow (). All analyses were conducted in R version 4.0.5.,2022-05-09
0,Scientific Data,41597,10.1038/s41597-022-01293-7,"USEEIO v2.0, The US Environmentally-Extended Input-Output Model v2.0",3,5,2022,,USEEIO v2.0 was built in . The environmental and employment datasets were created with  as flow-by-sector data products. The flow-by-sector method names for the corresponding datasets are shown in Table . The indicator characterization factors for all elementary flows were built in the  as LCIA data products. The USEEIO Modeling Framework for USEEIO v2.0 provides an overview of the source code along with links to  and supporting software packages.,2022-05-03
0,Scientific Data,41597,10.1038/s41597-022-01435-x,"A curated, ontology-based, large-scale knowledge graph of artificial intelligence tasks and benchmarks",17,6,2022,https://github.com/OpenBioLink/ITO; https://doi.org/10.5281/zenodo.6566103,Code to generate the summary statistics are available from the Github repository in the folder ‘notebooks’: . The entire Github repository for the v1.01 release is archived on Zenodo: .,2022-06-17
0,Scientific Data,41597,10.1038/s41597-022-01294-6,Auto-generated database of semiconductor band gaps using ChemDataExtractor,3,5,2022,https://github.com/QingyangDong-qd220/BandgapDatabase1; https://github.com/QingyangDong-qd220/BandgapDatabase1/tree/main/chemdataextractor2; http://www.chemdataextractor2.org/download,"All source code for this work is freely available under the MIT license. The source code used to generate the band gap database is available at  and . The updated patch for the modified Snowball algorithm, which is now compatible with nested models, is available at . A clean build of the ChemDataExtractor version 2.0 code is available at .",2022-05-03
0,Scientific Data,41597,10.1038/s41597-022-01321-6,"Text-mined dataset of gold nanoparticle synthesis procedures, morphologies, and size entities",26,5,2022,https://github.com/CederGroupHub/text-mined-aunp-synthesis_public,"Scripts developed for the generation of this dataset as well as notebooks for example data analysis are available at , along with an acknowledgement for this paper. The libraries use for this project are: , , , , , , , and .",2022-05-26
0,Scientific Data,41597,10.1038/s41597-022-01350-1,Plant phenotype relationship corpus for biomedical relationships between plants and phenotypes,26,5,2022,https://github.com/DMCB-GIST/PPRcorpus,The Python codes for the NER and RE experiments represented in Technical Validation can be accessed from .,2022-05-26
0,Scientific Data,41597,10.1038/s41597-022-01266-w,A multi-million image Serial Femtosecond Crystallography dataset collected at the European XFEL,12,4,2022,http://www.desy.de/twhite/crystfel/,"Data was analysed with CrystFEL 0.9.1. The CrystFEL 0.9.1 software suite is a free open source software available under the GNU Public License version 3 and can be downloaded from . The AGIPD data was calibrated using the EuXFEL calibration pipeline, release 3.0.0-beta. The raw data and calibration constants are also available for development of calibration algorithms.",2022-04-12
0,Scientific Data,41597,10.1038/s41597-022-01295-5,A database of refractive indices and dielectric constants auto-generated using ChemDataExtractor,3,5,2022,https://github.com/JIUYANGZH/opticalmaterials_database; https://github.com/JIUYANGZH/opticalmaterials_database/tree/master/chemdataextractor; https://github.com/JIUYANGZH/aws_test,The source code used to generate the database is available at . The code of ChemDataExtractor that was modified for database auto-generation in the optical-property domain is available at . The source code of the website can be found at .,2022-05-03
0,Scientific Data,41597,10.1038/s41597-022-01380-9,Implementing the reuse of public DIA proteomics datasets: from the PRIDE database to Expression Atlas,14,6,2022,https://github.com/PRIDE-reanalysis/DIA-reanalysis,"The complete open reanalysis pipeline description and documentation, workflows, container recipes, and custom code and visualisation scripts, as well as parameter input files are available through the GitHub repository at .",2022-06-14
0,Scientific Data,41597,10.1038/s41597-022-01352-z,FAIR and Interactive Data Graphics from a Scientific Knowledge Graph,27,5,2022,https://github.com/tetherless-world/whyis/; https://github.com/vega/vega-lite; https://www.w3.org/TR/sparql11-query/,Source code for the Whyis application framework can be found on Github at . Source code and documentation for the Vega-Lite project can be found on Github at . The W3C Recommendation for the SPARQL 1.1 Query Language can be found at .,2022-05-27
0,Scientific Data,41597,10.1038/s41597-022-01409-z,The two decades brainclinics research archive for insights in neurophysiology (TDBRAIN) database,14,6,2022,www.brainclinics.com/resources; www.synapse.org; https://doi.org/10.70303/syn25671079; https://github.com/BCD-gitprojects/TDBRAIN/,"The data presented in the database contains raw, full time-series EEG recordings and it is possible to analyse in any way. Nonetheless, for full transparency and replicability the complementary custom python code used for preprocessing (which was peer reviewed and beta-tested) as well as the code used for the neurophysiological validation is published together with the entire dataset on  as well as on  () in one package and available under the same conditions described above. In addition, we have also published the TD_BRAIN_code on github: .",2022-06-14
0,Scientific Data,41597,10.1038/s41597-022-01438-8,Novel inorganic crystal structures predicted using autonomous simulation agents,14,6,2022,http://github.com/TRI-AMDD/CAMD,"The CAMD code used to generate the data described herein is available at . Scripts used to generate and analyze the dataset, as well as reproduce the figures in this manuscript are all included in the above data repository.",2022-06-14
0,Scientific Data,41597,10.1038/s41597-022-01467-3,A multi-city urban atmospheric greenhouse gas measurement data synthesis,24,6,2022,https://github.com/uataq/co2usa_data_synthesis,All of the code used to create and extract the CO-USA synthesis data set is maintained in an open access GitHub repository: .,2022-06-24
0,Scientific Data,41597,10.1038/s41597-022-01239-z,Electrophysiological dataset from macaque visual cortical area MST in response to a novel motion stimulus,19,4,2022,,"['We provide MATLAB code that serves two purposes: (a) to give examples of how the data can be accessed; and (b) to perform some elementary data validation analyses. The code is available along with the data in the repository. We briefly summarize the MATLAB scripts that we provide along with the data.', {'bold': {'italic': 'h5_extract.'}}, 'h5_extract() is a function that takes as its input the name of an HDF5 file, a cell array of strings that specify the parameters of the experiment that are to be extracted (for this dataset, this should always be event_value, and event_time), and a cell array of strings that specify the events that are to be extracted (typically the list of event names that can be extracted from the HDF5 itself, as demonstrated in the example scripts). The output of the function are two MATLAB structure arrays - event_value and event_time - which correspond to the groups in the HDF5 file of the same name. Each field name of the structure arrays is the name of an event and the content in the field corresponds to the values and times associated with the event, respectively. Once extracted, the data are ready for visualization and analysis. The scripts technical_validation, example_raster_fr, and example_trial_histogram demonstrate the use of the h5_extract() function to read in all events saved in an HDF5 file.', {'bold': {'italic': 'technical_validation.'}}, 'The technical_validation script was used to perform all the plausibility checks described in the section “Technical Validation”. The script prints all the statements from that section of the manuscript that contain quantitative information about the data (such as, for example, average firing rate) to the console.', {'bold': {'italic': 'example_raster_fr.'}}, 'To demonstrate how individual spike times can be accessed and visualized, the script creates a raster plot of spike trains and a scatter plot of firing rates per trial (for trials lasting at least 500\u2009ms), colored by trial outcome, for one example file. The example file is specified in the first line of the script and can easily be changed by the user. The raster plot also includes the time of reward delivery and the end of each trial.', {'bold': {'italic': 'example_trial_histogram.'}}, 'To demonstrate how multiple files can be accessed for population analyses, a histogram of the number of trials across recording sessions is produced for each of the three experiments and color-coded by monkey.', {'bold': {'italic': 'example_probe_time.'}}, 'This script contains a function, probe_time(), which extracts the values of all events at a given time point, as well as some additional code that illustrates the use of the function.', {'bold': {'italic': 'example_rc_stim_extraction.'}}, 'This script demonstrates how the direction and speed values of the RC stimulus can be extracted and in particular, how those segments that were masked in some of the files (see Methods) can be determined.', {'bold': {'italic': 'example_eye_data.'}}, {'italic': ['n', 'n'], '#text': 'To demonstrate how the eye tracking data can be accessed, this function plots the x- and y-position of the monkey’s gaze as well as the size of the right and left pupil for the first  timesteps (where  can be specified by the user, default is 100).'}, {'bold': {'italic': 'example_spatial_mapping_analysis and example_tuning_analysis.'}}, 'To demonstrate how spiking activity can be related to stimulus features, these two functions plot firing rate as a function of probe location in the Spatial Mapping experiment (example_spatial_mapping_analysis) or as a function of motion direction and speed in the Tuning experiment (example_tuning_analysis).']",2022-04-19
0,Scientific Data,41597,10.1038/s41597-022-01297-3,Better force fields start with better data: A data set of cation dipeptide interactions,17,6,2022,,All custom codes used in this study have been uploaded to Github.,2022-06-17
0,Scientific Data,41597,10.1038/s41597-022-01269-7,"MASCDB, a database of images, descriptors and microphysical properties of individual snowflakes in free fall",3,5,2022,https://github.com/ltelab/pymascdb; https://pymascdb.readthedocs.io/en/latest/index.html,"The  package to manipulate the data and described in the previous section can be freely accessed at . Code documentation, examples and installation instructions are also available at .",2022-05-03
0,Scientific Data,41597,10.1038/s41597-022-01298-2,CPG: A FAIR Knowledge Graph of COVID-19 Publications,8,7,2022,https://github.com/dice-group/COVID19DS,Our source code to generate the new versions of our knowledge graph is publicly available at  and is maintained in parallel with the knowledge graph.,2022-07-08
0,Scientific Data,41597,10.1038/s41597-022-01354-x,Nine-day continuous recording of EEG and 2-hour of high-density EEG under chronic sleep restriction in mice,23,5,2022,,All the Python scripts used for analysis in the Technical Validation section for analysis and figure generation are available in the G-Node repository.,2022-05-23
0,Scientific Data,41597,10.1038/s41597-022-01410-6,"FutureStreams, a global dataset of future streamflow and water temperature",15,6,2022,,"[{'ext-link': [{'@xlink:href': 'https://github.com/UU-Hydro/PCR-GLOBWB_model', '@ext-link-type': 'uri', '#text': 'https://github.com/UU-Hydro/PCR-GLOBWB_model'}, {'@xlink:href': '10.5281/zenodo.247139', '@ext-link-type': 'doi', '#text': 'https://doi.org/10.5281/zenodo.247139'}, {'@xlink:href': '10.5281/zenodo.1045339', '@ext-link-type': 'doi', '#text': 'https://doi.org/10.5281/zenodo.1045339'}, {'@xlink:href': 'https://github.com/wande001/dynWat', '@ext-link-type': 'uri', '#text': 'https://github.com/wande001/dynWat'}, {'@xlink:href': '10.5281/zenodo.3337659', '@ext-link-type': 'doi', '#text': 'https://doi.org/10.5281/zenodo.3337659'}], 'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR32', '#text': '32'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR33', '#text': '33'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR13', '#text': '13'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR34', '#text': '34'}}], 'italic': 'et al', '#text': 'Weekly water temperature and flow estimates at 5 arcminute were created using PCR-GLOBWB and DynWat. The PCR-GLOBWB model code is available at  as well as , and the global input files are available through . The DynWat code is available via . Monthly output from Wanders . is available through .'}, {'ext-link': [{'@xlink:href': 'https://github.com/vbarbarossa/futurestreams_figures', '@ext-link-type': 'uri', '#text': 'https://github.com/vbarbarossa/futurestreams_figures'}, {'@xlink:href': 'https://github.com/JoyceBosmans/FutureStreams', '@ext-link-type': 'uri', '#text': 'https://github.com/JoyceBosmans/FutureStreams'}], 'xref': {'@rid': 'Fig4', '@ref-type': 'fig', '#text': '4'}, '#text': 'Scripts used for figures in this paper are available through . Lastly, the repository  contains the scripts used to create the ecologically relevant derived variables, scripts to mask out grid cells with unrealistic values (see User Notes above), the script used to create Fig.\xa0, an example configuration file for PCR-GLOBWB as well as a table with years in which warming levels are reached for each RCP and GCM combination.'}, 'All the model runs were carried out on the Dutch national e-infrastructure Cartesius.']",2022-06-15
0,Scientific Data,41597,10.1038/s41597-022-01469-1,Scaling up SoccerNet with multi-view spatial localization and re-identification,21,6,2022,https://github.com/SoccerNet/; https://www.soccer-net.org,"The codes are provided in a figshare repository and are also available in a public github repository ; the last also provides codes for different baselines for solving tasks, such as camera calibration or re-identification. Those data are used in the public SoccerNet Challenge 2022 for the tasks of camera calibration, pitch localization and player re-identification. Finally, all information about the dataset, the tasks and the challenges can be found on the  website.",2022-06-21
0,Scientific Data,41597,10.1038/s41597-022-01299-1,"A  extension, MEG recordings while watching the audio-visual movie “Forrest Gump”",13,5,2022,https://github.com/BNUCNL/MEG_Gump; https://mne.tools/stable/index.html; https://mne.tools/stable/install/mne_python.html; https://fmriprep.org/en/stable/; https://github.com/poldracklab/pydeface; https://github.com/rordenlab/dcm2niix,"All custom codes for data preprocessing and technical validation are available at . Preprocessing was performed using MNE-BIDS v0.8 (), MNE v0.22 (), fMRIPrep v20.2.1 (), pydeface v2.0.0 (), and dcm2niix v1.0.20180622 ().",2022-05-13
0,Scientific Data,41597,10.1038/s41597-022-01355-w,Perovskite- and Dye-Sensitized Solar-Cell Device Databases Auto-generated Using ChemDataExtractor,17,6,2022,https://github.com/edbeard/pv_database; https://github.com/edbeard/dsc_db; https://github.com/edbeard/chemdataextractor-pv,"The code used to generate the two databases can be found at . This code makes use of a new library that can be found at , which defines the logical processes used in the overall data-extraction algorithm. ChemDataExtractor-PV, the bespoke version of ChemDataExtractor, which contains additional models and parsers for the extraction of photovoltaic data, is located at .",2022-06-17
0,Scientific Data,41597,10.1038/s41597-022-01411-5,"VitalDB, a high-fidelity multi-parameter vital signs database in surgical patients",8,6,2022,,"[{'ext-link': {'@xlink:href': '10.5281/zenodo.6321507', '@ext-link-type': 'doi', '#text': 'https://doi.org/10.5281/zenodo.6321507'}, 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR20', '#text': '20'}}, '#text': 'An open-source project facilitating the use of VitalDB dataset has been launched, and a lot of code written in C/C++, Python, Javascript, and R languages is currently available from Zenodo ().'}, {'ext-link': {'@xlink:href': '10.5281/zenodo.6321522', '@ext-link-type': 'doi', '#text': 'https://doi.org/10.5281/zenodo.6321522'}, 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR21', '#text': '21'}}, '#text': 'Python codes that can be used as references of algorithm research are also available from Zenodo ().'}, 'The examples of sample codes for statistical analysis are as following:', '- General Characteristic: vitaldb_tableone.ipynb', '- Mortality: asa_mortality.ipynb', '- Acute Kidney Injury - mbp_aki.ipynb', 'The examples of sample codes for artificial intelligence algorithms are as following:', '- Drug effect estimation using Long Short-Term Memory: ppf_bis.ipynb', '- Hypotension prediction using Long Short-Term Memory: hypotension.ipynb', '- Mortality prediction using Gradient Boosting Machine: predict_mortality.ipynb']",2022-06-08
0,Scientific Data,41597,10.1038/s41597-022-01356-9,Spatial and temporal data to study residential heat decarbonisation pathways in England and Wales,27,5,2022,https://github.com/AlexandreLab/UKERC-data,The Python code that we used to produce the datasets presented in this paper is published at .,2022-05-27
0,Scientific Data,41597,10.1038/s41597-022-01412-4,The NEON Daily Isotopic Composition of Environmental Exchanges Dataset,21,6,2022,https://doi.org/10.5281/zenodo.3836875,"Python and R code and generated CSV files are available on HydroShare. For the NEON data processing packages, refer to the NEONiso package found at on CRAN or at .",2022-06-21
0,Scientific Data,41597,10.1038/s41597-022-01441-z,"REFLACX, a dataset of reports and eye-tracking data for localization of abnormalities in chest x-rays",18,6,2022,https://github.com/ricbl/eyetracking,"The code used for all automatic processes described in this paper, involving sampling, collection, processing, and validation of data, is available at  The software and versions we used were: MATLAB R2019a, Psychtoolbox 3.0.17, Python 3.7.7, edfapi 3.1, EYELINK II CL v5.15, Eyelink GL Version 1.2 Sensor = AC7, EDF2ASC 3.1, librosa 0.8.0, numpy 1.19.1, pandas 1.1.1, matplotlib 3.5.1, statsmodels 0.12.2, shapely 1.7.1, scikit-image 0.17.2, pyrubberband 0.3.0, pydicom 2.1.2, pydub 0.24.1, soundfile 0.10.3.post1, pyttsx3 2.90, pillow 8.0.1, scikit-learn 0.23.2, nltk 3.5, syllables 1.0.0, moviepy 1.0.3, opencv 3.4.2, Ubuntu 18.04.5 LTS, espeak 1.48.04, joblib 1.1.0, ffmpeg 3.4.8, and rubberband-cli 1.8.1.",2022-06-18
0,Scientific Data,41597,10.1038/s41597-022-01328-z,A pediatric wrist trauma X-ray dataset (GRAZPEDWRI-DX) for machine learning,20,5,2022,,"['All anonymization steps were computed in Python 3.8.2 on a Windows 8.1 platform as described in the Methods section. We are not able to publicly share the actual code involved in the de-identification procedure, as patient information was processed. The de-identification procedure should be sufficiently reproducible based onto the presented information.', {'italic': ['“annotation_preview.ipynb”', '“copy_files_by_csv.ipynb”', '“image_conversion.ipynb”'], 'ext-link': {'@xlink:href': '10.6084/m9.figshare.19330688.v1', '@ext-link-type': 'doi', '#text': 'https://doi.org/10.6084/m9.figshare.19330688.v1'}, 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR43', '#text': '43'}}, '#text': 'We attached a Jupyter notebook that is able to preview images including tags and labeled objects (). It can be found in the “notebooks” folder and online at Figshare under . The dataset is accompanied by a Jupyter notebook that is able to split files into different folders, based onto two columns of a CSV file (). It is intended to efficiently prepare datasets for image classification tasks with the included “dataset.csv” file. Image post-processing code is publicly provided as second Jupyter notebook with the dataset (), also located in the “notebooks” folder.'}]",2022-05-20
0,Scientific Data,41597,10.1038/s41597-022-01471-7,Daily precipitation dataset at 0.1° for the Yarlung Zangbo River basin from 2001 to 2015,18,6,2022,http://uw-hydro.github.io/,The data was processed in Python and ArcGIS. The VIC model code could be downloaded from .,2022-06-18
0,Scientific Data,41597,10.1038/s41597-022-01272-y,OSASUD: A dataset of stroke unit recordings for the detection of Obstructive Sleep Apnea Syndrome,19,4,2022,,"[{'italic': 'preprocess_dataset.ipynb', 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR24', '#text': '24'}}, '#text': 'To allow for an easier usage of our data, a Python Jupyter Notebook is also included with the dataset (file ). The notebook has been tested with the following packages versions: pandas\u2009=\u20091.3.3, numpy\u2009=\u20091.20.3, pickle\u2009=\u20094.0. The code performs a series of data pre-processing operations, that include:'}, '• loading the Pickle file that encodes the dataset as a Pandas DataFrame;', {'xref': {'@rid': 'Tab1', '@ref-type': 'table', '#text': '1'}, '#text': '• printing some validation results, including the values presented in Table\xa0;'}, '• generating a sample machine learning-ready dataset, in the form of a set of Numpy arrays.', 'The code provided significantly contributes to relieve the burden of data pre-processing, which typically absorbs a major part of time in the development and testing of machine learning solutions.']",2022-04-19
0,Scientific Data,41597,10.1038/s41597-022-01358-7,Paired field and water measurements from drainage management practices in row-crop agriculture,1,6,2022,,"Code written by the Data Management Team in support of this project can be found on github.com under an open and permissive usage license. Gap filling of the drain flow data and visualization of the results were performed within RStudio. The source code and corresponding input and output data are publicly available on GitHub. The code is split into three pieces corresponding to individual phases of the gap-filling method. In addition, there are separate scripts for the visualization of the data. At drainagedata.org, users can visualize the data with customized tools, query based on specific sites and measurements of interest, and access site photographs, maps, summaries, and publications. The code used for the backend data services and frontend web interfaces can be found on GitHub.",2022-06-01
0,Scientific Data,41597,10.1038/s41597-022-01387-2,A database of physical therapy exercises with variability of execution collected by wearable sensors,3,6,2022,,No custom code was used to generate or process the data.,2022-06-03
0,Scientific Data,41597,10.1038/s41597-022-01388-1,PAPILA: Dataset with fundus images and clinical data of both eyes of the same patient for glaucoma assessment,9,6,2022,https://doi.org/10.6084/m9.figshare.14798004.v1,"The PAPILA dataset is publicly available at . As detailed in the composition of the dataset, the clinical data of both eyes of each patient and the corresponding diagnosis are stored in spreadsheet and plain text format. In addition, the folder named  contains sample code in Python to read, handle and process the dataset. Jupyter Notebooks are also provided to exemplify the use of the PAPILA features.",2022-06-09
0,Scientific Data,41597,10.1038/s41597-022-01415-1,Enhancing the REMBRANDT MRI collection with expert segmentation labels and quantitative radiomic features,14,6,2022,https://github.com/ICBI/rembrandt-mri,The methods and tools applied in this paper use open-source tools detailed in respective publications . publication. The python code for extracting PyRadiomics features from Rembrandt and the TCGA segmented data (Supplementary File  and  respectively) is provided here. .,2022-06-14
0,Scientific Data,41597,10.1038/s41597-022-01274-w,Using open-source data to construct 20 metre resolution maps of children’s travel time to the nearest health facility,17,5,2022,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR25', '#text': '25'}}, '#text': 'The software/code works out of the box on a current Ubuntu system using Python, and all source data were derived from open and free datasets available for the whole of Sub-Saharan Africa. The Python software, scripts and a virtual environment are provided with the data and requires minimal input from the user.'}, 'The software is controlled by a configuration file. The GitHub repository contains an example configuration file, creation.cfg. In this file the following should be changed:', '•\u2003Path to input data sets', '•\u2003Names of input data sets should these have been changed.', '•\u2003Path to the location where the output files should be saved', '•\u2003Names of the output files.', 'The user can also choose to change the following settings in the configuration file:', '•\u2003Walking speed reduction to account for children. As standard it is set to 0.78 to generate a 22% reduction in the travel speeds. If a user wishes to generate a cost allocation surface for adult walking speeds they should change the factor to 1.0.', '•\u2003Water speed can be changed in this file. It is set to NA in the landcover.csv and should remain as such. The water speed can also be changed in the configuration file and will appear in the water passable cost allocation surface.', 'Additional steps required prior to running the code:', {'italic': 'apwith.ai', 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR32', '#text': '32'}}, '#text': '•\u2003The roads from the standard OSM roads download and m download should be merged. We used the merge function in ArcMap 10.7.1. The merged road data require a new ‘tag’ attribute to be created which contains the road type (name).'}, '•\u2003The user should check the roadcosts.csv file to ensure the road names match those in the shapefile.', '•\u2003The user can change the travel speeds assigned to each road type by editing the roadcosts.csv file.', '•\u2003The user can change the travel speeds assigned to each land cover type by editing the landcovercosts.csv file. If users want to use a different land cover map they can do so by ensuring that the land cover types in the landcovercosts.csv match those in the chosen land cover data.', {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR25', '#text': '25'}}, 'ext-link': {'@xlink:href': 'https://github.com/ChildPovetyAccesstoServices/cpas/tree/v1.0', '@ext-link-type': 'uri', '#text': 'https://github.com/ChildPovetyAccesstoServices/cpas/tree/v1.0'}, '#text': 'All of the analysis was conducted using Python-3 apart from merging the OSM roads with the MapwithAi roads. The code has been developed as an all-in-one software which can be downloaded from Zenodo and GitHub [] and is licensed under the GNU General Public License v3.0 only, meaning changes to the code are permitted as long as they are distributed under the same license.'}, {'bold': 'creation.cfg', 'italic': '.', '#text': 'The code includes an example configuration file  This is the only file the user needs to alter to repeat the process. Within this file the user specifies the location of the input data and the names and paths for the output files. The user can also specify the weighting if they wish to consider children’s travel speed, if not they can set the reduction factor to 0 which will generate adult cost allocation surfaces. The code runs in two steps (1) calculate the cost allocation surfaces (2) use the least cost path analysis to estimate the travel time from every pixel to the nearest health centre.'}, {'ext-link': {'@xlink:href': 'https://github.com/mapbox/rasterio', '@ext-link-type': 'uri', '#text': 'https://github.com/mapbox/rasterio'}, 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR62', '#text': '62'}}, '#text': 'All rasters are read and written using the rasterio python package () together with xarray. The steps and python packages used for pre-processing the roads data:'}, '•\u2003Walking speed values are assigned to an array based on land cover type.', {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR63', '#text': '63'}}, '#text': '•\u2003A CSV file of OSM road categories and associated walking speeds was used to input the assigned walking speeds into a pandas Data Frame.'}, '•\u2003A shapefile containing roads was rasterised using the walking speeds assigned to each type of road. The fastest walking speeds are used for pixels that contain roads of different types. There were a few steps to this which are detailed below:', 'a.\u2003The road types of the CSV file were matched to the road types of the shapefile using a fuzzy match to ensure a walking speed was assigned to every road type in the shapefile.', 'b.\u2003The road types were grouped by walking speed. Each group of roads was processed separately.', 'c.\u2003Roads from the shapefile were filtered by road type and rasterised using the rasterize function of the rasterio.features module. The walking speed is used as value for the rasterised roads. The all_touched option was used so that all grid squares in which road segments existed were counted as road segments in the new raster. This was essential so the rasterization of the roads retained the connections.', 'd.\u2003The resulting walking speed surfaces were merged taking the maximum value at each pixel for all road types.', 'Pre-processing steps of the SRTM data required:', {'inline-formula': {'@id': 'IEq1', 'alternatives': {'mml:math': {'@id': 'IEq1_Math', 'mml:mi': ['s', 'l', 'o', 'p', 'e'], 'mml:mo': '=', 'mml:msqrt': {'mml:mrow': {'mml:mfenced': {'@close': ')', '@open': '(', 'mml:mrow': {'mml:msup': [{'mml:mrow': [{'mml:mfenced': {'@close': ')', '@open': '(', 'mml:mrow': {'mml:mfrac': {'mml:mrow': [{'mml:mi': ['d', 'H']}, {'mml:mi': ['d', 'x']}]}}}}, {'mml:mn': '2'}]}, {'mml:mrow': [{'mml:mfenced': {'@close': ')', '@open': '(', 'mml:mrow': {'mml:mfrac': {'mml:mrow': [{'mml:mi': ['d', 'H']}, {'mml:mi': ['d', 'y']}]}}}}, {'mml:mn': '2'}]}], 'mml:mo': ['+', '∗'], 'mml:mfrac': {'mml:mrow': [{'mml:mn': '100'}, {'mml:mn': '111120'}]}}}}}}, 'tex-math': {'@id': 'IEq1_TeX', '#text': '\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym}\n\t\t\t\t\\usepackage{amsfonts}\n\t\t\t\t\\usepackage{amssymb}\n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$slope=\\sqrt{\\left({\\left(\\frac{dH}{dx}\\right)}^{2}+{\\left(\\frac{dH}{dy}\\right)}^{2}\\ast \\frac{100}{111120}\\right)}$$\\end{document}'}, 'inline-graphic': {'@specific-use': 'web', '@mime-subtype': 'GIF', '@xlink:href': '41597_2022_1274_Article_IEq1.gif'}}}, '#text': '•\u2003Re-sampled to 20\u2009m resolution, using bilinear interpolationSlope is calculated using the formula The factor 100 converts the gradient to a percentage slope and the factor 111120 coverts degree latitude/longitude to metres. This approximation is only valid close to the equator which is the case in this study.'}, {'xref': {'@rid': 'Equ1', '@ref-type': 'disp-formula', '#text': '1'}, '#text': 'As Eq.\xa0 does not account for gradients over 100%, these were removed and replaced with a NA value.'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR64', '#text': '64'}}, '#text': 'The cost allocation surface and health facilities locations were combined to calculate travel time from each cell in the array to the nearest health facility using the graph.MCP_Geometric() and find_costs() methods from the SciPy package.'}]",2022-05-17
0,Scientific Data,41597,10.1038/s41597-022-01301-w,Auto-generating databases of Yield Strength and Grain Size using ChemDataExtractor,9,6,2022,https://github.com/gh-PankajKumar/ChemDataExtractorStressEng,"The code used to generate the four databases can be found at . This repository contains the modified ChemDataExtractor 2.0, webscraping scripts and post-processing tools. The repository contains , which was used to automatically extract yield strength and grain size and serves as an example as to how the herein modified version of ChemDataExtractor can be used for extracting engineering-material properties. Also,  is an iPython notebook that walks through the basic steps to extract records from an input article. A static version of the repository is available to download from Figshare",2022-06-09
0,Scientific Data,41597,10.1038/s41597-022-01330-5,A benchmark dataset for Hydrogen Combustion,17,5,2022,https://doi.org/10.6084/m9.figshare.19601689,All the data and python scripts used to generate coordination number based PES surface to analyze the data for each reaction channel is provided at .,2022-05-17
0,Scientific Data,41597,10.1038/s41597-022-01389-0,"Inter-species cell detection - datasets on pulmonary hemosiderophages in equine, human and feline specimens",3,6,2022,https://github.com/ChristianMarzahl/EIPH_WSI/,"All code used in the experiments to generate results, plots and tables was written in Python and is available through our GitHub repository for EIPH analysis [] in the folder SDATA and is referenced on Zenodo.",2022-06-03
0,Scientific Data,41597,10.1038/s41597-022-01416-0,"The DeepFish computer vision dataset for fish instance segmentation, classification, and size estimation",9,6,2022,,"The images and Django labeller annotation files (JSON) are available on the mentioned Zenodo repository. Furthermore, the code to obtain JSON files in the MS COCO format is published alongside this dataset, and is made available online.",2022-06-09
0,Scientific Data,41597,10.1038/s41597-022-01331-4,"solar energy desalination analysis tool, , with data and models for selecting technologies and regions",20,5,2022,https://github.com/gyetman/DOE_CSP_PROJECT,"All source code for  is made available through github under the academic free license, at the time of writing. The code is hosted at .",2022-05-20
0,Scientific Data,41597,10.1038/s41597-022-01360-z,CDCDB: A large and continuously updated drug combination database,2,6,2022,https://github.com/Omer-N/CDCDB,"All of the source code for CDCDB database generation has been uploaded to GitHub: , where it is maintained. We also provide the code for parsing and visualizing the data (see Usage Notes above).",2022-06-02
0,Scientific Data,41597,10.1038/s41597-022-01417-z,Genotypic and tissue-specific variation of  transcriptome profiles in response to drought,14,6,2022,,"[{'ext-link': {'@xlink:href': '10.6084/m9.figshare.19382594.v1', '@ext-link-type': 'doi', '#text': 'https://doi.org/10.6084/m9.figshare.19382594.v1'}, 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR44', '#text': '44'}}, '#text': 'Figshare R Code DEG analysis: .'}, {'ext-link': {'@xlink:href': '10.6084/m9.figshare.17031869.v2', '@ext-link-type': 'doi', '#text': 'https://doi.org/10.6084/m9.figshare.17031869.v2'}, 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR38', '#text': '38'}}, '#text': 'Figshare Python Codes: .'}, {'ext-link': {'@xlink:href': '10.6084/m9.figshare.17031884.v2', '@ext-link-type': 'doi', '#text': 'https://doi.org/10.6084/m9.figshare.17031884.v2'}, 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR41', '#text': '41'}}, '#text': 'Figshare R Code for PCA analysis: .'}]",2022-06-14
0,Scientific Data,41597,10.1038/s41597-022-01303-8,Simulated carbon K edge spectral database of organic molecules,16,5,2022,,"A proprietary code, Academic Release CASTEP version 8.0 was used to perform DFT and core-loss spectra calculations. The configuration files used in the calculation is provided for reproducing the site C-K edge spectra at figshare along with some of the output files for confirmation of calculation condition. For making input files, parsing output files, creating database and visualization, we have used the following python libraries: Numpy, Pandas, h5py, rdkit and Matplotlib. The Python code used for parsing the CASTEP input and output files is available at GitHub under the MIT license. The Python code used for making the Gaussian smeared spectra dataset from the database HDF5 file of eigenvalues and dynamical structure factors is also available at GitHub under the MIT license, and can be used for making spectra with arbitrary smearing parameters.",2022-05-16
0,Scientific Data,41597,10.1038/s41597-022-01332-3,Global forest management data for 2015 at a 100 m resolution,10,5,2022,,"['To create the reference data set, including the control and validation data sets, we employed the Geo-Wiki application, which can be used to visually check available land cover and land use maps against very high-resolution imagery. Alternatively, users can employ the LACO-Wiki tool, which has similar functionalities and is openly available, but it requires users to upload the land cover and land use maps.', {'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR45', '#text': '45'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR46', '#text': '46'}}], '#text': 'All the geographical operations were done in QGIS 3.8.0, and the accuracy matrixes were calculated in R 3.6.1., using the raster and dtwSat libraries.'}, {'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR47', '#text': '47'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR48', '#text': '48'}}], '#text': 'The forest management layer was generated using the CGLS-LC100 collection 2 processing line developed by VITO NV on behalf of the European Commission Joint Research Centre (JRC). All code was written in Python 2.7. The regional models are available as Zenodo record, and the related biome cluster are also stored as a separate Zenodo record.'}]",2022-05-10
0,Scientific Data,41597,10.1038/s41597-022-01361-y,A multimodal sensor dataset for continuous stress detection of nurses in a hospital,1,6,2022,,The code is available on GitHub. The data collection was performed in Central Standard Time of the United States which is 6 hours behind GMT.,2022-06-01
0,Scientific Data,41597,10.1038/s41597-022-01390-7,"QMugs, quantum mechanical properties of drug-like molecules",7,6,2022,http://www.rdkit.org; https://rclone.org,"All analyses were supported by the Python programming language (version 3.7.7) and its scientific software stack. Molecular conformations were generated using RDKit (, version 2020.03.3) and GFN2-xTB (version 6.3.1). All quantum mechanical calculations were carried out with Psi4 (version 1.3.2). Molecular structure visualizations were created using PyMol (version 2.3.5) and ChemDraw (version 19.1.1.32). The rclone (, version 1.54.0) WebDAV client was used for all data uploading purposes.",2022-06-07
0,Scientific Data,41597,10.1038/s41597-022-01418-y,YouTube Dataset on Mobile Streaming for Internet Traffic Modeling and Streaming Analysis,13,6,2022,,"The complete dataset is available as zip file at figshare. The dataset includes all measured data and all post-processing and evaluation scripts. In addition, the publicly available wrapper app is freely available in case the dataset needs to be updated or expanded.",2022-06-13
0,Scientific Data,41597,10.1038/s41597-022-01219-3,The brainstem connectome database,12,4,2022,https://neuroviisas.med.uni-rostock.de/neuroviisas.shtml,The code written in JAVA for the generation of the BC database and the analysis of the connectome including the database is available at .,2022-04-12
0,Scientific Data,41597,10.1038/s41597-022-01333-2,"CYCLANDS: Cycling geo-located accidents, their details and severities",26,5,2022,https://opensource.org/licenses/MIT; https://github.com/U-Shift/cyclands,"Together with the collection of cycling safety datasets, we share the code used for curating the datasets. All code has been written for Python3. We present the code under Jupyter notebooks, which provide step-by-step instructions on how each dataset was curated. The code is available under the MIT license ( and is available at .",2022-05-26
0,Scientific Data,41597,10.1038/s41597-022-01391-6,Long-term ice phenology records spanning up to 578 years for 78 lakes around the Northern Hemisphere,16,6,2022,https://github.com/afilazzola/IcePhenologyDatabase; http://www.python.org,"We provide code in an open access repository that will assist in reviewing the database and examining patterns (). Within the repository, we provide the Python code used for the creation of the dataset, quality assurance, and quality control. We included a set of visualization options for technical validation in Python. All Python code was conducted in Python Version 3.8.1 () using numpy, pandas, textract libraries. We also provide R code that was used as a second reviewer for technical validation. The code in the qaqc.r file was used to convert the dataset into “long” format where there is only one column each for ice-on and ice-off date. The same file may also be used by future users for a quick visualization of patterns within the lake ice database. All R code was conducted in R version 3.5.1 using the tidyr, dplyr, ggplot, and broom packages. All code is freely available under the Massachusetts Institute of Technology license.",2022-06-16
0,Scientific Data,41597,10.1038/s41597-022-01448-6,Real-world sensor dataset for city inbound-outbound critical intersection analysis,21,6,2022,https://github.com/EEM0N/sathorndata.github.io/blob/main/sathorndata.ipynb,"The code implementation was performed in Python using a Jupyter notebook. The Python scripts to perform data preprocessing, visualization and technical validation are available at the sathorndata GitHub repository. (.)",2022-06-21
0,Scientific Data,41597,10.1038/s41597-022-01305-6,A dataset of winter wheat aboveground biomass in China during 2007–2015 based on data assimilation,11,5,2022,https://github.com/paperoses/CHN_Winter_Wheat_AGB,"Python scripts that implement model calibration, data assimilation, dataset generation, and mapping are available (). Further questions can be directed towards Hai Huang (haihuang@cau.edu.cn).",2022-05-11
0,Scientific Data,41597,10.1038/s41597-022-01449-5,"ReaLSAT, a global dataset of reservoir and lake surface area variations",21,6,2022,,The data repository contains a jupyter notebook that provides the code to access the input GSW dataset and process it using the ORBIT framework.,2022-06-21
0,Scientific Data,41597,10.1038/s41597-022-01505-0,"HistoML, a markup language for representation and exchange of histopathological features in pathology images",8,7,2022,https://github.com/Peiliang/HistoML,The source code of this work can be downloaded from .,2022-07-08
0,Scientific Data,41597,10.1038/s41597-022-01335-0,"Motor evoked potentials for multiple sclerosis, a multiyear follow-up dataset",16,5,2022,https://gin.g-node.org/JanYperman/motor_evoked_potentials,"The code to generate the dataset is not available, as it deals with privacy sensitive source data. Alongside the dataset itself we also provide a Jupyter notebook in Python which implements a machine learning pipeline on the dataset for the task of cross-sectional prediction. This code illustrates how to handle the data. The notebook, some helper code and the data are available at .",2022-05-16
0,Scientific Data,41597,10.1038/s41597-022-01364-9,"AnimalTraits - a curated animal trait database for body mass, metabolic rate and brain size",2,6,2022,https://github.com/animaltraits/animaltraits.github.io,"The observations database, all raw CSV files and the R scripts used to standardise and check the observations, as well as a sample script to aggregate the observations database into a species-trait data set are available in the auxiliary material. The auxiliary material also contains README.txt files that describe the structure and usage of the data and scripts. The auxiliary material is managed as a GitHub repository (). GitHub is also used to build and serve the website.",2022-06-02
0,Scientific Data,41597,10.1038/s41597-022-01393-4,"NASA Global Daily Downscaled Projections, CMIP6",2,6,2022,https://github.com/bthrasher/daily_BCSD,The NCL code used to generate the downscaled products can be found at .,2022-06-02
0,Scientific Data,41597,10.1038/s41597-022-01420-4,Improvement of eukaryotic protein predictions from soil metagenomes,16,6,2022,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR31', '#text': '31'}}, '#text': '• Project name: EukaProt_in_PublicSoilMetag'}, {'ext-link': {'@xlink:href': 'https://github.com/CaroleBelliardo/EukaProt_in_PublicSoilMetag.git', '@ext-link-type': 'uri', '#text': 'https://github.com/CaroleBelliardo/EukaProt_in_PublicSoilMetag.git'}, '#text': '• Project home page:'}, '• Operating system(s): Platform independent', '• Programming language: Python3', '• Other requirements: Python3.8 or higher', '• License: License: GNU General Public License v3.0']",2022-06-16
0,Scientific Data,41597,10.1038/s41597-022-01307-4,"Dynamic World, Near real-time global 10 m land use land cover mapping",9,6,2022,,"[{'monospace': 'ee.ImageCollection(‘GOOGLE/DYNAMICWORLD/V1’)', '#text': 'The Dynamic World NRT dataset has been made available as an Earth Engine Image Collection under “GOOGLE/DYNAMICWORLD/V1”. This is referenced in either the Earth Engine Python or JavaScript client library with: .'}, {'ext-link': {'@xlink:href': 'https://sites.google.com/view/dynamic-world/home', '@ext-link-type': 'uri', '#text': 'https://sites.google.com/view/dynamic-world/home'}, '#text': 'We provide a public web interface for rapid exploration of the dataset at: .'}, {'ext-link': {'@xlink:href': 'https://code.earthengine.google.com/710e2ae9d03cd994c6e8dc9213257cbc', '@ext-link-type': 'uri', '#text': 'https://code.earthengine.google.com/710e2ae9d03cd994c6e8dc9213257cbc'}, '#text': 'We also provide an example of accessing Dynamic World using the Earth Engine Code Editor in the following code snippet: .'}, {'ext-link': {'@xlink:href': '10.5281/zenodo.5602141', '@ext-link-type': 'doi', '#text': 'https://doi.org/10.5281/zenodo.5602141'}, 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR44', '#text': '44'}}, '#text': 'The Dynamic World model has been run for historic Sentinel-2 imagery and is being run for newly acquired Sentinel-2 imagery; users are therefore encouraged to work with outputs available in the NRT Image Collection available on Earth Engine. Nonetheless, to ensure reproducibility, we have archived the trained model, example code for running inference, and additional information on the model architecture in Zenodo at .'}]",2022-06-09
0,Scientific Data,41597,10.1038/s41597-022-01336-z,Machine-learning ready data on the thermal power consumption of the Mars Express Spacecraft,24,5,2022,https://kelvins.esa.int/mars-express-power-challenge/,The raw data are available on the ESA website  as provided by the MEX operations team at ESOC. These data are pre-processed using the above-described approaches.,2022-05-24
0,Scientific Data,41597,10.1038/s41597-022-01365-8,A harmonized chemical monitoring database for support of exposure assessments,16,6,2022,https://doi.org/10.23645/epacomptox.16674298,"All scripts used to obtain raw data, clean or process raw data, perform QA, and construct the database are available in the MMDB Processing Scripts folder at . Various versions of R and python were used in different project stages; the primary version for both data cleaning and building the database was R version 3.6.2. An example R script containing sample queries of MMDB by chemical and media is maintained in the Sample Queries folder. We will also maintain an SQL script (to be run in MySQL immediately after the MMDB dump file) to correct any identified curation mistakes in official MMDB releases in the MMDB Correction Scripts folder.",2022-06-16
0,Scientific Data,41597,10.1038/s41597-022-01280-y,AJILE12: Long-term naturalistic human intracranial neural recordings and pose,21,4,2022,https://github.com/BruntonUWBio/ajile12-nwb-data,Code to run our Jupyter Python dashboard and recreate all results in this paper can be found at . We used Python 3.8.5 and PyNWB 1.4.0. A requirements file listing the Python packages and versions necessary to run the code is provided in our code repository. Our code is publicly available without restriction other than attribution.,2022-04-21
0,Scientific Data,41597,10.1038/s41597-022-01337-y,Cloud-based applications for accessing satellite Earth observations to support malaria early warning,16,5,2022,https://github.com/EcoGRAPH/epidemia_reach; https://github.com/EcoGRAPH/epidemia_gee,"The REACH software, including JavaScript code, a sample R script for running REACH using the Python API, and a user guide, is accessible via the following GitHub archive: . Computer code for the REACH Python package is accessible via the following GitHub archive: .",2022-05-16
0,Scientific Data,41597,10.1038/s41597-022-01366-7,The International Bathymetric Chart of the Southern Ocean Version 2,7,6,2022,https://www.generic-mapping-tools.org/; https://gdal.org/; https://github.com/SeaBed2030/IBCSO_v2_Dorschel_et_al_2022,"The GMT and GDAL routines used in the SEAHORSE workflow are Open Source and can be accessed on their respective webpages ( and ). All relevant code related to the main SEAHORSE workflow are available at . Data for the technical validation are hosted on figshare. Since the SEAHORSE workflow was customised to fit the existing architecture of AWI’s high performance cluster, most of the code is specific and requires severe adjustments when moved to a different environment.",2022-06-07
0,Scientific Data,41597,10.1038/s41597-022-01338-x,"MusMorph, a database of standardized mouse morphology data for morphometric meta-analyses",25,5,2022,https://github.com/jaydevine/MusMorph,"Our code is freely available at . The scripts describe every stage of the MusMorph data acquisition and analysis, including image preprocessing (e.g., file conversion, image resampling and intensity correction), processing (e.g., atlas generation, non-linear registration, label propagation), and postprocessing (e.g., shape optimization, morphometric analysis). We developed and implemented the code with Bash 4.4.20, R 3.6.1, Python 3.6, and Julia 1.2.0 on Ubuntu. To facilitate MusMorph software installations, reproducibility, and data aggregation, we have created a comprehensive Docker image that can be downloaded as follows: . Further information about running the Docker container is available on GitHub. All code is distributed under the GNU General Public License v3.0.",2022-05-25
0,Scientific Data,41597,10.1038/s41597-022-01367-6,Historical long-term cultivar×climate suitability data to inform viticultural adaptation to climate change,6,6,2022,,No custom code was used in this study.,2022-06-06
0,Scientific Data,41597,10.1038/s41597-022-01224-6,Multi-decadal ocean temperature time-series and climatologies from Australia’s long-term National Reference Stations,7,4,2022,,"[{'ext-link': {'@xlink:href': 'https://github.com/aodn/imos-toolbox', '@ext-link-type': 'uri', '#text': 'https://github.com/aodn/imos-toolbox'}, '#text': 'The code for the IMOS mooring toolbox used to quality control IMOS data can be accessed here: .'}, {'ext-link': {'@xlink:href': 'https://github.com/aodn/python-aodntools/tree/master/aodntools/timeseries_products', '@ext-link-type': 'uri', '#text': 'https://github.com/aodn/python-aodntools/tree/master/aodntools/timeseries_products'}, '#text': 'The code used to aggregate the mooring time-series data is available here: .'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR46', '#text': '46'}}, '#text': 'MATLAB, Python, and R tutorials have been created to help users download, load, plot, and export data (as CSV files) contained in the products. These are publicly available under a Creative Commons Attribution 4.0 International license (CC BY 4.0) on line at figshare.'}]",2022-04-07
0,Scientific Data,41597,10.1038/s41597-022-01282-w,Laboratory modelling of urban flooding,11,4,2022,https://forge.irstea.fr/projects/fudaa-lspiv,"Algorithms for data processing were coded with Matlab 2018b, and the flow surface velocities were computed with the open-source software Fudaa-LSPIV (version 1.7.3). For further details, see .",2022-04-11
0,Scientific Data,41597,10.1038/s41597-022-01254-0,Deciphering Bitcoin Blockchain Data by Cohort Analysis,7,4,2022,https://github.com/SciEcon/UTXO,"The code used for the cohort analysis is available on GitHub (). The GitHub repository is also archived by Zenodo, with the code available in Python and written in Google Colab Notebook with Markdown. first release created on Github: 22 Apr 2021; license: GPL-3.0 License",2022-04-07
0,Scientific Data,41597,10.1038/s41597-022-01283-9,A compilation of experimental data on the mechanical properties and microstructural features of Ti-alloys,26,4,2022,https://gitlab.com/comari/dax-ti,"The dataset and the utility script (utils.py) are available on Zenodo, an open data repository. The python3 script is also available on GitLab (), in which the users might obtain static (dax-ti-static) or a rolling release version (dax-ti-sid) of the project. The rolling release version will be continuously updated based on external requests. Researchers are encouraged to contribute to the database through GitLab or via e-mail, sharing their published data to expand the dataset.",2022-04-26
0,Scientific Data,41597,10.1038/s41597-022-01369-4,"A cross-verified database of notable people, 3500BC-2018AD",9,6,2022,https://doi.org/10.21410/7E4/YLG6YR,"Code to replicate all analyses presented here, the intermediate datasets and the exhaustive dataset are available at the publicly accessible Sciences-Po Dataverse: .",2022-06-09
0,Scientific Data,41597,10.1038/s41597-022-01398-z,Optimising the classification of feature-based attention in frequency-tagged electroencephalography data,13,6,2022,https://osf.io/c689u/; https://github.com/air2310/FeatAttnClassification,The full repository containing all data and code folders described above is available through the Open Science Framework (). The analysis code and experimental task code are also available on Github ().,2022-06-13
0,Scientific Data,41597,10.1038/s41597-022-01284-8,A global record of annual terrestrial Human Footprint dataset from 2000 to 2018,19,4,2022,https://github.com/HaoweiGis/humanFootprintMapping/,The programs used to generate all the results were Python (3.11) and ArcGIS (10.4). Analysis scripts are available on GitHub ().,2022-04-19
0,Scientific Data,41597,10.1038/s41597-022-01340-3,Quantifying yeast colony morphologies with feature engineering from time-lapse photography,17,5,2022,,"['All algorithms used in this paper can be freely obtained from open source Python packages.', {'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR21', '#text': '21'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR32', '#text': '32'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR25', '#text': '25'}}], 'italic': ['numpy.linalg.svd', 'scipy.cluster.hierarchy'], '#text': 'The image pre-processing pipeline and feature engineering (Canny edge detection, circular Hough transform, convex hull, local binary pattern) were performed by using the algorithms implemented in the scikit-image library. The Principal Component Analysis was carried out with the Singular Value Decomposition using the numerical python library, NumPy (). Hierarchical clustering tools made available by SciPy in the module  were also used.'}, {'italic': ['protoclust', 'pyprotoclust', 'pyprotoclust'], 'ext-link': [{'@xlink:href': 'https://CRAN.R-project.org/package=protoclust', '@ext-link-type': 'uri', '#text': 'https://CRAN.R-project.org/package=protoclust'}, {'@xlink:href': 'https://pyprotoclust.readthedocs.io', '@ext-link-type': 'uri', '#text': 'https://pyprotoclust.readthedocs.io'}], 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR17', '#text': '17'}}, '#text': 'The algorithm for minimax linkage has been published as an R package on the CRAN repository under the name  (). Independently, the authors of this paper developed the  Python package (albeit with the retroactive incorporation of this previously established naming convention). Our Python implementation allows us to obtain seamless integration with the interface established by SciPy’s hierarchical clustering and the other Python-specific tools used in this study. Our implementation can also take advantage of multi-threading to improve algorithm performance. The open-source code for the  Python package can be obtained from the Python Package Index under the MIT license. The documentation is hosted publicly online by Read the Docs ().'}]",2022-05-17
0,Scientific Data,41597,10.1038/s41597-022-01455-7,A multi-scale time-series dataset with benchmark for machine learning in decarbonized energy grids,22,6,2022,https://github.com/tamu-engineering-research/Open-source-power-dataset,"A step-by-step guidance and the source-code for dataset generation and machine learning benchmarks can be found on GitHub. Specifically, we provide ready-to-use Pytorch data loaders with both data processing and splitting included, and also share the code of evaluators to support fair comparison among different ML-based algorithms, of which the dependencies and usage are also descibed on Github ().",2022-06-22
0,Scientific Data,41597,10.1038/s41597-022-01427-x,A global database for conducting systematic reviews and meta-analyses in innovation and quality management,14,6,2022,,"Data preprocessing tasks were performed in Python programming language. A Jupyter notebook (Geocode_cnet.ipynb) including the data preprocessing steps is provided alongside the paper. Example figures (geographical distribution, co-occurrence networks, citation network) were constructed in the R program language. The R script is provided to produce and modify the figures based on the needs of the researcher.",2022-06-14
0,Scientific Data,41597,10.1038/s41597-022-01228-2,Global nature run data with realistic high-resolution carbon weather for the year of the Paris Agreement,11,4,2022,https://www.ecmwf.int/en/forecasts/access-forecasts/ecmwf-web-api; https://apps.ecmwf.int/registration/; https://www.python.org,"The IFS forecast model and the Meteorological Archival and Retrieval System (MARS) software are not available for public use as the ECMWF Member States are the proprietary owners. However, the CHE global nature run dataset and the MARS data extraction features are freely available through ECMWF API () following a registration step (). The data can be accessed using python (). The commands and steps required are detailed in the Supplementary Information file  (S.",2022-04-11
0,Scientific Data,41597,10.1038/s41597-022-01343-0,California wildfire spread derived using VIIRS satellite observations and an object-based tracking system,30,5,2022,,"[{'italic': 'figshare', 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR33', '#text': '33'}}, '#text': 'The open-source Python code of the fire tracking system, as well as sample scripts for reading the dataset, are freely available at the  data repository, along with the 2012–2020 FEDS dataset.'}, {'italic': ['Numpy', 'Pandas', 'Geopandas', 'Xarray', 'Scipy', 'Shapely', 'Gdal', 'Pyproj'], '#text': 'Versions and packages of the Python script include  (1.17.5),  (1.0.1),  (0.7.0),  (0.15.0),  (1.4.1),  (1.7.1),  (3.0.4), and  (2.5.0).'}]",2022-05-30
0,Scientific Data,41597,10.1038/s41597-022-01372-9,eldBETA: A Large Eldercare-oriented Benchmark Database of SSVEP-BCI for the Aging Population,31,5,2022,,"Custom codes for generation and processing of the data and the figures are presented in the repository. A MATLAB script “eldbeta_convert.m” was provided for data processing in converting the raw data to the epoch data. The data preprocessing and technical validations were conducted in MATLAB R2018b and Python 3.6.10. A “README.md” file was used for a brief description of the code in the code repository. The Benchmark database and the BETA database as well as the classification algorithms can be found in their corresponding repositories related to the papers, and thus they are not provided in this data descriptor.",2022-05-31
0,Scientific Data,41597,10.1038/s41597-022-01429-9,Generating FAIR research data in experimental tribology,16,6,2022,,"[{'ext-link': {'@xlink:href': 'https://kadi.iam-cms.kit.edu/', '@ext-link-type': 'uri', '#text': 'https://kadi.iam-cms.kit.edu/'}, '#text': 'The virtual research environment Kadi4Mat, its documentation and source code, can be found at .'}, {'ext-link': {'@xlink:href': 'https://github.com/nick-garabedian/SurfTheOWL', '@ext-link-type': 'uri', '#text': 'https://github.com/nick-garabedian/SurfTheOWL'}, 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR36', '#text': '36'}}, '#text': 'The SurfTheOWL application (both source code and standalone executable) for deriving key-value pairs from the TriboDataFAIR Ontology is available at  with its most up-to-date changes, while main updates are listed as versions in Zenodo.'}]",2022-06-16
0,Scientific Data,41597,10.1038/s41597-022-01288-4,"GEOM, energy-annotated molecular conformations for property prediction and molecular generation",21,4,2022,https://github.com/learningmatter-mit/geom; https://github.com/learningmatter-mit/NeuralForceField; https://github.com/grimme-lab/crest/releases; https://github.com/grimme-lab/xtb/releases,"Tutorials for loading the dataset and code for training 3D-based neural network models are publicly available without restriction ( and ). CREST and xTB are both freely available online ( and ). CREST version 2.9 was used with xTB version 6.2.3 to generate the initial CREs. CENSO 1.1.2 was used with Orca 5.0.1 and xTB 6.4.1 to refine the ensembles. Orca 5.0.2 was used for all single-point calculations. A race condition bug in version 5.0.1 meant that some CENSO energies were clearly incorrect (conformational energies above 1,000 kcal/mol), while some energy calculations failed to converge for reasonable geometries. Therefore, we discarded ensembles with failed energy calculations or conformational energy ranges exceeding 30 kcal/mol at any stage of the optimization. We also performed new single-point calculations on all converged CENSO geometries with Orca 5.0.2; 0.44% of the energies were found to be incorrect and were replaced.",2022-04-21
0,Scientific Data,41597,10.1038/s41597-022-01315-4,Bottom-up estimates of reactive nitrogen loss from Chinese wheat production in 2014,25,5,2022,,All the code used to develop RF model is available in ‘source file’.,2022-05-25
0,Scientific Data,41597,10.1038/s41597-022-01373-8,"PISCOeo_pm, a reference evapotranspiration gridded database based on FAO Penman-Monteith in Peru",17,6,2022,https://github.com/adrHuerta/PISCOeo_pm,"Construction of the gridded data was performed using the R environment for statistical computing version 3.6.3. Python version 3.8.5 was also used. The code that describes the procedures (quality control, gap-filling, homogenization, spatial interpolation, and spatial downscaling) to obtain the gridded data of the meteorological subvariables and PISCOeo_pm is freely available at figshare and GitHub () under GNU public licence version 3.",2022-06-17
0,Scientific Data,41597,10.1038/s41597-022-01374-7,"Disaggregated data on age and sex for the first 250 days of the COVID-19 pandemic in Bucharest, Romania",31,5,2022,,No codes were developed for this research.,2022-05-31
0,Scientific Data,41597,10.1038/s41597-022-01401-7,"A large, curated, open-source stroke neuroimaging dataset to improve lesion segmentation algorithms",16,6,2022,https://github.com/npnl/atlas; https://github.com/BIC-MNI/minc-toolkit; https://surfer.nmr.mgh.harvard.edu/fswiki/mri_deface; https://github.com/npnl/PALS; https://github.com/npnl/isles_2022/,"The ATLAS v2 lesion segmentations were generated using ITK-SNAP version 3.8.0. Our protocols for lesion segmentation can be found on our Github (). Code used to preprocess the dataset were adapted from the MINC-toolkit (). T1w images were defaced using the “mri_deface” tool from FreeSurfer (v1.22) (). PALS, our open-source software to perform lesion analyses, can be accessed at . Finally, as part of the MICCAI ISLES 2022 challenge, we provide sample code on our Github () to assist users in getting started with the lesion segmentation challenge (e.g., code to obtain the data, load it, and save predictions in the format expected by our automatic evaluator).",2022-06-16
0,Scientific Data,41597,10.1038/s41597-022-01430-2,Hourly rainfall data from rain gauge networks and weather radar up to 2020 across the Hawaiian Islands,14,6,2022,,"The R version 4.0.2 along with the R packages, dataRetrieval (v2.7.6), lubridate (v1.7.9.2), dplyr (v1.0.2), and data.table (v1.13.6) are used to download and quality control hourly gauge rainfall data. We used the 2020 Lidar Radar Open Software Environment (LROSE) to derive hourly radar rainfall. The python version 3.7.10 along with the python packages, pandas (v1.3.5), numpy (v1.21.2), netCDF4 (v1.5.8), xarray (v0.19.0), and matplotlib (3.5.0) are used for the validation process.",2022-06-14
0,Scientific Data,41597,10.1038/s41597-022-01317-2,Dataset of solution-based inorganic materials synthesis procedures extracted from the scientific literature,25,5,2022,,"[{'ext-link': {'@xlink:href': 'https://github.com/CederGroupHub/text-mined-solution-synthesis_public', '@ext-link-type': 'uri', '#text': 'https://github.com/CederGroupHub/text-mined-solution-synthesis_public'}, '#text': 'The scripts used to classify paragraphs and extract procedures as well as to perform the data analysis are home-written codes which are publicly available at the GitHub repository  with acknowledgement of the current paper.'}, 'The underlying libraries used in this project are all open-source:', {'italic': 'Tensorflow', 'ext-link': {'@xlink:href': 'http://www.tensorflow.org', '@ext-link-type': 'uri', '#text': 'www.tensorflow.org'}, '#text': '()'}, {'italic': 'Keras', 'ext-link': {'@xlink:href': 'https://keras.iokeras.io/', '@ext-link-type': 'uri', '#text': 'keras.iokeras.io'}, '#text': '()'}, {'italic': 'SpaCy', 'ext-link': {'@xlink:href': 'https://spacy.iospacy.io/', '@ext-link-type': 'uri', '#text': 'spacy.iospacy.io'}, 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR60', '#text': '60'}}, '#text': '()'}, {'italic': 'NLTK', 'ext-link': {'@xlink:href': 'https://www.nltk.org/', '@ext-link-type': 'uri', '#text': 'https://www.nltk.org/'}, 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR62', '#text': '62'}}, '#text': '()'}, {'italic': 'gensim', 'ext-link': {'@xlink:href': 'https://radimrehurek.comradimrehurek.com/', '@ext-link-type': 'uri', '#text': 'radimrehurek.comradimrehurek.com'}, 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR59', '#text': '59'}}, '#text': '()'}, {'italic': 'scikit-learn', 'ext-link': {'@xlink:href': 'https://scikit-learn.org/', '@ext-link-type': 'uri', '#text': 'scikit-learn.org'}, 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR75', '#text': '75'}}, '#text': '()'}, {'italic': 'ChemDataExtractor', 'ext-link': {'@xlink:href': 'http://chemdataextractor.org/', '@ext-link-type': 'uri', '#text': 'chemdataextractor.org'}, 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR40', '#text': '40'}}, '#text': '()'}, {'italic': 'Material Parser', 'ext-link': {'@xlink:href': 'https://github.com/CederGroupHub/MaterialParser', '@ext-link-type': 'uri', '#text': 'github.com/CederGroupHub/MaterialParser'}, '#text': '()'}, {'italic': 'Borges', 'ext-link': {'@xlink:href': 'https://github.com/CederGroupHub/Borges', '@ext-link-type': 'uri', '#text': 'github.com/CederGroupHub/Borges'}, '#text': '()'}, {'ext-link': {'@xlink:href': 'https://github.com/CederGroupHub/LimeSoup', '@ext-link-type': 'uri', '#text': 'github.com/CederGroupHub/LimeSoup'}, '#text': 'LimeSoup ().'}]",2022-05-25
0,Scientific Data,41597,10.1038/s41597-022-01346-x,"GriddingMachine, a database and software for Earth system modeling at global and regional scales",1,6,2022,https://github.com/CliMA/GriddingMachine.jl,The code can be found at  under the Apache 2.0 License. The exact version of the package used to produce the results presented in this paper is also archived on CaltechDATA along with the datasets.,2022-06-01
0,Scientific Data,41597,10.1038/s41597-022-01402-6,"BIRAFFE2, a multimodal dataset for emotion-based personalization in rich affective game environments",7,6,2022,https://gitlab.geist.re/pro/biraffe2-supplementary-codes,"The recording of all data was controlled by a procedure written with PsychoPy 3.2.4 that established a connection to the gamepad, BITalino, and camera and recorded the data streams and images. The raw data thus collected was then converted to CSV and JSON formats using custom scripts in Python 3.8 with libraries: pandas for data manipulation, heartpy for ECG signal processing, and neurokit2 for EDA signal processing. All analyses supporting technical validation of the dataset were performed using statsmodels and bioinfokit libraries for Python. Finally, level maps were visualized using the bokeh library. The whole code is available to interested researchers in a dedicated repository: .",2022-06-07
0,Scientific Data,41597,10.1038/s41597-022-01347-w,"Understanding occupants’ behaviour, engagement, emotion, and comfort indoors with heterogeneous sensors and wearables",2,6,2022,https://github.com/cruiseresearchgroup/InGauge-and-EnGage-Datasets,Python code for prepossessing the data and implementing the segmentation based on different classes are available online .,2022-06-02
0,Scientific Data,41597,10.1038/s41597-022-01376-5,"The long-tail effect of the COVID-19 lockdown on Italians’ quality of life, sleep and physical activity",31,5,2022,,The code to reproduce the plots in the paper is publicly available on Figshare.,2022-05-31
0,Scientific Data,41597,10.1038/s41597-022-01403-5,A large-scale multi-label 12-lead electrocardiogram database with standardized diagnostic statements,7,6,2022,,"The Python code for reading the ECG data, attributes and diagnostic code dictionary, evaluating the signal quality, and dataset partition is available in figshare.",2022-06-07
0,Scientific Data,41597,10.1038/s41597-022-01432-0,"Lexibank, a public repository of standardized wordlists with computed phonological and lexical features",16,6,2022,https://github.com/lexibank/lexibank-analysed/tree/v0.2; https://doi.org/10.5281/zenodo.5227817; https://github.com/lexibank/lexibank-analysed/blob/v0.2/etc/lexibank.csv; https://zenodo.org/communities/lexibank/,The main software package underlying Lexibank is curated on GitHub () and archived with Zenodo (). Individual datasets belonging to the Lexibank wordlist collection are curated on individual repositories on GitHub (see our master list at ) and are also all archived with Zenodo (see ).,2022-06-16
0,Scientific Data,41597,10.1038/s41597-022-01461-9,Sea ice surface temperatures from helicopter-borne thermal infrared imaging during the MOSAiC expedition,25,6,2022,,"The data processing and analysis were performed with Python 3. The code responsible for processing and analyzing the maps and the images is published under open access. The used colormap for the temperature is individually defined and provided as well. It is suited to visualize temperature structures for Arctic winter conditions. In case of specific requests, please contact the corresponding author directly.",2022-06-25
0,Scientific Data,41597,10.1038/s41597-022-01291-9,"A high-resolution 4D terrestrial laser scan dataset of the Kijkduin beach-dune system, The Netherlands",28,4,2022,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR62', '#text': '62'}}, '#text': 'The transformation matrices for time-dependent alignment were calculated using the module ICP in the software package OPALS. The rigid alignment was calculated by minimizing point-to-plane distances using a search radius of 0.5\u2009m for plane fitting and a sampling distance of 0.05\u2009m. Stable parts were extracted with 0.5\u2009m radius at locations of planar surfaces.'}, 'Python and Matlab scripts are provided with the data, with basic functions to read and write the point cloud data from the LAZ files, to apply the rigid transformation matrices to the point cloud data for time-dependent alignment and georeferencing, and to read out information on scan settings from the metadata files of an epoch.']",2022-04-28
0,Scientific Data,41597,10.1038/s41597-022-01263-z,COVID-19 Open-Data a global-scale spatially granular meta-dataset for  disease,12,4,2022,github.com/GoogleCloudPlatform/covid-19-open-data,All the code to create the dataset is available at . Jupyter notebooks to reproduce the analyses in this paper are available under the examples folder.,2022-04-12
0,Scientific Data,41597,10.1038/s41597-022-01292-8,High-throughput inverse design and Bayesian optimization of functionalities: spin splitting in two-dimensional compounds,29,4,2022,github.com/simcomat/SS_2D_Materials,"The entire computational code employed in the SS analysis within this work is openly available at the GitHub repository . It is intensely built upon tools and methods from Pymatgen and ASE, and provide functions to identify, measure and classify SS effects that appear valence/conduction bands of 2D materials band structure calculations.",2022-04-29
0,Scientific Data,41597,10.1038/s41597-021-01093-5,COVID-19 Flow-Maps an open geographic information system on COVID-19 and human mobility for Spain,30,11,2021,,"All the code used in the process of data acquisition, processing and analysis have been written in Python Language (version 3.7) and executed in a Linux environment. For data handling, analysis and visualisation we have used the following python libraries: Pandas (0.11.0) SciPy (1.5.2) and Seaborn (0.11.0). All spatial operations have been conducted using GeoPandas (version 0.8.1). The database used to host COVID-19 Flow-Maps is MongoDB (version 4.2.8). The REST-API used to expose the data is implemented using python eve (version 1.1.2). The source code used in this work is available in the public GitHub repository that also hosts the data-set. COVID-19 Flow-Board interactive dashboards were implemented in plotly (version 4.11.0).",2021-11-30
0,Scientific Data,41597,10.1038/s41597-021-01094-4,"Mobile BCI dataset of scalp- and ear-EEGs with ERP and SSVEP paradigms while standing, walking, and running",20,12,2021,https://github.com/youngeun1209/MobileBCI_Data,"The MATLAB scripts are available for loading data, for evaluating classification performance or signal quality, and for plotting figures at .",2021-12-20
0,Scientific Data,41597,10.1038/s41597-022-01122-x,"ECD-UY, detailed household electricity consumption dataset of Uruguay",20,1,2022,https://github.com/jpchavat/ecd-uy,"Three Jupyter notebooks were implemented to facilitate the handling of the dataset (one notebook for each subset). The notebooks are publicly available to download from . For a correct execution of the notebooks, Python version 3 and the Pandas and Numpy libraries are required.",2022-01-20
0,Scientific Data,41597,10.1038/s41597-022-01180-1,1024-channel electrophysiological recordings in macaque V1 and V4 during resting state,11,3,2022,,"[{'ext-link': {'@xlink:href': 'https://gin.g-node.org/NIN/V1_V4_1024_electrode_resting_state_data', '@ext-link-type': 'uri', '#text': 'https://gin.g-node.org/NIN/V1_V4_1024_electrode_resting_state_data'}, 'xref': [{'@rid': 'Tab2', '@ref-type': 'table', '#text': '2'}, {'@rid': 'Tab5', '@ref-type': 'table', '#text': '1'}], '#text': 'All scripts used for the processing of data and our preliminary analyses are available alongside the data at , in the ‘code’ folder. All experiment control scripts are listed in Table\xa0 and the data processing scripts are listed in Online-only Table\xa0.'}, 'Matlab version R2015b, Python version 3.7 and Snakemake version 5.8.1 were used. The only Matlab dependency was the NPMK toolbox (version 5.0, Blackrock Microsystems), a copy of which is included in the data repository.', {'xref': {'@rid': 'Tab5', '@ref-type': 'table', '#text': '1'}, '#text': 'Direct Python dependencies include neo 0.9.0, nixio 1.5.0 elephant 0.10.0, odml 1.4.5 and odmltables 1.0. A full list of all Python dependencies and the specific versions used can be found in the Python environment specifications, along with the Python scripts (Online-only Table\xa0).'}]",2022-03-11
0,Scientific Data,41597,10.1038/s41597-021-01100-9,Landscape Dynamics (landDX) an open-access spatial-temporal database for the Kenya-Tanzania borderlands,18,1,2022,,No custom code was used to generate the data.,2022-01-18
0,Scientific Data,41597,10.1038/s41597-022-01208-6,Global land projection based on plant functional types with a 1-km resolution under socio-climatic scenarios,30,3,2022,http://www.geosimulation.cn/FLUS.html,"The land simulation in this study was performed by the FLUS model software (GeoSOS-FLUS V2.4), which can be downloaded for free from . Meanwhile, the tutorial on the operation of this software can be found in the user manual at this URL. The other spatial calculations and analyses in this study were performed by ArcGIS software as described in the Method section. The spatial data used for input are all publicly available online, with sources cited within the manuscript.",2022-03-30
0,Scientific Data,41597,10.1038/s41597-022-01123-w,A hydrological simulation dataset of the Upper Colorado River Basin from 1983 to 2019,20,1,2022,https://github.com/parflow/parflow/tree/v3.6.0/; https://numpy.org/; https://gdal.org/; https://pandas.pydata.org/,"These simulations were conducted using ParFlow version 3.6.0 (). The data processing step was done using Python3.5 programming language with necessary toolboxes including NumPy (), the Geospatial Data Abstraction Library (GDAL; ) and the Python Data Analysis Library (PANDAS; ).",2022-01-20
0,Scientific Data,41597,10.1038/s41597-022-01181-0,"MOFSimplify, machine learning models with extracted stability data of three thousand metal–organic frameworks",11,3,2022,,"[{'ext-link': {'@xlink:href': 'https://github.com/hjkgrp/text_mining_tools', '@ext-link-type': 'uri', '#text': 'https://github.com/hjkgrp/text_mining_tools'}, 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR69', '#text': '69'}}, '#text': 'All scripts used to mine the extant literature corresponding to the CoRE MOF 2019 database are commented and are available on a public GitHub repository at . Manuscript copyrights are retained by the publishers, preventing the complete dissemination of full-length articles, but the mined data is provided with an open source CC-BY license and is available on Zenodo (see also Data Records).'}, {'ext-link': [{'@xlink:href': 'https://mofsimplify.mit.edu', '@ext-link-type': 'uri', '#text': 'https://mofsimplify.mit.edu'}, {'@xlink:href': 'https://github.com/hjkgrp/MOFSimplify', '@ext-link-type': 'uri', '#text': 'https://github.com/hjkgrp/MOFSimplify'}], '#text': 'The MOFSimplify website is located at . The code backend for the MOFSimplify website is available in a public GitHub repository at . The repository contains a user manual for the website.'}]",2022-03-11
0,Scientific Data,41597,10.1038/s41597-021-01101-8,"Flipping food during grilling tasks, a dataset of utensils kinematics and dynamics, food pose and subject gaze",12,1,2022,,"['The authors make available the custom code:', '• written in MATLAB, to obtain the trajectories of points of interest in the utensils, from the motion capture data;', '• written in MATLAB, to display the movements in a 3D plot.', '• written in MATLAB, to interact with Nexus and automatically split C3D and ASCII files, because this may be useful for any study that requires processing a huge amount of MoCap data;', '• written in Python, to track food in video records;', '• written in Python, to calculate the metrics to assess the performance of the tracking algorithm.', '• written in C++, to synchronize the clocks of the two computers, that was run before every experimental session.', {'ext-link': [{'@xlink:href': 'https://github.com/deborapereira/foodFlippingDataset', '@ext-link-type': 'uri', '#text': 'https://github.com/deborapereira/foodFlippingDataset'}, {'@xlink:href': 'https://github.com/matterport/Mask_RCNN/releases/tag/v2.1', '@ext-link-type': 'uri', '#text': 'https://github.com/matterport/Mask_RCNN/releases/tag/v2.1'}], '#text': 'All the code is freely available on the following repository: . More detailed information on how to use it is embedded in the code files. The code to process the videos uses Python 3.6.4, OpenCV 4.1.2, Keras 2.2.4 and Tensorflow 1.15.0. The code for the mask R-CNN can be found at . The version 2.1 was used with this dataset.'}, {'ext-link': {'@xlink:href': 'https://github.com/rafaelpadilla/Object-Detection-Metrics', '@ext-link-type': 'uri', '#text': 'https://github.com/rafaelpadilla/Object-Detection-Metrics'}, 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR46', '#text': '46'}}, '#text': 'The code to calculate the performance metrics was adapted from , a tool developed by the authors of.'}]",2022-01-12
0,Scientific Data,41597,10.1038/s41597-022-01209-5,Updating global urbanization projections under the Shared Socioeconomic Pathways,31,3,2022,https://www.python.org,"All python codes (python 3.9.6, ) for creating urbanization level projections are stored in public repository Figshare.",2022-03-31
0,Scientific Data,41597,10.1038/s41597-022-01238-0,Categorized contrast enhanced mammography dataset for diagnostic and artificial intelligence research,30,3,2022,https://github.com/omar-mohamed/CDD-CESM-Dataset,"A Github repository is publicly available () which contains helper scripts to make training a DL model on the dataset easier like reading the annotations, pre-processing the images by resizing and normalizing, training different existing models, augmenting the images while training, and evaluating the different models and plotting the segmentation results. The scripts were written using Python 3.6 with Tensorflow 2.3 for the training process, and OpenCV 4.1 and Pillow 6.1 for the image processing.",2022-03-30
0,Scientific Data,41597,10.1038/s41597-021-01102-7,"Human EEG recordings for 1,854 concepts presented in rapid serial visual presentation streams",10,1,2022,https://doi.org/10.17605/OSF.IO/HD6ZK,"Code and detailed instructions to reproduce the technical validation analyses and figures presented in this manuscript are available from the Open Science Framework (), which also contains links to the data repositories.",2022-01-10
0,Scientific Data,41597,10.1038/s41597-022-01125-8,Dataset of human intracranial recordings during famous landmark identification,31,1,2022,,Example scripts are provided with the dataset. They contain code for reading and plotting the neural and behavioural data. Code examples are provided in Matlab and Python Jupyter Notebooks.,2022-01-31
0,Scientific Data,41597,10.1038/s41597-022-01154-3,Boosting the predictive performance with aqueous solubility dataset curation,3,3,2022,https://github.com/Mengjintao/SolCuration,"Python and C++ codes used to perform data curation, training workflow, and performance evaluation shown in this manuscript are publicly available on GitHub at  or one can cite our code by.",2022-03-03
0,Scientific Data,41597,10.1038/s41597-021-01069-5,A curated diverse molecular database of blood-brain barrier permeability with chemical descriptors,29,10,2021,https://github.com/theochem/B3DB; https://doi.org/10.6084/m9.figshare.15634230.v3,The codes used in this study have been deposited to  and  (version 3). All the calculation were done with  under a virtual environment created with  on Linux.,2021-10-29
0,Scientific Data,41597,10.1038/s41597-022-01155-2,A 14-year time series of marine megafauna bycatch in the Italian midwater pair trawl fishery,14,2,2022,,"The datasets were extracted via queries directly in MySQL Workbench and through the free software environment R, using: the  function from the RMySQL package, a Database Interface and ‘MySQL’ Driver for R. The maps in Fig.  were created using several R packages like  for data handling,  a package providing bindings to the “Geospatial Data Abstraction Library”,  package, a standardized way of encoding spatial vector data in R and the package  for graphical visualization. The package  was also used for the calculation of haul midpoints, starting from the coordinates of gear deploying and retrieving of each fishing operation. A few examples of these operations (MySQL and R) and of the R code for the creation of the maps are available among the shared datasets attached to this work as .",2022-02-14
0,Scientific Data,41597,10.1038/s41597-022-01184-x,"Housing unit and urbanization estimates for the continental U.S. in consistent tract boundaries, 1940–2019",11,3,2022,,"All source code used in this analysis is available on Open Science Framework (OSF). This code was run using R 4.0.5, ArcGIS Pro 2.8 (with Python 3), and ArcGIS Desktop 10.7.",2022-03-11
0,Scientific Data,41597,10.1038/s41597-022-01156-1,Dataset on electrical single-family house and heat pump load profiles in Germany,15,2,2022,https://github.com/ISFH/WPuQ,"The code implementation was done in Python3. The scripts to perform the download, restructuring, validation and visualization of the data are available at the ISFH GitHub repository ().",2022-02-15
0,Scientific Data,41597,10.1038/s41597-022-01185-w,VIB5 database with accurate ab initio quantum chemical molecular potential energy surfaces,11,3,2022,,"All the data generated at the MP2/cc-pVTZ and the CCSD(T)/cc-pVQZ levels of theory were performed with the CFOUR software package. TBE and other data were obtained using various software packages (MOLPRO, CFOUR, MRCC) as described in the Methods section.",2022-03-11
0,Scientific Data,41597,10.1038/s41597-021-01041-3,Multimodal dataset of real-time 2D and static 3D MRI of healthy French speakers,1,10,2021,https://github.com/IADI-Nancy/ArtSpeech; https://github.com/bbTomas/mPraat,"A MATLAB code which generates a video from the dynamic data is available on GitHub . This code also allows reading alignment annotation .trs and .textgrid files, and reading audio and dicom files. It uses mPraat third party toolbox which is available on bbTomas GitHub . The code was tested on Linux distribution of MATLAB2018b and MATLAB2020a. A Python toolbox for .textgrid files parsing also exists.",2021-10-01
0,Scientific Data,41597,10.1038/s41597-022-01213-9,"Daily motionless activities: A dataset with accelerometer, magnetometer, gyroscope, environment, and GPS data",25,3,2022,https://github.com/impires/DataAcquisitionADL; https://github.com/impires/FeatureExtractionMotionlessActivities; https://github.com/impires/JupyterNotebooksMotionlessActivities,"The Android project related to the mobile application used for the data acquisition from all sensors is available at . In addition, the Java project used for the automatic measurement of the parameters of related to the different sensors is available at . The code for preliminary data exploration and analysis is available as a Jupyter notebook at . The Jupyter notebook shows how the data can be loaded, and how the initial data exploration can be performed showing some charts and descriptive statistics. This will be more than sufficient to bootstrap future uses of the dataset.",2022-03-25
0,Scientific Data,41597,10.1038/s41597-021-01042-2,Task-evoked simultaneous FDG-PET and fMRI data for measurement of neural metabolism in the human visual cortex,15,10,2021,https://github.com/BioMedAnalysis/petmr-bids,Scripts used to insert required metadata into the published BIDS dataset are freely available at  under Apache License 2.0.,2021-10-15
0,Scientific Data,41597,10.1038/s41597-022-01242-4,Comparative microelectrode array data of the functional development of hPSC-derived and rat neuronal networks,30,3,2022,,"[{'sup': [{'xref': [{'@ref-type': 'bibr', '@rid': 'CR19', '#text': '19'}, {'@ref-type': 'bibr', '@rid': 'CR27', '#text': '27'}, {'@ref-type': 'bibr', '@rid': 'CR28', '#text': '28'}, {'@ref-type': 'bibr', '@rid': 'CR31', '#text': '31'}], '#text': ',,,'}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR33', '#text': '33'}}], 'ext-link': {'@xlink:href': 'https://gin.g-node.org/NeuroGroup_TUNI/Comparative_MEA_dataset/src/master/Codes', '@ext-link-type': 'uri', '#text': 'https://gin.g-node.org/NeuroGroup_TUNI/Comparative_MEA_dataset/src/master/Codes'}, '#text': 'The provided codes are modified versions of those published earlier and custom in-house scripts. Modification and further distribution fall under the restrictions described by the authors. The codes can be found in .'}, 'MATLAB 2020a (MathWorks) and RStudio version 1.3.959 were used during the preparation of the current publication.']",2022-03-30
0,Scientific Data,41597,10.1038/s41597-021-01106-3,"A longitudinal neuroimaging dataset on language processing in children ages 5, 7, and 9 years old",10,1,2022,,"Code used to create BIDS data is located in the code directory at the root level of the dataset. The ELP_convert_to_bids.py is the main code for BIDS organization from the pre-existing file storage. It calls two sub-functions: beh_process.py, which creates events files from compiled E-prime data and dicom_proc.py, which utilizes dcm2niix to convert DICOM to NIFTI files with its corresponding json file. The ELP_acc_rt.ipynb and four excel files (i.e., Gram.xlsx, Phon.xlsx, Plaus.xlsx, and Sem.xlsx), which are located in the code/func_qa/acc_rt folder, are the code and files used to produce the calculated_response, calculated_accuracy, and calculated_RT for each trial in the events files. The code used to evaluate the movement of the T2-weighted images is located in the code/func_qa/mv folder. The main_just_for_movement.m is the main code used for movement evaluation that calls other sub-functions: realignment_byrun.m and art_global_bdl.m. SPM12 and ArtRepair need to be added before running this code. The count_repaired_rt_acc.m is the code used to count the outlier volumes as marked by ArtRepair and calculates the accuracy and RTs for each condition of each run for each task. Code used to examine the quality of diffusion weighted images is located in the code/dwi_qa folder. The dwi_eddy.sh is the main code that runs FSL eddy which calls other parameters stored in the acqparam.txt, index1.txt, index2.txt, or index3.txt files. The eddy_qa_output.sh is the code that calls the FSL eddy outputs and calculate the quality metrics stored in the derivatives/dwi_mv_snr folder. The DTI_qa_composite.m is an additional code to calculate a composite quality score for the dwi. Code used to calculate the composite score for the quality of the T1-weighted images is located in the code directory and named T1w_qa_composite.m.",2022-01-10
0,Scientific Data,41597,10.1038/s41597-021-01071-x,Accessible data curation and analytics for international-scale citizen science datasets,22,11,2021,,"['All source code for ExeTera is made available through github under the Apache 2.0 license, at the time of writing. The code is split up into two separate projects.', {'bold': 'ExeTera', 'ext-link': {'@xlink:href': 'https://github.com/KCL-BMEIS/ExeTera', '@ext-link-type': 'uri', '#text': 'https://github.com/KCL-BMEIS/ExeTera'}, 'monospace': 'pip install exetera', '#text': 'ExeTera is hosted at  and is available through pypi via .'}, {'ext-link': {'@xlink:href': 'https://github.com/KCL-BMEIS/ExeTera/wiki', '@ext-link-type': 'uri', '#text': 'https://github.com/KCL-BMEIS/ExeTera/wiki'}, '#text': 'ExeTera has a wiki that can be found at .'}, {'bold': 'ExeTeraCovid', 'ext-link': {'@xlink:href': 'https://github.com/KCL-BMEIS/ExeTeraCovid', '@ext-link-type': 'uri', '#text': 'https://github.com/KCL-BMEIS/ExeTeraCovid'}, 'monospace': ['pip install exeteracovid', 'exeteracovid', 'exetera'], '#text': 'ExeTeraCovid is hosted at  and is available through pypi via . Installing  installs .'}, {'ext-link': {'@xlink:href': 'https://github.com/KCL-BMEIS/ExeTera/wiki', '@ext-link-type': 'uri', '#text': 'https://github.com/KCL-BMEIS/ExeTera/wiki'}, '#text': 'ExeTeraCovid has a wiki that can be found at .'}]",2021-11-22
0,Scientific Data,41597,10.1038/s41597-022-01158-z,Large scale dataset of real space electronic charge density of cubic inorganic materials from density functional theory (DFT) calculations,21,2,2022,http://www.carolinamatdb.org/,"Python-language-based codes for converting the JSON file to the original CHG file generated by VASP of each case are provided, which can be downloaded through Figshare and our Carolina Materials Database ().",2022-02-21
0,Scientific Data,41597,10.1038/s41597-022-01187-8,LHC physics dataset for unsupervised New Physics detection at 40 MHz,29,3,2022,,"[{'monospace': 'PYTHIA 8.240', 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR40', '#text': '40'}}, '#text': 'Data are generated using , setting the collision energy at 13 TeV. Unless otherwise specified, all parameters were fixed to their default values.'}, 'We set the beam parameters to produce proton-proton collisions at 13\u2009TeV', {'monospace': ['Beams:idA = 2212', '! first beam, p = 2212, pbar = −2212 \\\\']}, {'monospace': ['Beams:idB = 2212', '! second beam, p = 2212, pbar = −2212 \\\\']}, {'monospace': ['Beams:eCM = 13000', '.! CM energy of collision \\\\']}, {'monospace': 'PYTHIA', 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR40', '#text': '40'}}, 'italic': ['W', 'v'], '#text': 'while the rest of the card is configured specifically for each process, as indicated in the  manual. For example,  → ℓ decays are generated with the settings:'}, {'monospace': 'WeakSingleBoson:ffbar2W\u2009=\u2009on! switch on W production mode'}, {'monospace': '24::onMode\u2009=\u2009off! switch off any W decay'}, {'monospace': '24:onIfAny\u2009=\u200911 13 15! switch on W-\u2009>\u2009lv decays.'}, 'The signal-specific parameters for the four benchmark signal models are set as follows:', {'italic': ['A', 'Z', 'Z', 'Z', 'e', 'μ', 'τ'], 'sup': ['*', '*', '*'], '#text': '• For  → 4ℓ: set the Higgs mass to 50\u2009GeV, force the decay to  final states, and force  → ℓℓ decays (ℓ = ,,).'}, {'italic': ['LQ', 'bτ', 'LQ', 'b', 'τ'], '#text': '• For  → : set the  mass to 80\u2009GeV and force its decays to a  quark and a  lepton.'}, {'italic': ['h', 'ττ', 'ττ'], 'sup': '0', '#text': '• For  → : set the Higgs boson mass to 60\u2009GeV and switch off any decay mode other than .'}, {'italic': ['h', 'τv', 'τv'], 'sup': '+', '#text': '• For  → : set the charged Higgs boson mass to 60\u2009GeV and switch off any decay mode other than .'}, {'monospace': ['DELPHES 3.3.2', 'Python'], 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR41', '#text': '41'}}, '#text': 'We emulate the detector response with , using the default Phase-II CMS detector card. For simplicity, we avoid degrading the detector resolution to account for the coarser nature of L1T event reconstruction. This simplification does not affect the aim of the study, which is not focused on assessing the absolute physics performance but instead on comparing different algorithms and their resource consumption. We include the effect of parasitic proton collisions, sampling the number of collisions according to a Poisson distribution centered at 20. The Delphes outcome is processed by a custom  macro to store the aforementioned physics content on HDF5 files, which are then published.'}]",2022-03-29
0,Scientific Data,41597,10.1038/s41597-022-01214-8,Global spatiotemporally continuous MODIS land surface temperature dataset,1,4,2022,,"[{'ext-link': {'@xlink:href': 'https://github.com/YuPeiHPU/ReconstructGlobalMODIS-LST.git', '@ext-link-type': 'uri', '#text': 'https://github.com/YuPeiHPU/ReconstructGlobalMODIS-LST.git'}, '#text': 'All the codes used in this study to construct the dataset were written in the MATLAB language and will be openly available at  under GNU Affero General Public License v3.0 after this work is accepted.'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR60', '#text': '60'}}, 'ext-link': {'@xlink:href': 'https://github.com/aida-alvera/DINEOF.git', '@ext-link-type': 'uri', '#text': 'https://github.com/aida-alvera/DINEOF.git'}, '#text': 'The code used to implement the DINEOF method is openly shared by Azcarate at .'}]",2022-04-01
0,Scientific Data,41597,10.1038/s41597-021-01043-1,An ensemble reconstruction of global monthly sea surface temperature and sea ice concentration 1000–1849,4,10,2021,https://github.com/shamakson/PaleoSST_SIC_ARM.git; https://github.com/jf256/reuse.git; https://github.com/shamakson/AMIP_bc_interpolation.git,"Three main types of codes were used in generating the datasets. Another one is also developed to interpolate between mid-months, while still conserving the monthly mean values. For spatial regression coefficients, we utilize a shell script calculates spatial regression for different calendar months, implementing various Climate Data Operators commands, and another shell script is used to select best sea ice analogs based on correlation coefficients between reconstructed subpolar SSTs and its instrumental target are available on GitHub (). The basic form of the data assimilation code written in R, is available on GitHub () and a python script that implements the AMIP II interpolation scheme is also available via GitHub ().",2021-10-04
0,Scientific Data,41597,10.1038/s41597-022-01243-3,"SINBAD, structural, experimental and clinical characterization of STAT inhibitors and their potential applications",31,3,2022,https://doi.org/10.6084/m9.figshare.14975136.v1,All generated code and data are hosted within Figshare repository .,2022-03-31
0,Scientific Data,41597,10.1038/s41597-021-01107-2,A kinematic and EMG dataset of online adjustment of reach-to-grasp movements to visual perturbations,21,1,2022,https://github.com/tuniklab/scientific-data,The code used for post-processing of the kinematic data is available at .,2022-01-21
0,Scientific Data,41597,10.1038/s41597-022-01159-y,A reference set of clinically relevant adverse drug-drug interactions,4,3,2022,https://github.com/elpidakon/CRESCENDDI; https://athena.ohdsi.org/; https://github.com/OHDSI/usagi,"The code used to generate this dataset is publicly available on a GitHub repository (). This code was developed and tested using: OHDSI standard vocabulary version v5.0 18-JAN-19 (), which includes: RxNorm version 20181203, RxNorm Extension version 2019-01-17, and MedDRA version19.1. Database storage and operations were enabled using PostgreSQL 9.3. Drug and event mapping steps were performed using OHDSI Usagi version 1.2.7 (). Web data extraction was performed using Python 3.6. Scores for the SDAs were calculated using Python 3.6 (Omega), SAS (delta_add) and R version 4.0.0 (IntSS, PRR, EBGM and BCPNN); AUC scores and CI estimates were calculated using MATLAB R2020b ( function).",2022-03-04
0,Scientific Data,41597,10.1038/s41597-022-01130-x,Confocal imaging dataset to assess endothelial cell orientation during extreme glucose conditions,27,1,2022,,No custom code was used to generate or process the data described in this data descriptor.,2022-01-27
0,Scientific Data,41597,10.1038/s41597-022-01189-6,Global seasonal Sentinel-1 interferometric coherence and backscatter data set,11,3,2022,,"[{'ext-link': {'@xlink:href': 'https://www.gamma-rs.ch/software', '@ext-link-type': 'uri', '#text': 'https://www.gamma-rs.ch/software'}, 'italic': ['openSAR', 'global_coherence', 'code', 'notebooks'], 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR45', '#text': '45'}}, '#text': 'The interferometric processing of Sentinel-1 data was performed with processing scripts using the commercial software developed by GAMMA Remote Sensing (v20201216). Usage of the software is subject to licensing (). A open code repository for the core code developed for processing, data access, and visualization is available at the Earth Big Data LLC  github repository. Relevant code for this data set is available in the  folders in the  and  sections at this repository.'}, {'ext-link': {'@xlink:href': 'https://github.com/EarthBigData/openSAR/blob/master/code/global_coherence/global_coherence_mosaic_tool.py', '@ext-link-type': 'uri', '#text': 'https://github.com/EarthBigData/openSAR/blob/master/code/global_coherence/global_coherence_mosaic_tool.py'}, '#text': 'A python tool to mosaic and subset data from the tiles is available at: .'}]",2022-03-11
0,Scientific Data,41597,10.1038/s41597-022-01216-6,A comprehensive LFQ benchmark dataset on modern day acquisition strategies in proteomics,30,3,2022,https://github.com/compomics/ms2pip_c; https://doi.org/10.5281/zenodo.5714380,"MSPIP is open source, licensed under the Apache-2.0 License, and hosted on . The Jupyter notebooks used to generate Fig.  are available through Zenodo, under .",2022-03-30
0,Scientific Data,41597,10.1038/s41597-022-01245-1,A worldwide epidemiological database for COVID-19 at fine-grained spatial resolution,29,3,2022,https://github.com/covid19datahub/COVID19,All the code used to generate the database is open-source and available at .,2022-03-29
0,Scientific Data,41597,10.1038/s41597-021-01109-0,A FAIR and AI-ready Higgs boson decay dataset,14,2,2022,CERN Open Data Portal,"To make the CMS  Open Dataset more accessible, we provide notebooks from the course “Particle Physics and Machine Learning” at University of California San Diego. The course notebooks provide a guide for the use of the  format dataset. We also have released a second set of interactive Jupyter Notebooks on GitHub, where we visualize feature distributions and feature correlations, and provide machine learning examples on low-level features in this dataset. The Jupyter notebooks that we released show how the -formatted data can be accessed.",2022-02-14
0,Scientific Data,41597,10.1038/s41597-021-01074-8,A palaeoclimate proxy database for water security planning in Queensland Australia,2,11,2021,https://github.com/nickmckay/sisal2lipd,Code to reformat the relational database to the LiPD and Rdata formats was adapted from this example () and is available in PalaeoWISE. Code to produce the figures are available in PalaeoWISE. Correlations were all produced using code published within the original publications cited within.,2021-11-02
0,Scientific Data,41597,10.1038/s41597-022-01160-5,"Zeo-1, a computational data set of zeolite structures",22,2,2022,www.scm.com,"Downloads of the Atomic Simulation Environment (v. 3.21.1) and NumPy (v. 1.20.1) packages for Python are freely available. Amsterdam Modeling Suite (v. 2020.203, r92091) is a commercial software, for which a free trial may be requested at .",2022-02-22
0,Scientific Data,41597,10.1038/s41597-021-01046-y,"Dataset of concurrent EEG, ECG, and behavior with multiple doses of transcranial electrical stimulation",27,10,2021,https://github.com/ngebodh/GX_tES_EEG_Physio_Behavior,"The latest version of all accompanying code for this dataset can be acquired within this repository: . MATLAB, version 2018b and 2019b were utilized with functions from EEGlab, Raincloud plots toolbox, and ANT neuro’s import functions.",2021-10-27
0,Scientific Data,41597,10.1038/s41597-021-01075-7,China material stocks and flows account for 1978–2018,25,11,2021,,"The original input data, the amount of products quantified by per-household, per-capita, and absolute in each province, is stored as xlsx files and shared in Figshare (Input data.xlsx). Data processing is performed using MATLAB software (MatlabR2019), and the codes for creating provincial and national stocks and flows datasets are also stored in Figshare (code_calculation.m). We share these datasets and scripts for data transparency and computational reproducibility, and to assist users for further exploration and development.",2021-11-25
0,Scientific Data,41597,10.1038/s41597-022-01132-9,"GLOBathy, the global lakes bathymetry dataset",3,2,2022,,"['We provide two Python scripts to accompany the GLOBathy dataset:', {'italic': ['“Generate_Bathymetry_Rasters.py”', 'GLOBathy_basic_parameters(ALL_LAKES).csv', '“HydroLAKES_polys_v10.shp”', 'Dmax', 'Dmax'], '#text': 'prepares bathymetric maps of the GLOBathy dataset. It requires two inputs: 1) a csv file containing maximum depth of the waterbodies (e.g., “” can be used as a template), and 2) polygon shapefiles of the corresponding waterbodies (e.g.,  obtained from\xa0the HydroLAKES dataset can be used as a template). This script can be used to re-generate GLOBathy data with new  estimations/observations or for any other case study, as long as the waterbody  value and shapefile are available.'}, {'italic': '“WGS_84_cell_dimesion_calculator.py”', '#text': 'In addition, we provide the  script which can be used to calculate cell dimensions of the GLOBathy raster files in South-North and East-West directions. It will provide the cell dimensions for any given location, so that accurate distances and volumes may be calculated. This is necessary because the geocentric coordinate system of the input raster data (WGS84) does not preserve distances. This script requires either the average latitude of the domain to be updated in the script header manually or a path to at least one bathymetry raster file to obtain the dimensions. In the first case the outputs are average cell dimensions of the study area. In the latter case for each bathymetry raster input, a csv file is generated that includes cell dimensions for every cell in the raster file. This script can also be used for other cases with a similar geocentric coordinate system. Script options (at the beginning of the script) need to be updated based on the input raster file.'}]",2022-02-03
0,Scientific Data,41597,10.1038/s41597-022-01218-4,A spatially-explicit harmonized global dataset of critical infrastructure,1,4,2022,https://github.com/snirandjan/CISI,"The code developed to process the OSM data is publicly available through the following GitHub repository: . The procedure for the developed CI dataset can be simulated using the main script, which is divided into three sections: (1) extraction of CI from OSM files in .PBF format, and reclassification; (2) estimation of amount of CI; and (3) calculation of the CISI. We also provide code for the validation procedure, and for the development of the figures and supplementary files. Detailed information per section and on the applied functions can be found on the repository, README file, and throughout the code.",2022-04-01
0,Scientific Data,41597,10.1038/s41597-021-01076-6,"The Swiss data cube, analysis ready data archive using earth observations of Switzerland",8,11,2021,,"[{'ext-link': [{'@xlink:href': 'https://www.opendatacube.org', '@ext-link-type': 'uri', '#text': 'https://www.opendatacube.org'}, {'@xlink:href': 'https://opensource.org/licenses/Apache-2.0', '@ext-link-type': 'uri', '#text': 'https://opensource.org/licenses/Apache-2.0'}], '#text': 'The SDC is based on the Open Data Cube platform that can be obtained at:  and freely available under an Apache 2.0 license ().'}, {'ext-link': [{'@xlink:href': 'https://www.python.org', '@ext-link-type': 'uri', '#text': 'https://www.python.org'}, {'@xlink:href': 'https://cran.r-project.org', '@ext-link-type': 'uri', '#text': 'https://cran.r-project.org'}, {'@xlink:href': 'https://github.com/GRIDgva/SwissDataCube/tree/master/ingestors', '@ext-link-type': 'uri', '#text': 'https://github.com/GRIDgva/SwissDataCube/tree/master/ingestors'}, {'@xlink:href': 'https://www.gnu.org/licenses/gpl-3.0.en.html', '@ext-link-type': 'uri', '#text': 'https://www.gnu.org/licenses/gpl-3.0.en.html'}], '#text': 'The Analysis Ready Data workflows are developed as a suite of Python (3.6) and R (2.7) scripts. Both programming languages are freely available at:  and  respectively under Python Software Foundation License (PSFL) and GNU General Public License. Landsat, Sentinel-1, Sentinel-2 workflows are freely available on the Swiss Data Cube GitHub repository  under GNU GPL 3.0 license ().'}, {'ext-link': [{'@xlink:href': 'https://code.usgs.gov/espa', '@ext-link-type': 'uri', '#text': 'https://code.usgs.gov/espa'}, {'@xlink:href': 'https://www.usgs.gov/core-science-systems/nli/landsat/espa-and-product-related-code-repository-location-changes', '@ext-link-type': 'uri', '#text': 'https://www.usgs.gov/core-science-systems/nli/landsat/espa-and-product-related-code-repository-location-changes'}], '#text': 'The Landsat Ecosystem Disturbance Adaptive Processing System (LEDAPS) (version 3.4.0) is an atmospheric correction model developed by USGS. The repository was recently removed from Github and is being re-established on the USGS Official Source Code Archive  and will be soon freely available under Unlicense conditions ().'}, {'ext-link': [{'@xlink:href': 'https://code.usgs.gov/espa', '@ext-link-type': 'uri', '#text': 'https://code.usgs.gov/espa'}, {'@xlink:href': 'https://www.usgs.gov/core-science-systems/nli/landsat/espa-and-product-related-code-repository-location-changes', '@ext-link-type': 'uri', '#text': 'https://www.usgs.gov/core-science-systems/nli/landsat/espa-and-product-related-code-repository-location-changes'}], '#text': 'Land Surface Reflectance Code (LaSRC) (version 1.4.1) is an atmospheric correction model developed by USGS. The repository was recently removed from Github and is being re-established on the USGS Official Source Code Archive  and will be soon freely available under Unlicense conditions ().'}, {'ext-link': {'@xlink:href': 'https://step.esa.int/main/third-party-plugins-2/sen2cor/', '@ext-link-type': 'uri', '#text': 'https://step.esa.int/main/third-party-plugins-2/sen2cor/'}, '#text': 'Sen2cor (version2.5) is a processor for Sentinel-2 Level 2\u2009A product generation and formatting. It is freely available at:  under an Apache 2.0 license.'}, {'ext-link': [{'@xlink:href': 'https://www.geo.uzh.ch/geolean/en/department/Staff/?content=davidsmall', '@ext-link-type': 'uri', '#text': 'https://www.geo.uzh.ch/geolean/en/department/Staff/?content=davidsmall'}, {'@xlink:href': 'http://step.esa.int/main/download/snap-download/', '@ext-link-type': 'uri', '#text': 'http://step.esa.int/main/download/snap-download/'}], 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR46', '#text': '46'}}, '#text': 'The Sentinel-1 processing is performed using UZH’s in-house developed radiometric image simulation and geocoding software radsim v2.0, currently undergoing packaging for wider distribution. More information can be obtained by contacting Dr. David Small (University of Zurich): . However, the algorithm used to generate Sentinel-1 analysis ready radiometrically terrain-corrected (RTC) Synthetic Aperture Radar (SAR) gamma nought backscatter data described in is also available in the European Space Agency (ESA) Sentinel Application Platform (SNAP) that is freely available at:  under a GNU GPL v3 license.'}, {'ext-link': {'@xlink:href': 'https://geonetwork-opensource.org', '@ext-link-type': 'uri', '#text': 'https://geonetwork-opensource.org'}, '#text': 'GeoNetwork is a web-based application (version 3.10.5) used for cataloguing, publishing and managing metadata description of spatially referenced resources using OGC and ISO standards. It is freely available at:  under a GNU General Public License v2.0.'}, {'ext-link': {'@xlink:href': 'https://github.com/opendatacube/datacube-ows', '@ext-link-type': 'uri', '#text': 'https://github.com/opendatacube/datacube-ows'}, '#text': 'Datacube-ows has been used to expose data collections according to OGC standards for visualization (Web Map Service – WMS) and download (Web Coverage Service – WCS). It is freely available at:  under an Apache 2.0 license.'}]",2021-11-08
0,Scientific Data,41597,10.1038/s41597-021-01048-w,The changing face of floodplains in the Mississippi River Basin detected by a 60-year land use change dataset,15,10,2021,https://colab.research.google.com/drive/1vmIaUCkL66CoTv4rNRIWpJXYXp4TlAKd?usp = sharing; https://gishub.org/mrb-floodplain,"The MRB floodplain land use change dataset is derived entirely through ArcGIS 10.5 and ENVI 5.1 geospatial analysis platforms (see  section for details). We developed additional open-access codes and visualization interfaces, however, to promote reproducibility and widespread application of the dataset. The python code is accessible at: . The visualization interface is available online at: . See  section for details.",2021-10-15
0,Scientific Data,41597,10.1038/s41597-022-01163-2,FAIRly big: A framework for computationally reproducible processing of large-scale data,11,3,2022,https://www.github.com/psychoinformatics-de/fairly-big-processing-workflow; https://doi.org/10.5281/zenodo.6019782; https://doi.org/10.5281/zenodo.6021002; https://www.github.com/m-wierzba/cat-container,All scripts used to process the data are publicly available at  (). The recipe used to build the CAT Singularity container () is publicly available at .,2022-03-11
0,Scientific Data,41597,10.1038/s41597-022-01192-x,A building height dataset across China in 2017 estimated by the spatially-informed approach,11,3,2022,https://github.com/terryyangwhu/BH_China.git; https://github.com/terryyangwhu/BH_China.git,"The programs used to generate all the results were Python, Google Earth Engine (GEE) and ESRI ArcGIS (Pro 2.5). The scripts of data collection and preprocessing on GEE can be accessed on GitHub (). Furthermore, we have made the Si-GPR model’s source code publicly accessible on GitHub ().",2022-03-11
0,Scientific Data,41597,10.1038/s41597-021-01078-4,Short-read and long-read RNA sequencing of mouse hematopoietic stem cells at bulk and single-cell levels,29,11,2021,https://github.com/LuChenLab/hemato,The codes used in this article were deposited in .,2021-11-29
0,Scientific Data,41597,10.1038/s41597-022-01135-6,"UK daily meteorology, air quality, and pollen measurements for 2016–2019, with estimates for missing data",9,2,2022,,"[{'bold': 'Data download and processing scripts.', 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR18', '#text': '18'}}, 'italic': 'README.md', '#text': 'The scripts used to download, extract, and process the MEDMI and AURN datasets are implemented in Python and are available under the GPL-3.0 license. Instructions on how to use them are included in the associated  files.'}, {'bold': 'Regional estimations package.', 'ext-link': {'@xlink:href': 'http://geopandas.org/', '@ext-link-type': 'uri', '#text': 'http://geopandas.org/'}, 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR19', '#text': '19'}}, 'italic': 'README.md', '#text': 'The concentric regions algorithm described above is implemented in Python, using the publicly available GeoPandas library (). We have made this regional estimations code available as a Python library, accessible under the MIT licence. Instructions on how to use the package are included in the associated  file. This package receives inputs from users as three CSV files (site metadata, site measurement data and region metadata), and the algorithm iterates through each requested region and timestamp combination, outputting an estimation for each. As we present the concentric regions algorithm as a baseline method, this package is built in a modular fashion to facilitate users adding further regional estimation algorithms as new components, further to the two currently implemented (concentric regions and a simple shortest distance based algorithm).'}, {'bold': 'EMEP modelling system.', 'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR20', '#text': '20'}}, {'xref': [{'@ref-type': 'bibr', '@rid': 'CR21', '#text': '21'}, {'@ref-type': 'bibr', '@rid': 'CR22', '#text': '22'}], '#text': ','}], '#text': 'The EMEP model source code is available to download from Zenodo. The input files, and NAEI emissions used for this project are also available for download from Zenodo.'}]",2022-02-09
0,Scientific Data,41597,10.1038/s41597-022-01164-1,"PET-BIDS, an extension to the brain imaging data structure for positron emission tomography",2,3,2022,,"All code reported in this manuscript for converting, validating and analyzing PET-BIDS data is publicly available, and is referenced in the main text.",2022-03-02
0,Scientific Data,41597,10.1038/s41597-022-01193-w,"A global urban microwave backscatter time series data set for 1993–2020 using ERS, QuikSCAT, and ASCAT data",16,3,2022,,"['Code used in this paper is available in the following locations:', {'ext-link': {'@xlink:href': 'https://github.com/tmilliman/sir_to_netcdf', '@ext-link-type': 'uri', '#text': 'https://github.com/tmilliman/sir_to_netcdf'}, '#text': 'The github repository, tmilliman/sir_to_netcdf, ()'}, 'includes the python and bash scripts that:', '1. Convert NASA SCP backscatter SIR images to NetCDF files of seasonal mean and standard deviation backscatter at 0.05° (lat/lon) for all land, and with an urban mask. Separate code for each sensor (ERS, QuikSCAT, ASCAT).', {'ext-link': {'@xlink:href': 'https://github.com/tmilliman/urban_backscatter', '@ext-link-type': 'uri', '#text': 'https://github.com/tmilliman/urban_backscatter'}, '#text': 'A separate github repository, tmilliman/urban_backscatter, () includes scripts that:'}, '2. Extract from NetCDF files in #1 the backscatter data for 11\u2009×\u200911 grids around a lat-lon location and create CSV files from this data.', {'xref': {'@rid': 'Fig3', '@ref-type': 'fig', '#text': '3'}, '#text': '3. Create using the scripts in #2 CSV files with the mean backscatter for city-level grids around city centers and for also for the invariant regions shown in Fig.\xa0.'}, {'ext-link': {'@xlink:href': 'https://github.com/sfrolking/urban_backscatter_ERS_QSCAT_ASCAT', '@ext-link-type': 'uri', '#text': 'https://github.com/sfrolking/urban_backscatter_ERS_QSCAT_ASCAT'}, '#text': 'The github repository sfrolking/urban_backscatter_ERS_QSCAT_ASCAT () includes R scripts that:'}, {'xref': {'@rid': 'Fig2', '@ref-type': 'fig', '#text': '2'}, '#text': '4. Evaluate seasonal backscatter for invariant evergreen tropical forest sites and construct Fig.\xa0.'}, {'xref': {'@rid': 'Fig3', '@ref-type': 'fig', '#text': '3'}, '#text': '5. use the CSV files from #3 above and correlate 2015 summer mean ASCAT backscatter with building volume data and construct Fig.\xa0.'}, {'xref': [{'@rid': 'Fig4', '@ref-type': 'fig', '#text': '4'}, {'@rid': 'Fig6', '@ref-type': 'fig', '#text': '6'}], '#text': '6. use the CSV files from #3 above to generate annual mean summer backscatter time series plots for sample cities and construct Figs.\xa0–.'}]",2022-03-16
0,Scientific Data,41597,10.1038/s41597-021-01113-4,"A 24-hour population distribution dataset based on mobile phone data from Helsinki Metropolitan Area, Finland",4,2,2022,https://github.com/DigitalGeographyLab/mfd-helsinki,The developed codes and tools for generating and validating the population datasets are written in Python and openly available on GitHub: .,2022-02-04
0,Scientific Data,41597,10.1038/s41597-021-01079-3,Bias-corrected CMIP6 global dataset for dynamical downscaling of the historical and future climate (1979–2100),4,11,2021,,The code used to produce the bias-corrected global CMIP6 data is publicly available. The code consists of an NCL (version 6.6.2) script to compute non-linear trends and a few CDO (version 1.7.0) scripts to regrid data and correct CMIP6 data biases.,2021-11-04
0,Scientific Data,41597,10.1038/s41597-022-01165-0,Short-read and long-read full-length transcriptome of mouse neural stem cells across neurodevelopmental stages,2,3,2022,https://github.com/LuChenLab/Neuron,The codes used in this article were deposited in .,2022-03-02
0,Scientific Data,41597,10.1038/s41597-022-01194-9,Scientific data from precipitation driver response model intercomparison project,30,3,2022,https://www.dkrz.de/up/systems/wdcc; https://doi.org/10.26050/WDCC/PDRMIP_2012-2021,A code to extract the PDRMIP data is available at the storage of the data (see usage section) where all the PDRMIP data are freely available. The PDRMIP data are available through the World Data Center for Climate (WDCC)  with .,2022-03-30
0,Scientific Data,41597,10.1038/s41597-021-01050-2,"SMAP-HydroBlocks, a 30-m satellite-based soil moisture dataset for the conterminous US",11,10,2021,https://github.com/chaneyn/HydroBlocks,"Source code for the HydroBlocks land surface model is available at . The Random Forest model used to parameterize the merging scheme was implemented using the RandomForestRegressor class of the scikit-learn Python module. While not written as a portable library or toolset, code is available upon request.",2021-10-11
0,Scientific Data,41597,10.1038/s41597-022-01166-z,Southern ocean sea level anomaly in the sea ice-covered sector from multimission satellite observations,2,3,2022,https://github.com/MatthisAuger/SO_SLA,"The codes used to process the along track measurements and for the Optimal Interpolation (OI) are not available for public use as Collecte Localisation Satellite (CLS) and the Centre National des Etudes Spatiales (CNES) are the proprietary owners. However, these codes are extensively described in and. The python code used for the comparison of the product with external sources of data are available at .",2022-03-02
0,Scientific Data,41597,10.1038/s41597-022-01222-8,Multi-scanner and multi-modal lumbar vertebral body and intervertebral disc segmentation database,23,3,2022,https://doi.org/10.17605/OSF.IO/QX5RT; https://simpleitk.org/; https://nipy.org/nibabel/,"The database is available online within the OSF Repository, including the different imaging datasets, segmentation files, as well as metadata comprising patient characteristics and details on the available pulse sequences and used MRI systems per patient (Identifier: ). We additionally provide Python scripts to read and visualize the data by utilizing open-source image analysis libraries – SimpleITK () and NiBabel (), available within the repository wiki page.",2022-03-23
0,Scientific Data,41597,10.1038/s41597-022-01251-3,The Mexican magnetic resonance imaging dataset of patients with cocaine use disorder: SUDMEX CONN,31,3,2022,https://github.com/psilantrolab/SUDMEX_CONN,"For the code analysis presented here, please check:",2022-03-31
0,Scientific Data,41597,10.1038/s41597-021-01080-w,"CHIASM, the human brain albinism and achiasma MRI dataset",26,11,2021,https://brainlife.io/apps,"The processing was performed mainly using brainlife.io services (), which together with the code are available online. The offline preprocessing was performed with freely accessible neuroimaging tools. The preprocessing steps, together with the references to the Software/Apps are provided separately for the T1w, DW, fMRI data, and hand-curated ROIs and mask (Tables –, respectively). The web links to source code are provided separately in the Table .",2021-11-26
0,Scientific Data,41597,10.1038/s41597-022-01138-3,Efficient compressed database of equilibrated configurations of ring-linear polymer blends for MD simulations,8,2,2022,,"[{'xref': {'@ref-type': 'supplementary-material', '@rid': 'MOESM1', '#text': 'Supporting Information'}, 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR41', '#text': '41'}}, '#text': 'To decode the JHPCN-DF compression, no special attention was required. (The easy-to-use sample code for generating the LAMMPS input data file is attached in the\xa0) To encode/segment the data into two parts with JHPCN-DF compression, as shown for the above data, the main part of the reference code is as follows:'}, 'union fi64{', 'double f;', 'uint64_t i64;', '};', 'double fval0,fval1,allowerr,logallo;', 'int i,ntotal,ival,ival2,sval;', 'union fi64 fival,fival1;', 'double *posi_before_compress, *posi_after_compress;', 'uint64_t *tailing_fraction_bits_posi;', 'allowerr\u2009=\u20090.00001', 'logallo\u2009=\u2009log(allowerr)/log(2.0);', 'for(i\u2009=\u20090;i\u2009<\u20093*ntotal;i++){', 'fval0\u2009=\u2009posi_before_compress[i];', 'frexp(fval0,&ival);', 'ival2\u2009=\u2009(int)(-logallo\u2009+\u2009ival);', 'sval\u2009=\u2009(int)(53-ival2);', 'if(sval\u2009>\u200952) sval\u2009=\u200953;', 'do {', 'sval–;', 'fival.f\u2009=\u2009fval0;', 'fival.i64\u2009=\u2009(fival.i64 ≫ sval);', 'fival.i64\u2009=\u2009(fival.i64 ≪ sval);', 'fval1\u2009=\u2009fival.f;', '} while ((fval1-fval0)*(fval1-fval0) >allowerr*allowerr);', 'posi_after_compress[i]\u2009=\u2009fval1;', 'fival1.f\u2009=\u2009fval1;', 'fival.f\u2009=\u2009fval0;', 'tailing_fraction_bits_posi[i]\u2009=\u2009(fival1.i64 ^ fival.i64);', '}', {'ext-link': {'@xlink:href': 'https://github.com/avr-aics-riken/JHPCN-DF', '@ext-link-type': 'uri', '#text': 'https://github.com/avr-aics-riken/JHPCN-DF'}, '#text': 'For software developers, RIKEN has released the open library “JHPCN-DF” at the following GitHub repository: .'}]",2022-02-08
0,Scientific Data,41597,10.1038/s41597-021-01052-0,Downscaling SSP-consistent global spatial urban land projections from 1/8-degree to 1-km resolution 2000–2100,28,10,2021,,The dataset was created using Python scripts with ArcGIS 10.6.1. Codes are publicly available at GitHub through Zenodo. All input data are publicly available as described in “Data Records”.,2021-10-28
0,Scientific Data,41597,10.1038/s41597-022-01252-2,A residential labeled dataset for smart meter data analytics,31,3,2022,https://gitlab.com/alspereira/EMD-SF; https://gitlab.com/alspereira/EMD-DF; https://gitlab.com/mikemx55/Plugwise-2-M-ITI; https://osf.io/jcn2q/,"The code used to collect and store the aggregated consumption data is available at . This project used the EMD-DF library to create the audio files, which is available . The code runs using Java 8 or higher on a Windows machine. The code used to collect the individual appliance consumption is available at . The code runs using Python 3 on a Ubuntu machine. Finally, the Python 3 code to reproduce the examples presented in this paper is available on the dataset repository at .",2022-03-31
0,Scientific Data,41597,10.1038/s41597-021-01116-1,Source imaging of high-density visual evoked potentials with multi-scale brain parcellations and connectomes,19,1,2022,https://connectome-mapper-3.readthedocs.io; https://sites.google.com/site/cartoolcommunity,"The dataset contains a code/folder with scripts, and .ini files for generating each derivative. CMP3 is freely available at , the Cartool software via .",2022-01-19
0,Scientific Data,41597,10.1038/s41597-021-01081-9,A database of global coastal conditions,26,11,2021,,Code in R language to recreate the database and the figures in the Usage Notes is available on Figshare.,2021-11-26
0,Scientific Data,41597,10.1038/s41597-022-01139-2,Simultaneous visualization of DNA loci in single cells by combinatorial multi-color iFISH,10,2,2022,https://github.com/anacmota/miFISH; github.com/elgw/dotter; https://github.com/elgw/df_cc,All the custom code written in MATLAB used for image processing and for all the analyses presented is available at . The DOTTER suite written in MATLAB is available at . The MATLAB code for chromatic aberration correction is available at . The Python documentation for 3D segmentation and lamina distance calculation is available here.,2022-02-10
0,Scientific Data,41597,10.1038/s41597-022-01168-x,Vectorized rooftop area data for 90 cities in China,2,3,2022,https://github.com/ChanceQZ/RoofTopSegmatation,"The procedure of spatial sampling is executed in the ArcGIS Pro platform. The code of the deep learning model is available at . The program is described by Python3, packages of which are Pytroch, Numpy, and OpenCV mainly.",2022-03-02
0,Scientific Data,41597,10.1038/s41597-021-01117-0,"Psychophysiology of positive and negative emotions, dataset of 1157 cases and 8 biosignals",20,1,2022,,"[{'ext-link': {'@xlink:href': 'https://github.com/psychosensing/popane-2021', '@ext-link-type': 'uri', '#text': 'https://github.com/psychosensing/popane-2021'}, '#text': 'The code can be accessed on the public GitHub repository: . It is licensed under MIT OpenSource license, i.e., the permission is granted, free of charge, to obtaining a copy of this software and associated files (e.g., the Jupyter IPython Notebooks), subject to the following conditions: the copyright notice and the MIT license permission notice shall be included in all copies or substantial portions of the software based on the scripts we published.'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR83', '#text': '83'}}, '#text': 'Scripts that we used to transform the data from proprietary acquisition formats into coherent CSV files utilized Python 3.6. The list of the specific modules and their versions is available in the “requirements.txt” file in the GitHub repository.'}, {'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR84', '#text': '84'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR73', '#text': '73'}}], '#text': 'Jupyter Notebooks use Python version: 3.5.3, as well as the following Python modules: packages related to Jupyter Notebook: notebook module v. 6.1.4; jupyter-core module v. 4.6.3, jupyter-client v. 6.1.7; ipython v. 7.9.0; ipykernel v. 5.3.4; and a data organization and manipulation module – pandas v. 0.25.3.'}]",2022-01-20
0,Scientific Data,41597,10.1038/s41597-022-01169-w,Validation and refinement of cropland data layer using a spatial-temporal decision tree algorithm,2,3,2022,https://github.com/llin-csiss/RCDL,The scripts used to generate the R-CDL dataset are available in this GitHub repository: .,2022-03-02
0,Scientific Data,41597,10.1038/s41597-021-01083-7,Crash and disengagement data of autonomous vehicles on public roads in California,23,11,2021,https://github.com/geopandas/geopandas; https://github.com/gboeing/osmnx,"Primary data (crash and disengagement data) extraction does not utilise any automated pipelines or scripts due to implicit and subjective data in the original reports. Experts manually analysed each report to register the data into processable formats. GeoPandas, OSMnx, and Pandas Python custom modules are used for the extraction of secondary data. Code documentation for GeoPandas () and OSMnx () can be found in the corresponding GitHub repositories. A more elaborative procedure is explained in the Methods section.",2021-11-23
0,Scientific Data,41597,10.1038/s41597-022-01226-4,"The Dual Mechanisms of Cognitive Control dataset, a theoretically-guided within-subject task fMRI battery",29,3,2022,,"All custom code is freely available in several locations, depending on type. Code to work with the DMCC55B data records, including to reproduce the validation analyses, is in the DMCC55B Dataset Description component of the DMCC OSF site. Code elsewhere is not specific to the DMCC55B dataset, but used in the main DMCC project. Eprime task presentation scripts can be downloaded after completing the online form. Processing and summary scripts used after data collection are in the dualmechanisms GitHub or Docker Hub repositories, depending on type.",2022-03-29
0,Scientific Data,41597,10.1038/s41597-021-01055-x,A high-fidelity residential building occupancy detection dataset,28,10,2021,https://github.com/mhsjacoby/HPDmobile,"All code used to collect, process, and validate the data was written in Python and is available for download (). All image processing was done with the Python Image Library package (PIL) Image module, version 7.2.0. Audio processing was done with SciPy io module, version 1.5.0. Environmental data processing made extensive use of the pandas package, version 1.0.5. The code base that was developed for data collection with the HPDmobile system utilizes a standard client-server model, whereby the sensor hub is the server and the VM is the client. Note that the term “server” in this context refers to the SBC (sensor hub), and not the the on-site server mentioned above, which runs the VMs. All collection code on both the client- and server-side were written in Python to run on Linux systems. Technical validation of the audio and images were done in Python with scikit-learn version 0.24.1, and YOLOv5 version 3.0.",2021-10-28
0,Scientific Data,41597,10.1038/s41597-022-01255-z,"fastMRI+, Clinical pathology annotations for knee and brain fully sampled magnetic resonance imaging data",5,4,2022,https://github.com/facebookresearch/fastMRI,"Scripts used to generate the DICOM images for radiologists can be accessed from (‘ExampleScripts/fastmri-to-dicom.py’) in the open-source GitHub repository. The detailed method used has been specified in the Methods section. More open-source tools for reconstructing the original fastMRI dataset, including standardized evaluation criteria, standardized code, and PyTorch data loaders can be found in the fastMRI GitHub repository ().",2022-04-05
0,Scientific Data,41597,10.1038/s41597-021-01084-6,Global daily 1 km land surface precipitation based on cloud cover-informed downscaling,26,11,2021,https://gitlabext.wsl.ch/karger/chelsa_earthenv; https://gitlabext.wsl.ch/karger/chelsa_earthenv_validation,The code calculating the bias correction on the CHELSA V2.0 precipitation data is written in Python 2.7 and C++ (via the SAGA-GIS api). The code for the cloud cover refinement is available here: . The code for the validation is available here: .,2021-11-26
0,Scientific Data,41597,10.1038/s41597-022-01141-8,A construction classification system database for understanding resource use in building construction,9,2,2022,https://doi.org/10.5281/zenodo.5576147,"No custom code was used in the production of this dataset. Use of existing software (Microsoft Excel) is described in the Methods and Data Records sections. The translation code that converts the dataset into the format developed by Heeren and Fishman (2019) and the sample code for querying the dataset are coded in Python. The dataset, translation script and sample code are made available in an open source repository and the Python packages used in the sample code are listed in the “requirements.txt” file ().",2022-02-09
0,Scientific Data,41597,10.1038/s41597-021-01092-6,Comprehensive diffusion MRI dataset for in vivo human brain microstructure mapping using 300 mT/m gradients,18,1,2022,https://github.com/ksubramz/gradunwarp; https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FslInstallation; https://surfer.nmr.mgh.harvard.edu/fswiki/DownloadAndInstall; https://nrg.wustl.edu/software/face-masking; https://www.fil.ion.ucl.ac.uk/spm; https://www.mrtrix.org; https://www.dipy.org; https://github.com/qiyuantian/GDSI; https://github.com/NYU-DiffusionMRI/DESIGNER; http://mig.cs.ucl.ac.uk/index.php?n=Tutorial.NODDImatlab; https://github.com/ekaden/smt,"The Matlab codes for gradient nonlinearity correction have been modified into Python codes, which are publicly available (). The FMRIB Software Library used in the diffusion data pre-processing and DTI and BEDPOSTX model fitting is publicly available (). The FreeSurfer software is publicly available (). The software used for face masking is publicly available (). The SPM software for image intensity bias correction is publicly available (). The MRtrix3 software for the constrained spherical deconvolution is publicly available (). The DIPY software used for the CSA-QBI and GQI reconstruction is publicly available (). The codes for the GDSI reconstruction are publicly available (). The DESIGNER software for the DKI and WMTI model fitting is publicly available (). The NODDI Matlab toolbox is publicly available (). The SMT software for the MicroDTI and MCMicro model fitting are publicly available ().",2022-01-18
0,Scientific Data,41597,10.1038/s41597-022-01142-7,"Organic materials repurposing, a data set for theoretical predictions of new applications for existing compounds",14,2,2022,,Scripts to obtain plots starting from the database are available at the University of Liverpool repository.,2022-02-14
0,Scientific Data,41597,10.1038/s41597-022-01171-2,"Caltech Conte Center, a multimodal data resource for exploring social cognition and decision-making",31,3,2022,,"[{'ext-link': {'@xlink:href': 'https://github.com/adolphslab/ConteDataRelease', '@ext-link-type': 'uri', '#text': 'https://github.com/adolphslab/ConteDataRelease'}, '#text': 'We used containerized versions of fMRIPrep 20.2.1 and MRIQC for data preprocessing and quality control. Example calling scripts for fMRIPrep, jupyter lab notebooks for figure recreation and R code for the example factor analysis are provided at .'}, {'ext-link': {'@xlink:href': 'https://github.com/adolphslab/rsDenoise', '@ext-link-type': 'uri', '#text': 'https://github.com/adolphslab/rsDenoise'}, '#text': 'The code to reproduce resting-state and movie analyses are provided at . As outlined in detail in the source, this codebase can easily be adapted to run many different configurations of denoising decisions on the data.'}]",2022-03-31
0,Scientific Data,41597,10.1038/s41597-021-01028-0,Dataset for predicting single-spot proton ranges in proton therapy of prostate cancer,29,9,2021,,"All code is available in the figshare repository. The code includes scripts for the analyses presented in this paper. The scripts rely on open source Python packages such as numpy, pandas, matplotlib, scipy, sklearn and pickle.",2021-09-29
0,Scientific Data,41597,10.1038/s41597-022-01257-x,A three-year dataset supporting research on building energy management and occupancy analytics,5,4,2022,https://github.com/LBNL-ETA/Data-Cleaning,"The Python code for detecting and filling the data gaps, as well as for modifying outlier values, is available at the dataset’s GitHub page: .",2022-04-05
0,Scientific Data,41597,10.1038/s41597-022-01143-6,A large-scale study on research code quality and execution,21,2,2022,https://github.com/a,"To develop and execute the analysis code, we used Python 2.7. The code is released as a single version, which was used for both data collection and analysis. All Python dependencies with their versions are captured in a text file  at the root directory. All code files can be freely accessed on on GitHub at  trisovic/dataverse-r-study. The code is released under MIT license.",2022-02-21
0,Scientific Data,41597,10.1038/s41597-021-01058-8,Characterization of hormone-producing cell types in the teleost pituitary gland using single-cell RNA-seq,28,10,2021,https://github.com/sikh09/Medaka-pituitary-scRNA-seq,The R code used in the analysis of the scRNA-seq data is available on GitHub ().,2021-10-28
0,Scientific Data,41597,10.1038/s41597-022-01173-0,Open multimodal iEEG-fMRI dataset from naturalistic stimulation with a short audiovisual film,21,3,2022,https://github.com/UMCU-RIBS/ieeg-fmri-dataset-validation; https://github.com/UMCU-RIBS/ieeg-fmri-dataset-quickstart,The code used to perform technical validation on the (i)BIDS dataset is available at . We also provide a set of utility scripts to help new users get started with processing and visualizing the data ().,2022-03-21
0,Scientific Data,41597,10.1038/s41597-021-01059-7,"The normalised Sentinel-1 Global Backscatter Model, mapping Earth’s land surface with C-band microwaves",28,10,2021,,"[{'bold': ['SGRT', 'GDAL', 'NumPy', 'SNAP'], 'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR36', '#text': '36'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR45', '#text': '45'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR30', '#text': '30'}}], '#text': 'The S1GBM mosaics were produced with geodata management software and scientific algorithms contained in the SAR Geophysical Retrieval Toolbox ( v2.4) software suite, which embeds also open-access python libraries (, ) and Sentinel-1 preprocessing functions of the  v6.0 toolbox. The SGRT suite has been developed by TU Wien and is not openly accessible, and is only available under conditions to project- and research-partners of TU Wien.'}, {'bold': 'Equi7Grid', 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR33', '#text': '33'}}, '#text': 'For the usage of the  we provide data and tools via the openly accessible python package on GitHub.'}, {'bold': 'yeoda', 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR55', '#text': '55'}}, '#text': 'Furthermore, we encourage users to use TU Wien’s open-source Python package , a datacube storage access layer that offers functions to read, write, search, filter, split and load data from the S1GBM datacube. The yeoda package is openly accessible on GitHub.'}]",2021-10-28
0,Scientific Data,41597,10.1038/s41597-022-01174-z,High spatial resolution dataset of La Mobilière insurance customers,11,3,2022,https://github.com/alibatti/LaMobiliereDatasetCode,The code used to validate our data is available at  in the form of Python scripts.,2022-03-11
0,Scientific Data,41597,10.1038/s41597-022-01201-z,An improved daily standardized precipitation index dataset for mainland China from 1961 to 2018,30,3,2022,https://github.com/wangqianfeng23/DailySPI,All calculations of daily SPI are based on the Python language and are available at GitHub: . Any updates will also be published on GitHub.,2022-03-30
0,Scientific Data,41597,10.1038/s41597-022-01231-7,"Neurocognitive aging data release with behavioral, structural and multi-echo functional MRI measures",29,3,2022,https://github.com/ME-ICA/me-ica; https://tedana.readthedocs.io/,"ME-ICA uses AFNI and python, both of which are open-source software. ME-ICA processing code is available at . As the code base is unmaintained, readers are also directed to  for additional multi-echo de-noising options.",2022-03-29
0,Scientific Data,41597,10.1038/s41597-022-01118-7,"p3k14c, a synthetic global database of archaeological radiocarbon dates",27,1,2022,,Code used to prepare the datasets and for the data quality analyses reported above were developed using both the  and  computing languages.,2022-01-27
0,Scientific Data,41597,10.1038/s41597-022-01147-2,"Thinking out loud, an open-access EEG-based BCI dataset for inner speech recognition",14,2,2022,https://github.com/N-Nieto/Inner_Speech_Dataset,"In line with reproducible research philosophy, all codes used in this paper are publicly available and can be accessed at . The stimulation protocol and the auxiliary MatLab functions are also available. The code was run in PC1, and shows the stimulation protocol to the participants while sending the event information to PC2, via parallel port. The processing Python scripts are also available. The repository contains all the auxiliary functions to facilitate the load, use and processing of the data, as described above. By changing a few parameters in the main processing script, a completely different process can be obtained, allowing any interested user to easily build his/her own processing code. Additionally, all scripts for generating the Time-Frequency Representations and the plots here presented, are also available.",2022-02-14
0,Scientific Data,41597,10.1038/s41597-022-01203-x,Big data collection in pharmaceutical manufacturing and its use for product quality predictions,23,3,2022,https://doi.org/10.6084/m9.figshare.c.5645578.v1,"Python code is available at figshare within the same collection as the dataset:. The code is available for preprocessing, visualization and attribute extraction.",2022-03-23
0,Scientific Data,41597,10.1038/s41597-021-01032-4,"SOIL-WATERGRIDS, mapping dynamic changes in soil moisture and depth of water table from 1970 to 2014",6,10,2021,,"[{'ext-link': {'@xlink:href': 'https://sites.google.com/site/thebrtsimproject/home', '@ext-link-type': 'uri', '#text': 'https://sites.google.com/site/thebrtsimproject/home'}, '#text': 'SOIL-WATERGRIDS has been generated using the BRTSim (BioReactive Transport Simulator) computational solver version v4.1a (2020). The BRTSim software is multiplatform and can be deployed on Microsoft, Unix/Linux/Ubuntu, and Mac operating systems. The full BRTSim package, inclusive of executables, examples, basic post-processing scripts, and User Manual and Technical Guide are available for download at the BRTSim home page  under the CC BY4.0 license.'}, {'xref': {'@rid': 'Tab6', '@ref-type': 'table', '#text': '1'}, 'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR49', '#text': '49'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR49', '#text': '49'}}], '#text': 'Seeding datasets used in our modelling are not distributed here because they are publicly accessible from the links reported in Online-only Table\xa0. BRTSim input files required to produce the full-size global-scale simulation of SOIL-WATERGRIDS are distributed in this data release and are available at the Zenodo repository (about 50 GB). The BRTSim modelling distribution package for SOIL-WATERGRIDS includes 7 continental regions (Africa, Asia insular, Asia, Europe, North America, South America, Oceania) organized in compressed folders that include the BRTSim executable and license file, bash files (BRTSim_v41a_GLNXA64_R2019b, run_BRTSim_v41a_GLNXA64_R2019b.sh, license.txt), the Param_*.inp files to instruct BRTSim to run each grid cells of the computational domain, and the tables of boundary conditions (Table_*.txt, WB_*.txt, and RNF_*.txt) organized by continental regions. The license file is valid until the end of 2021 and a new license can be obtained with no charge from the BRTSim home page after expiry. The distributed package allows to model about 168,000 grid cells globally. Raw outputs of the full-scale model run occupy about 10TB (uncompressed) and is not distributed. Full details on the BRTSim modelling including data structure, file naming, supported operating system, modelling launching and workflow, and accompanying scripts are available in the SOIL-WATERGRIDS Technical Documentation.'}]",2021-10-06
0,Scientific Data,41597,10.1038/s41597-021-01061-z,Crop production and nitrogen use in European cropland and grassland 1961–2019,29,10,2021,,Source code in Python for the data processing is available in the data record.,2021-10-29
0,Scientific Data,41597,10.1038/s41597-022-01261-1,"AB-DB: Force-Field parameters, MD trajectories, QM-based data, and Descriptors of Antimicrobials",1,4,2022,,"QSAR calculations were performed using the ChemAxon’s Marvin suite of programs, version 21.14. For QM calculations we used the Gaussian16 package, revision A.03. The Amber18 package was used for MD simulations and FF generation. We used simple bash scripts to iteratively extract descriptors from outputs and generate AB-DB data-files.",2022-04-01
0,Scientific Data,41597,10.1038/s41597-022-01177-w,A dataset of 175k stable and metastable materials calculated with the PBEsol and SCAN functionals,2,3,2022,https://github.com/hyllios/utils/tree/main/ht_pd_scan,"All data can be easily processed with publicly available tools such as json and pymatgen. An example usage is provided with the data. The dataset was generated with VASP, the bash and python scripts to generate input files or manage the output files can be downloaded from github repository: .",2022-03-02
0,Scientific Data,41597,10.1038/s41597-021-01099-z,Intracranial electrophysiological recordings from the human brain during memory tasks with pupillometry,13,1,2022,https://gitlab.com/brainandmindlab/memory_encoding,"To provide an easy way to detect and analyze electrophysiological activity in iEEG signals we make our codes available at GitLab (). Currently, the repository contains a python script to automatically process the individual BIDS layers of the dataset (subject, task, run, channel) using the EPYCOM library. The library is focused on iEEG processing and contains a set of algorithms for automated detection of high frequency oscillations, interictal epileptiform discharges and for computation of univariate and bivariate feature calculation. The repository will be gradually updated with scripts for statistical analyses and result visualizations.",2022-01-13
0,Scientific Data,41597,10.1038/s41597-021-01062-y,high-resolution structural MRI-based atlas of human thalamic nuclei,28,10,2021,,The code for the segmentation is a shell script which is provided in the Zenodo repository. It performs an automatic cropping of the input dataset prior to registering to a cropped custom template. This is done to speed up registration and for accuracy by focusing on the thalami as the crop region encompasses both thalami. A mask for automatic cropping and the cropped custom template are also provided.,2021-10-28
0,Scientific Data,41597,10.1038/s41597-022-01262-0,"Emognition dataset: emotion recognition with self-reports, facial expressions, and physiology using wearables",7,4,2022,https://github.com/Emognition/Emognition-wearable-dataset-2020,The code used for the technical validation is publicly available at . The code was developed in Python 3.7. The repository contains several  with data manipulations and visualizations. All required packages are listed in  file. The repository may be used as a starting point for further data analyses. It allows you to easily load and preview the Emognition dataset.,2022-04-07
0,Scientific Data,41597,10.1038/s41597-022-01149-0,RNA-seq profiling of white and brown adipocyte differentiation treated with epigallocatechin gallate,8,2,2022,,"['In the current study, the following open access software was used as described in the Methods section. For all the software, we used default parameters, and no custom code was used beyond the tools listed.', {'ext-link': {'@xlink:href': 'http://www.bioinformatics.babraham.ac.uk/projects/fastqc/', '@ext-link-type': 'uri', '#text': 'http://www.bioinformatics.babraham.ac.uk/projects/fastqc/'}, '#text': '1. FastQC (version 0.11.9) was used to check the quality of raw FASTQ sequencing data: .'}, {'ext-link': {'@xlink:href': 'https://github.com/OpenGene/fastp', '@ext-link-type': 'uri', '#text': 'https://github.com/OpenGene/fastp'}, '#text': '2. Fastp (version 0.20.1) was used to trim adapters and filter quality reads: .'}, {'ext-link': {'@xlink:href': 'http://daehwankimlab.github.io/hisat2/', '@ext-link-type': 'uri', '#text': 'http://daehwankimlab.github.io/hisat2/'}, '#text': '3. HISAT2 (version 2.2.0) was used to map sequence reads to the mouse mm9 genome: .'}, {'ext-link': {'@xlink:href': 'https://bioconductor.org/packages/release/bioc/html/DESeq2.html', '@ext-link-type': 'uri', '#text': 'https://bioconductor.org/packages/release/bioc/html/DESeq2.html'}, '#text': '4. DESeq2 (version 1.10.1) was used to identify differentially expressed genes: .'}, {'ext-link': {'@xlink:href': 'https://cran.r-project.org/web/packages/FactoMineR/index.html', '@ext-link-type': 'uri', '#text': 'https://cran.r-project.org/web/packages/FactoMineR/index.html'}, '#text': '5. FactoMineR (version 2.4) was used to perform PCA: .'}, {'ext-link': {'@xlink:href': 'https://cran.r-project.org/web/packages/pheatmap/', '@ext-link-type': 'uri', '#text': 'https://cran.r-project.org/web/packages/pheatmap/'}, '#text': '6. Pheatmap (Version 1.0.12) was used to plot the heatmap: .'}, {'ext-link': {'@xlink:href': 'https://cran.r-project.org/web/packages/ggplot2/index.html', '@ext-link-type': 'uri', '#text': 'https://cran.r-project.org/web/packages/ggplot2/index.html'}, '#text': '7. Ggplot2 (version 3.3.4) was used to generate the volcano plot: .'}, {'ext-link': {'@xlink:href': 'https://bioconductor.org/packages/release/bioc/html/sva.html', '@ext-link-type': 'uri', '#text': 'https://bioconductor.org/packages/release/bioc/html/sva.html'}, '#text': '8. The Combat_Seq function from R package SVA was used to correct the batch effect of samples in each batch: .'}, {'ext-link': {'@xlink:href': 'https://sourceforge.net/projects/rseqc/files/', '@ext-link-type': 'uri', '#text': 'https://sourceforge.net/projects/rseqc/files/'}, '#text': '9. The GeneBodyCoverage.py script from RseQC package (version 4.0.0) was used to evaluate the quality of the reads: .'}]",2022-02-08
0,Scientific Data,41597,10.1038/s41597-022-01205-9,A citizen centred urban network for weather and air quality in Australian schools,30,3,2022,https://github.com/giuliaulpiani/SWAQ,The code used for technical validations is publicly available in the SWAQ repository on Github: .,2022-03-30
0,Scientific Data,41597,10.1038/s41597-021-01034-2,A curated dataset for data-driven turbulence modelling,30,9,2021,,"Both the code used for generating this dataset and input files for the OpenFOAM simulations are available on the Kaggle page for this dataset. The software used was OpenFOAM v2006, with all scripts written in Python 3.",2021-09-30
0,Scientific Data,41597,10.1038/s41597-021-00883-1,Continuous sensorimotor rhythm based brain computer interface learning in a large population,1,4,2021,https://github.com/bfinl/BCI_Data_Paper,The code used to produce the figures in this manuscript is available at .,2021-04-01
0,Scientific Data,41597,10.1038/s41597-021-00927-6,Dataset of low global warming potential refrigerant refrigeration system for fault detection and diagnostics,27,5,2021,,A Python code was developed to process the data set to compare the baseline test results with the faulted test results. The code was stored on figshare and on a shared platform that can be accessed publicly. The data acquisition system used LabVIEW. The data file format was automatically transferred from data loggers to storage on an Oak Ridge National Laboratory local PC with sample time of 1 or 3 s.,2021-05-27
0,Scientific Data,41597,10.1038/s41597-020-00772-z,A gene expression atlas for different kinds of stress in the mouse brain,16,12,2020,https://doi.org/10.6084/m9.figshare.c.4860843,"The bioinformatics analysis pipeline, with a detailed list of command lines, was deposited on the repository Figshare ().",2020-12-16
0,Scientific Data,41597,10.1038/s41597-021-00870-6,"The Amsterdam Open MRI Collection, a set of multimodal MRI datasets for individual difference analyses",19,3,2021,,"[{'ext-link': [{'@xlink:href': 'https://github.com/orgs/NILAB-UvA', '@ext-link-type': 'uri', '#text': 'https://github.com/orgs/NILAB-UvA'}, {'@xlink:href': 'https://github.com/NILAB-UvA/AOMIC-common-scripts', '@ext-link-type': 'uri', '#text': 'https://github.com/NILAB-UvA/AOMIC-common-scripts'}, {'@xlink:href': 'https://joblib.readthedocs.io', '@ext-link-type': 'uri', '#text': 'https://joblib.readthedocs.io'}], 'italic': 'joblib', '#text': 'All code used for curating, annotating, and (pre)processing AOMIC are version-controlled using git and can be found in project-specific Github repositories within the NILAB-UvA Github organization: . Many pre and postprocessing steps were identical across datasets, so the code for these procedures is stored in a single repository: . Possible parameters are all hard-coded within the scripts, except for a single positional parameter pointing to the directory to be processed. For custom Python-based scripts, we used Python version 3.7. All code was developed on a Linux system with 56 CPUs (Intel Xeon E5-2680 v4, 2.40\u2009GHz) and 126GB RAM running Ubuntu 16.04. All curation, preprocessing, and analyses were run on said Linux system, apart from the Fmriprep, MRIQC, and Freesurfer analyses, which were run in a Docker container provided by those software packages. Custom code was parallelized to run on multiple CPUs concurrently using the Python package  ().'}, {'italic': ['bidsify', 'dcm2niix', 'dcm2niix', 'r2aGUI', 'r2aGUI', 'angulation_correction_Achieva', 'pydeface', 'scanphyslog2bids', 'bids-validator'], 'ext-link': [{'@xlink:href': 'https://github.com/NILAB-UvA/bidsify', '@ext-link-type': 'uri', '#text': 'https://github.com/NILAB-UvA/bidsify'}, {'@xlink:href': 'http://r2agui.sourceforge.net/', '@ext-link-type': 'uri', '#text': 'http://r2agui.sourceforge.net/'}, {'@xlink:href': 'https://github.com/NILAB-UvA/ID1000/blob/master/code/bidsify/DTI_gradient_table_ID1000.m', '@ext-link-type': 'uri', '#text': 'https://github.com/NILAB-UvA/ID1000/blob/master/code/bidsify/DTI_gradient_table_ID1000.m'}, {'@xlink:href': 'https://github.com/lukassnoek/scanphyslog2bids', '@ext-link-type': 'uri', '#text': 'https://github.com/lukassnoek/scanphyslog2bids'}], 'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR94', '#text': '94'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR86', '#text': '86'}}], '#text': 'For curation, preprocessing, and analysis of the datasets, we used a combination of existing packages and custom scripts (written in Python or bash). To convert the data to the Brain Imaging Data Structure (BIDS), we used the in-house developed, publicly available software package  (v0.3; ), which in turn uses the  (v1.0.20181125) to convert the Philips PAR/REC files to compressed nifti files. In contrast to the data from PIOP1 and PIOP2 (which were converted to nifti using ),  (v2.7.0; ) was used to convert the data from ID1000. Because  does not correct the gradient table of DWI scans for slice angulation, we used the  Matlab script (version December 29, 2007) from Jonathan Farrell to do so (available for posterity at ). To remove facial characteristics from anatomical scans, we used the  package (v.1.1.0). Finally, to convert the raw physiology files (i.e., Philips “SCANPHYSLOG” files) to BIDS, we used the in-house developed, publicly available Python package  (v0.1; ). The outputs from the BIDS-conversion pipeline were checked using the  software package (v1.4.3).'}, {'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR42', '#text': '42'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR60', '#text': '60'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR95', '#text': '95'}}, {'xref': [{'@ref-type': 'bibr', '@rid': 'CR71', '#text': '71'}, {'@ref-type': 'bibr', '@rid': 'CR72', '#text': '72'}, {'@ref-type': 'bibr', '@rid': 'CR73', '#text': '73'}], '#text': ', –'}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR96', '#text': '96'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR75', '#text': '75'}}], 'ext-link': {'@xlink:href': 'http://www.mrtrix.org', '@ext-link-type': 'uri', '#text': 'www.mrtrix.org'}, '#text': 'Anatomical and functional MRI preprocessing were done using Fmriprep (v1.4.1; see the Derivatives section for extensive information about Fmriprep’s preprocessing pipeline). For our DWI preprocessing pipeline, we used tools from the MRtrix3 package (; v3.0_RC3) and FSL (v6.0.1). For the VBM and dual regression pipelines, we used FSL (v6.0.1). To create the files with Freesurfer-based metrics across all participants, we used Freesurfer version 6.0.0. Physiological nuisance regressors (RETROICOR and HRV/RVT regressors) were estimated using the TAPAS PhysIO Matlab package (v3.2.0).'}, {'italic': ['nistats', 'nilearn', 'seaborn'], 'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR59', '#text': '59'}}, {'xref': [{'@ref-type': 'bibr', '@rid': 'CR59', '#text': '59'}, {'@ref-type': 'bibr', '@rid': 'CR97', '#text': '97'}], '#text': ','}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR92', '#text': '92'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR98', '#text': '98'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR99', '#text': '99'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR100', '#text': '100'}}], 'ext-link': {'@xlink:href': 'http://brainiak.org', '@ext-link-type': 'uri', '#text': 'http://brainiak.org'}, '#text': 'First-level functional MRI analyses for technical validation were implemented using the Python package  (v0.0.1b2) and  (v0.6.2). For the inter-subject correlation analysis the Brain Imaging Analysis Kit was used (BrainIAK, , v0.10; RRID:SCR_014824). Plotting brain images was done using FSLeyes (v0.32) and plotting statistical plots was done using the Python packages  and Matplotlib.'}]",2021-03-19
0,Scientific Data,41597,10.1038/s41597-021-00812-2,"QM7-X, a comprehensive dataset of quantum-mechanical properties spanning the chemical space of small organic molecules",2,2,2021,,The initial structure generation was carried out by using . Further structure optimization and the creation of non-equilibrium structures was performed by utilizing an in-house version of +  together with ASE. Note that all necessary features regarding the utilized DFTB3+MBD approach are available in the current + version. All DFT calculations were carried out using  (version 180218).,2021-02-02
0,Scientific Data,41597,10.1038/s41597-020-00768-9,Expanded dataset of mechanical properties and observed phases of multi-principal element alloys,8,12,2020,https://github.com/CitrineInformatics/MPEA_dataset,"Data processing, validation and statistical plotting were performed using visualization tools on Citrination and Jupyter notebooks in a Python 3 environment. The code is available on GitHub ().",2020-12-08
0,Scientific Data,41597,10.1038/s41597-021-00866-2,"Harmonized and high-quality datasets of aerosol optical depth at a US continental site, 1997–2018",11,3,2021,https://github.com/ARM-Development/qc_aod; https://github.com/ARM-DOE/ADI; https://engineering.arm.gov/ADI_doc/index.html,The code used to generate the combined AOD product (sgpqcaod.c1) is available on Github (). All computations have been performed using the Python environment (v3.6.8) and the ARM Data Integration (ADI) library (). More information about ADI is available at the ADI Documentation website ().,2021-03-11
0,Scientific Data,41597,10.1038/s41597-021-01007-5,"MNI-FTD templates, unbiased average templates of frontotemporal dementia variants",24,8,2021,https://github.com/vfonov/build_average_model; https://github.com/vfonov/nist_mni_pipelines; http://nist.mni.mcgill.ca/?p=2148; https://github.com/philnovv/CNN_NeuroSeg/; https://github.com/NIST-MNI/falcon; https://github.com/vfonov/bic-pipelines,"The scripts for generating unbiased average templates are publicly available at , also re-implemented in Python and publicly available at (see : iplScoopGenerateModel.py as well as examples/synthetic_tests/test_model_creation/scoop_test_nl_sym.py). The scripts for tissue classification tools and FALCON are publicly available at , , and , respectively. To replicate our results or generate a new average template, the user needs to provide preprocessed T1w images to the pipeline. Raw T1w images can be preprocessed using our standard pipeline available at . Afterwards, either build_average_model.rb or scoop_test_nl_sym.py can be used to generate the average template.",2021-08-24
0,Scientific Data,41597,10.1038/s41597-020-00742-5,The landscape of childhood vaccine exemptions in the United States,18,11,2020,https://github.com/bansallab/exemptions-landscape,The code used to produce the figures included in the manuscript as well as the full cleaned and raw datasets are available on Github at . The code runs in Python 3.6.,2020-11-18
0,Scientific Data,41597,10.1038/s41597-021-01008-4,Proteomic profiling dataset of chemical perturbations in multiple biological backgrounds,25,8,2021,https://github.com/cmap/psp; https://github.com/SebVaca/Avant_garde; https://skyline.ms/skyts/home/software/Skyline/tools/details.view?name=AvantGardeDIA,The Proteomics Signature Pipeline (PSP) is available online at .  is available at  and can be downloaded from the Skyline Tool Store directly in the Skyline interface or at .,2021-08-25
0,Scientific Data,41597,10.1038/s41597-021-00849-3,Standardizing human brain parcellations,8,3,2021,https://github.com/neurodata/neuroparc,"[{'ext-link': {'@xlink:href': 'https://github.com/neurodata/neuroparc', '@ext-link-type': 'uri', '#text': 'https://github.com/neurodata/neuroparc'}, '#text': 'Code for processing is publicly available and can be found on GitHub under the scripts folder (). Examples of useful functions include resampling parcellations to a desired voxel size, the ability to register parcellations to any given reference image, and center calculation for regions of interest for 3D parcellations. Jupyter notebook tutorials are also available for learning how to prepare atlases for being added to Neuroparc. All code is provided under the Apache 2.0 License.'}, {'sup': {'xref': [{'@ref-type': 'bibr', '@rid': 'CR50', '#text': '50'}, {'@ref-type': 'bibr', '@rid': 'CR51', '#text': '51'}], '#text': ','}, 'xref': {'@rid': 'Fig1', '@ref-type': 'fig', '#text': '1'}, '#text': 'Visualizations are generated using both MIPAV 8.0.2 and FSLeyes 5.0.10 to view the brain volumes in 2D and 3D spaces. Figure\xa0 can be created using MIPAV triplanar views of each atlases with a striped LUT.'}]",2021-03-08
0,Scientific Data,41597,10.1038/s41597-021-00938-3,"TüEyeQ, a rich IQ test performance data set with eye movement, educational and socio-demographic information",16,6,2021,,"[{'ext-link': {'@xlink:href': 'https://github.com/haugjo/TueEyeQ', '@ext-link-type': 'uri', '#text': 'https://github.com/haugjo/TueEyeQ'}, '#text': 'All Python-code corresponding to the evaluations described in this work is distributed on GitHub under the MIT license . To run the evaluation, the following packages are required (note that older or more recent versions might also work):'}, '• python (v3.7.3)', '• numpy (v1.18.1)', '• pandas (v0.25.1)', '• scikit-learn (v0.21.3)', '• matplotlib (v3.1.3)', '• shap (v0.34.0)', '• lime (v0.2.0.1)', '• dcor (v0.5.2)', '• pickleshare (v0.7.5, only required to load the precomputed Distance Correlation scores).']",2021-06-16
0,Scientific Data,41597,10.1038/s41597-021-01009-3,"A statistics-based reconstruction of high-resolution global terrestrial climate for the last 800,000 years",27,8,2021,https://www.crummy.com/software/BeautifulSoup/,"Model code for the linear regression as well as the code for the analysis and visualisation of figures is publicly available in the project repository. NetCDF files have been processed using . We used the Python language for most of our scripts with a few bash scripts as wrappers. The workflow for the data generation process is managed by . The linear regression is based on the  package. All visualisations are made with  using  for maps. Other Python packages used are (in alphabetical order): ,  (), , , , , , and .",2021-08-27
0,Scientific Data,41597,10.1038/s41597-021-00823-z,Collegiate athlete brain data for white matter mapping and network neuroscience,11,2,2021,https://github.com/bacaron/athlete-brain-study,"Table  below reports the links to each web service and github.com URL implementing the processing pipeline. All code not found on brainlife.io, including visualization code, can be found at .",2021-02-11
0,Scientific Data,41597,10.1038/s41597-021-00970-3,Dataset of cortical activity recorded with high spatial resolution from anesthetized rats,15,7,2021,https://github.com/MouseLand/Kilosort; https://github.com/cortex-lab/phy; https://cellexplorer.org/; https://github.com/petersenpeter/CellExplorer; https://github.com/SpikeInterface/spikemetrics; https://github.com/SpikeInterface/spikeinterface; https://github.com/AllenInstitute/ecephys_spike_sorting/tree/master/ecephys_spike_sorting/modules/quality_metrics; https://github.com/NeurodataWithoutBorders/matnwb,"All software used for the visualization, processing and analysis of this dataset are open access or custom written, Python- and MATLAB-based programs. The Kilosort2 MATLAB package was used for spike sorting (; version 2.0; commit date, 8 April 2019), and the Phy Python module for subsequent manual curation of the data (; version 2.0b1; release date, 7 February 2020). Spatial and temporal features of the single unit spike waveforms were calculated using custom MATLAB scripts or the CellExplorer MATLAB module (; ; version 1.2; commit date, 25 September 2020). Single unit quality metrics were computed using the SpikeMetrics module (; version 0.2.2; commit date, 30 December 2020) of the SpikeInterface Python-based framework (; version 0.11.0; commit date, 10 December 2020). SpikeMetrics relies on the code of the quality metrics module developed at the Allen Institute for Brain Science (). NWB files were created using the MatNWB (; version 2.2.4.0; commit data, 9 February 2021) application programming interface.",2021-07-15
0,Scientific Data,41597,10.1038/s41597-021-00979-8,"MiREDiBase, a manually curated database of validated and putative editing events in microRNAs",4,8,2021,https://github.com/ncRNAome-OSU/miredibase; https://doi.org/10.5281/zenodo.4421966,The whole platform is openly available at  and at Zenodo ().,2021-08-04
0,Scientific Data,41597,10.1038/s41597-021-00810-4,The human -GlcNAcome database and meta-analysis,21,1,2021,https://github.com/glygener/glygen-backend-integration/blob/master/pipeline/integrator/make-proteoform-dataset.py,Source code for the GlyGen QC and integration can be found in the Github repository:,2021-01-21
0,Scientific Data,41597,10.1038/s41597-021-00819-9,A new vector-based global river network dataset accounting for variable drainage density,26,1,2021,https://github.com/peironglinlin/Variable_drainage_density,"The new global vector-based hydrography dataset, consisting of basins, watersheds, and river networks of variable and constant , is produced using Python v3.7.3 and the TauDEM software v5.3.8. All computations are completed using the Della high-performance computing clusters at Princeton University. For geospatial analysis, we use the freely available GeoPandas library in Python; for some figure displaying purposes, we use the ArcPro version 2.4.1. Key Python scripts developed for this work are openly shared with the scientific community at Github: .",2021-01-26
0,Scientific Data,41597,10.1038/s41597-021-00966-z,CO emission accounts of Russia’s constituent entities 2005–2019,13,7,2021,,The Python Code used to draw Fig.  and Fig.  is published at Supplementary File 1 to show how the data can be loaded and visualized.,2021-07-13
0,Scientific Data,41597,10.1038/s41597-021-00864-4,Data-driven curation process for describing the blood glucose management in the intensive care unit,10,3,2021,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR15', '#text': '15'}}, '#text': 'All code used for data extraction, processing, and visualization is available online (two JUPYTER notebooks in Python 3.7) and two MATLAB’s Live Script files) in the PhysionNet project associated with the present paper. These scripts are publicly available to allow for reproducibility and code reuse. The associated PhysioNet project contains 3 folders. The Data folder contains the data subsets described in this paper. The content of the other folders is explained below.'}, {'bold': {'italic': 'Queries folder'}}, 'Contains the queries to obtain the raw insulin entries and blood glucose readings. The queries can be run on Google’s BigQuery. The file glucose_readings.sql contains the code to extract glucose readings, and the file insulin.sql the codes for insulin entries.', {'bold': {'italic': 'Notebooks'}}, 'Contains the following files:', 'JUPYTER notebooks', '1.0-ara-data-curation-I.ipynb: This notebook contains the processing stages to obtain the curated entries of glucose readings and insulin inputs.', '2.0-ara-pairing-II.ipynb: This notebook contains the pairing rules to link a preceding glucose reading with a regular insulin input.', 'MATLAB Live Scripts', 'Glucose_Analysis.mlx: This contains a deeper statistics analysis on the glucose readings. It is a complementary analysis for 1.0-ara-data-curation-I.ipynb notebook.', 'Pairing.mlx: Contains the results related to the pairing of a preceding glucose reading and an insulin event. It is a complementary analysis for the 2.0-ara-pairing-II.ipynb notebook.', 'Glucose_Analysis.html & Pairing.html: Contain the same information as the scripts mentioned above, but readable in a web browser.', 'Functions subfolder:', 'Contains MATLAB functions that are called in the Live Scripts described above.']",2021-03-10
0,Scientific Data,41597,10.1038/s41597-021-00806-0,Synthetic skull bone defects for automatic patient-specific craniofacial implant design,29,1,2021,https://github.com/Jianningli/SciData,"We provide the python scripts to inject artificial defects to the healthy skulls on GitHub (), which can serve as a starting point for future development based on our skull dataset for other researchers. We also provide additional python scripts for the extraction of point clouds from 3D image volumes and Matlab scripts to convert the triangular, surface meshes of the skulls back to voxel grids (voxelization). The dependencies and usage of the scripts are described in our GitHub repository.",2021-01-29
0,Scientific Data,41597,10.1038/s41597-021-01010-w,A high-spatial-resolution dataset of human thermal stress indices over South and East Asia,1,9,2021,,"All codes for calculating the indoor and outdoor UTCI, MRT, and other empirical thermal indices, written in Python (3.8) using cdsapi (0.3.1), numpy (1.19.2), pandas (1.1.3), netCDF4 (1.5.4), and scipy (1.5.3) libraries, were developed on Linux (CentOS 6.10) and can be easily adapted to Windows and other platforms. The codes are freely available at the abovementioned repository.",2021-09-01
0,Scientific Data,41597,10.1038/s41597-020-00749-y,A detailed open access model of the PubMed literature,20,11,2020,https://github.com/vtraag/leidenalg,The Leiden algorithm was used for clustering and is freely available at .,2020-11-20
0,Scientific Data,41597,10.1038/s41597-021-00847-5,Tropical cyclone simulations over Bangladesh at convection permitting 4.4 km & 1.5 km resolution,16,2,2021,,"[{'ext-link': {'@xlink:href': 'https://www.metoffice.gov.uk/research/approach/collaboration/unified-model/partnership', '@ext-link-type': 'uri', '#text': 'https://www.metoffice.gov.uk/research/approach/collaboration/unified-model/partnership'}, '#text': 'The Met Office Unified Model is available for use under licence. Several research organisations and national meteorological services use the UM in collaboration with the Met Office to undertake basic atmospheric process research, produce forecasts, develop the UM code, and build and evaluate Earth system models. For further information on how to apply for a licence, see'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR46', '#text': '46'}}, '#text': 'Python and R code used to process the RAL2 data is available from Zenodo.'}]",2021-02-16
0,Scientific Data,41597,10.1038/s41597-021-00936-5,Database of ab initio L-edge X-ray absorption near edge structure,11,6,2021,,"The workflow for FEFF9 calculation including input generation, output parsing and workflow management is available in open-source materials science packages pymatgen, FireWorks and atomate. The error handler for automatic error detection and recovery can be found in custodian.",2021-06-11
0,Scientific Data,41597,10.1038/s41597-020-00777-8,Multichannel acoustic source and image dataset for the cocktail party effect in hearing aid and implant users,17,12,2020,,The code used to create and process the presented data is provided in or is part of open source repositories.,2020-12-17
0,Scientific Data,41597,10.1038/s41597-021-00964-1,Global soil moisture data derived through machine learning trained with  measurements,12,7,2021,https://github.com/osungmin/SciData2021_SoMo_v1; https://github.com/kratzert/ealstm_regional_modeling,The LSTM model implemented in this study and figure scripts are available from . Note that the LSTM model is built by adopting python modules obtained from .,2021-07-12
0,Scientific Data,41597,10.1038/s41597-020-00764-z,"HuskinDB, a database for skin permeation of xenobiotics",1,12,2020,https://github.com/RhDm/huskinDB_publication,The code which was used to create Figs. – and analyse the data records in huskinDB can be found under the following link:  This repository contains a detailed guide on how to install the requirements and run the code.,2020-12-01
0,Scientific Data,41597,10.1038/s41597-021-00951-6,Scenarios of future Indian electricity demand accounting for space cooling and electric vehicle adoption,15,7,2021,,The code used in the generation of the data sets is open-sourced on Github repository.,2021-07-15
0,Scientific Data,41597,10.1038/s41597-021-01014-6,"Gutenberg Gait Database, a ground reaction force database of level overground walking in healthy individuals",2,9,2021,,"A custom script for tracing and replicating the used processing of the force plate data in Matlab (The MathWorks, Inc., Natick, Massachusetts, United States, 2019a) and custom scripts for importing and merging (with the GaitRec dataset) the data in Matlab (The MathWorks, Inc., Natick, Massachusetts, United States, 2019a) and Python (Python Software Foundation, 3.7) are publicly available at figshare.",2021-09-02
0,Scientific Data,41597,10.1038/s41597-021-00890-2,An integrated landscape of protein expression in human cancer,23,4,2021,https://github.com/J-Andy/Protein-expression-in-human-cancer,The scripts used to generate the final quantification values (and selected intermediate files) are available at: .,2021-04-23
0,Scientific Data,41597,10.1038/s41597-020-00781-y,Multimodal pathophysiological dataset of gradual cerebral ischemia in a cohort of juvenile pigs,7,1,2021,https://ohridal.org/cimva/CIMVA-Core-Description.pdf,EEGLAB has been used which is available as open-source. CIMVA documentation is available online: . No proprietary code has been deployed in this study.,2021-01-07
0,Scientific Data,41597,10.1038/s41597-020-00734-5,Multiscale dynamic human mobility flow dataset in the U.S. during the COVID-19 epidemic,12,11,2020,https://github.com/GeoDS/COVID19USFlows,Data processing and data analysis were performed on a Linux server using the Python version 3.7. All codes used for analysis are available in the public GitHub repository that hosts the data: .,2020-11-12
0,Scientific Data,41597,10.1038/s41597-021-00832-y,"OutFin, a multi-device and multi-modal dataset for outdoor localization based on the fingerprinting approach",24,2,2021,,"Well-documented scripts, written in , are present alongside the dataset (also available on GitHub). These include the scripts used to generate the results described in the Technical Validation section as well as a script to calibrate magnetic field measurements against hard/soft-iron distortions. The data required to replicate the experiments reside in . Depending on the script, some of the following libraries may be required: . Additionally, a thorough description of the collection environment in the form of an interactive map (developed using ) is provided. The map is composed of several layers that display information such as RP coordinates (both ground truth and smartphone estimated), pictures of the collection sites, and building height and ground elevation (as provided by the City and County of Denver). High-resolution aerial imagery (3-inch), provided by the Denver Regional Council of Governments, are used as the basemap.",2021-02-24
0,Scientific Data,41597,10.1038/s41597-021-00921-y,"The IDEAL household energy dataset, electricity, gas, contextual sensor data and survey data for 255 UK homes",28,5,2021,,The lightweight processing that happened on the incoming sensor data is described above. Processing code will be available on request.,2021-05-28
0,Scientific Data,41597,10.1038/s41597-021-00988-7,European primary forest database v2.0,17,8,2021,,"[{'ext-link': {'@xlink:href': '10.6084/m9.figshare.13194095.v1', '@ext-link-type': 'doi', '#text': 'https://doi.org/10.6084/m9.figshare.13194095.v1'}, 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR33', '#text': '33'}}, '#text': 'The code to reproduce the composite layers, for post-processing and for assessing recent human disturbance with remote sensing is available together with the database in Figshare (). We included seven scripts:'}, {'italic': '00_ComposeMap.R', '#text': '•  – Identifies overlapping polygons across individual datasets.'}, {'italic': '01_CreateComposite_Points.py', '#text': '•  – Creates the composite point feature class.'}, {'italic': '02_CreateComposite_Polygons.py', '#text': '•  – Creates the composite polygon feature class.'}, {'italic': '03_PostProcessing.R', '#text': '•  – Extracts additional information on each primary forest.'}, {'italic': '04_Add_Postprocessing.py', '#text': '•  – Imports post-processing output into the geodatabase.'}, {'italic': '05_Summary_stats.R', '#text': '•  – Calculates summary statistics of primary forests'}, {'italic': '06_DisturbanceAssessment_Step1_exportIntermediateChangeImg.txt', '#text': '•  – Runs LandTrendr in Google Earth Engine, tiles the area of interest, creates Change-Images for each tile, and exports these as intermediate .tif files containing the LandTrendr metrics.'}, {'italic': '07_DisturbanceAssessment_Step2_extractPolygonValuesFromChangeImg.txt', '#text': '•  –\xa0Extracts LandTrendr metrics for each forest polygon from Change-Images and exports as\xa0.csv.'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR46', '#text': '46'}}, '#text': 'Python (.py) scripts were run in ESRI ArcGIS (v10.5) and are available also as ArcGIS Models inside the Geodatabase. R (.R) scripts were run using R (v 3.6.1). The remaining\xa0.txt scripts were run in Google Earth Engine.'}]",2021-08-17
0,Scientific Data,41597,10.1038/s41597-021-00886-y,Boston College daily sleep and well-being survey data during early phase of the COVID-19 pandemic,16,4,2021,https://doi.org/10.17605/OSF.IO/GPXWA,"All code for formatting, cleaning, and quality assurance was written in Python (python.org) with use of the NumPy (numpy.org) and Pandas (pandas.pydata.org) libraries. This code is available on the studies OSF page, along with the code used to produce Online-only Tables  and  All code is released under a free and open source license (BSD three-clause): .",2021-04-16
0,Scientific Data,41597,10.1038/s41597-020-00775-w,Genome-scale determination of 5´ and 3´ boundaries of RNA transcripts in  genomes,15,12,2020,,"Read count enriched positions and the RNA-Seq read density across the positions were determined using two source codes in Python (version 3.5.2) programming language, which are publicly available in Figshare.",2020-12-15
0,Scientific Data,41597,10.1038/s41597-021-00962-3,The 2021 update of the EPA’s adverse outcome pathway database,12,7,2021,,"All custom code created to process of manipulate external datasets in the construction or subsequent update of the AOP-DB v.2 relational database tables are made publicly available by the U.S. Environmental Protection Agency, Office of Research and Development (ORD).",2021-07-12
0,Scientific Data,41597,10.1038/s41597-021-00904-z,human whole-brain Connectom diffusion MRI dataset at 760 µm isotropic resolution,29,4,2021,https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FslInstallation; https://surfer.nmr.mgh.harvard.edu/fswiki/DownloadAndInstall,The MATLAB code for the preprocessing steps described above are included in the released dataset. The code make use of the functions in the FSL toolbox and Freesurfer that are freely available for download at  and .,2021-04-29
0,Scientific Data,41597,10.1038/s41597-020-00792-9,A gridded establishment dataset as a proxy for economic activity in China,11,1,2021,https://github.com/quanturban/firm,"The preprocess script, validation dataset and the R code that performs the statistical analysis are available through .",2021-01-11
0,Scientific Data,41597,10.1038/s41597-021-00843-9,A database of high-density surface electromyogram signals comprising 65 isometric hand gestures,18,2,2021,www.otbioelecttronica.com; https://github.com/Neuroengineering-LTH/HDsEMG-database-Associated-codes,"The signal recording was performed using two programs in parallel: OT BioLab version 2.0.6254 available at  for recording HD-sEMG and synchronization signals, and the custom recording software developed in LabVIEW 2016 for force signals recording, generating synchronization pulses, visualizing forces, and generating commands and cues. Data post-processing was done in Matlab and Python. The custom codes for temporal re-labeling and outlier scores are available at the GitHub repository: .",2021-02-18
0,Scientific Data,41597,10.1038/s41597-021-00932-9,"An open tool for creating battery-electric vehicle time series from empirical data,",11,6,2021,https://pypi.org/project/emobpy/; https://gitlab.com/diw-evu/emobpy/emobpy_examples,The tool can be installed from the Python Package Index (PyPI) at . The code is provided under a permissive license in Zenodo. We also provide the script created to generate the 200 BEV profiles for the current case study at .,2021-06-11
0,Scientific Data,41597,10.1038/s41597-021-00830-0,Accelerometer data collected with a minimum set of wearable sensors from subjects with Parkinson’s disease,5,2,2021,,The only data processing procedures that we performed on the dataset were the ones described above. The first procedure was carried out to temporally align the data collected using different sensors. The second procedure was carried out to obtain an evenly-sampled timeseries.,2021-02-05
0,Scientific Data,41597,10.1038/s41597-021-00897-9,Population cluster data to assess the urban-rural split and electrification in Sub-Saharan Africa,23,4,2021,https://github.com/babakkhavari/Clustering,The latest version of the code is available at  (GNU General Public License v3.0). The code is Python-based and runs in Jupyter Notebook. The code repository includes instructions for how to install and run the algorithm as well as a country example displaying the necessary inputs and expected outputs. The datasets published with this paper were ran using Python 3.6 and the packages listed in the full_project.yml file uploaded to the repository.,2021-04-23
0,Scientific Data,41597,10.1038/s41597-021-00839-5,"The Zoltar forecast archive, a tool to standardize and store interdisciplinary prediction research",11,2,2021,https://github.com/reichlab/forecast-repository/; http://reichlab.io/zoltr/; https://github.com/reichlab/zoltpy,"All code for the Zoltar system is available under a GPL-3.0 license from . Zoltr and zoltpy packages are available under GPL-3.0 licenses from  and , respectively.",2021-02-11
0,Scientific Data,41597,10.1038/s41597-021-00986-9,Quantum chemical calculations of lithium-ion battery electrolyte and interphase species,5,8,2021,,"[{'monospace': ['pymatgen', 'custodian', 'atomate'], 'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR63', '#text': '63'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR76', '#text': '76'}}], 'xref': {'@rid': 'Fig8', '@ref-type': 'fig', '#text': '8a'}, '#text': 'Our computational infrastructure for high-throughput and automated DFT calculations using the Q-Chem electronic structure code is implemented in existing open-source Python packages developed by the Materials Project, namely , , and . The modules in these codes used specifically for Q-Chem, along with their purposes, are described in Fig.\xa0.'}, {'monospace': 'pymatgen', 'italic': 'ω', '#text': 'The basic functionality to generate, process, analyze, and manipulate molecules is included in . We have added functionality to read and write Q-Chem input files and to parse Q-Chem output files. In addition, we have developed a number of “Sets”, pre-defined collections of input parameters appropriate for common types of calculations. While these sets can be used with any level of theory available in Q-Chem, it is especially facile to use the advanced level of theory used for the LIBE dataset (B97X-V/def2-TZVPPD/SMD).'}, {'monospace': ['custodian', 'atomate', 'custodian'], '#text': 'The  Q-Chem module defines the interface between Q-Chem and our automation framework in . It can execute arbitrary Q-Chem jobs and can automatically check for, detect, and correct errors in Q-Chem calculations.  also handles the logic for FFOpt calculations.'}, {'monospace': ['atomate', 'pymatgen', 'custodian', 'custodian', 'atomate', 'SinglePointFW'], 'xref': {'@rid': 'Fig8', '@ref-type': 'fig', '#text': '8b'}, '#text': 'The Q-Chem module in  combines the Q-Chem input and output modules in  and the Q-Chem interface and error handlers in  to perform Q-Chem jobs and analyze their data in a high-throughput fashion. An example calculation, or Firework, for a single-point optimization is shown schematically in Fig.\xa0. First, based on some input parameters, a Q-Chem input file for a geometry optimization calculation is written. Then, the optimization job is run, with  waiting for completion and, upon completion, checking for errors. If the job completes without errors, then the output is parsed and stored in a database. Individual Q-Chem calculations, represented in  by Fireworks like , can be combined to form more complex workflows.'}, {'monospace': ['pymatgen', 'custodian', 'atomate', 'deliberate'], 'ext-link': [{'@xlink:href': 'http://github.com/materialsproject/pymatgen', '@ext-link-type': 'uri', '#text': 'http://github.com/materialsproject/pymatgen'}, {'@xlink:href': 'http://github.com/materialsproject/custodian', '@ext-link-type': 'uri', '#text': 'http://github.com/materialsproject/custodian'}, {'@xlink:href': 'http://github.com/hackingmaterials/atomate', '@ext-link-type': 'uri', '#text': 'http://github.com/hackingmaterials/atomate'}, {'@xlink:href': 'http://github.com/espottesmith/deliberate', '@ext-link-type': 'uri', '#text': 'http://github.com/espottesmith/deliberate'}], '#text': 'Other than Q-Chem itself, all the necessary code used to generate and analyze the LIBE dataset (: ; : ; : ; and : ) can be found on Github.'}]",2021-08-05
0,Scientific Data,41597,10.1038/s41597-021-00915-w,DLBCL-Morph: Morphological features computed using deep learning for an annotated digital DLBCL image set,20,5,2021,https://github.com/stanfordmlgroup/DLBCL-Morph,"The code to compute all geometric features from all tumor nuclei in our dataset, along with notebooks to illustrate usage of our data and reproduce all survival regression results, is publicly available at .",2021-05-20
0,Scientific Data,41597,10.1038/s41597-020-00760-3,"Data-science ready, multisite, human diffusion MRI white-matter-tract statistics",30,11,2020,,"['All the code used in the generation of this dataset is in the public domain, and it has been linked in the corresponding sub-section in this manuscript. Here we provide a summary of the linked repositories:', {'ext-link': {'@xlink:href': 'https://github.com/garikoitz/paper-scidata', '@ext-link-type': 'uri', '#text': 'https://github.com/garikoitz/paper-scidata'}, '#text': 'Main repository for this publication: .'}, {'ext-link': {'@xlink:href': 'https://github.com/garikoitz/paper-reproducibility', '@ext-link-type': 'uri', '#text': 'https://github.com/garikoitz/paper-reproducibility'}, '#text': 'Useful examples on data handling: .'}, {'ext-link': {'@xlink:href': 'https://github.com/vistalab/RTP-preproc', '@ext-link-type': 'uri', '#text': 'https://github.com/vistalab/RTP-preproc'}, '#text': 'Code to create the preprocessing container: .'}, {'ext-link': {'@xlink:href': 'https://github.com/vistalab/RTP-pipeline', '@ext-link-type': 'uri', '#text': 'https://github.com/vistalab/RTP-pipeline'}, '#text': 'Code to create the tracking container: .'}, {'ext-link': {'@xlink:href': 'https://github.com/vistalab/scitran/', '@ext-link-type': 'uri', '#text': 'https://github.com/vistalab/scitran/'}, '#text': 'Code to systematically access data and create tables:'}, 'The Docker containers with the algorithms and the configuration parameters can be run in a computer with Linux, macos (tested) or Windows (not tested). The Docker containers can be downloaded and run both in Docker and Singularity for free.']",2020-11-30
0,Scientific Data,41597,10.1038/s41597-020-00769-8,"An open-source, end-to-end workflow for multidimensional photoemission spectroscopy",17,12,2020,https://github.com/momentoscope/hextof-processor; https://github.com/mpes-kit/mpes; https://gitlab.mpcdf.mpg.de/rpx/parser-mpes,"The code, including documentation and examples in Jupyter notebooks for implementing the data transformations in the workflow, is available as  () and  (). The parser for integrating preprocessed experimental data into the NOMAD database is available as  ().",2020-12-17
0,Scientific Data,41597,10.1038/s41597-020-00756-z,A completely annotated whole slide image dataset of canine breast cancer to aid human breast cancer research,27,11,2020,https://github.com/DeepPathology/MITOS_WSI_CMC/,All code used in the experiments described in the manuscript was written in Python 3 and is available through our GitHub repository (). We provide all necessary libraries as well as Jupyter Notebooks allowing tracing of our results. The code is based on fast.ai and OpenSlide and provides some custom data loaders for use of the dataset.,2020-11-27
0,Scientific Data,41597,10.1038/s41597-021-00895-x,"ODFM, an omics data resource from microorganisms associated with fermented foods",20,4,2021,https://github.com/yang4851/gdkm,The code used to build the systemic architecture of the GDKM is available on GitHub: .,2021-04-20
0,Scientific Data,41597,10.1038/s41597-021-01021-7,Geolocated dataset of Chinese overseas development finance,20,9,2021,https://doi.org/10.17605/OSF.IO/GFWHJ,"Two sets of code are available in conjunction with the resulting data, at . News aggregation code is available in Python. Geolocation code (querying Google Maps and Open Street Maps APIs) is freely available, in R, upon completion of the data use agreement, which is also available in the same repository.",2021-09-20
0,Scientific Data,41597,10.1038/s41597-020-00739-0,"A harmonised, high-coverage, open dataset of solar photovoltaic installations in the UK",13,11,2020,,"Source code for producing our dataset is freely available online. Requirements are Python 3.7 or later, as well as PostgreSQL with PostGIS extensions (we used PostgreSQL 10.12).",2020-11-13
0,Scientific Data,41597,10.1038/s41597-021-00882-2,Heidelberg colorectal data set for surgical data science in the sensor operating room,12,4,2021,https://phabricator.mitk.org/source/rmis2019/; https://github.com/wiesenfa/challengeR,"The data set can be used without any further code. As stated in the usage notes, we recommend using the scripts provided for the ROBUST-MIS and surgical workflow challenges ( and) as well as the challengeR package () for comparative benchmarking of algorithms.",2021-04-12
0,Scientific Data,41597,10.1038/s41597-021-00926-7,Aachen-Heerlen annotated steel microstructure dataset,26,5,2021,,"All codes that were used in the preparation and the analysis of this dataset are made available along with the dataset and in our code repository. Codes are written in Python language. Specifically, two Jupyter Notebooks were provided; one that contains the code that describes the data tables and calculates the morphological feature, and another for contour detection and evaluation of the segmentation model. The code includes all custom methods, references to common libraries, and a full set of stepwise instructions to replicate the calculations in this study.",2021-05-26
0,Scientific Data,41597,10.1038/s41597-021-00824-y,Computational scanning tunneling microscope image database,11,2,2021,https://github.com/usnistgov/jarvis,Python-language-based codes with examples are given at the JARVIS-Tools page .,2021-02-11
0,Scientific Data,41597,10.1038/s41597-021-01051-1,"Addendum: High-resolution terrestrial climate, bioclimate and vegetation for the last 120,000 years",29,9,2021,,,2021-09-29
0,Scientific Data,41597,10.1038/s41597-021-00900-3,"COVID-CT-MD, COVID-19 computed tomography scan dataset applicable in machine learning and deep learning",29,4,2021,,"The Python code used to generate the statistical analysis and plots is shared within the same Figshare link, with the name “Statistical_Analysis.py”.",2021-04-29
0,Scientific Data,41597,10.1038/s41597-021-00967-y,A DICOM dataset for evaluation of medical image de-identification,16,7,2021,https://pypi.org/project/Faker; https://code.imphub.org/projects/PT/repos/oneposda; https://imagemagick.org/index.php,"Synthetic Protected Health Information (PHI) was generated using the Faker software package () and inserted into selected DICOM Attributes using an extended version of the Posda tool suite (), the open source package used for curation and de-identification by TCIA. Posda incorporated the open source software package ImageMagick () to insert multiple lines of text into Pixel Data.",2021-07-16
0,Scientific Data,41597,10.1038/s41597-020-00767-w,"CREAM, a component level coffeemaker electrical activity measurement dataset",17,12,2020,,"The source files for the data collection using the MEDAL measurement units are available in the BLOND data repository. For completeness sake, we have also added these files to the CREAM repository in the  folder. This repository contains all the scripts used for the technical validation of the measurement hardware capabilities. The code to reproduce the extraction of the product and maintenance events through the serial maintenance ports of the coffeemakers is available in the coffeemaker project repository provided by Q42. We implemented the data processing, labelling tools, and utility functions in Python 3. The labelling tools were implemented in three Jupyter Notebooks, one corresponding to each step of the labelling pipeline. The individual source files are available in the CREAM repository. All labelling steps can be fully reproduced and extended if necessary, using the supplied tools. Furthermore, we provide the utility class containing all necessary functions for loading and pre-processing the signals.",2020-12-17
0,Scientific Data,41597,10.1038/s41597-021-00941-8,"Open-access quantitative MRI data of the spinal cord and reproducibility across participants, sites and manufacturers",16,8,2021,https://github.com/spine-generic/spine-generic/releases/tag/v2.6; https://spine-generic.rtfd.io/; https://spinalcordtoolbox.com; https://plotly.com; https://brainsprite.github.io/; https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FSLeyes,"Data were processed using Python and shell scripts contained in the spine-generic package (), which is distributed under the MIT license. A comprehensive procedure is described in the “Analysis pipeline” section of the spine generic website (). This procedure includes the list of dependent software packages to install, a step-by-step analysis procedure with a list of commands to run, a procedure for quality control and for manual correction of intermediate outputs (e.g. cord segmentation and vertebral labeling). The procedure includes embedded video tutorials and has been tested by external users. The analysis documentation also includes a section on how to generate the static figures that are shown in this article (in PNG format) as well as the interactive figures embedded in the spine-generic website. Notable software used in this study include: the Spinal Cord Toolbox v5.0.1 () to analyse the MRI data, pandas to perform statistics, plotly v4.12.0 () to display the interactive plots, brainsprite v0.13.3 () for embedding in the online documentation an interactive visualization of example datasets, pybids for checking the acquisition parameters on the BIDS datasets, FSLeyes v0.34.0 () for manually-correcting the segmentations.",2021-08-16
0,Scientific Data,41597,10.1038/s41597-021-01024-4,"ValLAI_Crop, a validation dataset for coarse-resolution satellite LAI products over Chinese cropland",20,9,2021,https://github.com/BowenSong123/Code,"In the data repository, the readme files explain the location of the files and folders. All raw measurements records can be found in one Excel sheet. All the field data and satellite images were processed and analysed in IDL and Python. The source codes are available at the Github. .",2021-09-20
0,Scientific Data,41597,10.1038/s41597-021-00893-z,"LoDoPaB-CT, a benchmark dataset for low-dose computed tomography reconstruction",16,4,2021,https://github.com/jleuschn/lodopab_tech_ref,"Python scripts for the simulation setup and the creation of the dataset are publicly available on Github (). They make use of the ASTRA Toolbox (version 1.8.3) and the Operator Discretization Library (ODL, version ≥0.7.0). In addition, the ground truth reconstructions from the LIDC/IDRI database are needed for the simulation process. A sample data split into training, validation, test and challenge part is also provided. It differs from the one used for the creation of this dataset in order to keep the ground truth data of the challenge set undisclosed. The random seeds used in the scripts are modified for the same reason. The authors acknowledge the National Cancer Institute and the Foundation for the National Institutes of Health, and their critical role in the creation of the free publicly available LIDC/IDRI database used in this study.",2021-04-16
0,Scientific Data,41597,10.1038/s41597-020-00724-7,A comprehensive spectral assay library to quantify the  proteome by DIA/SWATH-MS,12,11,2020,,,2020-11-12
0,Scientific Data,41597,10.1038/s41597-021-00822-0,Building schematic of Vienna in the late 1920s,3,2,2021,https://rstudio.com/; https://www.r-project.org/,"With respect to the technical validation section, we provide the code to reproduce the Figs. –. The code was created with R Studio () based on R Project for Statistical Computing (). The code file “Technical_validation.Rmd” as well as the corresponding input data are available on Zenodo.",2021-02-03
0,Scientific Data,41597,10.1038/s41597-021-01025-3,"Directional wave buoy data measured near Campbell Island, New Zealand",15,9,2021,,No custom code was used to generate or process the data described in this manuscript.,2021-09-15
0,Scientific Data,41597,10.1038/s41597-021-00965-0,"A multi-site, year-round turbulence microstructure atlas for the deep perialpine Lake Garda",22,7,2021,,"The scripts for shear spectrum integration and maximum likelihood estimation (MLE) fitting are available together with the dataset (except for some functions of the ODAS libraries released by Rockland Scientific International Inc., to which we refer the interested reader).",2021-07-22
0,Scientific Data,41597,10.1038/s41597-021-00863-5,Creation and validation of a chest X-ray dataset with eye-tracking and report dictation for AI development,25,3,2021,,"[{'ext-link': {'@xlink:href': 'https://github.com/cxr-eye-gaze/eye-gaze-dataset', '@ext-link-type': 'uri', '#text': 'https://github.com/cxr-eye-gaze/eye-gaze-dataset'}, '#text': 'Our Github repository () contains code (Python 3) for:'}, '1.Data Preparation', 'Inclusion and exclusion criteria on MIMIC dataset (see details in Inclusion and exclusion criteria section).', 'Case sampling and image preparation for eye gaze experiment (see details in Preparation of images section).', '2.Data Post -Processing', 'Speech-to-text on dictation audio (see details in Audio extraction and transcript generation section).', 'Mapping of eye gaze coordinates to original image coordinates (see details in Fixations and eye gaze spreadsheets section).', 'Generate heatmap images (i.e temporal or static) and videos given eye gaze coordinates. The temporal and static heatmap images were used in our demonstrations of machine learning methods in Use of the Dataset in Machine Learning section.', '3.Technical Validation', 'Validation of eye gaze fixation quality using calibration images (see details in Validation of eye gaze data).', 'Validation of quality in transcribed dictations (see details in Validation of transcripts section).', 'The t-test for eye gaze fixations for each anatomical structure and condition pairs (see details in Statistical analysis on fixations section)', '4.Machine Learning Experiments, as described in Use of the Dataset in machine learning section.', {'ext-link': {'@xlink:href': 'https://github.com/cxr-eye-gaze/eye-gaze-dataset', '@ext-link-type': 'uri', '#text': 'https://github.com/cxr-eye-gaze/eye-gaze-dataset'}, '#text': 'Software requirements are listed in .'}]",2021-03-25
0,Scientific Data,41597,10.1038/s41597-021-00907-w,Time series of useful energy consumption patterns for energy system modeling,31,5,2021,https://github.com/FCN-ESE/JERICHO-E-usage,"The code for compiling the time series of useful energy consumption and energy service profiles is published at  under the open MIT license. Detailed instructions for using the code are included in the repository. All code is implemented in Python. For easy use of the scripts, we have added a Jupyter Notebook with further instructions on the workflow. The required input data, comprising pre-calculated data and data from official reports, are included with references.",2021-05-31
0,Scientific Data,41597,10.1038/s41597-021-00850-w,Global 1-km present and future hourly anthropogenic heat flux,22,2,2021,https://urbanclimate.tse.ens.titech.ac.jp/,The codes for constructing the dataset were written in Python language and can be downloaded from  or may be requested directly from the corresponding author. The inputs used to construct the present and future AHE datasets are all publicly available online with sources cited within this manuscript. Specific pre-processed inputs may be requested from the corresponding author upon request.,2021-02-22
0,Scientific Data,41597,10.1038/s41597-020-00795-6,The traded water footprint of global energy from 2010 to 2018,11,1,2021,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR22', '#text': '22'}}, 'ext-link': {'@xlink:href': '10.5281/zenodo.3891722', '@ext-link-type': 'doi', '#text': 'https://doi.org/10.5281/zenodo.3891722'}, '#text': 'All code necessary for generating the overall virtual water trade network is published in Zenodo, . Most of the coding was completed using the R scripting language and open source packages, with one script written using Python to access the IEA API. See Methods for a description and breakdown of use for each script file.'}, 'Versions and Packages:', '• R-base: 4.0.0', '• comtradr (R): 0.2.2', '• Python: 3.7.4', '• pandas (Python): 0.25.3', '• numpy (Python): 1.18.1', '• requests (Python): 2.22.0', '• json (Python): 2.0.9']",2021-01-11
0,Scientific Data,41597,10.1038/s41597-021-00891-1,"Daily PM concentration estimates by county, ZIP code, and census tract in 11 western states 2008–2018",19,4,2021,,"[{'ext-link': {'@xlink:href': '10.5281/zenodo.4499264', '@ext-link-type': 'doi', '#text': 'https://doi.org/10.5281/zenodo.4499264'}, 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR55', '#text': '55'}}, '#text': 'All code used for downloading and processing the data used in this project, including the machine learning and technical validation code, may be accessed at .'}, 'To ensure that our work is reproducible, all code is written in open-source languages. Some scripts are in R and others are in Python. Python scripts need Python 3; R versions beyond 3.5.1 should suffice.', 'The scripts on Zenodo are a copy of our GitHub repository of scripts and contains the following files and directories:', {'bold': 'General_Project_Functions', '#text': '• : Scripts to obtain the prediction set locations and tools that are generally useful during the data processing, such as making buffers around points and reprojecting point coordinates.'}, {'bold': 'Get_PM25_Observations', 'sub': '2.5', '#text': '• : Scripts to process PM observations from across the western U.S. These observations are used to train our machine learning models.'}, {'bold': 'Get_Earth_Observations', 'italic': ['Overall_steps', 'README'], '#text': '• : Scripts to download and process observations from data sets that are used both as inputs for our machine learning models during training and as inputs for our models in the prediction stage. The file  provides all necessary directions. Individual  files (in each folder) provide more details if there are any.'}, {'bold': 'Merge_Data', '#text': '• : Scripts to merge all the data together and derive some spatio-temporal variables.'}, {'bold': 'Machine_Learning', '#text': '• : Scripts to run and evaluate our machine learning models. The folder Final_scripts contains all code used for our final analysis. The code in the Exploring_models folder was all preliminary testing.'}, {'bold': 'Estimate_PM25', '#text': '• : Scripts to use our machine learning models to make final predictions and to explore the prediction data sets over time and space.'}]",2021-04-19
0,Scientific Data,41597,10.1038/s41597-020-00782-x,"TreeMap, a tree-level model of conterminous US forests circa 2014 produced by imputation of FIA plot data",15,1,2021,,"['Code was written in R and Python for the purposes of this project. The arcpy module was run in the Python version that accompanies ArcGIS 10.5.1, which is the Python IDLE 2.7.13.', {'ext-link': {'@xlink:href': 'https://github.com/USDAForestService/TreeMap2014_scripts', '@ext-link-type': 'uri', '#text': 'https://github.com/USDAForestService/TreeMap2014_scripts'}, '#text': 'Code is available at . The code:'}, '1)\u2009\u2009\u2009\u200a\u2009\u2009\u200aprepared the target data rasters (script names: “reclass_Landfire_disturbance_rasters_for_tree_list.py” and “write_EVG_remap_files.r”)', '2)\u2009\u2009\u2009\u200a\u200a\u200aperformed the random forests imputation, using the R package yaimpute (script name: “yai-parallel_v02202019-final-Yes-disturbance_z1.r”)', '3)\u2009\u200a\u200a\u200a\u200avalidated the output grid of imputed plot IDs by comparing it to FIA plots measured in 2014 and to the target rasters (script names: “national_validation_plots_Landfire.py” and “national_validation_plots_Landfire_step2.r”)', '4)\u200a\u200a\u200a\u200a\u200a\u200a\u2009\u200a\u200acompared the imputed raster to the target rasters as a measure of imputation accuracy (script name: “analyze_national_tree_list_output.r”)']",2021-01-15
0,Scientific Data,41597,10.1038/s41597-020-00735-4,"An fMRI dataset in response to “The Grand Budapest Hotel”, a socially-rich, naturalistic movie",11,11,2020,https://github.com/mvdoc/budapest-fmri-data,"All code is available in the github repository. The code includes scripts to process the stimuli, presentation scripts, and scripts for the analyses presented in this paper. The scripts rely heavily on open source Python packages such as PyMVPA, nilearn, pycortex, scipy, and numpy.",2020-11-11
0,Scientific Data,41597,10.1038/s41597-021-00833-x,Quantum chemical benchmark databases of gold-standard dimer interaction energies,10,2,2021,,,2021-02-10
0,Scientific Data,41597,10.1038/s41597-021-00980-1,A curated dataset of modern and ancient high-coverage shotgun human genomes,4,8,2021,,"[{'xref': [{'@rid': 'Tab3', '@ref-type': 'table', '#text': '3'}, {'@rid': 'Tab3', '@ref-type': 'table', '#text': '3'}], '#text': 'All newly generated sequencing raw reads (see Table\xa0) have been deposited in the NCBI Sequence Read Archive (SRR12854172, SRR12854173, SRR12854174, SRR12854175). Six compressed fastq files per sample were uploaded. The fastq files have the same names as the libraries described in Table\xa0.'}, {'ext-link': {'@xlink:href': 'https://github.com/EvolEcolGroup/data_paper_genetic_pipeline', '@ext-link-type': 'uri', '#text': 'https://github.com/EvolEcolGroup/data_paper_genetic_pipeline'}, '#text': 'The genetic pipeline used to process the data is available at .'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR30', '#text': '30'}}, '#text': 'The filtered compressed vcf file used for the analyses has been uploaded to figshare with the title “A curated dataset of modern and ancient high-coverage shotgun human genomes”.'}]",2021-08-04
0,Scientific Data,41597,10.1038/s41597-020-00780-z,Obstacles to the reuse of study metadata in ClinicalTrials.gov,18,12,2020,https://doi.org/10.6084/m9.figshare.12743939,"The data used and generated throughout the study described in this paper are available in Figshare at . Stanford University’s PRS test environment was used for qualitative analysis of the PRS system. This environment is not publicly accessible, but our results can be verified in any PRS environment to which the reader has access.",2020-12-18
0,Scientific Data,41597,10.1038/s41597-021-00798-x,A database framework for rapid screening of structure-function relationships in PFAS chemistry,18,1,2021,http://madeatub.buffalo.edu,"The code supporting the finding of this study have been deposited at figshare. All code needed for the user interface of PFAS, as well as the repeating of data pre-processing are included in “PFAS-Map” folder. The folder also contains a detailed PDF instruction and a demonstration video for the installation and use of the PFAS-Map. The code is also available on the Materials Data Engineering Laboratory - MaDE@UB portal ().",2021-01-18
0,Scientific Data,41597,10.1038/s41597-021-00989-6,A synthetic building operation dataset,10,8,2021,https://lbnl-eta.github.io/AlphaBuilding-SyntheticDataset,"A step-by-step guidance and the source-code to generate this dataset, and a notebook to explore and visualize the data can be found at the dataset’s GitHub page ().",2021-08-10
0,Scientific Data,41597,10.1038/s41597-021-00820-2,A compilation of North American tree provenance trials and relevant historical climate data for seven species,26,1,2021,https://github.com/clara-risk/tree-provenance-trials,We used various Python scripts to process the data for input into the databases. These scripts were used to calculate climate value summaries and convert phenology observations to a uniform reference date (January 1). The scripts are available at .,2021-01-26
0,Scientific Data,41597,10.1038/s41597-021-00976-x,A multispeaker dataset of raw and reconstructed speech production real-time MRI video and 3D volumetric images,20,7,2021,https://github.com/usc-mrel/usc_speech_mri.git,"This dataset is accompanied by a code repository () that contains examples of software and parameter configurations necessary to load and reconstruct the raw RT-MRI in MRD format. Specifically, the repository contains demonstrations to illustrate and replicate results of Figs. –. Code samples are available in MATLAB and Python programming languages. All software is provided free to use and modify under the MIT license agreement.",2021-07-20
0,Scientific Data,41597,10.1038/s41597-021-00963-2,"DEDDIAG, a domestic electricity demand dataset of individual appliances in Germany",15,7,2021,https://DEDDIAG.github.io; https://github.com/DEDDIAG/DEDDIAG-loader.git,"The full data collection system is published under MIT license and is available under . The dataset itself is published as tab-separated text files together with code to import all data into a PostgreSQL instance. An SQL function called  is provided to get seconds-based measurements, where readings are converted from value-changes to seconds-based readings using interpolation; timestamps are rounded to nearest seconds. There is also a python package available  that assists in retrieving data into a pandas-DataFrame/numpy-array.",2021-07-15
0,Scientific Data,41597,10.1038/s41597-021-00950-7,COVID-19 European regional tracker,15,7,2021,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR1', '#text': '1'}}, 'ext-link': {'@xlink:href': 'https://github.com/asjadnaqvi/COVID19-European-Regional-Tracker', '@ext-link-type': 'uri', '#text': 'https://github.com/asjadnaqvi/COVID19-European-Regional-Tracker'}, '#text': 'The latest code to process the files can be downloaded from the Zenodo release or accessed from the GitHub repository (). The code is compatible with Stata versions 16 or higher\xa0 which are also recommended due to\xa0improved functionality with maps and graphs. General data processing can be done in any version.'}, {'bold': '/02 dofiles/', '#text': 'As specified in the Data Records section, the dofiles are in thefolder. Within this folder dofiles exist for each country plus a set of five dofiles that setup, merge, map, and validate the final data file. These files are explained as follows:'}, 'COUNTRY_SETUP.do initializes the code for running the country files. One can run each country file independently as well, but they need the directory structure and packages to be loaded in order to function correctly. Directory and packages can be initialized using the first few lines marked in the beginning of the COUNTRY_SETUP.do file. This syntax is as follows:', {'monospace': 'clear'}, {'monospace': 'global coviddir “<your directory path>/<your directory name>”'}, {'monospace': 'package for maps and correcting map projections', '#text': '*'}, {'monospace': 'ssc install spmap, replace'}, {'monospace': 'ssc install geo2xy, replace'}, {'monospace': 'packages for color schemes', '#text': '*'}, {'monospace': 'ssc install palettes, replace'}, {'monospace': 'ssc install colrspace, replace'}, {'monospace': 'package for replicating plots', '#text': '*'}, {'monospace': 'net install tsg_schemes, ///'}, {'monospace': ['from(“', '”'], 'ext-link': {'@xlink:href': 'https://raw.githubusercontent.com/asjadnaqvi/Stata-schemes/main/schemes/', '@ext-link-type': 'uri', '#text': 'https://raw.githubusercontent.com/asjadnaqvi/Stata-schemes/main/schemes/'}, '#text': ')'}, {'monospace': 'set scheme white_w3d, perm'}, {'monospace': 'set the detault graph font', '#text': '*'}, {'monospace': 'graph set window fontface “Arial Narrow”'}, 'Each country .do file is annotated with notes where necessary.', {'bold': ['04_master', '04_master'], '#text': 'COUNTRY_MERGE.do combines all the country datasets saved in  in one file EUROPE_COVID19 master.dta. The master file is also saved in the  folder.'}, 'COUNTRY_GIS_setup do sets up the GIS layers in Stata format for the combined NUTS regions and for individual countries. A mixed NUTS3 and NUTS2 shapefile is also created accommodate the data from Poland and Greece. The logic can also be applied to add data at the provincial (NUTS 1) or country (NUTS 0) level if one needs to add other countries not in the dataset. This file also extracts shapefiles for individual countries and generates a file used for labeling the individual country maps.', {'bold': '05_figures', 'xref': {'@rid': 'Fig5', '@ref-type': 'fig', '#text': '5'}, 'ext-link': {'@xlink:href': 'https://github.com/asjadnaqvi/COVID19-European-Regional-Tracker', '@ext-link-type': 'uri', '#text': 'https://github.com/asjadnaqvi/COVID19-European-Regional-Tracker'}, '#text': 'COUNTRY_GIS_map do create the maps that are saved in the  folder. See Fig.\xa0 for the overall map. Individual country COVID-19 maps can be viewed on GitHub ().'}, {'xref': {'@rid': 'Fig7', '@ref-type': 'fig', '#text': '7'}, '#text': 'COUNTRY_validation do collapses the Tracker to a country-date level and merges it with OWID COVID-19 dataset, for validation. This file also produces Fig.\xa0.'}, {'ext-link': {'@xlink:href': '10.5281/zenodo.4244878', '@ext-link-type': 'doi', '#text': 'https://doi.org/10.5281/zenodo.4244878'}, 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR1', '#text': '1'}}, '#text': 'GitHub and Zenodo files are updated every four weeks. Each Zenodo release is assigned a unique DOI, but the generic , will always link to the latest version.'}]",2021-07-15
0,Scientific Data,41597,10.1038/s41597-020-00750-5,16 years of topographic surveys of rip-channelled high-energy meso-macrotidal sandy beach,20,11,2020,,"All data files created and used in processing are formatted in the Network Common Data Form (NetCDF), providing detailed metadata for each variable within the file, and can be read using MATLAB, Python, Fortran, C, C++, Java, and other languages. A Code file used to interpolate the raw sand elevation data is included in the repository folder. Code is written in MATLAB (R2019a) and is fully commented. Although MATLAB is a proprietary language, the.m files can be read with a text viewer.",2020-11-20
0,Scientific Data,41597,10.1038/s41597-021-01030-6,"Introduction to a community dataset from an infrasound array experiment at Mt. Etna, Italy",23,9,2021,,"A Python notebook is provided in the supplementary material section of this manuscript, which demonstrates data retrieval from IRIS DMC. The notebook also demonstrates how to process data to obtain the results shown in Fig. . The user will require a working installation of the ObsPy package to execute the notebook.",2021-09-23
0,Scientific Data,41597,10.1038/s41597-021-00946-3,An automatic multi-tissue human fetal brain segmentation benchmark using the Fetal Tissue Annotation Dataset,6,7,2021,https://doi.org/10.7303/syn23747212,The code used during the development of the dataset can be found on Synapse under Fetal Tissue Annotation Challenge FeTA Dataset (website: ).,2021-07-06
0,Scientific Data,41597,10.1038/s41597-021-00991-y,OPERA tau neutrino charged current interactions,12,8,2021,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR76', '#text': '76'}}, '#text': 'The code to make the display of a neutrino event is provided as Auxiliary files. In the example shown, the event 9190097972 is used, but the code can be adapted to draw your own display of any neutrino candidate downloaded from the Open Data Repository. The code (Visualization.ipynb) is written as a Jupyter Notebook.'}, {'ext-link': {'@xlink:href': 'https://jupyter.org/install.html', '@ext-link-type': 'uri', '#text': 'https://jupyter.org/install.html'}, '#text': 'The installation of Python and Jupyter using the Anaconda Distribution is recommended. Anaconda Distribution includes Python, the Jupyter Notebook, and other commonly used packages for scientific computing and data science. More details can be found at: .'}, {'ext-link': {'@xlink:href': 'https://tinyurl.com/binder-OPERA', '@ext-link-type': 'uri', '#text': 'https://tinyurl.com/binder-OPERA'}, '#text': 'Among the auxiliary files, the one called visualization archive (Visualization.zip) has all the necessary files to run the display. Data folder contains input files, which have been downloaded from the Open Data Repository. Python script (opera_tools.py) provides auxiliary functions that were used in the Notebook. Running Visualization.ipynb requires dedicated libraries to be installed, as reported in the file requirements.txt. There is also a possibility to access the code via binder interactive environment ().'}]",2021-08-12
0,Scientific Data,41597,10.1038/s41597-021-00831-z,Limb and trunk accelerometer data collected with wearable sensors from subjects with Parkinson’s disease,5,2,2021,,The only data processing procedures that we performed on the dataset were the ones described above. The first procedure was carried out to temporally align the data collected using different sensors. The second procedure was carried out to obtain an evenly-sampled timeseries.,2021-02-05
0,Scientific Data,41597,10.1038/s41597-021-01031-5,"COVID Border Accountability Project, a hand-coded global database of border closures introduced during 2020",29,9,2021,https://github.com/COBAPteam/COBAP,"Codes for the database, raw data outputs and data visualizations appearing on our website are available on our project GitHub (). The full text of the survey used to code each policy is available in the .",2021-09-29
0,Scientific Data,41597,10.1038/s41597-021-00885-z,Database of Wannier tight-binding Hamiltonians using high-throughput density functional theory,13,4,2021,https://github.com/usnistgov/jarvis.,Python-language based scripts for obtaining and analyzing the dataset are available at,2021-04-13
0,Scientific Data,41597,10.1038/s41597-021-00929-4,A deep database of medical abbreviations and acronyms for natural language processing,2,6,2021,https://zenodo.org/record/4266962; https://bit.ly/github-clinical-abbreviations,We used the Python programming language for all activities. The entire code is permanently available in Zenodo () or GitHub ().,2021-06-02
0,Scientific Data,41597,10.1038/s41597-021-00827-9,The 10-m crop type maps in Northeast China during 2017–2019,2,2,2021,,JavaScript code used to generate the cropland layer and crop type maps are available from the figshare repository.,2021-02-02
0,Scientific Data,41597,10.1038/s41597-021-00974-z,"OPTIMADE, an API for exchanging materials data",12,8,2021,https://github.com/Materials-Consortia,All associated code is hosted under the Materials-Consortia organisation on GitHub ().,2021-08-12
0,Scientific Data,41597,10.1038/s41597-021-00916-9,"A 120,000-year long climate record from a NW-Greenland deep ice core at ultra-high resolution",26,5,2021,https://github.com/vgkinis/neem_isotope_data_descriptor_code,"The Python code used for the transfer, organising of the data, estimation of the precision and accuracy metrics as well as the plots included in this manuscript can be found in . In the repository, we also provide auxiliary code with basic routines for post-processing of the PANGAEA data file.",2021-05-26
0,Scientific Data,41597,10.1038/s41597-021-01003-9,Hourly potential evapotranspiration at 0.1° resolution for the global land surface from 1981-present,24,8,2021,https://github.com/Dagmawi-TA/hPET; https://doi.org/10.5523/bris.qb8ujazzda0s2aykkv0oq0ctp,"The codes we developed for computing hPET and dPET are available at . In addition, we are providing users with a simple Python script to enable easy access to specific parts of the complete hPET and dPET datasets, based on user needs. This latter script allows the user to specify a geographical box and a selection of years for which data are required. The code then accesses the raw data files and downloads the data for relevant temporal and spatial domains from an open-access data server (). Since the data are freely available, one can download the script and run it on a local machine to download all or a portion of the data. The scripts are all documented, and readme files are provided from the relevant repository.",2021-08-24
0,Scientific Data,41597,10.1038/s41597-020-00761-2,Mining potentially actionable kinase gene fusions in cancer cell lines with the KuNG FU database,30,11,2020,https://doi.org/10.5281/zenodo.3996125; https://github.com/BadSeby/KuNG-FU-Database,The Python scripts used for KuNG Fu database curation are available at Zenodo (). The source code is available at .,2020-11-30
0,Scientific Data,41597,10.1038/s41597-021-01004-8,"A multi-site, multi-disorder resting-state magnetic resonance image database",30,8,2021,https://github.com/bicr-resource/deface,The face-masking code is available on our GitHub project. ().,2021-08-30
0,Scientific Data,41597,10.1038/s41597-021-01033-3,The “Narratives” fMRI dataset for evaluating models of naturalistic language comprehension,28,9,2021,https://github.com/snastase/narratives; https://datasets.datalad.org/?dir=/labs/hasson/narratives,All code used for aggregating and analyzing the data is version-controlled and publicly available via the associated GitHub repository () and the  directory in the top-level BIDS directory distributed via DataLad (). The GitHub repository contains both scripts used to prepare the data for sharing () and scripts used to analyze the BIDS-formatted data (). See Table  for a brief description of the scripts used to process the “Narratives” data.,2021-09-28
0,Scientific Data,41597,10.1038/s41597-021-00896-w,Generation of a mouse SWATH-MS spectral library to quantify 10148 proteins involved in cell reprogramming,26,4,2021,https://github.com/M-Russell/Mouse_iPSC_Spectral_Library,"The workflow for spectral library generation was scripted using a gnu-make. The make file and a companion document are included with the data in the pride repository, the make file companion document is also included with this article as supplementary file 1. The make file and the companion document are available on github (). These files should enable precise replication of the library from raw data as presented here and re-use of the raw data through varied processing. The library was created with a series of open source software packages, the precise versions and sources of these programs are given in the documentation. Python scripts are required for the pipeline and instructions are given on how to install the versions used.",2021-04-26
0,Scientific Data,41597,10.1038/s41597-020-0434-6,A synthetic energy dataset for non-intrusive load monitoring in households,2,4,2020,,"[{'ext-link': [{'@xlink:href': 'https://creativecommons.org/licenses/by/4.0/', '@ext-link-type': 'uri', '#text': 'https://creativecommons.org/licenses/by/4.0/'}, {'@xlink:href': 'https://github.com/klemenjak/synd/', '@ext-link-type': 'uri', '#text': 'https://github.com/klemenjak/synd/'}], '#text': 'We selected Python 3 as main programming language and identify the following dependencies of SynD: Pandas 0.22, Numpy 1.15, and NILMTK 0.3. We aimed at providing compatibility to the latest versions of these software packages and released code examples, an extensive user guide, and supplemental material under the licence Attribution 4.0 International () on our GitHub repository ().'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR22', '#text': '22'}}, 'ext-link': {'@xlink:href': 'https://creativecommons.org/publicdomain/zero/1.0/', '@ext-link-type': 'uri', '#text': 'https://creativecommons.org/publicdomain/zero/1.0/'}, '#text': 'Along with the dataset SynD, we release the first public version of our dataset generator tool via figshare. This tool was used to create SynD and can also serve to generate new datasets on-demand. We release this early version of our tool under the licence CC0 ().'}]",2020-04-02
0,Scientific Data,41597,10.1038/s41597-020-00581-4,Cultivar-specific transcriptome and pan-transcriptome reconstruction of tetraploid potato,24,7,2020,fairdomhub.org/projects/161; github.com/NIB-SI/pISA; github.com/NIB-SI/pisar,"All used Bash, Perl, Python, and R/Markdown custom code and scripts complemented with intermediate and processed data (input and output files), and all other supporting information that enable reproduction and re-use are available at FAIRDOMHub under project name _p_stRT () under CC BY 4.0 licenses. Data were locally stored in a ISA-tab compliant project directory tree generated using pISA-tree () and uploaded to FAIRDOMHub repository using FAIRDOMHub API and R package pisar ().",2020-07-24
0,Scientific Data,41597,10.1038/s41597-020-00670-4,"Individual Brain Charting dataset extension, second release of high-resolution fMRI data for cognitive mapping",16,10,2020,,"[{'ext-link': {'@xlink:href': 'https://github.com/hbp-brain-charting/public_protocols', '@ext-link-type': 'uri', '#text': 'https://github.com/hbp-brain-charting/public_protocols'}, 'italic': ['(1)', '(2)', '(3)', '(4)'], '#text': 'Metadata concerning the stimuli presented during the BOLD fMRI runs are publicly available at . They include:  the task-stimuli protocols;  demo presentations of the tasks as video annotations;  instructions to the participants; and  scripts to extract paradigm descriptors from log files for the GLM estimation.'}, 'Task-stimuli protocols from Preference, TOM and VSTM\u2009+\u2009Enumeration batteries were adapted from the original studies in order to comply with the IBC experimental settings, without affecting the design of the original paradigms. MTT battery pertains to an original protocol developed in Python under the context of the IBC project. Protocols of Self and Bang tasks were re-written from scratch in Python with no change of the design referring to the original paradigms.', 'The scripts used for data analysis are available on GitHub under the Simplified BSD license:', {'ext-link': {'@xlink:href': 'https://github.com/hbp-brain-charting/public_analysis_code', '@ext-link-type': 'uri', '#text': 'https://github.com/hbp-brain-charting/public_analysis_code'}, 'xref': {'@rid': 'Sec31', '@ref-type': 'sec', '#text': 'Derived statistical maps'}, '#text': '. Additionally, a full description of all contrasts featuring data derivatives (see Section “” for details) as well as a list of the main contrasts are also provided under the folder hbp-brain-charting/public_analysis_code/ibc_data.'}]",2020-10-16
0,Scientific Data,41597,10.1038/s41597-020-0386-x,"A 12-lead electrocardiogram database for arrhythmia research covering more than 10,000 patients",12,2,2020,https://github.com/zheng120/ECGConverter; https://www.mathworks.com/; https://github.com/zheng120/ECGDenoisingTool,"The source code of the converter tool that transfers ECG data files from XML format to CSV format can be found at , which contains binary executable files, source code, and a user manual. Both the MATLAB () and Python version programs for ECG noise reduction are available at .",2020-02-12
0,Scientific Data,41597,10.1038/s41597-020-00666-0,"Genome assembly and annotation of , an emerging parthenogenetic root-knot nematode",5,10,2020,,,2020-10-05
0,Scientific Data,41597,10.1038/s41597-020-0564-x,Laboratory-scale hydraulic fracturing dataset for benchmarking of enhanced geothermal system simulation tools,8,7,2020,,"Python scripts for basic processing of seismic data and aggregate visualization of both acoustic events and pressure profiles are published in the data repository. Requirements for scripts execution are Python 2.7 and the additional modules  and . The code in the repository relies on methods widely available in literature, and is therefore provided “as-is”, with no other warranties, expressed or implied, of correctness and completeness. It is not to be considered as a software for professional data elaboration but rather as an example in support of the research community to develop specific codes by providing a starting template. Users are encouraged to view the code and make necessary changes as required.",2020-07-08
0,Scientific Data,41597,10.1038/s41597-020-0462-2,High resolution temporal profiles in the Emissions Database for Global Atmospheric Research,17,4,2020,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR62', '#text': '62'}}, '#text': 'Most of the temporal profiles data processing has been done using the software R version 3.5 and Python version 3.6. The computation of heating degree days maps was based on the 2\u2009m air temperature of ECMWF ERA5 re-analysis and produced by using IDL8.6 programming software. Further computations, such as mapping sectors and countries have been performed using Microsoft Access 2010.'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR20', '#text': '20'}}, '#text': 'The implementation of the EDGAR_temporal_profiles_r1 library into the Emissions Database for Global Atmospheric Research has been developed using a dedicated EDGAR development tool of the Joint Research Centre named EOLO based on Php and Microsoft SQL Server. This system cannot be accessed outside the institution but further details can be provided upon request. All scripts related with this work are available at figshare.'}]",2020-04-17
0,Scientific Data,41597,10.1038/s41597-020-00608-w,An annotated fluorescence image dataset for training nuclear segmentation methods,11,8,2020,https://github.com/perlfloccri/NuclearSegmentationPipeline,"We provide code to transform predicted annotation masks in TIFF-format to SVG-files for curation by experts as well as the transformation from SVG-files to TIFF-files. The contour sampling rate when transforming mask objects to SVG-descriptions can be set in accordance to the size of predicted nuclei. Therefore, new nuclei image annotation datasets can easily be created utilizing the proposed framework and a tool to modify SVG-objects, such as Adobe Illustrator. The code is written in python and is publicly available under .",2020-08-11
0,Scientific Data,41597,10.1038/s41597-020-0547-y,Probabilistic identification of saccharide moieties in biomolecules and their protein complexes,3,7,2020,http://ctpic.nmrfam.wisc.edu; https://github.com/htdashti/ctpic,"The cheminformatic tool for probabilistic identification of carbohydrate (CTPIC) program was developed using Python and is available on our website () as a web server. In addition, the source codes are available through GitHub ().",2020-07-03
0,Scientific Data,41597,10.1038/s41597-020-0445-3,A global database of Holocene paleotemperature records,14,4,2020,https://github.com/nickmckay/LiPD-utilities; https://github.com/nickmckay/Temperature12k,"Code for working with the LiPD data files, including basic functionality in three programming languages, is available on GitHub (). MATLAB code used to map site locations (Fig. ) and to compute composites (Figs. –) is available at  under an MIT license.",2020-04-14
0,Scientific Data,41597,10.1038/s41597-020-0534-3,The FLUXNET2015 dataset and the ONEFlux processing pipeline for eddy covariance data,9,7,2020,https://github.com/AmeriFlux/ONEFlux,"The ONEFlux collection of codes used to create data intercomparable with FLUXNET2015 has been packaged to be executed as a complete pipeline and is available in both source-code and executable forms under a 3-clause BSD license on GitHub: . The complete environment to run this pipeline requires a GCC compatible C compiler (or capability to run pre-compiled Windows, Linux, and/or Mac executables), a MATLAB Runtime Environment, and a Python interpreter with a few numeric and scientific packages installed. All of these can be obtained at no cost.",2020-07-09
0,Scientific Data,41597,10.1038/s41597-020-0397-7,"Tesco Grocery 1.0, a large-scale dataset of grocery purchases in London",18,2,2020,,We provide the code we used to validate our data in the form of a python script.,2020-02-18
0,Scientific Data,41597,10.1038/s41597-020-00725-6,An annotated dataset of bioacoustic sensing and features of mosquitoes,11,11,2020,,"[{'xref': {'@rid': 'Sec6', '@ref-type': 'sec', '#text': '1'}, '#text': 'Box\xa0 describes the algorithm to segment the audio (WAV file) into snippets of 300\u2009ms. Supervised segmentation is a critical process for most of the audio analysis applications, its purpose being to split an audio stream into homogeneous segments.'}, {'xref': {'@rid': 'Sec7', '@ref-type': 'sec', '#text': '2'}, 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR22', '#text': '22'}}, '#text': 'We used the pyaudioanalysis library to generate and extract 34 features, represented in Box\xa0. This is an open python library that provides audio related functionalities such as: audio features, visualization, classification and segmentation.'}, {'xref': {'@rid': 'Sec7', '@ref-type': 'sec', '#text': '2'}, 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR24', '#text': '24'}}, '#text': 'Box\xa0 shows the function to create, extract and manipulate the 34 features. Both algorithms can be found in the Github repository.'}, {'italic': 'dirWavFeatureExtraction()', '#text': 'We used the library audio feature extraction from pyaudioanalysis using the method  to extract the short-term feature sequences of WAV files (one audio signal per specimen), using a sliding window with a time overlap of 50% (frame size of 300\u2009ms and frame step of 150\u2009ms). The resulting 34-element feature vector for each audio file is extracted by mid-term averaging the short-term features.'}]",2020-11-11
0,Scientific Data,41597,10.1038/s41597-020-0521-8,"Nappe oscillations on free-overfall structures, data from laboratory experiments",17,6,2020,,The characterization techniques were developed on Matlab 2017b. Futher details on these methods are given by Lodomez.,2020-06-17
0,Scientific Data,41597,10.1038/s41597-020-00588-x,"Quantum chemical calculations for over 200,000 organic radical species and 40,000 associated closed-shell molecules",21,7,2020,https://github.com/pstjohn/bde,"Code used to perform the high-throughput calculations are available at . The code relies on cclib and RDKit to process molecular information in Python, Gaussian to perform the DFT calculation, and pandas for data processing. Some of the code relating to the PostgreSQL database and NREL’s HPC infrastructure is site-specific and will likely need to altered to run these types of calculations on alternative HPC systems.",2020-07-21
0,Scientific Data,41597,10.1038/s41597-020-00712-x,"The Building Data Genome Project 2, energy meter data from the ASHRAE Great Energy Predictor III competition",27,10,2020,https://github.com/buds-lab/building-data-genome-project-2,The Building Data Genome 2 data set and the custom code used for its creation and analysis is hosted in a public Github repository () and its v1.0 release has been deposited in Zenodo. This codebase includes several Jupyter notebooks with Python and R data analysis workflows that can be easily reproduced.,2020-10-27
0,Scientific Data,41597,10.1038/s41597-020-00575-2,"COVID-19 outbreak response, a dataset to assess mobility changes in Italy following national lockdown",8,7,2020,,All data records were generated using code developed in Python 3. The code is available upon request from the corresponding author.,2020-07-08
0,Scientific Data,41597,10.1038/s41597-020-0473-z,"The ANI-1ccx and ANI-1x data sets, coupled-cluster and density functional theory properties for molecules",1,5,2020,https://github.com/aiqm/torchani,"All electronic structure calculations were computed with the Gaussian 09 [cite] or ORCA electronic structure packages [cite]. All molecular dynamics simulations for sampling were carried out with the atomic simulation environment (ASE). The analysis and active learning scripts are available upon request. The C++/CUDA implementation of our ANI code is available online in binary format [ref], but source code is not publicly released. Alternatively a PyTorch version ANI is available as open source. [].",2020-05-01
0,Scientific Data,41597,10.1038/s41597-020-00619-7,A fine resolution dataset of accessibility under different traffic conditions in European cities,25,8,2020,,The Python scripts for the generation of accessibility indicators are available upon request.,2020-08-25
0,Scientific Data,41597,10.1038/s41597-020-0415-9,A NWB-based dataset and processing pipeline of human single-neuron activity during a declarative memory task,4,3,2020,https://github.com/rutishauserlab/recogmem-release-NWB,"All code associated with this project is available as open source. The code is available on GitHub under the BSD license (). Both Python and MATLAB scripts are included in this repository along with the matNWB API. We also provide a streamlined workflow as a Jupyter Notebook. Note, we tested our code with the following versions of the Python Packages: numpy (1.17.2), pandas (0.23.0), scipy (1.1.0), matplotlib (2.2.2), pynwb (1.1.0), hdmf (1.2.0), and seaborn (0.9.0). Detailed instructions on installing and running the code in this repository are found in our online documentation on GitHub.",2020-03-04
0,Scientific Data,41597,10.1038/s41597-020-0460-4,"Reactants, products, and transition states of elementary chemical reactions based on quantum chemistry",8,5,2020,,The code used to generate the data is freely available on GitHub under the MIT license. Further details on how to use it to generate the data are given in the Usage Notes.,2020-05-08
0,Scientific Data,41597,10.1038/s41597-020-00692-y,A three-dimensional thalamocortical dataset for characterizing brain heterogeneity,20,10,2020,https://github.com/nerdslab/xray-thc,"Code for downloading the data and annotations in bossDB can be found in the ‘data_access_notebooks’ folder here: . A Jupyter notebook for generating the results in Figs. ,  can be found in the ‘analysis_notebooks’ folder in the same repo. Annotations, images, and analysis notebooks used for the inter-rater reliability study, are also provided through figshare to facilitate reproducibility. All of these examples are written in Python 3 and executed using Jupyter notebooks, a cross platform Python solution.",2020-10-20
0,Scientific Data,41597,10.1038/s41597-020-0532-5,Multicenter intracranial EEG dataset for classification of graphoelements and artifactual signals,16,6,2020,,,2020-06-16
0,Scientific Data,41597,10.1038/s41597-020-00621-z,High-density EEG of auditory steady-state responses during stimulation of basal forebrain parvalbumin neurons,8,9,2020,,All the Python scripts used in the  section for analysis and figure generation are available online. Python scripts for simulation are also available in G-Node repository ().,2020-09-08
0,Scientific Data,41597,10.1038/s41597-020-00688-8,A cross-country database of COVID-19 testing,8,10,2020,https://github.com/owid/covid-19-data/tree/master/scripts/scripts/testing,"Code used for the creation of this database is not included in the files uploaded to figshare. Our scripts for data collection, processing, and transformation, are available for inspection in the public GitHub repository that hosts our data ().",2020-10-08
0,Scientific Data,41597,10.1038/s41597-020-00719-4,Paired rRNA-depleted and polyA-selected RNA sequencing data and supporting multi-omics data from human T cells,9,11,2020,https://github.com/LuChenLab/40Tcells,The codes used in this article were deposited in .,2020-11-09
0,Scientific Data,41597,10.1038/s41597-020-00617-9,910 metagenome-assembled genomes from the phytobiomes of three urban-farmed leafy Asian greens,25,8,2020,,Custom scripts were not used to generate or process this dataset. Software versions and non-default parameters used have been appropriately specified where required.,2020-08-25
0,Scientific Data,41597,10.1038/s41597-020-0413-y,Longitudinal spatial dataset on travel times and distances by different travel modes in Helsinki Region,4,3,2020,,"['All the tools and source codes for the accessibility tools used for producing these datasets are openly available from the following links:', 'MetropAccess-Reititin: helsinkimatrix.github.io/reititin', 'DORA: helsinkimatrix.github.io/dora', 'In addition, the full analytical workflow that was used to produce these matrices are published openly that makes it possible for anyone to assess the process pipeline:', 'helsinkimatrix.github.io']",2020-03-04
0,Scientific Data,41597,10.1038/s41597-020-0502-y,X-ray nano-tomography of complete scales from the ultra-white beetles Lepidiota stigma and Cyphochilus,29,5,2020,https://software.pan-data.eu/software/74/pyhst2,"The ESRF High Speed Tomography in Python (PyHST2) software which was used to reconstruct the phase images is open source and can be found at: . The current pipeline for processing the raw data prior to its use in the PyHST2 algorithm is a large collection of scripts in MATLAB, Python and GNU Octave which makes it difficult to bundle into a single tomography pipeline. However, the ESRF is currently working to convert all of the scripts to Python to create a completely open source pipeline, though additional computing power, such a high performance computing cluster will likely be necessary. All additional image processing was done using open source Python libraries; these have been noted at the appropriate stages in the text.",2020-05-29
0,Scientific Data,41597,10.1038/s41597-020-0569-5,A dataset describing data discovery and reuse practices in research,13,7,2020,,"All R scripts used in data preparation and technical validation, along with the un-prepared data, are available upon request from the corresponding author. Examples of how to load the data and how to change factor/category columns to character columns in R (Box ) and Python (Box ) are provided. Additionally, the code used to create Fig.  in R (Box ) is listed as an example of how to combine data from both data files into a single plot.",2020-07-13
0,Scientific Data,41597,10.1038/s41597-020-0467-x,"Simultaneous human intracerebral stimulation and HD-EEG, ground-truth for source localization methods",28,4,2020,https://github.com/iTCf/mikulan_et_al_2020,"Usage demonstration scripts and the code used for the preparation, pre-processing and technical validation of the Localize-MI dataset are publicly available at .",2020-04-28
0,Scientific Data,41597,10.1038/s41597-020-0556-x,Chromatin accessibility and transcriptome landscapes of  brain,8,7,2020,,"['Data processing was performed using open source software. The approach of tools and parameters used were as below.', {'ext-link': {'@xlink:href': 'https://github.com/BGI-flexlab/SOAPnuke', '@ext-link-type': 'uri', '#text': 'https://github.com/BGI-flexlab/SOAPnuke'}, '#text': 'SOAPnuke: . Version: 1.5.2. Parameters: filter -A 0.5 -M 2 -l 10 -q 0.3 -n 0.05 -Q 2 -d.'}, {'ext-link': {'@xlink:href': 'https://cutadapt.readthedocs.io/en/stable/', '@ext-link-type': 'uri', '#text': 'https://cutadapt.readthedocs.io/en/stable/'}, '#text': 'Cutadapt: . Version: 1.16. Parameters: -m 5 -e 0.10.'}, {'ext-link': {'@xlink:href': 'http://www.ccb.jhu.edu/software/hisat', '@ext-link-type': 'uri', '#text': 'http://www.ccb.jhu.edu/software/hisat'}, '#text': 'HISAT2: . Version 2.0.1-beta. Parameters: -p 4 –phred33 –sensitive –no-discordant –no-mixed -I 1 -X 1000.'}, {'ext-link': {'@xlink:href': 'http://subread.sourceforge.net/', '@ext-link-type': 'uri', '#text': 'http://subread.sourceforge.net/'}, '#text': 'featureCounts: . Version 1.5.3. Parameters: -T 5 -p -t exon -g gene_id.'}, {'ext-link': {'@xlink:href': 'https://github.com/taoliu/MACS', '@ext-link-type': 'uri', '#text': 'https://github.com/taoliu/MACS'}, '#text': 'MACS2: . Version 2.1.2. Parameters: macs2 callpeak -t input.bam -f BAM -g 259040147 -n name.output -B -q 0.01 --nomodel.'}, {'ext-link': {'@xlink:href': 'https://bedtools.readthedocs.io/en/latest/content/tools/intersect.html', '@ext-link-type': 'uri', '#text': 'https://bedtools.readthedocs.io/en/latest/content/tools/intersect.html'}, '#text': 'Bedtools: . Version: 2.26.0. Parameters: bedtools intersect -a standardpeak.bed -b input.bam -c\u2009>\u2009output.count.'}, 'The R code used for calculating the correlation and comparative analysis are available in the supplementary materials.']",2020-07-08
0,Scientific Data,41597,10.1038/s41597-020-00597-w,Printable 3D vocal tract shapes from MRI data and their acoustic and aerodynamic properties,5,8,2020,,"All the software used in this study are open source (see the previous sections for references or URLs). In addition, the dataset contains several custom-made Matlab and Python scripts (see section “Data Records”).",2020-08-05
0,Scientific Data,41597,10.1038/s41597-020-0495-6,"PTB-XL, a large publicly available electrocardiography dataset",25,5,2020,,The code for dataset preparation is not intended to be released as it does not entail any potential for reusability. We provide the stratified sampling routine in Supplementary File  to allow users to create stratification folds based on user-defined preferences.,2020-05-25
0,Scientific Data,41597,10.1038/s41597-020-0526-3,"BAGLS, a multihospital Benchmark for Automatic Glottis Segmentation",19,6,2020,http://www.hno-klinik.uk-erlangen.de/phoniatrie/forschung/computational-medicine/gat-software/; https://github.com/anki-xyz/pipra; https://github.com/anki-xyz/bagls,"We provide the Glottis Analysis Tools software on request (). The Pixel-Precise Annotator tool (PiPrA) is available open source online (). We provide a Jupyter notebook for training, evaluating and using the deep neural network as used in the Usage Notes section online under an open source license ().",2020-06-19
0,Scientific Data,41597,10.1038/s41597-020-0389-7,A voltage and current measurement dataset for plug load appliance identification in households,12,2,2020,,"The complete PLAID dataset and all mentioned scripts are available in. In the same repository, code written to capture the data can be found. The files are two scripts, namely ‘collecting_data.vi’ (written with LabVIEW) and ‘collecting_data.m’ (written in MATLAB).",2020-02-12
0,Scientific Data,41597,10.1038/s41597-019-0346-5,Global karst springs hydrograph dataset for research and management of the world’s fastest-flowing groundwater,20,2,2020,https://github.com/KarstHub/WoKaS,"The R code to download datasets directly from the hydrological databases and to combine them with the spring discharge time series obtained from the other sources (see above) is available at . The code is provided in R programming language version 3.5.0, and commented following a recommended programming comment guidelines. Comprehensive instructions on how to run the code and system requirements are provided by a “README” file included in the GitHub repository.",2020-02-20
0,Scientific Data,41597,10.1038/s41597-020-0411-0,"Very high resolution, altitude-corrected, TMPA-based monthly satellite precipitation product over the CONUS",3,3,2020,https://github.com/JVFayne/HRAC-Precip_v1,"R programming language and Matlab scripts used to produce (Eq. ) and validate (Eq. ) this data as well as the Monte Carlo coefficient analysis are publicly available with a public access license through GitHub: . Due to the simplicity of the correction formula, the scripts can be easily translated to other programming languages; the free to use open source packages ‘raster’, ‘rgdal’, and ‘rgeos’ are required to use the R scripts, although the code functions of these packages that are used in the scripts (such as reading and writing geospatial files) do not change over the course of version updates, and many other programming languages such as Matlab and Python use similar packages to read and write raster files. Additional software packages are not required to produce these data.",2020-03-03
0,Scientific Data,41597,10.1038/s41597-020-0478-7,"Estimating nitrogen and phosphorus concentrations in streams and rivers, within a machine learning framework",28,5,2020,,"['We used the following open source software packages to compute the full processing chain:', {'sup': {'xref': [{'@ref-type': 'bibr', '@rid': 'CR51', '#text': '51'}, {'@ref-type': 'bibr', '@rid': 'CR52', '#text': '52'}], '#text': ','}, '#text': '● Geospatial Data Abstraction Library (GDAL, version number 2.1.2).'}, {'sup': {'xref': [{'@ref-type': 'bibr', '@rid': 'CR33', '#text': '33'}, {'@ref-type': 'bibr', '@rid': 'CR53', '#text': '53'}, {'@ref-type': 'bibr', '@rid': 'CR54', '#text': '54'}], '#text': ',,'}, '#text': '● Geographic Resources Analysis Support System software (GRASS, version number 7.4.0).'}, {'sup': {'xref': [{'@ref-type': 'bibr', '@rid': 'CR55', '#text': '55'}, {'@ref-type': 'bibr', '@rid': 'CR56', '#text': '56'}], '#text': ','}, '#text': '● Processing Kernel for geospatial data (PKTOOLS, version number 2.6.3).'}, {'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR57', '#text': '57'}}, {'xref': [{'@ref-type': 'bibr', '@rid': 'CR34', '#text': '34'}, {'@ref-type': 'bibr', '@rid': 'CR35', '#text': '35'}], '#text': ','}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR58', '#text': '58'}}, {'xref': [{'@ref-type': 'bibr', '@rid': 'CR59', '#text': '59'}, {'@ref-type': 'bibr', '@rid': 'CR60', '#text': '60'}], '#text': ','}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR61', '#text': '61'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR62', '#text': '62'}}, {'xref': [{'@ref-type': 'bibr', '@rid': 'CR63', '#text': '63'}, {'@ref-type': 'bibr', '@rid': 'CR64', '#text': '64'}], '#text': ','}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR65', '#text': '65'}}, {'xref': [{'@ref-type': 'bibr', '@rid': 'CR66', '#text': '66'}, {'@ref-type': 'bibr', '@rid': 'CR67', '#text': '67'}], '#text': ','}], '#text': '● R: a language and environment for statistical computing, with the following libraries: randomForestSRC, geoR, plyr, moments, data.table, reshape, dplyr, ggplot2'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR68', '#text': '68'}}, '#text': 'All of these tools provide fast and scalable functions for raster-based workflows that are easily automated using a scripting language, such as Bash or Python. They also allow for the processing of very large geo-datasets owing to efficient algorithms and optimised memory management.'}, {'ext-link': {'@xlink:href': 'https://gitlab.com/Ferdinand18/np_us_streams', '@ext-link-type': 'uri', '#text': 'https://gitlab.com/Ferdinand18/np_us_streams'}, '#text': 'In the spirit of reproducible research we provide the scripting procedure at the GitLab repository (). The full procedure, starting from the N and P observations treatment to the 30-arc-second raster predictions, is provided below.'}, '● 01_Cleaning.sh: cleaning the raw observation data.', '● 02_Snapping.sh: snapping the observation data points onto the gridded stream network.', '● 03_Extraction.sh: extracting descriptors corresponding to the snapped points.', '● 04_Modelling.sh: building predictive models based on the observation data.', '● 05_Prediction.sh: making predictions for all the US streams and building gridded GeoTiff maps as the final output.']",2020-05-28
0,Scientific Data,41597,10.1038/s41597-020-00602-2,A database of battery materials auto-generated using ChemDataExtractor,6,8,2020,https://github.com/ShuHuang/batterydatabase; https://github.com/ShuHuang/batterydatabase/tree/master/chemdataextractor_batteries; https://github.com/ShuHuang/batterygui,The source code used to generate the database is available at . The code of ChemDataExtractor 1.5 that has been modified for database auto-generation in the battery domain is available at . The GUI application source code can be found at .,2020-08-06
0,Scientific Data,41597,10.1038/s41597-020-0567-7,"GlobalFungi, a global database of fungal occurrences from high-throughput-sequencing metabarcoding studies",13,7,2020,https://github.com/VetrovskyTomas/GlobalFungi,The workflow included several custom made python scripts (labelled by star in the Fig. ) which are accessible here: .,2020-07-13
0,Scientific Data,41597,10.1038/s41597-020-0407-9,An automatically curated first-principles database of ferroelectrics,3,3,2020,,"[{'ext-link': [{'@xlink:href': 'http://www.cryst.ehu.es', '@ext-link-type': 'uri', '#text': 'http://www.cryst.ehu.es'}, {'@xlink:href': 'https://pypi.org/', '@ext-link-type': 'uri', '#text': 'https://pypi.org/'}], 'sup': {'xref': [{'@ref-type': 'bibr', '@rid': 'CR80', '#text': '80'}, {'@ref-type': 'bibr', '@rid': 'CR82', '#text': '82'}, {'@ref-type': 'bibr', '@rid': 'CR83', '#text': '83'}, {'@ref-type': 'bibr', '@rid': 'CR84', '#text': '84'}, {'@ref-type': 'bibr', '@rid': 'CR85', '#text': '85'}, {'@ref-type': 'bibr', '@rid': 'CR86', '#text': '86'}], '#text': ',, , , –'}, '#text': 'VASP version 5.3.5 used to perform DFT calculations is a proprietary code. The Bilbao Crystallographic Server (BCS) is freely available on-line at . Fireworks, atomate, and pymatgen are python packages accessible on GitHub. Fireworks and atomate are released under a modified Berkeley Software Distribution (BSD) License. pymatgen is released under a Massachusetts Institute of Technology (MIT) License. Both MIT and BSD licenses are open-source and permit both commercial and non-commercial use. Our workflow code is included since atomate version 0.6.7 and our analysis code is available in pymatgen since v2019.2.4. We also use the following python packages in our analysis and Graphical Representation of Results: numpy, scipy, matplotlib, ipython, and jupyter. These packages are freely available through the Python Package Index ().'}, {'monospace': ['pymatgen.analysis.ferroelectricity', 'atomate.vasp.workflows.base.ferroelectric'], 'ext-link': [{'@xlink:href': 'http://blondegeek.github.io/ferroelectric_search_site/', '@ext-link-type': 'uri', '#text': 'http://blondegeek.github.io/ferroelectric_search_site/'}, {'@xlink:href': 'http://github.com/blondegeek/ferroelectric_search_site', '@ext-link-type': 'uri', '#text': 'http://github.com/blondegeek/ferroelectric_search_site'}], '#text': 'Our code for recovering the same branch polarization from polarization calculations has been contributed to pymatgen under the  module. Our code for the DFT and polarization analysis workflows for performing polarization calculations has been contributed to atomate under the  module. We also provide code for the interface that we used to view our candidates in aggregate. The web interface for the current work is hosted at . The code for the interface can be found at .'}]",2020-03-03
0,Scientific Data,41597,10.1038/s41597-020-0554-z,ERA5-based global meteorological wildfire danger maps,7,7,2020,https://git.ecmwf.int/projects/CEMSF/repos/geff; https://github.com/cvitolo/paper_geff_era5,The fire indices have been generated using the open source model GEFF v3.1 (). The code to reproduce the results of this manuscript is openly available on a public repository on GitHub ().,2020-07-07
0,Scientific Data,41597,10.1038/s41597-020-00630-y,"K-EmoCon, a multimodal sensor dataset for continuous emotion recognition in naturalistic conversations",8,9,2020,,"[{'ext-link': [{'@xlink:href': 'https://github.com/Kaist-ICLab/K-EmoCon_SupplementaryCodes', '@ext-link-type': 'uri', '#text': 'https://github.com/Kaist-ICLab/K-EmoCon_SupplementaryCodes'}, {'@xlink:href': 'https://github.com/pln-fing-udelar/fast-krippendorff', '@ext-link-type': 'uri', '#text': 'https://github.com/pln-fing-udelar/fast-krippendorff'}], 'italic': 'Krippendorff', '#text': 'Python codes implementing outlier detection using Chauvenet’s criterion, majority voting, mode-subtraction, and other utility functions, including the generation of heatmap plots, are available on . The  package () was used for the computation of Krippendorff’s alpha. Python version 3.6.9 was used throughout.'}, {'italic': 'SQLAlchemy', '#text': 'Codes for preprocessing the raw log-level data in SQL databases to CSV files were implemented in Python with the  package. However, these codes and the raw log-level data are not made available as they include privacy-sensitive information outside the agreed boundary for public sharing of the dataset, which was collected only for logistic reasons. Nevertheless, we welcome the dataset users to contact the corresponding authors if they need any further assistance or information regarding the raw data, and it’s preprocessing.'}]",2020-09-08
0,Scientific Data,41597,10.1038/s41597-020-00595-y,A data resource from concurrent intracranial stimulation and functional MRI of the human brain,5,8,2020,https://github.com/wiheto/esfmri_data_descriptor,"See  for code used for: fMRIPrep execution, MRIQC comparision, and confound differences between pre and postop.",2020-08-05
0,Scientific Data,41597,10.1038/s41597-020-00639-3,Access to mass rapid transit in OECD urban areas,8,9,2020,https://gitlab.iscpif.fr/vverbavatz/mrt-access-project,Detailed code generating the database can be accessed from the source code hosted via  ().,2020-09-08
0,Scientific Data,41597,10.1038/s41597-020-00582-3,"CU-BEMS, smart building electricity consumption and indoor environmental sensor datasets",20,7,2020,https://nbviewer.jupyter.org/github/mpipatta/mpipatta.github.io/blob/master/CHAM5.ipynb,"The code implementation was done in Python3 using Jupyter notebook. The scripts to perform data pre-processing, technical validation, visualization are available at SGRU github repository ().",2020-07-20
0,Scientific Data,41597,10.1038/s41597-020-0489-4,"The Ontario Climate Data Portal, a user-friendly portal of Ontario-specific climate projections",19,5,2020,,"[{'ext-link': {'@xlink:href': 'https://github.com/LAMPSYORKU/OntarioClimateDataPortal/tree/master/pythonCode', '@ext-link-type': 'uri', '#text': 'https://github.com/LAMPSYORKU/OntarioClimateDataPortal/tree/master/pythonCode'}, '#text': 'All codes associated with the data portal are open source; they are available on GitHub under the MIT license. We created the codes in two popular languages, JavaScript for the formal OCDP website and Python to facilitate data availability and interpretation. The GitHub repository hosts these codes. The 12 Python programs are designed for data reading, plotting, mapping and exporting on local server ().'}, {'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR50', '#text': '50'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR50', '#text': '50'}}], 'ext-link': {'@xlink:href': 'https://github.com/LAMPSYORKU/OntarioClimateDataPortal/tree/master/JavaScripts', '@ext-link-type': 'uri', '#text': 'https://github.com/LAMPSYORKU/OntarioClimateDataPortal/tree/master/JavaScripts'}, '#text': 'All of these Python files have also been uploaded to the Figshare data repository. Users can directly clone and download the repository to their local machine and run the Python programs in the Jupyter notebook environment to manipulate the data. HTML5 and JavaScript files for the formal data portal can be found in Figshare and GitHub (). Users, who are interested in web application development, can download and use them for their own projects.'}]",2020-05-19
0,Scientific Data,41597,10.1038/s41597-020-0418-6,High dielectric ternary oxides from crystal structure prediction and high-throughput screening,6,3,2020,,"All reported crystal structure prediction calculations were performed using the USPEX code, which is based on evolutionary algorithms to predict structures with only elemental information. Relaxation of structures and DFPT method were carried out by the VASP code.",2020-03-06
0,Scientific Data,41597,10.1038/s41597-020-0552-1,"High-resolution terrestrial climate, bioclimate and vegetation for the last 120,000 years",14,7,2020,,Code used to generate our dataset is available on the Open Science Framework.,2020-07-14
0,Scientific Data,41597,10.1038/s41597-020-0548-x,Multivariate time series dataset for space weather data analytics,10,7,2020,,"Our open-source repositories for MVTS generation, task-based sampling, and model validation is available on Bitbucket. Interested parties are encouraged to get involved in the ongoing development for and extensions to the dataset.",2020-07-10
0,Scientific Data,41597,10.1038/s41597-020-0446-2,A global ensemble of ocean wave climate projections from CMIP5-driven models,27,3,2020,,"[{'bold': ['Fortran code:', {'italic': 'getStat.f'}, {'italic': 'getStatDir.f'}, {'italic': 'getHsEx.f'}], '#text': ', ,'}, {'ext-link': {'@xlink:href': 'https://cowclip.org/data-access', '@ext-link-type': 'uri', '#text': 'https://cowclip.org/data-access'}, 'italic': 'getStat.f, getStatDir.f and getHsEx.f', 'sup': {'xref': [{'@ref-type': 'bibr', '@rid': 'CR30', '#text': '30'}, {'@ref-type': 'bibr', '@rid': 'CR31', '#text': '31'}, {'@ref-type': 'bibr', '@rid': 'CR32', '#text': '32'}], '#text': ', –'}, '#text': 'The Fortran code developed to derive the COWCLIP statistics can be requested via the COWCLIP website (). The code - as described in the Data Generation Method section, consists of a set of commands () which can be compiled with a Fortran compiler, linked against netCDF4 and HDF5 libraries. The documentation for setup, usage and requirements for the code is described within the technical reports that complement this manuscript. These commands can be executed by COWCLIP contributors to generate the set of wave statistics from their raw simulations. With the specific purpose of sharing in an open data format, and adhering to relevant data standards, the processed data is given in netCDF format, the global metadata attributes from the submitted netCDF data recorded, and additional information added where possible to ensure both CF Conventions and Attribute Convention for Dataset Discovery (‘ACDD’) standards compliance.'}, {'bold': ['Python code:', {'italic': 'COWCLIP_stats_mat2nc.py'}, 'and', {'italic': 'COWCLIP_extremes_mat2nc.py'}]}, {'ext-link': {'@xlink:href': 'https://cowclip.org/data-access', '@ext-link-type': 'uri', '#text': 'https://cowclip.org/data-access'}, '#text': 'The Python commands developed to produce the final standardised netCDF files (which comprise this data publication) are available in the COWCLIP website (). The code is written in Python 3 as well as standard python modules, depends on numpy, pandas, scipy, tables and netCDF4 python modules. Both python scripts require setting of the descriptive metadata location (path to file COWCLIP-GlobalProj-Metadata-merged.xlsx, structured to be readily usable with python’s pandas library), and the location of the Matlab matrix (.mat) and script (.m) files for the standardised data. The python scripts take as command line arguments the climate modelling group (e.g., ‘LBNL’) and time-slice simulation period (i.e., ‘Historical’ or ‘Future’). They produce, where possible, CF and ACDD standards-compliant output files in a Data Reference Syntax (DRS) structure akin to that used in CMIP and CORDEX modelling projects.'}]",2020-03-27
0,Scientific Data,41597,10.1038/s41597-020-0491-x,"CAVD, towards better characterization of void space for ionic transport analysis",22,5,2020,https://gitee.com/shuhebing/cavd,Source codes of CAVD are freely accessed in the repository: .,2020-05-22
0,Scientific Data,41597,10.1038/s41597-020-00637-5,"Materials Cloud, a platform for open computational science",8,9,2020,archive.materialscloud.org,"[{'ext-link': [{'@xlink:href': 'http://github.com/oschuett/appmode', '@ext-link-type': 'uri', '#text': 'github.com/oschuett/appmode'}, {'@xlink:href': 'http://github.com/aiidateam', '@ext-link-type': 'uri', '#text': 'github.com/aiidateam'}, {'@xlink:href': 'http://github.com/aiidalab', '@ext-link-type': 'uri', '#text': 'github.com/aiidalab'}], 'monospace': ['aiidateam', 'aiidalab'], '#text': 'The source code of AiiDA, the AiiDAlab, Appmode (), and most AiiDAlab applications is released under the MIT open-source license, and made available under the  () and  () GitHub organisations.'}, {'ext-link': [{'@xlink:href': 'http://materialscloud.org/quantum-mobile', '@ext-link-type': 'uri', '#text': 'materialscloud.org/quantum-mobile'}, {'@xlink:href': 'http://github.com/marvel-nccr', '@ext-link-type': 'uri', '#text': 'github.com/marvel-nccr'}], 'monospace': 'marvel-nccr', '#text': 'The Quantum Mobile virtual machine can be downloaded from . Its source code (in the form of ansible roles and playbooks) is released under the MIT license and made available under the  GitHub organisation ().'}]",2020-09-08
0,Scientific Data,41597,10.1038/s41597-020-00682-0,"ClimActor, harmonized transnational data on climate network participation by city and regional governments",6,11,2020,https://github.com/datadrivenenvirolab/ClimActor,Data and code for the ClimActor R package functions is available on GitHub: .,2020-11-06
0,Scientific Data,41597,10.1038/s41597-020-00580-5,"Outlining where humans live, the World Settlement Footprint 2015",20,7,2020,,"The WSF2015 is the result of several processing steps involving tens of sub-modules run on multiple architectures and using different software. While S1 pre-processing and feature extraction has been supported by Google through its Earth Engine platform, the computation of Landsat-8 temporal statistics, the training point extraction and classification tasks have been performed in the IT4Innovations Czech supercomputing center by means of DLR proprietary software, GDAL (Geospatial Data Abstraction Library v.2.4) and Pktools (Processing Kernels for geospatial data v2.6) scripts. Post-classification has been carried out in the Calvalus system available at DLR’s Earth Observation Center by means of proprietary software and dedicated Python (v3.5) scripts. Given the use of proprietary tools, the code cannot be openly released to the public.",2020-07-20
0,Scientific Data,41597,10.1038/s41597-020-0398-6,Building fault detection data to aid diagnostic algorithm creation and performance testing,24,2,2020,,"[{'sup': [{'xref': [{'@ref-type': 'bibr', '@rid': 'CR30', '#text': '30'}, {'@ref-type': 'bibr', '@rid': 'CR31', '#text': '31'}], '#text': ','}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR29', '#text': '29'}}], '#text': 'The Modelica Buildings Library and EnergyPlus are freely available for download. EnergyPlus runs on Windows, Mac OSX, and Linux operating systems. A Windows or Linux-based computer and Dymola solver are required to run Modelica, and Dymola can be licensed from Modelica Buildlings Library. HVACSim+ is also freely available, upon request from NIST, and has no operating system requirements. The HVACSim+ AHU model that was used in this work is available within the ASHRAE Research Project 1312 Results.'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR32', '#text': '32'}}, '#text': 'The data acquisition system that is implemented in FLEXLAB comprises a custom-built National Instruments platform utilizing distributed Compact RIOs (cRIOs), PC-based servers and workstations, and a Unix-based database running sMAP2.0. Data are typically collected and recorded at a one-second (1\u2009Hz) rate with averaging to one minute for database storage – suitable for most research purposes. 1\u2009Hz data are stored in CSV files for use as needed. Control and data acquisition are implemented over the same architecture, with most control sequences running in the NI TestStand environment. A simple API also allows remote data acquisition and control using text-based messages, which allows use of other programming or scripting environments such as python or java. Experimental data are accessed from sMAP using a browser-based GUI or via text-based query.'}, 'At the Flexible Research Platform, the data acquisition system utilizes Campbell Scientific data loggers. It includes measurements of the zone set point temperature and humidity, supply and return air temperature and flow rates, and energy consumption of individual components including compressor, condenser, supply fan, VAV reheating. Data are typically collected and recorded at a one-second rate with averaging thirty seconds for database storage. The data file format is CSV, with automated transfer from the data loggers to storage on an ORNL internal server, at time resolutions of 30\u2009seconds, 1\u2009min, 15\u2009min, and 60\u2009min intervals.']",2020-02-24
0,Scientific Data,41597,10.1038/s41597-020-00624-w,A global-scale data set of mining areas,8,9,2020,,"[{'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR61', '#text': '61'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR35', '#text': '35'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR58', '#text': '58'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR59', '#text': '59'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR62', '#text': '62'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR63', '#text': '63'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR64', '#text': '64'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR65', '#text': '65'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR65', '#text': '65'}}], 'ext-link': {'@xlink:href': 'http://www.github.com/fineprint-global/app-mining-area-polygonization', '@ext-link-type': 'uri', '#text': 'www.github.com/fineprint-global/app-mining-area-polygonization'}, '#text': 'All the code and geoprocessing scripts used to produce the results of this paper are distributed under the GNU General Public License v3.0 (GPL-v3) from the repository . The processing scripts were written in R, Python, and GDAL (Geospatial Data Abstraction Library). The web application to delineate the polygons was written in R Shiny using a PostgreSQL database with PostGIS extension for storage. The full app setup uses Docker containers to facilitate management, portability, and reproducibility.'}, {'ext-link': {'@xlink:href': 'http://www.github.com/fineprint-global/app-mining-area-polygonization', '@ext-link-type': 'uri', '#text': 'www.github.com/fineprint-global/app-mining-area-polygonization'}, 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR33', '#text': '33'}}, '#text': 'The web application supports the delineation of areas from the satellite images layers. It systematically displays the regions of interest (e.g., buffer around the mines) and several background options of satellite images, which the users can take into account to draw and edit polygons. Note that mining coordinates are not part of the web application and must be fed into the database by the user. To learn more about the application setup see . The current version of app provides image layers from Sentinel-2 Cloudless, Google Satellite, and Microsoft Bing Imagery. Further sources of satellite images can be added to the application via WMS.'}]",2020-09-08
0,Scientific Data,41597,10.1038/s41597-020-0487-6,Large database for the analysis and prediction of spliced and non-spliced peptide generation by proteasomes,15,5,2020,,The algorithm generating all possible  and  spliced peptides was originally described by Liepe .. Scripts for MySQL database setup have been deposited in the MySQL database dump to the Mendeley repository with the dataset access doi:10.17632/nr7cs764rc.1.,2020-05-15
0,Scientific Data,41597,10.1038/s41597-020-0385-y,"Atomic structures and orbital energies of 61,489 crystal-forming organic molecules",18,2,2020,https://aimsclub.fhi-berlin.mpg.de/aims_obtaining_simple.php,"All electronic structure data contained in this work was generated with the FHI-aims code. The code is available for a license fee from . Parsing of outputs and data collection were performed with custom-made Python scripts, which will be available upon request. Finally, the published archive contains a tutorial detailing how to access the dataset.",2020-02-18
0,Scientific Data,41597,10.1038/s41597-020-0429-3,An open data infrastructure for the study of anthropogenic hazards linked to georesource exploitation,11,3,2020,www.epos-ip.org; www.tcs.ah-epos.eu,Applications are available on the IS-EPOS Platform of Thematic Core Service Anthropogenic Hazards: tcs.ah-epos.eu. The source code of the platform itself is available at bit.ly/ISEPOSsourcecode.,2020-03-11
0,Scientific Data,41597,10.1038/s41597-020-0474-y,High-throughput screening platform for solid electrolytes combining hierarchical ion-transport prediction algorithms,21,5,2020,https://www.bmaterials.cn/static/help/SPSE-UserManuals.pdf,"The CAVD, BVSE and hierarchical computational codes have been integrated in SPSE, and they can only be run if the user has access to a SPSE account. The SPSE manuals are available in the website: . Source codes of SPSE are freely available for download at figshare.",2020-05-21
0,Scientific Data,41597,10.1038/s41597-020-0563-y,A database of human gait performance on irregular and uneven surfaces collected by wearable sensors,8,7,2020,,"[{'ext-link': {'@xlink:href': 'https://github.com/UF-ISE-HSE/UnevenWalkingSurface', '@ext-link-type': 'uri', '#text': 'https://github.com/UF-ISE-HSE/UnevenWalkingSurface'}, '#text': 'The custom MATLAB script to process data is provided on the following Github repository: .'}, 'A Python script (python_version.py) was also provided for converting the processed data into Python compatible format. The .h5py file can be directly use as a standard file object in Python to process.']",2020-07-08
0,Scientific Data,41597,10.1038/s41597-020-0403-0,All-hazards dataset mined from the US National Incident Management System 1999–2014,21,2,2020,,,2020-02-21
0,Scientific Data,41597,10.1038/s41597-020-0559-7,"IDEAL, the Infectious Diseases of East African Livestock project open access database and biobank",9,7,2020,,,2020-07-09
0,Scientific Data,41597,10.1038/s41597-020-0498-3,"Simultaneous EEG-fMRI during a neurofeedback task, a brain imaging dataset for multimodal data integration",10,6,2020,https://github.com/glioi/BIDS_fMRI_analysis_nipype,"A detailed description of the bimodal EEG-fMRI NF platform is given in: the platform software package for real-time analysis and visualization is well documented but not publicly available. Python pipelines for the analysis of structural and functional MRI are available on github (), in form of commented jupyter notebooks. Other scripts used for the technical validation in this paper can be provided by the authors upon request.",2020-06-10
0,Scientific Data,41597,10.1038/s41597-020-0396-8,Benchmark classification dataset for laser-induced breakdown spectroscopy,13,2,2020,,"Custom code for loading in the training and testing datasets is available in the data repository for Python, R, and MATLAB. The Python code was tested in Python 3.6 and requires the following libraries: “os”, “h5py”, and “numpy”. The R code was tested in R 3.5.2 and requires the following libraries: “rhdf5”. Lastly, the MATLAB code was tested in MATLAB 2016. The codes are intended to load in the data from the hdf5 files in a user-friendly manner.",2020-02-13
0,Scientific Data,41597,10.1038/s41597-020-00689-7,A late Pleistocene dataset of Agulhas Current variability,11,11,2020,,,2020-11-11
0,Scientific Data,41597,10.1038/s41597-020-0520-9,The International Bathymetric Chart of the Arctic Ocean Version 4.0,9,7,2020,https://www.generic-mapping-tools.org/; https://gdal.org/,"The gridding and statistical calculation procedures described in the Methods section are based on open source routines, provided within GMT () and GDAL (), embedded in Python scripts. Codes are available upon request.",2020-07-09
0,Scientific Data,41597,10.1038/s41597-020-0485-8,"Pofatu, a curated and open-access database for geochemical sourcing of archaeological materials",11,5,2020,,"[{'ext-link': [{'@xlink:href': 'https://pypi.org/project/pypofatu', '@ext-link-type': 'uri', '#text': 'https://pypi.org/project/pypofatu'}, {'@xlink:href': 'https://github.com/pofatu/pofatu-data/releases/tag/v1.0.0', '@ext-link-type': 'uri', '#text': 'https://github.com/pofatu/pofatu-data/releases/tag/v1.0.0'}], 'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR182', '#text': '182'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR176', '#text': '176'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR183', '#text': '183'}}], '#text': 'The pypofatu Python package is open-source software, maintained on GitHub and distributed via the Python Package Index (), with released versions archived with Zenodo. The two output formats listed above are created and stored as part of the GitHub repository where the dataset is curated (), and each release of the dataset is also archived on Zenodo. Additionally, the dataset is loaded into a clld web application, providing an online, browsable user interface for “window-shopping”, before downloading and using the dataset locally.'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR177', '#text': '177'}}, '#text': 'Released versions of the Pofatu dataset meet the requirements on FAIR data as laid out by Wilkinson and colleagues. The data is findable thanks to Zenodo’s integration in the research data landscape on the web, and the metadata we provide. It is accessible via the DOI doled out by Zenodo. “It is interoperable due to the open standards” used to encode the data and reusable because it is provided under an open CC-BY license.'}]",2020-05-11
0,Scientific Data,41597,10.1038/s41597-020-0427-5,Gut microbiome diversity detected by high-coverage 16S and shotgun sequencing of paired stool and colon sample,16,3,2020,,"[{'xref': {'@rid': 'Tab7', '@ref-type': 'table', '#text': '8'}, '#text': 'Software versions used are listed in Table\xa0.'}, {'ext-link': {'@xlink:href': 'https://gitlab.com/JoanML/colonbiome-pilot', '@ext-link-type': 'uri', '#text': 'https://gitlab.com/JoanML/colonbiome-pilot'}, '#text': 'Code for sequence quality control and trimming, shotgun and 16S metagenomics profiling and generation of figures in this paper is freely available and thoroughly documented at . This repository includes instructions for the analysis and reproduction of the figures on this paper from the publicly available samples, as well as pipelines used for the analysis. This repository is arranged in folders, each containing a README:'}, {'monospace': 'qc', '#text': '• : Scripts for quality control and preprocessing of samples'}, {'monospace': 'analysis_shotgun', '#text': '• : Scripts to run softwares for metagenomics analysis'}, {'monospace': 'regions_16s', '#text': '• : In-house scripts for splitting IonTorrent reads into new FASTQ files'}, {'monospace': 'analysis_16s', '#text': '• : DADA2 pipeline adapted to this dataset'}, {'monospace': 'assembly', '#text': '• : Scripts to run the assembly, binning and quality control software'}, {'monospace': 'figures', '#text': '• : Scripts used to generate the figures in this manuscript'}, {'monospace': 'shannon_index_subsamples', '#text': '• : Scripts used to compute alpha diversity in subsampled FASTQs'}]",2020-03-16
0,Scientific Data,41597,10.1038/s41597-020-0574-8,"TAASRAD19, a high-resolution weather radar reflectivity dataset for precipitation nowcasting",13,7,2020,https://github.com/MPBA/TAASRAD19,"All the software described in Technical Validation is available in a public GitHub repository (), along with the Python scripts for sequence pre-processing, installation scripts for the MXNet framework, pre-trained network model weights, and examples of radar prediction output sequences. All the code was written in Python 3.6 and tested on Ubuntu releases 16.04/18.04. Some pre-processing steps (e.g. sequence and outlier mask generation) require a non trivial amount of computing resources and memory. Training the deep learning model with the same parameters described in the paper requires either two GPUs with 8GB of RAM or one GPU with 16GB. Please refer to the  files in the code release for further instructions.",2020-07-13
0,Scientific Data,41597,10.1038/s41597-020-0516-5,Large-scale metabolic interaction network of the mouse and human gut microbiota,26,6,2020,,"Our Python code that converts the JSON format of NJC19 network data (NJC19_network.json) to the format of Supplementary Table  can be downloaded from the Dryad Digital Repository. For the taxonomic profiling of microbiome samples for the NJC19 construction, we used MetaPhlAn v2.0 with the “sensitive-local” mapping option and QIIME v1.8.0 with Greengenes v13_8_pp reference files, as described above. The aforementioned cys file of NJC19 for network visualization was produced by Cytoscape v3.7.2.",2020-06-26
0,Scientific Data,41597,10.1038/s41597-020-00707-8,Density functional theory-based electric field gradient database,21,10,2020,https://github.com/usnistgov/jarvis,Python-language based codes for carrying out calculations and analyzing the results are provided at the JARVIS-Tools GitHub page ().,2020-10-21
0,Scientific Data,41597,10.1038/s41597-020-00605-z,Combining expert and crowd-sourced training data to map urban form and functions for the continental US,11,8,2020,,All Google Earth Engine codes to pre-process the earth observation input features and perform the actual LCZ classification are available upon request. The pixel-based random-sampling assessment was done using the randomForest v4.16-14 package available in R-project. The CONUS LCZ map figure is produced with QGIS v3.4. All other data processing and visualizations are done in Python v3.6.9.,2020-08-11
0,Scientific Data,41597,10.1038/s41597-020-0557-9,"CerebrA, registration and manual label correction of Mindboggle-101 atlas for MNI-ICBM152 template",15,7,2020,https://gin.g-node.org/anamanera/CerebrA/src/master/,"The scripts used to perform both the linear and nonlinear registrations (including the ANTs code with all the selected registration parameters), the obtained transformations that were used to register the DKT atlas to the MNI-ICBM2009c template, the code for resampling the labels based on these transformations, as well as the registered DKT atlas in the MNI space, after applying the transformations are available at .",2020-07-15
0,Scientific Data,41597,10.1038/s41597-020-0455-1,Database of pharmacokinetic time-series data and parameters for 144 environmental chemicals,20,4,2020,,The Python code to identify possible CvT data sources in literature is available at our GitHub repository. Download of the database is required to train the model.,2020-04-20
0,Scientific Data,41597,10.1038/s41597-020-00646-4,Mapping twenty years of corn and soybean across the US Midwest using the Landsat archive,15,9,2020,,"[{'ext-link': {'@xlink:href': 'https://github.com/LobellLab/csdl', '@ext-link-type': 'uri', '#text': 'https://github.com/LobellLab/csdl'}, '#text': 'Code used to generate the training samples, compute harmonics, train a random forest classifier, create the final maps, and produce the validation analyses are available publicly as Google Earth Engine scripts, R markdown files, or Jupyter notebooks. Links to scripts and data for analyses can be found in the GitHub repository at .'}, 'The software used in this work include:', '• R version 3.5.1, dplyr 0.8.0.1, sf 0.6–3, raster 2.6–7, rgdal 1.3–4, salustools 0.1.0, sp 1.3–1', '• Python 3.7.3, numpy 1.16.4, pandas 0.24.2, matplotlib 3.1.0, sklearn 0.21.2, plotly 4.5.0']",2020-09-15
0,Scientific Data,41597,10.1038/s41597-020-0442-6,Processing citizen science- and machine-annotated time-lapse imagery for biologically meaningful metrics,26,3,2020,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR12', '#text': '12'}}, '#text': 'Code used to produce files within Dataset 1:'}, {'italic': ['Penguin Watch', 'R'], 'ext-link': {'@xlink:href': 'https://github.com/zooniverse/Data-digging/blob/master/example_scripts/Penguin_Watch/Penguin_Watch_ImageProcessingScript.R', '@ext-link-type': 'uri', '#text': 'https://github.com/zooniverse/Data-digging/blob/master/example_scripts/Penguin_Watch/Penguin_Watch_ImageProcessingScript.R'}, 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR26', '#text': '26'}}, '#text': '• Raw  time-lapse photographs are renamed and resized using an  (currently v3.6.0) script. The code is publically available via GitHub at . A static version (written using v3.4.1) is archived on Figshare.'}, {'italic': 'Penguin Watch', 'xref': {'@rid': 'Fig1', '@ref-type': 'fig', '#text': '1'}, 'sup': [{'xref': [{'@ref-type': 'bibr', '@rid': 'CR10', '#text': '10'}, {'@ref-type': 'bibr', '@rid': 'CR14', '#text': '14'}], '#text': ','}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR14', '#text': '14'}}], 'ext-link': {'@xlink:href': 'https://github.com/zooniverse/aggregation/blob/master/penguins/aggregate.py', '@ext-link-type': 'uri', '#text': 'https://github.com/zooniverse/aggregation/blob/master/penguins/aggregate.py'}, '#text': '• Raw  volunteer classifications (xy coordinate clicks) were clustered into ‘consensus clicks’ using agglomerative hierarchical clustering (Fig.\xa0, left, ‘Step 2’). The aggregation algorithm is written in Python (v 2.7) and can be found on GitHub at . A static version of this script is also archived on Figshare.'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR15', '#text': '15'}}, '#text': 'Code used to produce files within Dataset 2:'}, {'italic': ['Kraken Script', 'Kraken Files', 'R'], 'xref': {'@rid': 'Fig1', '@ref-type': 'fig', '#text': '1'}, 'ext-link': {'@xlink:href': 'https://github.com/zooniverse/Data-digging/blob/master/example_scripts/Penguin_Watch/Kraken_Script.R', '@ext-link-type': 'uri', '#text': 'https://github.com/zooniverse/Data-digging/blob/master/example_scripts/Penguin_Watch/Kraken_Script.R'}, 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR27', '#text': '27'}}, '#text': '• The  (Fig.\xa0, left, ‘Step 3’; output\u2009=\u2009) is written in  (v3.6.0); it can be accessed through GitHub at , and a static version is archived on Zenodo.'}, {'italic': ['Narwhal Script', 'Narwhal Files', 'R'], 'xref': {'@rid': 'Fig1', '@ref-type': 'fig', '#text': '1'}, 'ext-link': {'@xlink:href': 'https://github.com/zooniverse/Data-digging/blob/master/example_scripts/Penguin_Watch/Narwhal_Script.R', '@ext-link-type': 'uri', '#text': 'https://github.com/zooniverse/Data-digging/blob/master/example_scripts/Penguin_Watch/Narwhal_Script.R'}, 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR28', '#text': '28'}}, '#text': '• The  (Fig.\xa0, left, ‘Step 4’; output\u2009=\u2009) is written in  (v3.6.0); it can be accessed through GitHub at  and a static version is archived on Zenodo.'}, {'italic': ['Narwhal Plotting Script', 'Narwhal Plots', 'Narwhal', 'R'], 'ext-link': {'@xlink:href': 'https://github.com/zooniverse/Data-digging/blob/master/example_scripts/Penguin_Watch/Narwhal_Plotting_Script.R', '@ext-link-type': 'uri', '#text': 'https://github.com/zooniverse/Data-digging/blob/master/example_scripts/Penguin_Watch/Narwhal_Plotting_Script.R'}, 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR29', '#text': '29'}}, '#text': '• The  (output\u2009=\u2009 – graphs displaying  summary statistics) is written in  (v3.6.0); it can be accessed through GitHub at  and a static version is archived on Zenodo.'}, {'italic': 'Pengbot', 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR11', '#text': '11'}}, 'ext-link': {'@xlink:href': 'https://www.robots.ox.ac.uk/~vgg/data/penguins/', '@ext-link-type': 'uri', '#text': 'https://www.robots.ox.ac.uk/~vgg/data/penguins/'}, '#text': '• The  model, associated code and instructions, and the dataset used to train the neural network can be found at the following address: .'}, {'italic': ['Pengbot Counting Script', 'R'], 'xref': {'@rid': 'Fig1', '@ref-type': 'fig', '#text': '1'}, 'ext-link': {'@xlink:href': 'https://github.com/zooniverse/Data-digging/blob/master/example_scripts/Penguin_Watch/Pengbot_Counting_Script.R', '@ext-link-type': 'uri', '#text': 'https://github.com/zooniverse/Data-digging/blob/master/example_scripts/Penguin_Watch/Pengbot_Counting_Script.R'}, 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR30', '#text': '30'}}, '#text': '• The  (Fig.\xa0, right, ‘Step 2’ and ‘Step 3’) is written in  (v3.6.0); it can be accessed through GitHub at  and a static version is archived on Zenodo.'}]",2020-03-26
0,Scientific Data,41597,10.1038/s41597-020-0483-x,Developing reliable hourly electricity demand data through screening and imputation,26,5,2020,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR12', '#text': '12'}}, '#text': 'The code used to clean and analyze the data is published in Zenodo (ref.\u2009) with the corresponding DOIs:'}, {'ext-link': {'@xlink:href': '10.5281/zenodo.3737085', '@ext-link-type': 'doi', '#text': 'https://doi.org/10.5281/zenodo.3737085'}, '#text': '• v1.1 (specific version):'}, {'ext-link': {'@xlink:href': '10.5281/zenodo.3678854', '@ext-link-type': 'doi', '#text': 'https://doi.org/10.5281/zenodo.3678854'}, '#text': '• Concept (all versions):'}, 'The code is available under the MIT License. The repository contains four notebooks that reproduce the following workflow:', {'bold': 'Step 1:', '#text': 'Query the EIA database for raw demand data'}, {'bold': 'Step 2:', '#text': 'Screen the data for anomalous values'}, {'bold': 'Step 3:', '#text': 'Impute missing and anomalous values with the Multiple Imputation by Chained Equations (MICE) procedure'}, {'bold': 'Step 4:', '#text': 'Distribute the imputation results to publication-ready files'}, {'bold': ['Step 1', 'Step 2', 'Step 4', 'Step 3'], 'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR13', '#text': '13'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR14', '#text': '14'}}, {'xref': [{'@ref-type': 'bibr', '@rid': 'CR15', '#text': '15'}, {'@ref-type': 'bibr', '@rid': 'CR16', '#text': '16'}], '#text': ','}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR11', '#text': '11'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR10', '#text': '10'}}], 'italic': ['pandas', 'numpy', 'mice'], '#text': ', , and  are based on python code, were written in python, and use the  and  packages.  is written in the R programming language and relies on the  package. The versions of python, high-level python packages, R, and high-level R packages used in the analysis are:'}, '• python\u2009=\u20093.7.3', 'numpy\u2009=\u20091.16.2', 'pandas\u2009=\u20090.24.2', 'urllib3\u2009=\u20091.24.1', '• r-base\u2009=\u20093.5.1', 'r-mice\u2009=\u20093.6.0', 'r-dplyr\u2009=\u20090.7.6', 'r-data.table\u2009=\u20091.11.4', 'r-zoo\u2009=\u20091.8_3', 'parallel\u2009=\u200920200122', 'r-reshape2\u2009=\u20091.4.3', 'r-markdown\u2009=\u20090.8', 'r-rmarkdown\u2009=\u20091.10', 'r-lubridate\u2009=\u20091.7.4', 'multiprocess\u2009=\u20090.70.9', {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR17', '#text': '17'}}, '#text': 'The archived code used to clean and analyze the data is supplemented by an archived version of the Conda computing environment used for the analysis (ref.\u2009) with the corresponding DOIs:'}, {'ext-link': {'@xlink:href': '10.5281/zenodo.3736784', '@ext-link-type': 'doi', '#text': 'https://doi.org/10.5281/zenodo.3736784'}, '#text': '• v1.0 (specific version):'}, {'ext-link': {'@xlink:href': '10.5281/zenodo.3736783', '@ext-link-type': 'doi', '#text': 'https://doi.org/10.5281/zenodo.3736783'}, '#text': '• Concept (all versions):'}, {'bold': ['package-list.txt', 'environment.yml', 'environment.yml'], '#text': 'The data cleaning was run on Mac OSX 10.14.6. A complete list of every package present in the Conda computing environment can be found in the  file in the code archive. The archived Conda environment can be used on Mac OSX systems\xa0circa 2020. Instructions for setting up the environment are included in both the code and computing environment archives. For other operating systems, we include an  file containing the high-level packages mentioned above. Instructions for setting up a Conda computing environment using the  file are also included in the code archive.'}]",2020-05-26
0,Scientific Data,41597,10.1038/s41597-020-0514-7,Integrated analysis of a compendium of RNA-Seq datasets for splicing factors,16,6,2020,http://SFMetaDB.yubiolab.org,The scripts and source codes of raw DAS and DEG analyses are deployed in Docker image and deposited at Docker Hub with the public tag sfrs/dasdegdocker:latest. The docker image for signature comparison analysis workflow also was deposited at Docker Hub with the public tag sfrs/sfsigdb:latest.,2020-06-16
0,Scientific Data,41597,10.1038/s41597-020-0412-z,A multi-omics dataset of heat-shock response in the yeast RNA binding protein Mip6,27,2,2020,https://github.com/ConesaLab/MultiMip6,Preprocessing scripts for each of the omics datasets are available at the Github repository ().,2020-02-27
0,Scientific Data,41597,10.1038/s41597-020-0479-6,"Geomorpho90m, empirical evaluation and accuracy assessment of global high-resolution geomorphometric layers",28,5,2020,,"[{'bold': 'Geomorphometric layer computation'}, {'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR38', '#text': '38'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR36', '#text': '36'}}], 'italic': 'gdalwarp', '#text': 'Prior to computing the geomorphometric layers, we reprojected the DEMs (MERIT, 3DEP-1, LiDAR DMS and DTM) to the Equi7 projection with a cell size 100\u2009m (the projection parameters are available from). To harmonise the different spatial grains, we used a bilinear algorithm implemented in  within the open-source Geospatial Data Abstraction Library (GDAL). We kept the seven projection zones as defined in, and employed the T6 tiling method to allow parallel and distributed processing of our work-flow. Tile size was 600\u2009×\u2009600\u2009km where we buffered the borders by 401\u2009km to avoid border artefacts between tiles. These overlapping and duplicate grid cells were removed when merging all tiles to seamless, continental maps. These large tile size increments were needed to avoid border effects, especially for multiscale deviation and multiscale roughness.'}, {'xref': {'@rid': 'Tab1', '@ref-type': 'table', '#text': '1'}, '#text': 'We used the following open source software packages to compute the geomorphometric layers (Table\xa0 reports the software and the specific commands used for each derived variable calculation):'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR66', '#text': '66'}}, '#text': 'Geospatial Data Abstraction Library (GDAL),version number 2.1.2.'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR67', '#text': '67'}}, '#text': 'Geographic Resources Analysis Support System software (GRASS), version number 7.3.0.'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR68', '#text': '68'}}, '#text': 'Whitebox Geospatial Analysis Tools (Whitebox GAT), version number 3.3.0.'}, {'sup': {'xref': [{'@ref-type': 'bibr', '@rid': 'CR69', '#text': '69'}, {'@ref-type': 'bibr', '@rid': 'CR70', '#text': '70'}], '#text': ','}, '#text': 'Processing Kernel for geospatial data (Pktools), version number 2.6.3.'}, {'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR71', '#text': '71'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR35', '#text': '35'}}], '#text': 'All of these tools provide fast and scalable computation features and functions for raster-based workflows that are easily automated using a scripting language, such as Bash or Python. They also allow for the processing of very large datasets owing to efficient algorithms and optimised memory management. After computing all geomorphometric layers within the Equi7 projection, the layers were reprojected back to the WGS84 coordinate reference system (EPSG:4326 code) with a bilinear algorithm (or near for categorical variables) implemented in GDAL. This reversion of Geomorpho90m to WGS84 allows for the data to be seamlessly integrated with a broad set of global datasets.'}, 'We used a tiling system identical to MERIT-DEM, in terms of dimension and nomenclature, i.e. 5\u2009×\u20095 degree tiles with 6000\u2009×\u20096000 cells each, to ensure data integration and comparisons with the original MERIT-DEM. All calculations were processed in parallel using open-source software at the Center for Research Computing, Yale University.', {'bold': 'LiDAR data processing'}, {'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR72', '#text': '72'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR70', '#text': '70'}}], '#text': 'Two common products that can be extracted from LiDAR data are: DTM and DSM. The DTM is generated using ground echoes from the LiDAR point cloud, in conjunction with an interpolation technique. The approach employed in this paper for calculating the DTMs used the LiDAR processing tools found in the pktools software. This is a two-stage approach, where the first stage uses a minimum composite rule, which retains the LiDAR pulse for the minimum height of each cell.'}, {'monospace': 'pklas2img -a_srs EPSG:26911 -dx 100 -dy 100 -comp min -i input.las -o dtm_min.tif'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR73', '#text': '73'}}, '#text': 'For the second stage, the output is then filtered using a progressive morphological filter, which uses an iterative filter based on increasing kernel sizes to remove non-ground points from the final DTM.'}, {'monospace': 'pkfilterdem -f promorph -dim 3 -dim 11 -i dtm_min.tif -o dtm_min_promorph.tif'}, 'The calculation of the DSM is more straightforward and uses either a maximum composite rule (pulse retention with the maximum height of the cell) or a defined percentile composite rule. In our case, we use the rule to retain the pulse with the value corresponding to the 95th percentile of all pulses within the cell.', {'monospace': 'pklas2img -a_srs EPSG:26911 -dx 100 -dy 100 -comp percentile -percentile 95 -i input.las -o dsm2.tif'}, 'The LiDAR projection parameters (EPSG code) were set to EPSG:26911 in accordance with the associated metadata. The DSM and DTM cell size was set to 100 m to allow a simple reprojection to Equi7, which enables a comparison with the other discussed layers.']",2020-05-28
0,Scientific Data,41597,10.1038/s41597-020-00603-1,A consistent Great Lakes ice cover digital data set for winters 1973–2019,6,8,2020,https://github.com/NOAA-GLERL/icegridresampling,"We developed R scripts to compute the spatial and temporal interpolated ice cover values. Spatial interpolation for Grid-510 is processed by “Resampling_Raster.R”, and temporal interpolation for non-daily data is estimated by “Time_Interp.R”. Both scripts utilize RStudio version 1.1.463, and are available on the NOAA GLERL GitHub Repository at . This repository also contains sample scripts (Python, MATLAB and R) to demonstrate how to load the ASCII data into memory.",2020-08-06
0,Scientific Data,41597,10.1038/s41597-020-00657-1,Multiyear in-situ L-band microwave radiometry of land surface processes on the Tibetan Plateau,30,9,2020,,"The codes for processing the collected data, for plotting the figures, and for downloading SMAP data are included in the dataset. The codes for filtering the reflected solar signals and for plotting the results are included in the update together with the updated brightness temperature that contain corrected local time stamps. Overviews of the codes, data and explanations can be found in Online-only Table  and Table .",2020-09-30
0,Scientific Data,41597,10.1038/s41597-020-0408-8,Transcriptomic resources for evolutionary studies in flat periwinkles and related species,3,3,2020,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR17', '#text': '17'}}, '#text': '1. FASTQC v0.11.5, options: default'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR18', '#text': '18'}}, '#text': '2. Trimmomatic v0.36, options: PE ILLUMINACLIP:Adapters.fa:2:30:10 HEADCROP:10 SLIDINGWINDOW:4:15 LEADING:20 TRAILING:20 MINLEN:25'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR20', '#text': '20'}}, '#text': '3. Pseudo-it v1, options: –PE1 R1.fastq.gz –PE2 R2.fastq.gz –iupac 5 REF NAME'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR24', '#text': '24'}}, '#text': '4. Trinity v2.2.0, default'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR25', '#text': '25'}}, '#text': '5. Transrate v1.0.3, options: –left –right –reference'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR26', '#text': '26'}}, '#text': '6. CD-HIT-EST v4.6.4, options: -c 0.95 -M 0'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR27', '#text': '27'}}, '#text': '7. TransDecoder.LongOrfs v3.0.0, default'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR47', '#text': '47'}}, '#text': '8. hmmscan v v3.1b2, options: default'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR27', '#text': '27'}}, '#text': '9. TransDecoder.Predict v3.0.0, options: –retain_pfam_hits–retain_blastp_hits –single_best_only'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR29', '#text': '29'}}, '#text': '10. crb-blast v0.6.9, options: -e 1.0e-05'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR21', '#text': '21'}}, '#text': '11. BWA v0.7.15, options: mem -M -R ‘@RG\\tID:\\tLB:\\tPL:\\tSM:’'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR23', '#text': '23'}}, '#text': '12. Samtools v1.3.1, options: sort; fixmate, index'}, {'ext-link': {'@xlink:href': 'http://broadinstitute.github.io/picard/', '@ext-link-type': 'uri', '#text': 'http://broadinstitute.github.io/picard/'}, '#text': '13. Picard v1.140 (“Picard Toolkit.” 2019. Broad Institute, GitHub Repository: ), options: MarkDuplicates REMOVE_DUPLICATES\u2009=\u2009true ASSUME_SORTED\u2009=\u2009true'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR22', '#text': '22'}}, '#text': '14. GATK v3.7, options: RealignerTargetCreator; IndelRealigner'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR31', '#text': '31'}}, '#text': '15. BCFtools v1.6, options: mpileup -f REF.fa -Ou -a DP -b list|bcftools call -m -f GQ,GP'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR32', '#text': '32'}}, '#text': '16. Plink v1.90b3.45, options: –bp-space 10000 –pca'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR34', '#text': '34'}}, '#text': '17. VCFtools v. 0.1.14, vcftools –gzvcf target.vcf.gz–max-missing 1.0 –min-alleles 2 –max-alleles 2 –mac 2 –remove-indels–recode–stdout|bgzip -c\u2009>\u2009Filtered.biallelic.vcf.gz'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR35', '#text': '35'}}, '#text': '18. fastStructure v1.0, options: –full –cv\u2009=\u200910'}, {'ext-link': {'@xlink:href': 'https://github.com/simonhmartin/genomics_general', '@ext-link-type': 'uri', '#text': 'https://github.com/simonhmartin/genomics_general'}, '#text': '19. Simon Martin’s scripts (Simon Martin, GitHub Repository:  accessed on July 2018), options: parseVCF.py –skipIndels –minQual 30 –gtf flag\u2009=\u2009DP min\u2009=\u200920 (or 5); filterGenotypes.py –fixedDiffs–minCalls 8'}, {'sup': {'xref': [{'@ref-type': 'bibr', '@rid': 'CR40', '#text': '40'}, {'@ref-type': 'bibr', '@rid': 'CR41', '#text': '41'}], '#text': ','}, '#text': '20. Genepop v4.2, options: 1. Hardy Weinberg Exact Tests and 2. Linkage disequilibrium, default'}, {'sup': {'xref': [{'@ref-type': 'bibr', '@rid': 'CR42', '#text': '42'}, {'@ref-type': 'bibr', '@rid': 'CR43', '#text': '43'}, {'@ref-type': 'bibr', '@rid': 'CR44', '#text': '44'}], '#text': ', –'}, '#text': '21. STRUCTURE v2.3.4, options: burnin\u2009=\u2009100,000, numreps\u2009=\u20091,000,000, usepopinfo\u2009=\u20090; inferalpha\u2009=\u20091; maxpops\u2009=\u20092.'}]",2020-03-03
0,Scientific Data,41597,10.1038/s41597-020-0542-3,A rasterized building footprint dataset for the United States,29,6,2020,https://github.com/mehdiheris/RasterizingBuildingFootprints,Our software is available through U.S. Geological Survey code repository (https://doi.org/10.5066/P9XZCPMT). Our serial code is also available in our Github page: .,2020-06-29
0,Scientific Data,41597,10.1038/s41597-020-0440-8,A 12-Lead ECG database to identify origins of idiopathic ventricular arrhythmia containing 334 patients,23,3,2020,https://www.mathworks.com/; https://github.com/zheng120/PVCVTECGDenoising,The MATALB () program for ECG denoising is put under .,2020-03-23
0,Scientific Data,41597,10.1038/s41597-020-00631-x,"PPDIST, global 0.1° daily and 3-hourly precipitation probability distribution climatologies for 1979–2018",11,9,2020,,"The neural networks used to produce the PPDIST dataset were implemented using the MLPRegressor class of the scikit-learn Python module. The  occurrence for different thresholds was calculated using the percentileofscore function of the scipy Python module, whereas the  magnitudes for different return periods were calculated using the percentile function of the numpy Python module. The other codes are available upon request from the first author. The predictor, IMERG, and ERA5 data are available via the URLs listed in Table . Most of the gauge observations are available via the URLs provided in the “Gauge observations and quality control” subsection. Part of the GSDR database and some of the national databases are only available upon request.",2020-09-11
0,Scientific Data,41597,10.1038/s41597-020-00720-x,Estimation of global tropical cyclone wind speed probabilities using the STORM dataset,10,11,2020,https://github.com/NBloemendaal/STORM-return-periods; https://github.com/NBloemendaal/STORM; https://github.com/NBloemendaal/STORM-preprocessing,All Python algorithms used to generate the datasets described here are publicly available via . The STORM Python script can be found on  and the data-preprocessing is available via .,2020-11-10
0,Scientific Data,41597,10.1038/s41597-020-0481-z,"GaitRec, a large-scale ground reaction force dataset of healthy and impaired gait",12,5,2020,,,2020-05-12
0,Scientific Data,41597,10.1038/s41597-020-00627-7,An Asian-centric human movement database capturing activities of daily living,8,9,2020,,,2020-09-08
0,Scientific Data,41597,10.1038/s41597-020-00672-2,Data for training and testing radiation detection algorithms in an urban environment,5,10,2020,,"The position- and energy-dependent flux data were generated with MAVRIC, a serial-only code in the SCALE 6.2 package. Some custom codes were developed to make the mesh-based sources outside of MAVRIC, but these codes are not available. However, the methods used in these custom codes have been adopted by Shift, a new parallel Monte Carlo code, which will be released in the next version of SCALE. Shift will be able to use the same geometry and materials as MAVRIC and perform similar calculations.",2020-10-05
0,Scientific Data,41597,10.1038/s41597-020-0570-z,"A three-dimensional, population-based average of the C57BL/6 mouse brain from DAPI-stained coronal slices",13,7,2020,https://doi.org/10.12751/g-node.16wrxa,"All the code used for the pre-processing, brain reconstruction, and template creation can be found in the repository () along with the code for automatic slice segmentation.",2020-07-13
0,Scientific Data,41597,10.1038/s41597-020-0410-1,A socio-environmental geodatabase for integrative research in the transboundary Rio Grande/Río Bravo basin,6,3,2020,,We created the individual output datasets using geoprocessing scripts in the Python programming language and the arcpy package. All geoprocessing scripts are available on GitHub and are accessible through the Open Science Framework (OSF) repository.,2020-03-06
0,Scientific Data,41597,10.1038/s41597-020-00703-y,Reference exome data for a Northern Brazilian population,21,10,2020,,,2020-10-21
0,Scientific Data,41597,10.1038/s41597-020-0508-5,A clinically and genomically annotated nerve sheath tumor biospecimen repository,19,6,2020,http://github.com/sage-bionetworks/JHU-biobank,"A Github repository () con tains the codes required to generate the figures with a versioned repository available at Zenodo. The tutorials are provided in R and Python languages, contained in the r_demos and py_demos directories respectively. All of the analytical code is provided in the directory marked “analysis”. Additionally, we have provided Docker containers and R scripts to facilitate reproducibility of the figures in the paper.",2020-06-19
0,Scientific Data,41597,10.1038/s41597-020-00655-3,"TILES-2018, a longitudinal physiologic and behavioral data set of hospital workers",16,10,2020,https://github.com/usc-sail/tiles-dataset-release,"All code for collecting, formatting, processing, and learning on the data is made freely available at . Information about the code dependencies and package requirements are available in the same Github repository.",2020-10-16
0,Scientific Data,41597,10.1038/s41597-020-00594-z,"A comprehensive CHO SWATH-MS spectral library for robust quantitative profiling of 10,000 proteins",11,8,2020,,,2020-08-11
0,Scientific Data,41597,10.1038/s41597-020-00638-4,"AiiDA 1.0, a scalable computational infrastructure for automated reproducible workflows and data provenance",8,9,2020,github.com/aiidateam/aiida-core; pypi.org/project/aiida-core,The source code of AiiDA is released under the MIT open-source license and is made available on GitHub (). It is also distributed as an installable package through the Python Package Index ().,2020-09-08
0,Scientific Data,41597,10.1038/s41597-019-0104-8,"EEG-BIDS, an extension to the brain imaging data structure for electroencephalography",25,6,2019,,,2019-06-25
0,Scientific Data,41597,10.1038/s41597-019-0010-0,MHz data collection of a microcrystalline mixture of different jack bean proteins,3,4,2019,,"[{'ext-link': {'@xlink:href': 'https://gitlab.gwdg.de/p.lfoucar/cass', '@ext-link-type': 'uri', '#text': 'https://gitlab.gwdg.de/p.lfoucar/cass'}, '#text': 'Data were processed with CrystFEL 0.6.3. CrystFEL 0.6.3 is a free open source software under the GNU Public License version 3 and can be downloaded from http://www.desy.de/~twhite/crystfel/. CASS is publicly available on GitLab ().'}, {'ext-link': {'@xlink:href': 'https://github.com/tbarends/pygeom', '@ext-link-type': 'uri', '#text': 'https://github.com/tbarends/pygeom'}, '#text': 'The script used to optimize detector geometry is publicly available on GitHub ().'}]",2019-04-03
0,Scientific Data,41597,10.1038/s41597-019-0158-7,A global view on the effect of water uptake on aerosol particle light scattering,22,8,2019,,"[{'italic': 'T', '#text': 'The Matlab code used to generate this dataset is available from the corresponding authors upon request. The repository contains 26 different scripts due to the site dependent characteristics. These scripts read the WetNeph and DryNeph raw files as well as the /RH sensor files. The sample RH is established for each particular case as explained in this text, and flags for size cut and valid/invalid measurements are also set taking into consideration each particular case.'}, {'italic': ['σ', 'σ', 'σ', 'σ'], 'sub': ['sp', 'dry', 'bsp', 'dry', 'sp', 'wet', 'bsp', 'wet'], '#text': 'The user can find detailed explanations of the various corrections applied to the raw measurements in the literature as cited in this paper. The codes apply the corrections to the raw measurements, obtaining the resulting corrected quantities for (RH), (RH), (RH) and (RH).'}, {'italic': 'f', '#text': 'The user can use this dataset to apply their own methodology for obtaining (RH) and/or applying empirical regressions to the measured humidograms.'}]",2019-08-22
0,Scientific Data,41597,10.1038/s41597-019-0057-y,Flow and detailed 3D morphodynamic data from laboratory experiments of fluvial dike breaching,13,5,2019,,The laser profilometry technique algorithm was developed on Matlab 2014b. Further details on the geometry 3D reconstruction and the algorithm structure are given by Rifai . and Rifai.,2019-05-13
0,Scientific Data,41597,10.1038/sdata.2019.16,"A dynamic view of the proteomic landscape during differentiation of ReNcell VM cells, an immortalized human neural progenitor line",19,2,2019,,,2019-02-19
0,Scientific Data,41597,10.1038/s41597-019-0145-z,Linking  MS/MS spectra with chemistry data to improve identification of unknowns,2,8,2019,http://sourceforge.net/projects/cfm-id; https://github.com/USEPA/CFM-ID_generation_of_CompTox_Chemicals_Dashboard_Structures_Paper,All code for predicting the MS/MS spectra including model parameters and settings are available via . Additional scripts used to implement the prediction algorithm and query the compiled database are available on GitHub ().,2019-08-02
0,Scientific Data,41597,10.1038/s41597-019-0199-y,Time series of heat demand and heat pump efficiency for energy system modeling,1,10,2019,,"[{'ext-link': {'@xlink:href': 'https://github.com/oruhnau/when2heat', '@ext-link-type': 'uri', '#text': 'https://github.com/oruhnau/when2heat'}, '#text': 'All code is implemented in Python and published at  under the open MIT license. The entire processing workflow is documented in a single Jupyter Notebook (processing.ipynb), which draws on custom functions that are structured in different Python scripts (download.py, read.py, preprocess.py, demand.py, cop.py, write.py, metadata.py, misc.py).'}, {'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR7', '#text': '7'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR8', '#text': '8'}}], '#text': 'While the download of weather and population data is automated, the input data from the EU Building Database, BGW, and BDEW, as well as the COP curve parameters are included in the code repository.'}]",2019-10-01
0,Scientific Data,41597,10.1038/s41597-019-0288-y,"Global sea-surface iodide observations, 1967–2018",26,11,2019,https://doi.org/10.5281/zenodo.3271678,"The Python code used to prepare the archived data, and to enable incorporation of any subsequent observational data files, has also been made permanently available ().",2019-11-26
0,Scientific Data,41597,10.1038/sdata.2019.39,A dataset of neonatal EEG recordings with seizure annotations,5,3,2019,,,2019-03-05
0,Scientific Data,41597,10.1038/s41597-019-0275-3,"flEECe, an energy use and occupant behavior dataset for net-zero energy affordable senior residential buildings",26,11,2019,,An example of the Python code used to analyze the raw energy monitor data is publicly available on the Open Science Framework data repository. The data can also be analyzed using software which handles tabular timeseries data such as R or MATLAB. The code used to aggregate and plot the data for this paper are publicly available on the data repository on OSF.,2019-11-26
0,Scientific Data,41597,10.1038/sdata.2019.2,"Qresp, a tool for curating, discovering and exploring reproducible scientific papers",29,1,2019,,,2019-01-29
0,Scientific Data,41597,10.1038/s41597-019-0306-0,Comparative dataset of experimental and computational attributes of UV/vis absorption spectra,5,12,2019,https://github.com/edbeard/chemdataextractor-uvvis2018; http://chemdataextractor.org/download; https://doi.org/10.6084/m9.figshare.7619672.v2; https://github.com/alvarovm/qmwf,"The compound data were extracted from the scientific literature using a UV/vis absorption spectroscopy tailored version of ChemDataExtractor, which is available at . A clean build of the current release of ChemDataExtractor version 1.3 can be found at . The scripts used to filter the data to leave chemically valid compounds are available alongside the database at  in the ‘scripts.zip’ directory. Scripts used for the QMWF pipeline in Stage II can be found at .",2019-12-05
0,Scientific Data,41597,10.1038/s41597-019-0341-x,"The Database of Cross-Linguistic Colexifications, reproducible analysis of cross-linguistic polysemies",13,1,2020,,"The workflow by which CLDF datasets are analyzed and published within the CLICS framework, is available as a testable virtual container that can be freely used on-line in the form of a .",2020-01-13
0,Scientific Data,41597,10.1038/s41597-019-0073-y,"The open diffusion data derivatives, brain data upcycling via integrated publishing of derivatives and reproducible open cloud services",23,5,2019,,,2019-05-23
0,Scientific Data,41597,10.1038/s41597-019-0290-4,A large-scale dataset for mitotic figure assessment on whole slide images of canine cutaneous mast cell tumor,21,11,2019,https://github.com/maubreville/MITOS_WSI_CCMCT/,All code used in the experiments described in the manuscript was written in Python 3 and is available through our GitHub repository (). We provide all necessary libraries as well as Jupyter Notebooks allowing tracing of our results. The code is based on fast.ai and OpenSlide and provides some custom data loaders for use of the dataset.,2019-11-21
0,Scientific Data,41597,10.1038/s41597-019-0334-9,Expansion of the cassava brown streak pandemic in Uganda revealed by annual field survey data for 2004 to 2017,18,12,2019,,"The data cleaning, summarizing, merging and supplementing with additional columns were done in Python and R and the custom code used for this project can be provided upon request.",2019-12-18
0,Scientific Data,41597,10.1038/s41597-019-0038-1,Statistically downscaled climate dataset for East Africa,15,4,2019,https://sdsm.org.uk/software.html,"SDSM version 4.2, freely available (), is used to statically downscale the projection from the second generation Canadian Earth System Model (CanESM2). The predictors derived from CanESM2 and the NCEP reanalysis data are exported into SDSM directory for model calibration and projection. The CanESM2 is one of the GCMs used in the Coupled Model Inter-comparison Project Phase 5 (CMIP5). A free code written in R (mean-R.txt) is provided to compute the ensembles mean for a single predictand.",2019-04-15
0,Scientific Data,41597,10.1038/s41597-019-0286-0,"Experiment design driven FAIRification of omics data matrices, an exemplar",12,12,2019,,,2019-12-12
0,Scientific Data,41597,10.1038/s41597-020-0354-5,United States wildlife and wildlife product imports from 2000–2014,16,1,2020,https://github.com/ecohealthalliance/lemis,"Our custom R package, which provides access to the data described here, is publicly available at . Installation of the package and subsequent download of the data enables efficient, on-disk manipulation of the entire cleaned dataset. Basic package usage is outlined in the main package README file on the GitHub site. The code implementation of the data cleaning process is also available in the package codebase (via the ‘data-raw’ directory) and is outlined in the associated developer README file. These scripts span the entirety of our data processing and cleaning workflow, from importation and collation of the raw USFWS LEMIS data files through to generation of the single, cleaned data file as discussed in this manuscript. Thus, the scripts serve as transparent, reproducible documentation of our data processing in full.",2020-01-16
0,Scientific Data,41597,10.1038/s41597-019-0273-5,Longitudinal dataset of human-building interactions in U.S. offices,26,11,2019,,"['All MATLAB scripts used to implement the post-processing steps described in the Data Records section are available upon request, as are the raw data files described in steps 1 and 2 (e.g., pre- and post- cleaning). Interested readers should contact the corresponding author for access to the scripts so that further instructions on their use may be provided.', {'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR27', '#text': '27'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR29', '#text': '29'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR30', '#text': '30'}}], '#text': 'Additionally, links to the background survey instrument, daily survey instrument, and retrospective survey instrument conducted after each two week daily surveying period are available for testing.'}]",2019-11-26
0,Scientific Data,41597,10.1038/s41597-019-0171-x,A multi-omics digital research object for the genetics of sleep regulation,31,10,2019,https://gitlab.unil.ch/mjan/Systems_Genetics_of_Sleep_Regulation,"The scripts used for analytics were made available on gitlab (). The master branch contains the scripts used for our publication and mm9 analysis. A second branch was created for analysis performed on a mm10 mouse references (see Technical Validation). The intermediate files required to run these scripts were made available at Figshare. Finally, a documentation file was generated documenting the hierarchical relationship between the scripts and datasets in a form of a dynamic html document (see Workflow documentation).",2019-10-31
0,Scientific Data,41597,10.1038/s41597-019-0113-7,fMRI data of mixed gambles from the Neuroimaging Analysis Replication and Prediction Study,1,7,2019,www.psychtoolbox.org; https://github.com/rotemb9/NARPS_scientific_data,"The code for the mixed gambles task were adopted from a previous study. All codes were executed using Matlab version 2014b and the Psychtoolbox (). Code is available on GitHub (). Preprocessed data included in the dataset were preprocessed with fMRIprep version 1.1.4, available from fmriprep.org The quality assessment reports were generated with MRIQC version 0.14.2, available from mriqc.org.",2019-07-01
0,Scientific Data,41597,10.1038/s41597-019-0087-5,Activity/exercise-induced changes in the liver transcriptome after chronic spinal cord injury,13,6,2019,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR41', '#text': '41'}}, '#text': 'All analyses were performed using open sources software tools. Raw sequencing files were downloaded from Illumina BaseSpace using the Illumina Python Run Downloader. Individual samples were initially divided across four lanes for sequencing, and these files were concatenated into one single end Fastq file using the UNIX cat command.'}, {'italic': ['FN1', 'FN2', 'FN3', 'FN4', 'COND_REP'], '#text': 'cat <>.fastq <>.fastq <>.fastq <>.fastq> <>.fastq'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR26', '#text': '26'}}, '#text': 'The concatenated sequences were input to FastQC v.0.10.1 for quality control analysis using default parameters.'}, {'italic': 'COND_REP', '#text': 'fastqc <>.fastq –o <FASTQC_DIRECTORY>'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR27', '#text': '27'}}, '#text': 'Fastq files were input to Star 2.6 for alignment specifying BAM file format sorted by coordinate and requesting unmapped read files.'}, {'italic': ['COND_REP', 'COND_REP'], '#text': 'STAR–runMode alignReads–outSAMtype BAM SortedByCoordinate –outSAMstrandField intronMotif–outReadsUnmapped Fastx–readFilesIn <>.fastq.gz –outFileNamePrefix <>–runThreadN 16–genomeDir Rnor_6.0 –readFilesCommand zcat'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR28', '#text': '28'}}, 'italic': 'reverse', '#text': 'Read counts for each sample were extracted using HTSeq 0.10.0. The  option was used to indicate strand orientation. Illumina’s TruSeq Stranded mRNA protocol was used to sequence the data and produces libraries where the first read is on the opposite strand to the RNA molecule.'}, {'italic': ['COND_REP', 'COND_REP'], '#text': 'htseq-count -f bam–stranded=reverse–mode=intersection-nonempty -r name <>/Aligned.sortedByCoord.out.bam\\Rattus_norvegicus.Rnor_6.0.93_PARSED.gtf> <>/gene_counts_Reversed.htseq'}, {'sup': [{'xref': [{'@ref-type': 'bibr', '@rid': 'CR31', '#text': '31'}, {'@ref-type': 'bibr', '@rid': 'CR32', '#text': '32'}], '#text': ','}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR42', '#text': '42'}}], 'italic': 'estimateSizeFactors', '#text': 'The raw counts were normalized using DESeq2’s default procedure, relative log expression (RLE), using the  function. Detailed instructions can be found on the Bioconductor website for DESeq2.'}]",2019-06-13
0,Scientific Data,41597,10.1038/s41597-019-0044-3,"Harmonising topographic & remotely sensed datasets, a reference dataset for shoreline and beach change analysis",26,4,2019,https://laszip.org/,"The datasets included in this paper were manipulated using ESRI ArcGIS v10.2 and later versions. All the ArcMap Tools referred to in the Data Usage section are available in version 10.2 up to the current version, 10.6. The digital shoreline analysis was performed using the open access R-package, AMBUR. LiDAR point cloud manipulation was performed using the open access software, lasizp ().",2019-04-26
0,Scientific Data,41597,10.1038/s41597-019-0304-2,"Eyasi Plateau Paleontological Expedition, Laetoli, Tanzania, fossil specimen database 1998–2005",3,12,2019,https://github.com/paleocore/paleocore110/blob/master/eppe/import_1998_2005.py,"Data were imported into the Paleo Core data repository using Python (version 3.6) standard libraries (re, datetime, pytz), the xlrd library (version 1.20) for reading Excel files, and the database API included with the Django web framework (version 1.11.20). All of the original source code used to process the data are freely and publicly available through the Paleo Core github repository at: .",2019-12-03
0,Scientific Data,41597,10.1038/s41597-019-0202-7,"STATegra, a comprehensive multi-omics dataset of B-cell differentiation in mouse",31,10,2019,,"Preprocessing scripts for each of the omics datasets, together with relevant intermediate files, are available at the STATegraData GitHub repository. Preprocessing scripts collect in one.txt file all code and parameters required to transform raw data files into one consolidated data matrix with  quantitative data, where samples are arranged in columns and features are arranged in rows. Script files may contain code for multiple programming languages or simply list parameters used in commercial software when applicable.",2019-10-31
0,Scientific Data,41597,10.1038/s41597-019-0048-z,High-resolution global urban growth projection based on multiple applications of the SLEUTH urban growth model,18,4,2019,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR17', '#text': '17'}}, 'ext-link': {'@xlink:href': 'https://www.r-project.org/', '@ext-link-type': 'uri', '#text': 'https://www.r-project.org/'}, '#text': 'The R code developed for preparing the inputs to the SLEUTH model and integrating the modelling results into global maps is publicly and freely available. The code consists of two R programming language scripts (version R 3.4.3; ), which prepare simulation inputs and integrate outputs. The script is internally documented to assist understanding and customisation for further use.'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR17', '#text': '17'}}, 'ext-link': {'@xlink:href': 'https://pypi.org/project/sleuth-automation/', '@ext-link-type': 'uri', '#text': 'https://pypi.org/project/sleuth-automation/'}, '#text': 'We have also shared the modified SLEUTH model, as well as the scenario.jinja file in the Python package sleuth-automation (version 1.0.2; ).'}]",2019-04-18
0,Scientific Data,41597,10.1038/s41597-019-0167-6,Sixteen years of bathymetry and waves at San Diego beaches,29,8,2019,,"Code is written in MATLAB (R2018b). Although MATLAB is a proprietary language, the.m files can be read with a text viewer.",2019-08-29
0,Scientific Data,41597,10.1038/s41597-020-0382-1,A multilayer and spatial description of the Erasmus mobility network,6,2,2020,,"['The following R and Python codes used to process the cleaning and datasets can be accessed along with their data without any restrictions.', {'monospace': '1-erasmus_edgelist_merge_clean.Rmd', '#text': 'The  shows how datasets were cleaned and standardized. The file can be read using the freely available computer program RStudio and contains R codes of data wrangling in the form of chunks and comments about the processing steps all the way from the open datasets to the merged and cleaned data.'}, {'monospace': '2-findplacefromtext.py', '#text': 'The program  helped to harvest the geocodes of the HEIs from the POI database. HEIs were also recorded as POIs in the database, so their names, addresses and coordinates could be collected. The power of the database was proven by the historical institutional data.'}, {'monospace': '3-POI_Here_neighbour.py', '#text': 'The geographically nearest POIs, e.g. pubs and museums, to the HEIs were identified by the programme .'}, {'monospace': ['4-merge_eter_DB.Rmd', '5-merge_grid_DB.Rmd'], '#text': 'Based on the geocoordinates of the HEIs, the nearest ETER and GRID institutions that included university type institutes were matched using files  and , respectively. The aim was to identify matching Erasmus and ETER as well as GRID institutions.'}, {'monospace': '6-data_validation.Rmd', '#text': 'The validation determined using R are shown in the file . All the calculations made according to the published database can be seen in the Rmd file.'}, {'monospace': ['Network on map', '6-data_validation.Rmd', '7-network_on_map.gephi'], 'xref': {'@rid': 'Fig3', '@ref-type': 'fig', '#text': '3'}, '#text': 'The Gephi software, which is popular among network scientists, can also read the data. The  subsection in the file  includes the data preparation steps to read three subject layers, moreover, maps in Fig.\xa0 are provided with the help of the file .'}]",2020-02-06
0,Scientific Data,41597,10.1038/s41597-019-0332-y,Highly multiplexed immunofluorescence images and single-cell data of immune markers in tonsil and lung cancer,17,12,2019,https://github.com/jmuhlich/ashlar; https://github.com/BodenmillerGroup/histoCAT,All code used to process and generate the data in this study can be found alongside the dataset. A description of each script is provided in Online-only Table . Source code for ASHLAR is available on GitHub (). The newest histoCAT version can also be found GitHub ().,2019-12-17
0,Scientific Data,41597,10.1038/s41597-019-0097-3,"2DMatPedia, an open computational database of two-dimensional materials from top-down and bottom-up approaches",12,6,2019,http://pymatgen.org/; https://atomate.org,"The code used in this work relies heavily on the open-source tools developed by Materials Project, in particular, Python Materials Genomics () and Atomate (). The versions used in this work are pymatgen/4.7.3 and atomate/0.5.1 for all the calculations.",2019-06-12
0,Scientific Data,41597,10.1038/sdata.2018.308,"A mind-brain-body dataset of MRI, EEG, cognition, emotion, and peripheral physiology in young and old adults",12,2,2019,,,2019-02-12
0,Scientific Data,41597,10.1038/s41597-019-0328-7,Pan-European groundwater to atmosphere terrestrial systems climatology from a physically consistent simulation,16,12,2019,https://www.terrsysmp.org; https://github.com/parflow/parflow; http://www.cgd.ucar.edu/tss/clm/distribution/clm3.5/index.html; http://www.cosmo-model.org/content/support/software/default.htm,"Stable release versions of TSMP are provided through a git development repository available at the model’s website (). The release version includes extensive instructions for installing the system, including sample reference test cases for typical application examples, as well as a suite of pre-processing and post-processing tools. TSMP is essentially released without its component models, i.e., the release contains the built system, all configuration files, such as namelists, for the sample cases, the component model code patches and all coupler related modifications. The user must download the component models from their respective separate repositories: All ParFlow releases are available via GitHub (). The official CLM website () offers all links to documentation, source code, and input data for the stand-alone version release of CLM as used in this study. The COSMO model is available only after registration (cosmo-licence@cosmo-model.org) and is also free of charge for research applications. More information on the procedure and licensing terms are available at the COSMO model website (). It must be noted that the TSMP model system supports various combinations of different component model versions; e.g. ParFlow only, ParFlow-CLM, CLM-COSMO, ParFlow-CLM-COSMO.",2019-12-16
0,Scientific Data,41597,10.1038/s41597-019-0019-4,"Synthesis, optical imaging, and absorption spectroscopy data for 179072 metal oxides",27,3,2019,,,2019-03-27
0,Scientific Data,41597,10.1038/s41597-019-0060-3,Urban link travel speed dataset from a megacity road network,16,5,2019,,"[{'italic': 'et al', 'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR26', '#text': '26'}}, {'xref': [{'@ref-type': 'bibr', '@rid': 'CR26', '#text': '26'}, {'@ref-type': 'bibr', '@rid': 'CR27', '#text': '27'}], '#text': ','}], '#text': 'We cannot provide access to the raw source data due to their proprietary nature. As mentioned in Step 1 of the Methods section, the source data mainly contain a total of 3.01 billion GPS trajectory samples produced by more than 12,000 taxis during 45 days. As stated by Poulis ., the publication of the trajectories of personal movement could lead to identity disclosure, even if directly identifying information (e.g., names of taxi drivers and passengers) is not published. Moreover, existing trajectory anonymization techniques cannot be used in our research because existing techniques do not care about travel speeds in trajectories and do not need the information of taxi status. However, to obtain the travel speed dataset accurately, we have to use the information of taxi status (as described in Step 3) to indicate when each taxi picks up or drops off passengers.'}, 'Python (version 2.7.12) is used to produce the link travel dataset in this research. We have not shared the code because the code is dedicatedly designed for our raw source data and researchers cannot benefit from the code without the source data. Meanwhile, the code might reveal the identity of taxi drivers and raw real-time trajectory information of taxis in the road network. However, the code is straightforward, and its steps have been described in detail in the section of ‘Methods’. It is easy for a third party to exactly repeat the method.']",2019-05-16
0,Scientific Data,41597,10.1038/s41597-019-0350-9,"transcriptome assembly and annotation for gene discovery in avocado, macadamia and mango",8,1,2020,,"[{'bold': 'Trimmomatic v. 0.35 parameters:'}, {'italic': 'trimmomatic-0.35.jar PE -phred33 in_forward.fq.gz in_reverse.fq.gz out_forward_paired.fq.gz out_forward_unpaired.fq.gz out_reverse_paired.fq.gz out_reverse_unpaired.fq.gz ILLUMINACLIP: TruSeq3-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36'}, {'bold': 'HISAT2 v 2.1.0 parameters:'}, {'italic': 'hisat2-build reference_index_name genome.fa'}, {'italic': 'hisat2 –x reference_index -1 reads_1a.fq,reads_1b.fq, reads_1c.fq,reads_1d.fq -2 reads_2a.fq,reads_2b.fq,reads_2c.fq,reads_2d.fq -S output.sam'}, {'bold': 'SamTools v. 1.9.0 parameters:'}, {'italic': 'samtools view -b -o output.bam samfile_from_hisat2.sam'}, {'italic': 'samtools sort -o sorted.bam output.bam'}, {'italic': ['samtools depth sorted.bam', 'awk ‘{sum', '3} END', 'print “Average', '”,sum', 'NR'], '#text': '| +=$ {\u2009=\u2009/}’'}, {'bold': 'Trinity v. 2.7.0 parameters:'}, {'italic': ['Trinity', 'seqType fq', 'left reads_1a.fq,reads_1b.fq,reads_1c.fq,reads_1d.fq', 'right reads_2a.fq,reads_2b.fq,reads_2c.fq,reads_2d.fq', 'CPU 6', 'max_memory 20G'], '#text': '-- -- -- -- --'}, {'bold': 'CD-HIT-EST v. 4.8.1 parameters:'}, {'italic': 'cd-hit-est –i trinity_transcripts.fasta –o output file –c 0.9', '#text': '.'}, {'bold': 'TransDecoder v.5.5.0 parameters:'}, {'italic': 'TransDecoder.LongOrfs -t cd-hit-est__0.95_transcripts.fasta'}, {'bold': 'BUSCO v. 3 parameters:'}, {'italic': 'python BUSCO.py -i unigenes -l OrthoDB v9 -o output_name'}, {'bold': 'BLAST v. 2.7.1 parameters:'}, {'italic': 'makeblastdb -in reference_trancriptome assembly.fasta -dbtype “nucl”'}, {'italic': 'blastn -query unigenes.fasta -db reference_trancriptome assembly.fasta -out outputfile.txt -evalue 1e-5 -max_target_seqs. 20 -outfmt 6'}, {'italic': 'makeblastdb -in -in uniprot_sprot.fasta -dbtype “prot”'}, {'italic': 'blastx -query unigenes.fasta -db uniprot_sprot.fasta -out outputfile.txt -evalue 1e-3 -max_target_seqs. 20 -outfmt 6', '#text': '.'}]",2020-01-08
0,Scientific Data,41597,10.1038/s41597-019-0200-9,"SEEDS, simultaneous recordings of high-density EMG and finger joint angles during multiple hand movements",30,9,2019,,"['The preprocessing and normalisation of the EMG and kinematics data, as well as the synchronisation between them were performed using custom Python code, which can be obtained from the authors upon request.', {'monospace': 'functions', '#text': 'The Python script used for the classification of movements (see the Technical Validation section) can be found in a folder called .'}]",2019-09-30
0,Scientific Data,41597,10.1038/s41597-019-0021-x,A database seed for a community-driven material intensity research platform,9,4,2019,,,2019-04-09
0,Scientific Data,41597,10.1038/s41597-020-0380-3,"Gaze, visual, myoelectric, and inertial data of grasps for intelligent prosthetics",10,2,2020,,"The post-processing procedure described above was implemented in Matlab and executed in compiled form with Matlab 2016b. The relabeling procedure was implemented in Python and interpreted with Python 3.6. Censoring of the videos was done using a custom Bash script that interfaced with the FFmpeg 4.1.3 video manipulation tool. The GNU Parallel tool was used to run jobs in parallel. A copy of the code that was used to create the data records from the raw recordings is publicly available in the . archive within the MDSScript. This archive also contains a  file that lists and describes the main processing steps. Although the original data files cannot be released for privacy reasons, the code is made available to allow the user to thoroughly investigate and comprehend the procedure we used to process the data as well as to provide the community with a tool that can be re-used and re-adapted for similar tasks.",2020-02-10
0,Scientific Data,41597,10.1038/s41597-019-0152-0,"HENA, heterogeneous network-based data set for Alzheimer’s disease",14,8,2019,,"Data integration and analysis were performed using R language for statistical computing version 3.0.2 (2013-05-16), R version 3.3.1 (2016-06-21), R version 3.4.2 (2017-09-18). The case study was built using Python 3.6. The project repository is accessible via GitHub HENA repository and accompanied by DOI.",2019-08-14
0,Scientific Data,41597,10.1038/s41597-019-0241-0,Distributed radiomics as a signature validation study using the Personal Health Train infrastructure,22,10,2019,,"[{'ext-link': {'@xlink:href': 'https://gitlab.com/UM-CDS/distributedradiomics', '@ext-link-type': 'uri', '#text': 'https://gitlab.com/UM-CDS/distributedradiomics'}, '#text': 'The code used in this study is made publicly available on the Maastricht University Clinical Data Science (UM-CDS) GitLab repository (). The code repository has the following organization:'}, 'a.\u2003D2RQ folder: contains the raw feature value to RDF mapping (D2RQ) script and the SPARQL query used to retrieve the local data into the local VLP connector application.', 'b.\u2003VLP folder: contains the MATLAB codes submitted by the user into VLP, which then transmits it to the participating site for model validation and analysis.', 'c.\u2003Analysis Centralized Learning folder: contains the Jupyter notebook from Radboudumc for model development and evaluation on the aggregated datasets.', {'ext-link': {'@xlink:href': 'https://bioportal.bioontology.org/ontologies/RO', '@ext-link-type': 'uri', '#text': 'https://bioportal.bioontology.org/ontologies/RO'}, '#text': 'The open-access Radiomics Ontology (RO) is published via the National Center for Biomedical Ontology (NCBO) ontology registry. It is available to download in a range of formats from the following URL: . As a domain ontology, the RO defines histogram-based, morphology-based and texture-based radiomic features, including (since v.1.6, 08 November 2018) all feature entities presented in the International Biomarker Standardization Initiative. The ontology also defines software properties, digital imaging filter operations and feature extraction settings, together with relational predicates to link these to each feature entity.'}]",2019-10-22
0,Scientific Data,41597,10.1038/s41597-020-0363-4,Four high-quality draft genome assemblies of the marine heterotrophic nanoflagellate,21,1,2020,,"[{'ext-link': [{'@xlink:href': '10.5281/zenodo.355113321', '@ext-link-type': 'doi', '#text': 'https://doi.org/10.5281/zenodo.355113321'}, {'@xlink:href': 'http://github.com/thackl/cr-genomes', '@ext-link-type': 'uri', '#text': 'http://github.com/thackl/cr-genomes'}], '#text': 'All custom code used to generate and analyze the data presented here is available from  and from .'}, {'bold': 'Software versions and relevant parameters:'}, 'Trimmomatic v0.32 (ILLUMINACLIP:TruSeq3-PE.fa:2:30:10 SLIDINGWINDOW:10:20 MINLEN:75 LEADING:3 TRAILING:3); proovread v2.12 (config settings: ‘seq-filter’ => {‘–trim-win’ => ‘10,1’, ‘–min-length’ => 500}, ‘sr-sampling’ => {DEF => 0}); Canu v1.8; Flye v2.3.7; WTDBG v2.1; SPAdes v3.6.1 (–diploid); minimap2 v2.13-r858-dirty (PacBio reads: -x map-pb; MiSeq readsL -x sr); bam-junctions SHA: 28dc943 (-a200 -b200 -c5 -d2 -f30 -e30); Redundans v0.14a (–noscaffolding –norearrangements –nogapclosing); Kaiju v1.6.3 (-t kaijudb/nodes.dmp -f kaijudb/kaiju_db_nr_euk.fmi); Prokka v1.13 (–kingdom Mitochondria –gcode 4); DEXTRACTOR rev-844cc20; jellyfish v2.2.4; samtools v1.7; Racon v1.3.1; BUSCO v3.1.0; WindowMasker 1.0.0; tRNAscan v2.0; BRAKER v.2.1.1; BLAST v.2.6.0+; Augustus v3.3.2; GeneMark-ES v.4.38; HISAT v.2.1.0; MAFFT v7.310; trimAl v1.4.rev22 (-strictplus); RAxML v8.2.9 (-p 13178 -f a -x 13178 -N 100 -m PROTGAMMAWAG -q part.txt); phytools v0.6-60; ggtree v1.14.4; barrnap (–kingdom euk); bcftools v1.9 (mpileup; call -mv -Ob);']",2020-01-21
0,Scientific Data,41597,10.1038/s41597-019-0326-9,A global database of historic and real-time flood events based on social media,9,12,2019,https://github.com/jensdebruijn/Global-Flood-Monitor,"The datasets generated have been created using code for Python 3.6, Elasticsearch 6.6 and PostgreSQL 10.6. The code is available through .",2019-12-09
0,Scientific Data,41597,10.1038/sdata.2019.12,Characterization of deep neural network features by decodability from human brain activity,12,2,2019,,,2019-02-12
0,Scientific Data,41597,10.1038/s41597-019-0224-1,Text-mined dataset of inorganic materials synthesis recipes,15,10,2019,https://github.com/CederGroupHub/text-mined-synthesis_public; www.tensorflow.org; keras.io; spacy.io; radimrehurek.com; scikit-learn.org; chemdataextractor.org,"The scripts utilized to classify paragraphs and extract recipes as well as to perform the data analysis are home-written codes which are publicly available at the github repository  with acknowledgement of the current paper. The underlying machine-learning libraries used in this project are all open-source:  (),  (),  (),  () and  () ().",2019-10-15
0,Scientific Data,41597,10.1038/s41597-020-0359-0,Reference tool kinematics-kinetics and tissue surface strain data during fundamental surgical acts,15,1,2020,https://simtk.org/projects/multis; https://docs.anaconda.com/anaconda/,"A source code repository is available at (), which provides scripts to parse the data files, create data overview plots, and register surgical tools data to the image (MRI or CT) coordinate systems. Install Python 2.7 with Anaconda () and the npTDMS library (reading LabVIEW data files). Additional libraries can be installed using Anaconda (“conda install” command). A static download package is also available containing all raw and derivative data pertinent to the current study.",2020-01-15
0,Scientific Data,41597,10.1038/s41597-019-0080-z,High-throughput calculations of catalytic properties of bimetallic alloy surfaces,28,5,2019,https://github.com/SUNCAT-Center/; https://github.com/SUNCAT-Center/CatHub/tree/master/tutorials/1_bimetallic_alloys/; https://github.com/SUNCAT-Center/CatHub/tree/master/cathub/classification.py,"The CatHub python API and the CatKit software packages are available open-source from the GitHub repository at . In addition, the latest stable version of the CatHub module is available from the Zenodo repository. The Python scripts used for plotting the data shown in Fig.  is made available as a tutorials at . The code used to classify the adsorption sites is made available at .",2019-05-28
0,Scientific Data,41597,10.1038/s41597-019-0084-8,"Mapping global development potential for renewable energy, fossil fuels, mining and agriculture sectors",27,6,2019,www.esri.com,"For DPI replication and integration of potential future data updates, we provide via figshare all Python scripts and spatial data necessary to replicate each DPI. Because these DPI scripts require input criteria data totaling 65 GBs in size, we bundled both scripts and data into four compressed files, DPI_InputsAndCode_Part01-04.zip. These scripts use ArcPy, a Python module associated with ESRI’s ArcGIS Desktop software () and require both the Advance license of this mapping software and an accompanying Spatial Analyst extension license. A  accompanies these zip files and provides all necessary instructions to setup the database and run any of the DPI scripts.",2019-06-27
0,Scientific Data,41597,10.1038/sdata.2019.35,High resolution annual average air pollution concentration maps for the Netherlands,12,3,2019,,,2019-03-12
0,Scientific Data,41597,10.1038/s41597-019-0105-7,"iEEG-BIDS, extending the Brain Imaging Data Structure specification to human intracranial electrophysiology",25,6,2019,,,2019-06-25
0,Scientific Data,41597,10.1038/s41597-019-0150-2,"MOD-LSP, MODIS-based parameters for hydrologic modeling of North American land cover change",9,8,2019,https://github.com/tbohn/VIC_Landcover_MODIS_NLCD_INEGI/releases/tag/v1.6; https://github.com/tbohn/NLCD_INEGI/releases/tag/v1.6; https://github.com/UW-Hydro/tonic/releases/0.2,"The MOD-LSP parameter sets were created via Python scripts (using the xarray package) archived on GitHub (). The NLCD_INEGI harmonized US-Mexico land cover classifications are available for download at Zenodo, with scripts archived on GitHub (). The L2015 parameter set was converted from ascii to VIC-5-compliant NetCDF format by the “tonic” tool ().",2019-08-09
0,Scientific Data,41597,10.1038/s41597-019-0088-4,Transcriptome of dorsal root ganglia caudal to a spinal cord injury with modulated behavioral activity,7,6,2019,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR41', '#text': '41'}}, '#text': 'All analyses were performed using open sources software tools. Raw sequencing files were downloaded from Illumina BaseSpace using the Illumina Python Run Downloader. Individual samples were initially divided across four lanes for sequencing, and these files were concatenated into one single end Fastq file using the UNIX cat command.'}, {'italic': ['FN1', 'FN2', 'FN3', 'FN4', 'COND_REP'], '#text': 'cat <>.fastq <>.fastq <>.fastq <>.fastq > <>.fastq'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR27', '#text': '27'}}, '#text': 'The concatenated sequences were input to FastQC v.0.10.1 for quality control analysis using default parameters.'}, {'italic': 'COND_REP', '#text': 'fastqc <>.fastq -o <FASTQC_DIRECTORY>'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR28', '#text': '28'}}, '#text': 'Fastq files were input to Star 2.6 for alignment specifying BAM file format sorted by coordinate and requesting unmapped read files.'}, {'italic': ['COND_REP', 'COND_REP'], '#text': 'STAR–runMode alignReads–outSAMtype BAM SortedByCoordinate –outSAMstrandField intronMotif–outReadsUnmapped Fastx–readFilesIn <>.fastq.gz –outFileNamePrefix <>–runThreadN 16–genomeDir Rnor_6.0 –readFilesCommand zcat'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR29', '#text': '29'}}, 'italic': 'reverse', '#text': 'Read counts for each sample were extracted using HTSeq. 0.10.0. The  option was used to indicate strand orientation. Illumina’s TruSeq Stranded mRNA protocol was used to sequence the data and produces libraries where the first read is on the opposite strand to the RNA molecule.'}, {'italic': ['COND_REP', 'COND_REP'], '#text': 'htseq-count -f bam–stranded=reverse–mode=intersection-nonempty -r name <>/Aligned.sortedByCoord.out.bam Rattus_norvegicus.Rnor_6.0.93_PARSED.gtf ><>/gene_counts_Reversed.htseq'}, {'sup': [{'xref': [{'@ref-type': 'bibr', '@rid': 'CR31', '#text': '31'}, {'@ref-type': 'bibr', '@rid': 'CR32', '#text': '32'}], '#text': ','}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR42', '#text': '42'}}], 'italic': 'estimateSizeFactors', '#text': 'The raw counts were normalized using DESeq2’s default procedure, relative log expression (RLE), using the  function. Detailed instructions can be found on the Bioconductor website for DESeq2.'}]",2019-06-07
0,Scientific Data,41597,10.1038/s41597-019-0049-y,"A dataset of egg size and shape from more than 6,700 insect species",3,7,2019,https://github.com/shchurch/Insect_Egg_Evolution; https://github.com/sdonoughe/Insect_Egg_Image_Parser; https://github.com/brunoasm/TaxReformer,"All code used to generate the insect egg dataset as well as reproduce the tables and plots shown here is made freely available. Python code used to compile the dataset and extract text information from text sources, as well as the R code used to convert the raw dataset to the final dataset and to generate the tables and figures shown here is available at . Python code used to measure published images of eggs is available at , and Python code to cross-reference the egg dataset with taxonomic tools is available at . Statistical analyses were performed using R version 3.4.2.",2019-07-03
0,Scientific Data,41597,10.1038/s41597-019-0090-x,"PathoPhenoDB, linking human pathogens to their phenotypes in support of infectious disease research",3,6,2019,https://github.com/bio-ontology-research-group/pathophenodb,The source code for PathoPhenoDB is freely available at .,2019-06-03
0,Scientific Data,41597,10.1038/s41597-019-0235-y,A cone-beam X-ray computed tomography data collection designed for machine learning,22,10,2019,,"[{'ext-link': {'@xlink:href': 'https://github.com/cicwi/WalnutReconstructionCodes', '@ext-link-type': 'uri', '#text': 'https://github.com/cicwi/WalnutReconstructionCodes'}, '#text': 'Python and MATLAB scripts for loading, pre-processing and reconstructing the projection data in the way described above are published on github: .'}, {'ext-link': {'@xlink:href': 'http://www.astra-toolbox.com', '@ext-link-type': 'uri', '#text': 'www.astra-toolbox.com'}, 'monospace': 'conda install -c astra-toolbox/label/dev astra-toolbox', 'sup': ['3', '3'], 'italic': 'μ', '#text': 'They make use of the ASTRA toolbox, which is openly available on  or accessible as a conda package (use  to install the development version). ASTRA is currently only fully supported for Windows and Linux. Installing it on Mac OS is possible but in the current state\xa0very involved and version-dependent. For obtaining a comparable scaling of the image intensities between FDK and iterative reconstructions, it is required to use a development version of the ASTRA toolbox more recent than 1.9.0\u2009dev. For each dataset, a text file containing information about motor positions (source 3D position, detector position and detector orientation) is provided and used by the aforementioned Python/MATLAB scripts to set up the reconstruction geometry. All reference reconstructions provided have been computed with the Python scripts. Furthermore, while the scripts allow to sub-sample the projections and to choose a different image resolution, the reference reconstructions were computed with all projections and within a volume of 501 voxels of size 100\u2009m as mentioned above.'}]",2019-10-22
0,Scientific Data,41597,10.1038/sdata.2019.19,Sex-specific median life expectancies from  populations for 330 animal species,19,2,2019,,,2019-02-19
0,Scientific Data,41597,10.1038/s41597-019-0051-4,Characterization of CA-MRSA TCH1516 exposed to nafcillin in bacteriological and physiological media,26,4,2019,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR21', '#text': '21'}}, '#text': 'The pipeline used to analyze RNA sequence data is available on figshare.'}, {'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR22', '#text': '22'}}, '#text': 'The script used to remove batch effects from RNA sequencing data is available on figshare.'}]",2019-04-26
0,Scientific Data,41597,10.1038/sdata.2019.21,The variable quality of metadata about biological samples used in biomedical experiments,19,2,2019,,,2019-02-19
0,Scientific Data,41597,10.1038/s41597-020-0368-z,Reference values for resting and post exercise hemodynamic parameters in a 6–18 year old population,21,1,2020,,,2020-01-21
0,Scientific Data,41597,10.1038/sdata.2018.307,A functional connectome phenotyping dataset including cognitive state and personality measures,12,2,2019,,,2019-02-12
0,Scientific Data,41597,10.1038/s41597-019-0220-5,Tracing diagnosis trajectories over millions of patients reveal an unexpected risk in schizophrenia,15,10,2019,https://www.ahrq.gov/research/data/hcup/,"['The following software and source codes were used for the presented analysis and described in the main text:', {'ext-link': {'@xlink:href': 'https://www.mysql.com/downloads/', '@ext-link-type': 'uri', '#text': 'https://www.mysql.com/downloads/'}, '#text': '1. MySQL v14.12 was used for analysis of HCUP SIDCA data.'}, '2. An in-house developed Java script was used to trace the diagnosis trajectories of over 10.2 million patients. The source codes are available via GitHub repository for academic researches.', {'ext-link': {'@xlink:href': 'https://github.com/hypaik/HCUPSIDCA_trajectory_tracking', '@ext-link-type': 'uri', '#text': 'https://github.com/hypaik/HCUPSIDCA_trajectory_tracking'}}, '3. R package version 3.2, Java v1.8.0 and Python v2.7 were used for the presented statistical analysis.', {'ext-link': {'@xlink:href': 'https://www.r-project.org/', '@ext-link-type': 'uri', '#text': 'https://www.r-project.org/'}}, {'ext-link': {'@xlink:href': 'https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html', '@ext-link-type': 'uri', '#text': 'https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html'}}, {'ext-link': {'@xlink:href': 'https://www.python.org/download/releases/2.7/', '@ext-link-type': 'uri', '#text': 'https://www.python.org/download/releases/2.7/'}}, {'ext-link': {'@xlink:href': 'https://cytoscape.org/', '@ext-link-type': 'uri', '#text': 'https://cytoscape.org/'}, '#text': '4. Cytoscape, an open source platform, was used for the network analysis of disease trajectories and figure preparations.'}, '5. WebGL was used for the dynamic visualization of the presented disease trajectories.', {'ext-link': {'@xlink:href': 'https://developer.mozilla.org/en-US/docs/Web/API/WebGL_API', '@ext-link-type': 'uri', '#text': 'https://developer.mozilla.org/en-US/docs/Web/API/WebGL_API'}}, '6. All the traced disease trajectories for over 10.4 million patients in California, US was presented via our web resource.', {'ext-link': {'@xlink:href': 'http://52.89.56.137:3000/#/intro', '@ext-link-type': 'uri', '#text': 'http://52.89.56.137:3000/#/intro'}, '#text': '(Optimized only for Chrome)'}, {'xref': {'@ref-type': 'supplementary-material', '@rid': 'MOESM1', '#text': 'S1'}, '#text': '7. We also have a website and video explaining these disease trajectories. (Appendix Movie\xa0. Dynamic presentation of the traced diagnosis trajectories at scale)'}, {'ext-link': {'@xlink:href': 'https://www.youtube.com/watch?v=jJMds31-e2g', '@ext-link-type': 'uri', '#text': 'https://www.youtube.com/watch?v=jJMds31-e2g'}}, {'xref': {'@ref-type': 'supplementary-material', '@rid': 'MOESM1', '#text': 'S1'}, 'ext-link': {'@xlink:href': 'https://github.com/hypaik/HCUPSIDCA_trajectory_tracking', '@ext-link-type': 'uri', '#text': 'https://github.com/hypaik/HCUPSIDCA_trajectory_tracking'}, '#text': '8. Appendix Data\xa0. All of traced diagnosis trajectories. ().'}]",2019-10-15
0,Scientific Data,41597,10.1038/s41597-020-0355-4,"PIC, a paediatric-specific intensive care database",13,1,2020,https://github.com/Healthink/PIC,"The code that was used to create the PIC database, calculate statistics of this paper, demonstrate a machine learning task and source code which underpins the PIC website and documentation is openly available, and contributions from the research community are encouraged: .",2020-01-13
0,Scientific Data,41597,10.1038/s41597-019-0261-9,"A 2D hyperspectral library of mineral reflectance, from 900 to 2500 nm",11,11,2019,,"[{'ext-link': {'@xlink:href': 'https://github.com/unine-chyn/hics', '@ext-link-type': 'uri', '#text': 'https://github.com/unine-chyn/hics'}, '#text': 'The data acquisition and the hyperspectral data processing was performed using software developed inhouse. The software developed is available on GitHub: .'}, 'As the software setup for acquisition depends on the configuration and the hardware used, only an enumeration of the modules used is given. Three different classes of modules where used. Hardware related modules are specific to the hardware setup, and are likely to be modified with a different setup. Preview and control modules are used by the operator to control the camera and check that the process is performing satisfactorily. Finally, the plugins used for capturing data contain the algorithmic logic of the capture.', {'monospace': ['hics.hardware.specimswir.camera', 'hics.hardware.specimswir.scanner', 'hics.hardware.specimswir.framegrabber', 'hics.hardware.ev3.ev3focus', 'hics.hardware.webcam'], '#text': 'Hardware related:  (camera control),  (mirror scanner control),  (frame grabber),  (LEGO focus control), hics.hardware.ev3.ev3rotater (LEGO rotating device),  (webcam capture)'}, {'monospace': ['hics.daemon.frameconverter', 'hics.gui'], '#text': 'Preview and control:  (basic correction for preview),  (graphical user interface)'}, {'monospace': ['hics.plugin.photogrammetry', 'hics.plugin.record', 'hics.plugin.autofocus', 'hics.plugin.labelprint', 'hics.plugin.autoscan'], '#text': 'Plugins for capturing data:  (capture data from the webcam),  (record hyperspectral data),  (find the focus maximizing contrast),  (print the label for the sample),  (automate all steps required for the scan, using the various plugins)'}, 'Once data is captured, the following commands were used (@ replaces the scan name):', 'python3 –m hics.datafile.calibrate–input @.scan —output @.calibrated —max 14000', 'python3 –m hics.datafile.merge —input @.calibrated —output @.hdr —clean–required_images 2', '# A mask should be generated by the user in @.mask.png', 'python3 –m hics.datafile.maskdata —input @.hdr —mask @.mask.png —output @.mhdr.', {'sup': [{'xref': {'@ref-type': 'bibr', '@rid': 'CR8', '#text': '8'}}, {'xref': {'@ref-type': 'bibr', '@rid': 'CR12', '#text': '12'}}], '#text': 'For the 3D reconstruction, after multiple experiments with various tools, it was found that the best output quality was obtained using COLMAP for the keypoints detection and OpenMVS for the construction of the pointcloud, the mesh, and the texturing. A small change was applied to COLMAP in order to orient the sample correctly. This change however doesn’t affect the quality of the result. OpenMVS was used without any modification.'}, {'ext-link': {'@xlink:href': 'https://github.com/unine-chyn/sfm-bundle', '@ext-link-type': 'uri', '#text': 'https://github.com/unine-chyn/sfm-bundle'}, '#text': 'The code to automate the build of the tools and the processing of the images is available at .'}]",2019-11-11
0,Scientific Data,41597,10.1038/sdata.2019.30,Assessing data availability and research reproducibility in hydrology and water resources,26,2,2019,,,2019-02-26
0,Scientific Data,41597,10.1038/s41597-019-0168-5,"UASOL, a large-scale high-resolution outdoor stereo dataset",29,8,2019,project; GPS Visualizer,"We uploaded a selection of Python scripts to the public repository of the . First, the . is in charge of taking a raw ZED recording file SVO and saving the left and right color images, and the corresponding depth map. It also saves the manifest JSON and the log files. This script is largely based on the ZED sample script and needs GPU capabilities to compute the depth maps. It also needs the ZED API v2.3.3. Then, the . is a sample script that takes a manifest JSON file and loads the sequence into python variables. We used the script named . to connect to the GPS device. This script queries the current geolocalization in an infinite loop. When closed, it saves the data to a JSON file. Next, the script . takes a JSON file generated by the previous script and dumps the data to a comma-separated text file. This file is intended to be uploaded to a . in order to visually inspect the accuracy of the GPS records.",2019-08-29
0,Scientific Data,41597,10.1038/s41597-019-0081-y,"Catalysis-Hub.org, an open electronic structure database for surface reactions",28,5,2019,https://github.com/SUNCAT-Center; https://github.com/SUNCAT-Center/CatHub/tree/master/tutorials/1_bimetallic_alloys/heatmaps.py,"All code developed for the Catalysis-Hub platform is made available open source from the SUNCAT Center’s GitHub repository at , which includes the database backend, frontend and the CatHub python API. The Python script used for plotting the data shown in Fig. , using the CatHub API, is made available as a tutorial at .",2019-05-28
0,Scientific Data,41597,10.1038/s41597-019-0085-7,"YSTAFDB, a unified database of material stocks and flows for sustainability science",7,6,2019,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR33', '#text': '33'}}, 'italic': ['cross_boundary_flows', 'cross_boundary_flows_citations', 'flows', 'flows_citations', 'processes', 'processes_citations'], '#text': 'Each filled template file was converted into csv format and then parsed using a Python script titled ‘templates_0.2’. This script converts STAF data in template files into csv files with the same table format as present in data and mapping tables in YSTAFDB. The Python script produces the following csv files: , , , , , . Therefore, this set of six csv files were produced ~60 times to develop the material cycles part of YSTAFDB, one for each filled template (i.e., for each material cycle publication).'}, {'italic': ['cross_boundary_flows', 'cross_boundary_flows', 'cross_boundary_flows_citations', 'flows', 'flows_citations', 'processes', 'processes_citations', 'criticality', 'recycling'], 'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR33', '#text': '33'}}, '#text': 'All csv files of the same type, e.g., , were then merged using another Python script titled ‘merged_1.0’. This procedure yielded six merged csv files, one for each type: , , , , , and . Some manual cleaning was then performed to correct any formatting errors identified. All other tables in YSTAFDB were manually produced directly from publications, including , , and their related tables.'}]",2019-06-07
0,Scientific Data,41597,10.1038/s41597-019-0231-2,Comparative transcriptome profiling of immune response against  infection in Chinese tongue sole,22,10,2019,,"['The softwares used for data processing are included in the methods and available in the following list:', {'ext-link': {'@xlink:href': 'http://www.bioinformatics.babraham.ac.uk/projects/fastqc/', '@ext-link-type': 'uri', '#text': 'http://www.bioinformatics.babraham.ac.uk/projects/fastqc/'}, '#text': '1. FastQC v0.11.6 was used for quality assessment of FASTQ data: .'}, {'ext-link': {'@xlink:href': 'https://pypi.python.org/pypi/multiqc', '@ext-link-type': 'uri', '#text': 'https://pypi.python.org/pypi/multiqc'}, '#text': '2. MultiQC was used for combining fastqc results into one: .'}, {'ext-link': {'@xlink:href': 'http://bioinfo.single-cell.cn/rna-qc-chain.html', '@ext-link-type': 'uri', '#text': 'http://bioinfo.single-cell.cn/rna-qc-chain.html'}, '#text': '3. RNA-QC-Chain was used for data preprocessing of raw data: .'}, {'ext-link': {'@xlink:href': 'http://ccb.jhu.edu/software/tophat/downloads/', '@ext-link-type': 'uri', '#text': 'http://ccb.jhu.edu/software/tophat/downloads/'}, '#text': '4. TopHat v2.0.12 was used for clean reads aligned to the reference genome: .'}, {'ext-link': {'@xlink:href': 'http://cole-trapnell-lab.github.io/cufflinks/', '@ext-link-type': 'uri', '#text': 'http://cole-trapnell-lab.github.io/cufflinks/'}, '#text': '5. Cufflinks v2.1.1 was used for transcript assembly of samples: .'}, {'ext-link': {'@xlink:href': 'https://htseq.readthedocs.io/en/release_0.11.1/history.html#version-0-6-1', '@ext-link-type': 'uri', '#text': 'https://htseq.readthedocs.io/en/release_0.11.1/history.html#version-0-6-1'}, '#text': '6. HTSeq v0.6.1 was used for counting the reads numbers mapped to each gene: .'}, {'ext-link': {'@xlink:href': 'https://bioconductor.riken.jp/packages/3.0/bioc/html/DESeq.html', '@ext-link-type': 'uri', '#text': 'https://bioconductor.riken.jp/packages/3.0/bioc/html/DESeq.html'}, '#text': '7. DESeq package v1.18.0 was used for differential expression analysis of two groups with biological replicates: .'}, {'ext-link': {'@xlink:href': 'http://www.sthda.com/english/wiki/ggcorrplot-visualization-of-a-correlation-matrix-using-ggplot2', '@ext-link-type': 'uri', '#text': 'http://www.sthda.com/english/wiki/ggcorrplot-visualization-of-a-correlation-matrix-using-ggplot2'}, '#text': '8. Ggplot2 package was used for visualization of a correlation matrix between samples: .'}]",2019-10-22
0,Scientific Data,41597,10.1038/sdata.2019.14,An open source web application for distributed geospatial data exploration,12,2,2019,,,2019-02-12
0,Scientific Data,41597,10.1038/s41597-019-0138-y,High-throughput computation and evaluation of raman spectra,26,7,2019,,"The proprietary VASP-code is primarily used in the DFPT calculations. The processing and modifications of the simulations were implemented using Pymatgen and FireWorks. Pymatgen (Python Materials Genomics) is an open-source Python library under Massachusetts Instrutute of Technology (MIT) license for materials analysis. The workflow shown in Fig.  is implemented using FireWorks in Atomate, which stores, executes, and manages calculation workflows and is free to public through Atomate’s Github site under a modified GNU General Public License.",2019-07-26
0,Scientific Data,41597,10.1038/s41597-019-0329-6,"The heterogeneity and change in the urban structure of metropolitan areas in the United States, 1990–2010",16,12,2019,,The  subdirectory at the above data repository contains the python scripts used to produce the urban class data layers and the zonal statistics tools upon which our analysis is drawn.,2019-12-16
0,Scientific Data,41597,10.1038/s41597-019-0214-3,Global humid tropics forest structural condition and forest structural integrity maps,25,10,2019,,"['The SCI and FSII were generated in GEE. The Hansen Global Forest Change v1.5 (2000–2017) data set used to obtain loss year is served by GEE. Tree cover for 2010 was uploaded as a GEE asset from GLAD at the University of Maryland. This was done for each continental portion of the study area using the Python API Earth Engine shell on a parallel processor computer at Northern Arizona University. The same procedure was used for the canopy height data set. Human footprint data were uploaded using the GEE Asset Upload procedure.', 'The SCI weights were generated in GEE by assigning a mask of 0 where conditions are not met and 1 when they are met. An example conditional statement is:', 'var sciImage1\u2009=\u2009lossYear.gte(13).and(lossYear.lte(17)).or(forest.lt(25)).or(height.lte(5)).', 'This formulation was then used to multiply the cells with mask\u2009=\u20091 times the appropriate SCI weight. This was done for each combination of input data values for each SCI weight. The resulting maps for each category of SCI weight were combined into a single map using the GEE add command. Similar coding was used for the FSII classification. The resulting SCI and FSII maps for each continent were exported from GEE to Google Drive. GEE code for the classifications are available at', {'ext-link': {'@xlink:href': 'https://code.earthengine.google.com/f46e40ba74164a595f9be5067a7b26cf', '@ext-link-type': 'uri', '#text': 'https://code.earthengine.google.com/f46e40ba74164a595f9be5067a7b26cf'}, '#text': 'Africa:'}, {'ext-link': {'@xlink:href': 'https://code.earthengine.google.com/e6595f8040af90a7b30a9b89e42c12f6', '@ext-link-type': 'uri', '#text': 'https://code.earthengine.google.com/e6595f8040af90a7b30a9b89e42c12f6'}, '#text': 'SE Asia:'}, {'ext-link': {'@xlink:href': 'https://code.earthengine.google.com/285ecd4784d7d4e0bc9a6e6aa966b54c', '@ext-link-type': 'uri', '#text': 'https://code.earthengine.google.com/285ecd4784d7d4e0bc9a6e6aa966b54c'}, '#text': 'South America:'}]",2019-10-25
0,Scientific Data,41597,10.1038/s41597-019-0052-3,"BOLD5000, a public fMRI dataset while viewing 5000 visual images",6,5,2019,BOLD5000.org; http://psychtoolbox.org; BOLD5000.org,"The complete set of Psychtoolbox Matlab scripts for running this study are available for download at . The Psychtoolbox Version 3 Matlab toolbox and documentation are both available for download at . As per convention in the field, the complete set of images used as stimuli are available for download at .",2019-05-06
0,Scientific Data,41597,10.1038/s41597-019-0056-z,A multi-species repository of social networks,29,4,2019,https://github.com/bansallab/asnr/,All code for data characterization has been written in  using the  package. The code is open source at .,2019-04-29
0,Scientific Data,41597,10.1038/s41597-020-0381-2,Generation of a global synthetic tropical cyclone hazard dataset using STORM,6,2,2020,,,2020-02-06
0,Scientific Data,41597,10.1038/sdata.2019.29,Draft genomic and transcriptome resources for marine chelicerate,26,2,2019,,,2019-02-26
0,Scientific Data,41597,10.1038/s41597-019-0242-z,Integrated open-source software for multiscale electrophysiology,25,10,2019,https://github.com/brainstorm-tools/brainstorm3,The toolbox can be acquired as part of Brainstorm’s GitHub repository: .,2019-10-25
0,Scientific Data,41597,10.1038/s41597-019-0331-z,Profiling the effect of nafcillin on HA-MRSA D712 using bacteriological and physiological media,17,12,2019,,The complete RNAseq pipeline used in analysis of RNAseq data is available on Figshare. The script to remove batch effects from RNAseq data is also available of Figshare.,2019-12-17
0,Scientific Data,41597,10.1038/s41597-019-0066-x,A data set of inland lake catchment boundaries for the Qiangtang Plateau,16,5,2019,,"As mentioned above, some catchment delineation steps can be programmed using Python scripts which was provided in our data set. It contains two scripts, one is for single-lake catchment delineation named “Single-lake catchment.py” and the other is for tandem-lake catchment and mixed-lake catchment delineation named “Tandem-lake and Mixed-lake catchment.py”. Before running these two scripts, it needs to install ArcGIS 10.1 or above version and install the python function carried by it in the Windows system. Then, the scripts can be opened by the IDLE (python GUI). As for “Single-lake catchment.py”, line 9 is for setting a workspace for program running. It should put the initial peripheral boundary shapefile named “bd.shp” and lake shapefile named “lake.shp” in the workspace folder. Line 10 is for the original DEM grid file setting. By finishing all the above steps, it will automatically generates all the catchment units named “basin.shp”. As for “Tandem-lake and Mixed-lake catchment.py”, add the “river.kmz” file which is the river depicted from Google Earth into the workspace folder, the rest setting is the same as the “Single-lake catchment.py”.",2019-05-16
0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08
0,Scientific Data,41597,10.1038/s41597-019-0327-8,Facial model collection for medical augmented reality in oncologic cranio-maxillofacial surgery,9,12,2019,https://numpy.org/; https://pypi.org/project/pynrrd/; https://github.com/cgsaxner/PET-CT_preprocessing,"Aside from the image and 3D model processing steps performed with free or open source software described in this section, we used a custom script written in Python 3.6 with NumPy 1.15.4 () for cropping CT and PET scans in NRRD format, to cover the head and neck region only. This script furthermore outputs information about the volumetric medical data, such as the size and resolution, as seen in Table . This python script utilizes the pynrrd 0.4.0 library () to parse information about slice resolution and volume size from the header of NRRD files. With this information, we calculate the ideal number of slices to be included in the final NRRD files for upload, as described by formula. These slices are then again automatically converted to NRRD format and saved to disk. These are the NRRD files available in our figshare repository. The Python script is available on GitHub ().",2019-12-09
0,Scientific Data,41597,10.1038/s41597-019-0279-z,Chromosome genome assembly and annotation of the yellowbelly pufferfish with PacBio and Hi-C sequencing data,8,11,2019,,No specific code or script was used in this work. All commands used in the processing were executed according to the manual and protocols of the corresponding bioinformatics software.,2019-11-08
0,Scientific Data,41597,10.1038/s41597-019-0177-4,"BioExcel Building Blocks, a software library for interoperable biomolecular simulation workflows",10,9,2019,,BioBB’s source code is available at GitHub. URLs for the code and documentation repositories and the alternative installation and execution options are summarized in Online-only Table .,2019-09-10
0,Scientific Data,41597,10.1038/sdata.2019.15,", a campus-scale commercial and residential buildings electrical energy dataset",19,2,2019,,,2019-02-19
0,Scientific Data,41597,10.1038/s41597-019-0266-4,Travel times to hospitals in Australia,1,11,2019,https://github.com/sebbarb/times_to_hospitals_AU,The Python 3 code related to this project is available for download at .,2019-11-01
0,Scientific Data,41597,10.1038/s41597-019-0076-8,The FLUXCOM ensemble of global land-atmosphere energy fluxes,27,5,2019,https://git.bgc-jena.mpg.de/skoirala/fluxcom_ef_figures,Python code to synthesise the results and to generate the figures of FLUXCOM results can be obtained through the public repository at . MATLAB code for generating the flux products and ensemble estimates is available on request to Martin Jung (mjung@bgc-jena.mpg.de) for the sake of reproducibility. The collaborative nature of the FLUXCOM initiative and the demanding computing resulted in complex and large amounts of code that was customized to the HPC and file system of MPI-BGC and is therefore challenging to use. Code for processing MODIS satellite data is available on request to Kazuhito Ichii (ichii@chiba-u.jp).,2019-05-27
0,Scientific Data,41597,10.1038/s41597-019-0151-1,"AqSolDB, a curated reference set of aqueous solubility and 2D descriptors for a diverse set of compounds",8,8,2019,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR25', '#text': '25'}}, '#text': 'The reproducibility of the curation algorithm can be verified by executing the provided scripts on Code Ocean. The code has been developed and tested using Python 3.5 on Linux operating system and is available under the MIT license.'}, {'ext-link': {'@xlink:href': 'http://www.rdkit.org', '@ext-link-type': 'uri', '#text': 'http://www.rdkit.org'}, '#text': 'The RDKit cheminformatics software is freely available under the BSD licence ().'}, {'ext-link': {'@xlink:href': 'http://www.vcclab.org/lab/alogps/', '@ext-link-type': 'uri', '#text': 'http://www.vcclab.org/lab/alogps/'}, '#text': 'ALOGPS 2.1 used for reference value generation is freely available online ().'}]",2019-08-08
0,Scientific Data,41597,10.1038/sdata.2018.300,A cloud-free MODIS snow cover dataset for the contiguous United States from 2000 to 2017,15,1,2019,,,2019-01-15
0,Scientific Data,41597,10.1038/s41597-019-0147-x,Dataset of three-dimensional traces of roads,5,8,2019,,,2019-08-05
0,Scientific Data,41597,10.1038/s41597-019-0338-5,A longitudinal neuroimaging dataset on multisensory lexical processing in school-aged children,20,12,2019,,Code used to create event data files from compiled E-prime data and to deface T1-weighted images are located in the code directory at the root level of the dataset.  uses .csv containing merged data from all subjects per task and outputs events.tsv files into each subject folder as described in data records.  and  remove facial features from all T1-weighted images.  confirms that all stimuli referenced in participant events.tsv files exist in the stimuli directory at the root level of the dataset.,2019-12-20
0,Scientific Data,41597,10.1038/sdata.2019.40,A longitudinal neuroimaging dataset on arithmetic processing in school children,5,3,2019,,,2019-03-05
0,Scientific Data,41597,10.1038/s41597-019-0086-6,"Posterior samples of the parameters of binary black holes from Advanced LIGO, Virgo’s second observing run",3,6,2019,http://pycbc.org,"The posterior probability density functions presented in this paper were sampled using the PyCBC Inference software. The PyCBC Inference toolkit uses the Bayesian inference methodology described in this paper; a more detailed description of the toolkit is presented in ref.. The source code and documentation of PyCBC Inference is available as part of the PyCBC software package at . The results in this paper were generated with the PyCBC version 1.12.3 release. In the data release repository for this work we provide scripts and configuration files for replicating our analysis. The scripts document our command line calls to the  executable which performs the ensemble MCMC analyses. The command line call to  contains options for: the ensemble MCMC configuration, data conditioning, and locations of the configuration file and gravitational-wave detector data files. The configuration files included in the repository, and used as an input to , specify the prior probability density functions used in the analyses, including sections for: initializing the distribution of Markov-chain positions in the ensemble MCMC, declaring transformations between the parameters that define the prior and the parameters that the ensemble MCMC samples (eg. ), and defining additional constraints to the prior probability density function.",2019-06-03
0,Scientific Data,41597,10.1038/s41597-019-0325-x,Interaction data from the Copenhagen Networks Study,11,12,2019,,"Alongside the data, we provide an iPython notebook showing basic data loading and use (see  in the Figshare data repository). The notebook is intended to showcase the basic approaches to working with the released data.",2019-12-11
0,Scientific Data,41597,10.1038/s41597-019-0121-7,Energy refinement and analysis of structures in the QM9 database via a highly accurate quantum chemical method,3,7,2019,,The QM9-G4MP2 database contains raw outputs and scripts to parse the data addressed in this paper. All scripts are released with the BSD license. Other details on the scripts are discussed in the Usage Notes.,2019-07-03
0,Scientific Data,41597,10.1038/sdata.2018.309,"GFPLAIN250m, a global high-resolution dataset of Earth’s floodplains",15,1,2019,,,2019-01-15
0,Scientific Data,41597,10.1038/s41597-020-0358-1,Reference data on  anatomy and indentation response of tissue layers of musculoskeletal extremities,15,1,2020,https://simtk.org/projects/multis; https://simtk.org/frs/?group_id=1032; https://docs.anaconda.com/anaconda/,"The source code repository is available at (). Additionally, two static packages containing thickness analysis software and all data/source code used in the technical validation section are available (). In order to use the provided scripts, one needs to install Python 2.7 with Anaconda (). The two additional libraries required to read ultrasound and data files respectively are DICOM and npTDMS. Additional libraries used in the provided scripts can be installed using Anaconda (“conda install” command).",2020-01-15
0,Scientific Data,41597,10.1038/s41597-019-0312-2,A global wildfire dataset for the analysis of fire regimes and fire behaviour,29,11,2019,,"The code is available on demand from the EFFIS or GWIS team of the Joint Research Centre of the European Commission. Two parameters have been used in the execution of the application to the MCD64A1. First, the fire is considered as ended when it has been 16 days without activity. The second parameter is the time between two burnt areas that touch each other: if one of them has been active in the last 5 days it will become a single fire event. The code developed in GWIS uses the previously mentioned libraries and processes a single delivery of a burnt area product. The spatio-temporal clustering is defined during the creation process of the sparse matrix.",2019-11-29
0,Scientific Data,41597,10.1038/s41597-019-0117-3,Metagenomics and transcriptomics data from human colorectal cancer,5,7,2019,,"[{'sup': {'xref': {'@ref-type': 'bibr', '@rid': 'CR17', '#text': '17'}}, 'monospace': ['scripts', 'scripts', 'used_packages_and_their_versions.tsv', 'used_parameters.tsv.'], '#text': 'All the code used to process the genomic data is freely available as a part of the provided Zenodo repository and the code is located in the folder named . The  folder also contains dependencies listed in the file  and the used parameter values listed in  Depending on the scripts’ functionality, they are separated into various folders:'}, {'monospace': 'rnaseq-subtype-classification', '#text': 'The folder  contains scripts used for read mapping, gene expression quantification, and profile classification.'}, {'monospace': 'kraken/human-unmapped', '#text': 'The folder  contains scripts to assign reads to bacterial taxa.'}, {'monospace': 'kraken/diff-expr-taxa', '#text': 'The folder  contains scripts for differential analysis of bacterial species in CMS.'}, {'monospace': '16S-metabarcoding', '#text': 'The folder  contains scripts for metabarcoding data analysis.'}]",2019-07-05
0,Scientific Data,41597,10.1038/s41597-019-0264-6,Geocoding of worldwide patent data,6,11,2019,,"['The data extraction and parsing were done in Python 3.4 and 3.6.', 'The construction of address fields, the cleaning, data processing, matching with Geonames, and imputation of missing information was implemented in PostgreSQL 9.6.6. The assignment of regions and cities was done in PostGIS 2.4.', {'ext-link': {'@xlink:href': 'https://github.com/seligerf/Imputation-of-missing-location-information-for-worldwide-patent-data', '@ext-link-type': 'uri', '#text': 'https://github.com/seligerf/Imputation-of-missing-location-information-for-worldwide-patent-data'}, '#text': 'All Python and PostgreSQL code produced for this project can be accessed upon request. The PostgreSQL codes for the imputation of missing country codes and location information is available on Github ().'}]",2019-11-06
0,Scientific Data,41597,10.1038/s41597-019-0162-y,Global tissue-specific transcriptome analysis of  fruit across six developmental stages,21,8,2019,,,2019-08-21
0,Scientific Data,41597,10.1038/sdata.2017.55,Se-SAD serial femtosecond crystallography datasets from selenobiotinyl-streptavidin,25,4,2017,,,2017-04-25
0,Scientific Data,41597,10.1038/sdata.2018.5,RNA-seq transcriptomic analysis of adult zebrafish inner ear hair cells,6,2,2018,,,2018-02-06
0,Scientific Data,41597,10.1038/sdata.2018.55,Massively parallel recordings in macaque motor cortex during an instructed delayed reach-to-grasp task,10,4,2018,,,2018-04-10
0,Scientific Data,41597,10.1038/sdata.2016.134,High-throughput screening of inorganic compounds for the discovery of novel dielectric and optical materials,31,1,2017,,,2017-01-31
0,Scientific Data,41597,10.1038/sdata.2018.159,A spatio-temporal land use and land cover reconstruction for India from 1960–2010,14,8,2018,,,2018-08-14
0,Scientific Data,41597,10.1038/sdata.2018.258,A reference set of curated biomedical data and metadata from clinical case reports,20,11,2018,,,2018-11-20
0,Scientific Data,41597,10.1038/sdata.2018.165,Regional-scale management maps for forested areas of the Southeastern United States and the US Pacific Northwest,28,8,2018,,,2018-08-28
0,Scientific Data,41597,10.1038/sdata.2018.80,High-resolution hydrometeorological data from a network of headwater catchments in the tropical Andes,3,7,2018,,,2018-07-03
0,Scientific Data,41597,10.1038/sdata.2018.168,Residential wearable RSSI and accelerometer measurements with detailed location annotations,21,8,2018,,,2018-08-21
0,Scientific Data,41597,10.1038/sdata.2017.175,"RE-Europe, a large-scale dataset for modeling a highly renewable European electricity system",28,11,2017,,,2017-11-28
0,Scientific Data,41597,10.1038/sdata.2018.273,"Columbia Open Health Data, clinical concept prevalence and co-occurrence from electronic health records",27,11,2018,,,2018-11-27
0,Scientific Data,41597,10.1038/sdata.2017.184,"Verification of  stock collections using SNPmatch, a tool for genotyping high-plexed samples",19,12,2017,,,2017-12-19
0,Scientific Data,41597,10.1038/sdata.2018.203,Open grid model of Australia’s National Electricity Market allowing backtesting against historic data,23,10,2018,,,2018-10-23
0,Scientific Data,41597,10.1038/sdata.2017.108,"Spatiotemporal database of US congressional elections, 1896–2014",15,8,2017,,,2017-08-15
0,Scientific Data,41597,10.1038/sdata.2018.110,"MEG-BIDS, the brain imaging data structure extended to magnetoencephalography",19,6,2018,,,2018-06-19
0,Scientific Data,41597,10.1038/sdata.2018.206,"GHWR, a multi-method global heatwave and warm-spell record and toolbox",30,10,2018,,,2018-10-30
0,Scientific Data,41597,10.1038/sdata.2017.193,"ANI-1, A data set of 20 million calculated off-equilibrium conformations for organic molecules",19,12,2017,,,2017-12-19
0,Scientific Data,41597,10.1038/sdata.2018.189,Global-scale phylogenetic linguistic inference from lexical resources,9,10,2018,,,2018-10-09
0,Scientific Data,41597,10.1038/sdata.2018.209,In-depth data on the network structure and hourly activity of the Central Chilean power grid,23,10,2018,,,2018-10-23
0,Scientific Data,41597,10.1038/sdata.2017.48,Diffraction data of core-shell nanoparticles from an X-ray free electron laser,11,4,2017,,,2017-04-11
0,Scientific Data,41597,10.1038/sdata.2017.117,Advancing The Cancer Genome Atlas glioma MRI collections with expert segmentation labels and radiomic features,5,9,2017,,,2017-09-05
0,Scientific Data,41597,10.1038/sdata.2018.195,Evaluation of a novel cloud-based software platform for structured experiment design and linked data analytics,3,10,2018,,,2018-10-03
0,Scientific Data,41597,10.1038/sdata.2018.48,"BLOND, a building-level office environment dataset of typical electrical appliances",27,3,2018,,,2018-03-27
0,Scientific Data,41597,10.1038/sdata.2017.79,Coherent soft X-ray diffraction imaging of coliphage PR772 at the Linac coherent light source,27,6,2017,,,2017-06-27
0,Scientific Data,41597,10.1038/sdata.2017.81,Targeted metagenomic sequencing data of human gut microbiota associated with  colonization,27,6,2017,,,2017-06-27
0,Scientific Data,41597,10.1038/sdata.2018.125,"The Chemical and Products Database, a resource for exposure-relevant data on chemicals in consumer products",10,7,2018,,,2018-07-10
0,Scientific Data,41597,10.1038/sdata.2018.230,A large-scale dataset of  pharmacology assay results,23,10,2018,,,2018-10-23
0,Scientific Data,41597,10.1038/sdata.2018.42,A database linking Chinese patents to China’s census firms,27,3,2018,,,2018-03-27
0,Scientific Data,41597,10.1038/sdata.2018.73,Spatiotemporal incidence of Zika and associated environmental drivers for the 2015-2016 epidemic in Colombia,24,4,2018,,,2018-04-24
0,Scientific Data,41597,10.1038/sdata.2018.65,High-throughput density-functional perturbation theory phonons for inorganic materials,1,5,2018,,,2018-05-01
0,Scientific Data,41597,10.1038/sdata.2017.57,A hybrid organic-inorganic perovskite dataset,9,5,2017,,,2017-05-09
0,Scientific Data,41597,10.1038/sdata.2018.146,"BARM and BalticMicrobeDB, a reference metagenome and interface to meta-omic data for the Baltic Sea",31,7,2018,,,2018-07-31
0,Scientific Data,41597,10.1038/sdata.2018.297,The OpenEar library of 3D models of the human temporal bone based on computed tomography and micro-slicing,8,1,2019,,,2019-01-08
0,Scientific Data,41597,10.1038/sdata.2017.88,A global multiproxy database for temperature reconstructions of the Common Era,11,7,2017,,,2017-07-11
0,Scientific Data,41597,10.1038/sdata.2017.153,"High-throughput DFT calculations of formation energy, stability and oxygen vacancy formation energy of ABO perovskites",17,10,2017,,,2017-10-17
0,Scientific Data,41597,10.1038/sdata.2018.20,"A new, short-recorded photoplethysmogram dataset for blood pressure monitoring in China",27,2,2018,,,2018-02-27
0,Scientific Data,41597,10.1038/sdata.2018.90,"Gridded birth and pregnancy datasets for Africa, Latin America and the Caribbean",22,5,2018,,,2018-05-22
0,Scientific Data,41597,10.1038/sdata.2018.161,"The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions",14,8,2018,,,2018-08-14
0,Scientific Data,41597,10.1038/sdata.2018.257,Anthropomorphic breast model repository for research and development of microwave breast imaging technologies,20,11,2018,,,2018-11-20
0,Scientific Data,41597,10.1038/sdata.2018.82,Computational screening of high-performance optoelectronic materials using OptB88vdW and TB-mBJ formalisms,8,5,2018,,,2018-05-08
0,Scientific Data,41597,10.1038/sdata.2018.43,"U-Index, a dataset and an impact metric for informatics tools and databases",20,3,2018,,,2018-03-20
0,Scientific Data,41597,10.1038/sdata.2018.170,Transcriptomic analyses of murine ventricular cardiomyocytes,21,8,2018,,,2018-08-21
0,Scientific Data,41597,10.1038/sdata.2018.74,A mobile brain-body imaging dataset recorded during treadmill walking with a brain-computer interface,24,4,2018,,,2018-04-24
0,Scientific Data,41597,10.1038/sdata.2017.177,A curated mammography data set for use in computer-aided detection and diagnosis research,19,12,2017,,,2017-12-19
0,Scientific Data,41597,10.1038/sdata.2018.275,Transcript- and protein-level analyses of the response of human eosinophils to glucocorticoids,4,12,2018,,,2018-12-04
0,Scientific Data,41597,10.1038/sdata.2018.66,Curated compendium of human transcriptional biomarker data,17,4,2018,,,2018-04-17
0,Scientific Data,41597,10.1038/sdata.2018.182,Seafloor crustal deformation data along the subduction zones around Japan obtained by GNSS-A observations,11,9,2018,,,2018-09-11
0,Scientific Data,41597,10.1038/sdata.2017.89,Sub-national mapping of population pyramids and dependency ratios in Africa and Asia,19,7,2017,,,2017-07-19
0,Scientific Data,41597,10.1038/sdata.2018.106,Pharmacological and genomic profiling of neurofibromatosis type 1 plexiform neurofibroma-derived schwann cells,12,6,2018,,,2018-06-12
0,Scientific Data,41597,10.1038/sdata.2018.205,"Cross-Linguistic Data Formats, advancing data sharing and re-use in comparative linguistics",16,10,2018,,,2018-10-16
0,Scientific Data,41597,10.1038/sdata.2018.9,The Brain/MINDS 3D digital marmoset brain atlas,13,2,2018,,,2018-02-13
0,Scientific Data,41597,10.1038/sdata.2018.89,A collection of public transport network data sets for 25 cities,15,5,2018,,,2018-05-15
0,Scientific Data,41597,10.1038/sdata.2018.109,Sixty-one thousand recent planktonic foraminifera from the Atlantic Ocean,28,8,2018,,,2018-08-28
0,Scientific Data,41597,10.1038/sdata.2018.290,Transcriptome profiling of a beach-adapted wild legume for dissecting novel mechanisms of salinity tolerance,11,12,2018,,,2018-12-11
0,Scientific Data,41597,10.1038/sdata.2017.116,A new global anthropogenic heat estimation based on high-resolution nighttime light data,22,8,2017,,,2017-08-22
0,Scientific Data,41597,10.1038/sdata.2018.293,Outlier analyses of the Protein Data Bank archive using a probability-density-ranking approach,11,12,2018,,,2018-12-11
0,Scientific Data,41597,10.1038/sdata.2017.125,Precision annotation of digital samples in NCBI’s gene expression omnibus,19,9,2017,,,2017-09-19
0,Scientific Data,41597,10.1038/sdata.2018.124,Time-lapse imagery and volunteer classifications from the Zooniverse Penguin Watch project,26,6,2018,,,2018-06-26
0,Scientific Data,41597,10.1038/sdata.2018.44,A Mediterranean coastal database for assessing the impacts of sea-level rise and associated hazards,27,3,2018,,,2018-03-27
0,Scientific Data,41597,10.1038/sdata.2018.232,"LogMPIE, pan-India profiling of the human gut microbiome using 16S rRNA sequencing",30,10,2018,,,2018-10-30
0,Scientific Data,41597,10.1038/sdata.2018.28,Tracking vegetation phenology across diverse North American biomes using PhenoCam imagery,13,3,2018,,,2018-03-13
0,Scientific Data,41597,10.1038/sdata.2018.6,An open repository for single-cell reconstructions of the brain forest,27,2,2018,,,2018-02-27
0,Scientific Data,41597,10.1038/sdata.2017.30,"Molecular, phenotypic, and sample-associated data to describe pluripotent stem cell lines and derivatives",28,3,2017,,,2017-03-28
0,Scientific Data,41597,10.1038/sdata.2016.126,An extensive dataset of eye movements during viewing of complex images,31,1,2017,,,2017-01-31
0,Scientific Data,41597,10.1038/sdata.2017.149,Building a locally diploid genome and transcriptome of the diatom,10,10,2017,,,2017-10-10
0,Scientific Data,41597,10.1038/sdata.2018.151,High-throughput computational X-ray absorption spectroscopy,31,7,2018,,,2018-07-31
0,Scientific Data,41597,10.1038/sdata.2017.92,A metagenomic survey of forest soil microbial communities more than a decade after timber harvesting,25,7,2017,,,2017-07-25
0,Scientific Data,41597,10.1038/sdata.2018.53,An open experimental database for exploring inorganic materials,3,4,2018,,,2018-04-03
0,Scientific Data,41597,10.1038/sdata.2017.161,"Nasopharyngeal metagenomic deep sequencing data, Lancaster, UK, 2014–2015",24,10,2017,,,2017-10-24
0,Scientific Data,41597,10.1038/sdata.2018.37,A dataset on human navigation strategies in foreign networked systems,13,3,2018,,,2018-03-13
0,Scientific Data,41597,10.1038/sdata.2018.169,A global yield dataset for major lignocellulosic bioenergy crops based on field measurements,21,8,2018,,,2018-08-21
0,Scientific Data,41597,10.1038/sdata.2018.172,The first annotated set of scanning electron microscopy images for nanoscience,28,8,2018,,,2018-08-28
0,Scientific Data,41597,10.1038/sdata.2018.175,"HISDAC-US, historical settlement data compilation for the conterminous United States over 200 years",4,9,2018,,,2018-09-04
0,Scientific Data,41597,10.1038/sdata.2018.178,"The eICU Collaborative Research Database, a freely available multi-center database for critical care research",11,9,2018,,,2018-09-11
0,Scientific Data,41597,10.1038/sdata.2018.277,Genome analysis of  subspecies  strain 109,4,12,2018,,,2018-12-04
0,Scientific Data,41597,10.1038/sdata.2018.105,"Individual Brain Charting, a high-resolution fMRI dataset for cognitive mapping",12,6,2018,,,2018-06-12
0,Scientific Data,41597,10.1038/sdata.2017.188,Two-colour serial femtosecond crystallography dataset from gadoteridol-derivatized lysozyme for MAD phasing,12,12,2017,,,2017-12-12
0,Scientific Data,41597,10.1038/sdata.2018.190,An underwater observation dataset for fish classification and fishery assessment,9,10,2018,,,2018-10-09
0,Scientific Data,41597,10.1038/sdata.2018.286,On the privacy-conscientious use of mobile phone data,11,12,2018,,,2018-12-11
0,Scientific Data,41597,10.1038/sdata.2018.111,Auto-generated materials database of Curie and Néel temperatures via semi-supervised relationship extraction,19,6,2018,,,2018-06-19
0,Scientific Data,41597,10.1038/sdata.2018.210,Gridded emissions and land-use data for 2005–2100 under diverse socioeconomic and climate mitigation scenarios,16,10,2018,,,2018-10-16
0,Scientific Data,41597,10.1038/sdata.2018.23,"Datasets2Tools, repository and search engine for bioinformatics datasets, tools and canned analyses",27,2,2018,,,2018-02-27
0,Scientific Data,41597,10.1038/sdata.2018.193,Reference data on thickness and mechanics of tissue layers and anthropometry of musculoskeletal extremities,25,9,2018,,,2018-09-25
0,Scientific Data,41597,10.1038/sdata.2018.292,The metabolic regimes of 356 rivers in the United States,11,12,2018,,,2018-12-11
0,Scientific Data,41597,10.1038/sdata.2017.85,An  electronic transport database for inorganic materials,4,7,2017,,,2017-07-04
0,Scientific Data,41597,10.1038/sdata.2017.118,A gene expression atlas of adult  and their gonads,22,8,2017,,,2017-08-22
0,Scientific Data,41597,10.1038/sdata.2018.15,"ImmPort, toward repurposing of open access immunological assay data for translational and clinical research",27,2,2018,,,2018-02-27
0,Scientific Data,41597,10.1038/sdata.2018.295,Effective fetch and relative exposure index maps for the Laurentian Great Lakes,18,12,2018,,,2018-12-18
0,Scientific Data,41597,10.1038/sdata.2018.117,Sustainable data and metadata management at the BD2K-LINCS Data Coordination and Integration Center,19,6,2018,,,2018-06-19
0,Scientific Data,41597,10.1038/sdata.2018.8,Dynamic contrast-enhanced magnetic resonance imaging for head and neck cancers,13,2,2018,,,2018-02-13
0,Scientific Data,41597,10.1038/sdata.2017.46,"Spatiotemporal historical datasets at micro-level for geocoded individuals in five Swedish parishes, 1813–1914",11,4,2017,,,2017-04-11
0,Scientific Data,41597,10.1038/sdata.2018.199,Transcriptomes of cochlear inner and outer hair cells from adult mice,2,10,2018,,,2018-10-02
0,Scientific Data,41597,10.1038/sdata.2017.127,Machine-learned and codified synthesis parameters of oxide materials,12,9,2017,,,2017-09-12
0,Scientific Data,41597,10.1038/sdata.2018.40,"A suite of global, cross-scale topographic variables for environmental and biodiversity modeling",20,3,2018,,,2018-03-20
0,Scientific Data,41597,10.1038/sdata.2018.141,"MANTA2, update of the Mongo database for the analysis of transcription factor binding site alterations",24,7,2018,,,2018-07-24
0,Scientific Data,41597,10.1038/sdata.2018.63,A high-resolution probabilistic  atlas of human subcortical brain nuclei,17,4,2018,,,2018-04-17
0,Scientific Data,41597,10.1038/sdata.2016.125,Evaluation and comparison of classical interatomic potentials through a user-friendly interactive web-interface,31,1,2017,,,2017-01-31
0,Scientific Data,41597,10.1038/sdata.2017.94,"The research infrastructure of Chinese foundations, a database for Chinese civil society studies",25,7,2017,,,2017-07-25
0,Scientific Data,41597,10.1038/sdata.2017.151,"Clustergrammer, a web-based heatmap visualization and analysis tool for high-dimensional biological data",10,10,2017,,,2017-10-10
0,Scientific Data,41597,10.1038/sdata.2018.147,Selection analyses of paired HIV-1  and  sequences obtained before and after antiretroviral therapy,24,7,2018,,,2018-07-24
