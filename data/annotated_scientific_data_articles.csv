,Unnamed: 0,journalName,journalId,doi,title,pubDay,pubMonth,pubYear,codeLink,codeText,pubDate,issues_1,issues_2,issues_3
0,0,Scientific Data,41597,10.1038/s41597-024-03158-7,Mapping Road Surface Type of Kenya Using OpenStreetMap and High-resolution Google Satellite Imagery,3,4,2024,https://github.com/Dsayddd/RoadSurface,The data files and the python scripts used for model training are available online through GitHub repository: .,2024-04-03,,,
1,0,Scientific Data,41597,10.1038/s41597-024-03214-2,High spatial resolution elevation change dataset derived from ICESat-2 crossover points on the Tibetan Plateau,17,4,2024,https://github.com/snowhydro/icesat-cross-point,The script used to process the ICESat-2 data and extract ICESat-2 crossover points from it is available at the following link: .,2024-04-17,,,
3,0,Scientific Data,41597,10.1038/s41597-024-03358-1,CheXmask: a large-scale dataset of anatomical segmentation masks for multi-center chest x-ray images,17,5,2024,https://github.com/ngaggion/CheXmask-Database,"The code associated with this study is available in our Github repository: . The repository encompasses Python 3 code for various components, including data preparation, data post-processing, technical validation, and the deep learning models. The data preparation section includes scripts for preprocessing the images. The data post-processing section provides scripts for converting the segmentation masks from run-length encoding to a binary mask format, examples of how to read the model and the necessary code to revert the pre-processing steps for each dataset. The technical validation section includes the code for the individual RCA analysis and the processing of the physician results. Additionally, the repository includes the code for the deep learning models used for image segmentation, including the HybridGNet model architecture, weights, training and inference scripts. The software prerequisites for running the code are outlined in the repository’s README file.",2024-05-17,,,
6,0,Scientific Data,41597,10.1038/s41597-024-03273-5,Mapping annual 10-m soybean cropland with spatiotemporal sample migration,2,5,2024,https://github.com/ZihangLou/ChinaSoybean10,"The programs used to generate the datasets and all the results were ESRI ArcGIS (10.6), Python (3.7 or 3.8) and Google Earth Engine (GEE). The scripts utilized for ChinaSoybean10 described in this paper can be accessed at .",2024-05-02,,,
9,0,Scientific Data,41597,10.1038/s41597-024-03330-z,"A large and diverse brain organoid dataset of 1,400 cross-laboratory images of 64 trackable brain organoids",20,5,2024,https://github.com/LabTrivedi/MOrgAna,"The code for training MOrgAna and the SegFormer is publicly available on GitHub:  and. The data splits for MOrgAna and SegFormer training and evaluation, the configuration files for SegFormer training, the CellProfiler project as well as the workflow for the Technical Validation are publicly available on GitHub and co-deposited on Zenodo.",2024-05-20,,,
11,0,Scientific Data,41597,10.1038/s41597-024-03217-z,Dataset of weekly intra-treatment diffusion weighted imaging in head and neck cancer patients treated with MR-Linac,11,5,2024,https://github.com/kwahid/Weekly_DWI_Data_Descriptor,Codes used for data annotation are available through GitHub: ().,2024-05-11,,,
12,0,Scientific Data,41597,10.1038/s41597-024-03275-3,"A new commercial boundary dataset for metropolitan areas in the USA and Canada, built from open data",24,4,2024,https://github.com/schoolofcities/commercial-boundaries,"The codes used in this study are available from Figshare or the School of cities GitHub (). In addition, modified metropolitan areas, a saved Pytorch state file and commercial boundary result data can also be obtained from those pages. Readers can easily replicate the identified commercial boundary using the given codes and datasets.",2024-04-24,,,
13,0,Scientific Data,41597,10.1038/s41597-024-03302-3,Parsimonious estimation of hourly surface ozone concentration across China during 2015–2020,14,5,2024,https://github.com/Wenxiu0902/Ozone_prediction,"The code is available on GitHub () primarily using Python and R languages. It includes data preprocessing, model training, testing, prediction, and visualization sections. Additionally, sample model input data is also provided.",2024-05-14,,,
15,0,Scientific Data,41597,10.1038/s41597-024-03218-y,Large-scale annotated dataset for cochlear hair cell detection and classification,23,4,2024,https://github.com/indzhykulianlab/hcat-data,"All associated code for downloading, loading, and preprocessing this dataset may be found at: .",2024-04-23,,,
19,0,Scientific Data,41597,10.1038/s41597-024-03304-1,A unified dataset for pre-processed climate indicators weighted by gridded economic activity,24,5,2024,https://github.com/CoMoS-SA/climaterepo,"Python code running the  dashboard and scripts for aggregating data are available at . The  leverages . We employed  to process the data, exploiting package  for the weighted aggregations.",2024-05-24,,,
23,0,Scientific Data,41597,10.1038/s41597-024-03392-z,ACcoding: A graph-based dataset for online judge programming,29,5,2024,https://github.com/KarryBramley/ACcoding-Dataset,All the code is freely accessible in .,2024-05-29,,,
24,0,Scientific Data,41597,10.1038/s41597-024-03193-4,Cataract-1K Dataset for Deep-Learning-Assisted Analysis of Cataract Surgery Videos,12,4,2024,https://github.com/Negin-Ghamsarian/Cataract-1K,"We provide all code for mask creation using JSON annotations and phase extraction using CSV files, as well as the training IDs for four-fold validation and usage instructions in the GitHub repository of the paper ().",2024-04-12,,,
26,0,Scientific Data,41597,10.1038/s41597-024-03364-3,A daily gap-free normalized difference vegetation index dataset from 1981 to 2023 in China,22,5,2024,https://github.com/mainearth/Daily-Gap-free-NDVI-Code.git,The Python codes for generating and processing the daily gap-free NDVI data in China can be accessed through GitHub ().,2024-05-22,,,
28,0,Scientific Data,41597,10.1038/s41597-024-03250-y,The Imperial College Storm Model (IRIS) Dataset,24,4,2024,https://github.com/njsparks/iris,The IRIS code is publicly available () and a release of the version described here has been archived.,2024-04-24,,,
31,0,Scientific Data,41597,10.1038/s41597-024-03222-2,A multimodal physiological dataset for driving behaviour analysis,12,4,2024,https://github.com/zwqzwq0/MPDB,"Readers can access the tutorials and code of our original and preprocessed datasets on Github (). Two folders called preprocessing and classification can be found, which contain MATLAB code for preprocessing and python code for classification.",2024-04-12,,,
32,0,Scientific Data,41597,10.1038/s41597-024-03196-1,A document-level information extraction pipeline for layered cathode materials for sodium-ion batteries,11,4,2024,https://github.com/GGNoWayBack/cathodedataextractor,The source code of the document-level information extraction pipeline is available at .,2024-04-11,,,
35,0,Scientific Data,41597,10.1038/s41597-024-03281-5,Binary dataset for machine learning applications to tropical cyclone formation prediction,3,5,2024,https://github.com/kieucq/tcg-binary-dataset,Both the code and dataset presented herein are fully accessible on our GitHub repository at . All Python codes follow the standard GNU Open Source Licence.,2024-05-03,,,
36,0,Scientific Data,41597,10.1038/s41597-024-03367-0,Metricizing policy texts: Comprehensive dataset on China’s Agri-policy intensity spanning 1982–2023,22,5,2024,https://github.com/YNAU-WYH/Agricultural-policy-dataset.git,The code data utilized for computations and analyses within this paper is accessible via the GitHub repository at ().,2024-05-22,,,
46,0,Scientific Data,41597,10.1038/s41597-024-03398-7,ChineseEEG: A Chinese Linguistic Corpora EEG Dataset for Semantic Alignment and Neural Decoding,29,5,2024,https://github.com/ncclabsustech/Chinese_reading_task_eeg_processing,"The code for all modules is openly available on GitHub (). All scripts were developed in Python 3.10. Package openpyxl v3.1.2 was utilized to export segmented text in Excel (.xlsx) files, and egi-pynetstation v1.0.1, g3pylib v0.1.1, psychopy v2023.2.3 were used to implement the scripts for EGI device control, Tobii eye-tracker control, stimuli presentation respectively. In the data pre-processing scripts, MNE v1.6.0, pybv v0.7.5, pyprep v0.4.3, mne-iclabel v0.5.1 were used to implement the pre-processing pipeline, while mne-bids v0.14 was used to organize the data into BIDS format. The text embeddings were calculated using Hugging Face transformers v4.36.2. For more details about code usage, please refer to the GitHub repository.",2024-05-29,,,
50,0,Scientific Data,41597,10.1038/s41597-024-03371-4,Exploring SureChEMBL from a drug discovery perspective,16,5,2024,https://github.com/Fraunhofer-ITMP/patent-clinical-candidate-characteristics,The Python scripts and Jupyter notebooks supporting the conclusions of this study can be accessed and downloaded via GitHub (). The repository is structured into “Data” and “Notebook” sections. The “Data” section is the exact replica of the Zenodo dump mentioned previously. The “Notebook” section includes all the analyses presented in this study organized based on the sections within the results.,2024-05-16,,,
54,0,Scientific Data,41597,10.1038/s41597-024-03144-z,MultiSenseBadminton: Wearable Sensor–Based Biomechanical Dataset for Evaluation of Badminton Performance,5,4,2024,https://github.com/dailyminiii/MultiSenseBadminton,"Software is available on GitHub and can be accessed via the following link. . This comprehensive software package includes examples for reading and parsing HDF5 files, performing data preprocessing by extracting and filtering, and displaying the results. Additionally, it offers functionality for training a deep-learning model using the preprocessed data, generating a T-SNE plot based on the preprocessed data, and creating a visualization video based on the raw data presented in Fig. .",2024-04-05,,,
57,0,Scientific Data,41597,10.1038/s41597-024-03317-w,A Dataset for Evaluating Contextualized Representation of Biomedical Concepts in Language Models,4,5,2024,https://github.com/hrouhizadeh/BioWiC,"The entire process, including the development of the dataset and the conduction of experiments, was implemented using the Python programming language. The complete code and dataset are hosted on GitHub at: .",2024-05-04,,,
61,0,Scientific Data,41597,10.1038/s41597-024-03290-4,A high-resolution dataset of water bodies distribution over the Tibetan Plateau,4,5,2024,https://github.com/Siyu1993/WaterPreprocessing,"Codes for the dataset pre-processing are written using python, including TIFF read, morphological opening-and-closing operation, TIFF write and mosaic process. The codes are available at: . Then the image could be visualized in QGIS software (V3.16).",2024-05-04,,,
65,0,Scientific Data,41597,10.1038/s41597-024-03234-y,An EEG Dataset of Neural Signatures in a Competitive Two-Player Game Encouraging Deceptive Behavior,16,4,2024,https://github.com/yiyuchen-lab/DeceptionGame,"The EEG preprocessing, ERP analysis code and code used for classification is avaliable at .",2024-04-16,,,
67,0,Scientific Data,41597,10.1038/s41597-024-03179-2,Dataset of building locations in Poland in the 1970s and 1980s,5,4,2024,https://github.com/Szubbi/WallToWallMapingBuildingsPoland,Data processing was performed using the ESRI Arcpy Python Library. Spatial analysis and map production was done using ArcGIS Pro software. Plots were generated with Matplotlib Python Library. Code written for maps processing and detections postprocessing is available at GitHub repository: .,2024-04-05,,,
71,0,Scientific Data,41597,10.1038/s41597-024-03236-w,Sea-surface CO maps for the Bay of Bengal based on advanced machine learning algorithms,13,4,2024,https://github.com/APJ1812/INCOIS_pCO2,The code used to create the final product (different machine learning models) is available at . The study uses general machine learning codes available in Python.,2024-04-13,,,
72,0,Scientific Data,41597,10.1038/s41597-024-03321-0,"A tree-based corpus annotated with Cyber-Syndrome, symptoms, and acupoints",10,5,2024,https://github.com/Xiduoduosci/CS_A_corpus,"Write data analysis code using Python and install packages such as Nltk, Numpy, and Pandas to assist. The code runs on the local computer. The data annotation in this paper uses the Brat tool (version: Brat-1.3p1), running on a Linux system. The code is mainly used for CS-A corpus generation and the quality analysis of the corpus. The code has been uploaded to the  repository and is accessible using the following link: .",2024-05-10,,,
73,0,Scientific Data,41597,10.1038/s41597-024-03122-5,Registered multi-device/staining histology image dataset for domain-agnostic machine learning models,3,4,2024,https://github.com/p024eb/PLISM-registration,All codes used in the image registration between WSI and smartphone images described in the manuscript were written in Python 3 and are available through our GitHub repository (). We have provided all the necessary libraries and python scripts that allow the tracing of our results.,2024-04-03,,,
75,0,Scientific Data,41597,10.1038/s41597-024-03208-0,The Plegma dataset: Domestic appliance-level and aggregate electricity demand with metadata from Greece,12,4,2024,https://github.com/sathanasoulias/Plegma-Dataset,"The dataset can be efficiently managed, visualized and preprocessed using four Jupyter notebooks. These notebooks are accessible for download at  To ensure the proper functioning of these notebooks, it is necessary to have Python version 3 along with the Pandas, Plotly and Numpy libraries installed. Moreover, the primary Javascript functions used in the data collection process (Z-wave service and DataBroker service) are located in the data_collection folder giving more details about the implementation of such a system.",2024-04-12,,,
76,0,Scientific Data,41597,10.1038/s41597-024-03322-z,A chromosome-level genome assembly of an avivorous bat species (),10,5,2024,https://github.com/life404/genome-NycAvi.git,"In this study, all analyses were conducted following the manuals and tutorials of software and pipeline. The detailed software versions are specified in the methods section. Unless specified otherwise, default or author-recommended parameters were used for software and analysis pipeline. Detailed information about the parameters and custom scripts utilized in this research can be obtained by downloading them from .",2024-05-10,,,
83,0,Scientific Data,41597,10.1038/s41597-024-03353-6,An fNIRS dataset for driving risk cognition of passengers in highly automated driving scenarios,28,5,2024,https://github.com/benchidefeng/fNIRS-experiment-for-automated-driving-scenarios.git,The relational codes and example mentioned in this study and a brief description (readme.md) have been uploaded in github  Or Please contact the corresponding author with any further queries regarding code availability.,2024-05-28,,,
99,0,Scientific Data,41597,10.1038/s41597-024-03386-x,"A large-scale multivariate soccer athlete health, performance, and position monitoring dataset",30,5,2024,http://www.github.com/simula/soccermon,"We provide a sample codebase that can be used to import and structure the raw data from the  dataset, as a public software repository on GitHub: .",2024-05-30,,,
102,0,Scientific Data,41597,10.1038/s41597-024-02959-0,ezBIDS: Guided standardization of neuroimaging data interoperable with major data archives and platforms,8,2,2024,https://github.com/brainlife/ezbids,All code is publicly available on our GitHub repository: .,2024-02-08,,,
106,0,Scientific Data,41597,10.1038/s41597-024-03074-w,High resolution climate change observations and projections for the evaluation of heat-related extremes,1,3,2024,https://github.com/emilylynnwilliams/CHC-CMIP6_SourceCode,"The equations used to calculate RH, VPD, and WBGT are available in R on GitHub (). The CHC-CMIP6 dataset was processed using code written in the Interactive Data Language and Python.",2024-03-01,,,
108,0,Scientific Data,41597,10.1038/s41597-024-02960-7,Pig-eRNAdb: a comprehensive enhancer and eRNA dataset of pigs,1,2,2024,https://github.com/WangYF33/CNNEE,All CNNEE code for enhancer prediction and eRNA identification is publicly available at .,2024-02-01,,,
112,0,Scientific Data,41597,10.1038/s41597-024-03018-4,Homologous Recombination Deficiency Unrelated to Platinum and PARP Inhibitor Response in Cell Line Libraries,6,2,2024,https://github.com/shirotak/CellLine_HRD_DrugRes,Codes to reproduce the results of this work are available on the Figshare and the GitHub project page ().,2024-02-06,,,
113,0,Scientific Data,41597,10.1038/s41597-024-03132-3,Engagnition: A multi-dimensional dataset for engagement recognition of children with autism spectrum disorder,15,3,2024,https://github.com/dailyminiii/Engagnition,The Engagnition software is available to the public through its official repository (). This repository mainly includes the code for analyzing data distribution and technical validation.,2024-03-15,,,
114,0,Scientific Data,41597,10.1038/s41597-024-02933-w,A dataset of oracle characters for benchmarking machine learning algorithms,18,1,2024,https://github.com/wm-bupt/oracle-mnist,Oracle-MNIST are freely available online at GitHub (). Tutorials for loading the dataset and code for training and testing oracle character recognition models are also publicly available without restriction.,2024-01-18,,,
119,0,Scientific Data,41597,10.1038/s41597-024-02963-4,"High frequency Lunar Penetrating Radar quality control, editing and processing of Chang’E-4 lunar mission",24,1,2024,https://github.com/Giacomo-Roncoroni/LPR_CE4,The codes for the described algorithm will be available in Figshare and at .,2024-01-24,,,
122,0,Scientific Data,41597,10.1038/s41597-023-02886-6,Electricity and natural gas tariffs at United States wastewater treatment plants,23,1,2024,https://github.com/we3lab/wwtp-energy-tariffs,The data is provided as Excel and CSV spreadsheets that can be used without code for manipulation. Sample Python scripts are available on GitHub to ease analysis and demonstrate technical validation procedures ().,2024-01-23,,,
124,0,Scientific Data,41597,10.1038/s41597-024-03164-9,Harnessing Big Data in Critical Care: Exploring a new European Dataset,28,3,2024,https://github.com/nrodemund/sicdb,"All publicly available code can be accessed from the SICdb GitHub Code Repository (). However, due to the partial use of the code to appropriately remove sensitive patient information in accordance with HIPAA regulations, not all codes are fully publicly accessible. Furthermore, the GDPR restricts the sharing of certain code components to ensure the highest level of anonymization.",2024-03-28,,,
125,0,Scientific Data,41597,10.1038/s41597-024-02994-x,A 10-m national-scale map of ground-mounted photovoltaic power stations in China of 2020,13,2,2024,https://github.com/MrSuperNiu/PV_ScientificData_Classification_Code,"The GEE code for PV power stations classification based on Sentinel-2 imagery and DEM data is available at . The code is written in JavaScript, including all the mentioned steps in this paper, including feature calculation, random forest training, etc.",2024-02-13,,,
129,0,Scientific Data,41597,10.1038/s41597-024-02908-x,Annotated dataset for training deep learning models to detect astrocytes in human brain tissue,19,1,2024,https://github.com/qbeer/coco-froc-analysis,In order to do the evaluation we made the Python package () accessible. We generated FROC curves with this tool and generally it is possible to use it for binary evaluation for COCO formatted data.,2024-01-19,,,
133,0,Scientific Data,41597,10.1038/s41597-024-03081-x,An Integrated Multi-Source Dataset for Measuring Settlement Evolution in the United States from 1810 to 2020,7,3,2024,https://github.com/YoonjungAhn/HISTPLUS,Code for analysis and validation is available at .,2024-03-07,,,
135,0,Scientific Data,41597,10.1038/s41597-024-03053-1,County-scale dataset indicating the effects of disasters on crops in Taiwan from 2003 to 2022,14,2,2024,https://github.com/YuanChihSu/Crop_Disaster_Dataset,Most of the weather data used in this study were downloaded using a Python script. Only weather data obtained from agricultural weather stations were manually downloaded. All datasets were processed and analyzed using SAS. The Python and SAS codes are available at . A full list of weather station codes and altitudes is also provided.,2024-02-14,,,
137,0,Scientific Data,41597,10.1038/s41597-024-03025-5,A synthetic digital city dataset for robustness and generalisation of depth estimation models,16,3,2024,https://github.com/ReparkHjc/SDCD,"A git repository is publicly available at , in this repository several python scripts for visualisation, benchmarking and data pre-processing are available.",2024-03-16,,,
138,0,Scientific Data,41597,10.1038/s41597-024-03054-0,Sea water temperature and light intensity at high-Arctic subtidal shallows – 16 years perspective,22,2,2024,https://github.com/8ernabemoreno/Isfjorden-shallows_longterm-seawater-temp-lux,"The code that accompanies this data descriptor is publicly available in the GitHub repository . It contains Python code that might be useful for basic level users to create CF-NetCDF (.nc) files from .csv, and (ii) minimally process long-term data (e.g., annual, and monthly means).",2024-02-22,,,
143,0,Scientific Data,41597,10.1038/s41597-024-02913-0,A 31-year (1990–2020) global gridded population dataset generated by cluster analysis and statistical learning,24,1,2024,https://github.com/lulingliu/GlobPOP,The fully reproducible codes are publicly available at GitHub ().,2024-01-24,,,
147,0,Scientific Data,41597,10.1038/s41597-024-02943-8,Dataset of human-single neuron activity during a Sternberg working memory task,18,1,2024,https://github.com/rutishauserlab/workingmem-release-NWB,All code associated with this project is available as open source. The code is available on GitHub (). MATLAB scripts are included in this repository to reproduce all figures shown and to illustrate how to use the data.,2024-01-18,,,
150,0,Scientific Data,41597,10.1038/s41597-023-02780-1,Remotely sensed above-ground storage tank dataset for object detection and infrastructure assessment,12,1,2024,https://github.com/celinerobi/ast-data-pipeline,The raw aerial imagery and annotation tools used in this study are publicly accessible. The source code developed by the authors to process the imagery and develop the tank inventory dataset are available on GitHub ().,2024-01-12,,,
155,0,Scientific Data,41597,10.1038/s41597-024-02945-6,Manually annotated and curated Dataset of diverse Weed Species in Maize and Sorghum for Computer Vision,23,1,2024,https://github.com/grimmlab/MFWD,The code to download the dataset is publicly available for download on GitHub: .,2024-01-23,,,
157,0,Scientific Data,41597,10.1038/s41597-023-02896-4,Wind and structural loads data measured on parabolic trough solar collectors at an operational power plant,19,1,2024,https://github.com/NREL/NSO_processing_scripts,"The Python processing routines for the met masts, lidar and loads data are publicly available at .",2024-01-19,,,
158,0,Scientific Data,41597,10.1038/s41597-024-03089-3,A city-level dataset of heavy metal emissions into the atmosphere across China from 2015–2020,29,2,2024,https://github.com/Olivia-2012/HMEAs_DataSet,"Data processing was performed in Python 3.10, and data used for the computation of HMEAs at city level are available can be accessed at Github repository located at . We implemented the procedure described in the Methods section.",2024-02-29,,,
159,0,Scientific Data,41597,10.1038/s41597-024-02975-0,A clinical microscopy dataset to develop a deep learning diagnostic test for urinary tract infection,1,2,2024,https://github.com/casus/UMOD,All code is available from  under MIT open source licence.,2024-02-01,,,
160,0,Scientific Data,41597,10.1038/s41597-023-02897-3,A database of thermally activated delayed fluorescent molecules auto-generated from scientific literature with ChemDataExtractor,17,1,2024,https://github.com/Dingyun-Huang/chemdataextractorTADF,The code used to generate the databases in this work can be found at . The repository contains ChemDataExtractor v2.1 which has been modified for text-mining TADF properties; iPython notebooks that demonstrate an example data-extraction pipeline and data-cleaning and post-processing are also provided.,2024-01-17,,,
161,0,Scientific Data,41597,10.1038/s41597-024-03031-7,The Allen Ancient DNA Resource (AADR) a curated compendium of ancient human genomes,10,2,2024,https://github.com/DReichLab/adna-workflow,"The pipeline used for processing raw data generated within the Reich lab is available in the ‘Workflow Description Language’ (WDL) here: , and includes individual python scripts for components of the pipeline.",2024-02-10,,,
163,0,Scientific Data,41597,10.1038/s41597-024-03146-x,The extrachromosomal circular DNA atlas of aged and young mouse brains,27,3,2024,https://github.com/XiaoningHong/MouseBrain_ScientificData,The codes used to analyze the data in this study are available in the GitHub repository at the following URL: ().,2024-03-27,,,
164,0,Scientific Data,41597,10.1038/s41597-024-02918-9,RailFOD23: A dataset for foreign object detection on railroad transmission lines,16,1,2024,https://github.com/CV-Altrai2023/RailFOD23,We released and shared the code for our data synthesis().,2024-01-16,,,
165,0,Scientific Data,41597,10.1038/s41597-024-02976-z,miR-Blood – a small RNA atlas of human blood components,2,2,2024,https://github.com/gitHBDX/mirblood-code,"The code used for data pre-processing has been deposited on . The following software versions were used: unitas v1.7.7, SeqMap v1.0.13, SPORTS v1.1, Bowtie v1.3, SCANPY v1.8.2, Python v3.10.6, Plotly v5.10.0, Plotly Express v0.4.1, SciPy v1.9.1, Seaborn v0.12.2, and UpSetPlot v0.8.0.",2024-02-02,,,
167,0,Scientific Data,41597,10.1038/s41597-024-03061-1,A simulated ‘sandbox’ for exploring the modifiable areal unit problem in aggregation and disaggregation,24,2,2024,https://github.com/jjniev01/areal_sandbox,"The code utilised in producing this dataset was originally a series of individual scripts in R and, for submitting jobs, to the HPC, in . We have compiled these scripts, including job submission scripts, into a single ordered  notebook to ease comprehension and replicability. All packages indicated in the notebook utilised the most recent version available on November 1, 2021. The code notebook is available at the following Github repository release: .",2024-02-24,,,
173,0,Scientific Data,41597,10.1038/s41597-024-02950-9,An open dataset on individual perceptions of transport policies,22,1,2024,https://github.com/Urban-Analytics/UTM-Hanoi,"The code used for exploratory data analysis, validation and visualization in this study is openly available for access and use. The codebase, which includes Jupyter Notebooks, Python scripts, and relevant libraries, is hosted on a public GitHub repository (). The code is distributed under the MIT License, allowing for modification, distribution, and reuse, as long as proper credit is given to the original authors and the license terms are followed.",2024-01-22,,,
176,0,Scientific Data,41597,10.1038/s41597-024-02922-z,Mass spectrometry-based proteomics data from thousands of HeLa control samples,23,1,2024,github.com/RasmussenLab/hela_qc_mnt_data,"The code used for preparing the PRIDE data upload, the creation of curated data views on the University of Copenhagen FTP large file storage called ERDA, the workflows for sample raw file processing are available on . The software used for processing is provided as a python package in the provided GitHub repository.",2024-01-23,,,
177,0,Scientific Data,41597,10.1038/s41597-023-02844-2,Fine-grained urban blue-green-gray landscape dataset for 36 Chinese cities based on deep learning network,4,3,2024,https://github.com/Zhiyu-Xu/Fine-grained-urban-blue-green-gray-landscape-dataset-for-36-Chinese-cities,"The programs used to generate the dataset were ENVI (5.3), ESRI ArcGIS (10.6) and Pytorch deep learning framework. All used codes to generate the dataset are available in the following GitHub ().",2024-03-04,,,
179,0,Scientific Data,41597,10.1038/s41597-024-02981-2,Preoperative CT and survival data for patients undergoing resection of colorectal liver metastases,6,2,2024,https://github.com/lassoan/LabelmapToDICOMSeg,Code for converting DICOM images with segmentation masks to standard DICOM segmentation objects is available on GitHub: .,2024-02-06,,,
181,0,Scientific Data,41597,10.1038/s41597-024-03067-9,Wind turbine database for intelligent operation and maintenance strategies,29,2,2024,https://github.com/alecuba16/fuhrlander,"The turbine dataset was generated by aggregating the SCADA data obtained from the entire wind farm. It consists of five wind turbines, all of them of the same model and manufacturer: Fuhrländer FL2500 2.5 MW. To facilitate the manipulation and pre-processing of the data, we have developed functions in the programming languages R and MATLAB to serve as an interface. These functions efficiently transform the raw data into a structured table format. In this format, each variable corresponds to a column, while each entry represents a five-minute interval of data recorded in the rows. The database and the code are freely available at and at the GitHub page .",2024-02-29,,,
189,0,Scientific Data,41597,10.1038/s41597-023-02877-7,Mock community taxonomic classification performance of publicly available shotgun metagenomics pipelines,17,1,2024,https://github.com/mvee18/benchmarkingpaper,All relevant code used in these analyses can be found at  and in the figshare repository. The README in either repository provides additional useful information for the usage and description of files.,2024-01-17,,,
191,0,Scientific Data,41597,10.1038/s41597-024-03155-w,A Dataset of Electrical Components for Mesh Segmentation and Computational Geometry Research,22,3,2024,https://github.com/bensch98/eec-analysis,"Figures ,  were generated using the data provided in the data set using the open source library Open3D. To facilitate the reproduction of these figures, a copy of the raw data is included in the repository’s corresponding directory. Data set preparation was done with a combination of libraries, including Open3D, numpy and bpy. Blender version 3.3.0 was used to label the 234 triangle meshes. Further on, various functions were programmed to interact with the data set. These include functions for scaling labels up and down, cropping triangle meshes by labels, converting vertex labels to triangle or edge labels and calculating surface, volume and cluster centroids of the cropped out meshes like the labelled regions. Additionally, based on the technology, category, and subcategory, the components can be filtered. All the software used in the study are open source available at .",2024-03-22,,,
195,0,Scientific Data,41597,10.1038/s41597-024-02928-7,A comparative wordlist for investigating distant relations among languages in Lowland South America,18,1,2024,https://github.com/pano-tacanan-history/blumpanotacana,"All code that has been used during the creation of this dataset is published on Zenodo (v0.2) and curated on GitHub (). For converting the data to CLDF, we have used the Python tools cldfbench (v1.13.0) using the pylexibank plugin (v3.4.0). The dataset is linked to Concepticon (v3.1.0), Glottolog (v4.7), and CLTS (v2.2.0). The code for integrating data with other datasets via SQL is presented in the main README.md. The scripts that were used to create the plots and to compute the coverage and synonymy is part of the ‘analysis’ folder, where another README.md file leads through the replication of all necessary steps. The code for the initial addition of IDS data is added to ‘raw/archive/‘ for documentation. This list was then filtered while finalizing the concept list. All the orthography profiles that are used during the conversion of graphemes are part of ‘etc/orthography’.",2024-01-18,,,
197,0,Scientific Data,41597,10.1038/s41597-024-03042-4,Mapping urban form into local climate zones for the continental US from 1986–2020,13,2,2024,https://github.com/QiMengEnv/CONUS_Longitudinal_LCZ,"Python scripts for training data sampling, earth observation and census input feature collection, random forest model fine tuning and mode prediction on Google Earth Engine are available at . All data processing and visualizations are done in Python 3.9. The post-classification processing is done in JavaScript on Google Earth Engine Code Editor and is also available with the same URL.",2024-02-13,,,
198,0,Scientific Data,41597,10.1038/s41597-024-02958-1,"Ethnicity data resource in population-wide health records: completeness, coverage and granularity of diversity",22,2,2024,https://github.com/BHFDSC/CCU037_01,All code for data preparation and analysis are publicly available on GitHub ().,2024-02-22,,,
202,0,Scientific Data,41597,10.1038/s41597-023-02695-x,Improved global 250 m 8-day NDVI and EVI products from 2000–2021 using the LSTM model,14,11,2023,https://github.com/Xiongkovsky/glass_vis_lstm_code,The Python codes for generating and processing data and be accessed through GitHub ().,2023-11-14,,,
204,0,Scientific Data,41597,10.1038/s41597-023-02696-w,Reconstructing aerosol optical depth using spatiotemporal Long Short-Term Memory convolutional autoencoder,30,11,2023,https://github.com/lu-liang-geo/AOD-reconstruction,All code for processing the raw MCD19A2 HDF-EOS files as well as reconstructing the missing data is available on GitHub:  or Figshare. The code is all provided in Python using open-source libraries for reproducibility.,2023-11-30,,,
205,0,Scientific Data,41597,10.1038/s41597-023-02752-5,Introducing MEG-MASC a high-quality magneto-encephalography dataset for evaluating natural speech processing,4,12,2023,https://github.com/kingjr/meg-masc/,The code is available on .,2023-12-04,,,
206,0,Scientific Data,41597,10.1038/s41597-023-02781-0,"HALD, a human aging and longevity knowledge graph for precision gerontology and geroscience analyses",1,12,2023,https://github.com/zexuwu/hald,All code used in this paper can be downloaded on GitHub at .,2023-12-01,,,
223,0,Scientific Data,41597,10.1038/s41597-023-02786-9,High-resolution electric power load data of an industrial park with multiple types of buildings in China,6,12,2023,https://github.com/Industrialpark/SEMLab_HFUT-Building-Electricpowerloaddata,"The code implementation is done in the R programming language version 4.1.0 and MATLAB R2018a. The custom code used for data processing, technical validation, visualization is available on the github page ().",2023-12-06,,,
232,0,Scientific Data,41597,10.1038/s41597-023-02816-6,"ChillsDB 2.0: Individual Differences in Aesthetic Chills Among 2,900+ Southern California Participants",21,12,2023,https://github.com/ChillsTV/AffectiveStimuliScraper,The code for parsing YouTube and Reddit networks is available under an MIT license at .,2023-12-21,,,
233,0,Scientific Data,41597,10.1038/s41597-023-02646-6,The smarty4covid dataset and knowledge base as a framework for interpretable physiological audio data analysis,6,11,2023,https://github.com/smarty4covid/smarty4covid.git,"The audio classifier and the algorithm for extracting breathing features are available in a public repository (). Furthermore, the repository includes the weights of the CNNs used by the classifier and a script for generating triples from the available data for the purpose of customizing the smarty4covid OWL knowledge base.",2023-11-06,,,
234,0,Scientific Data,41597,10.1038/s41597-023-02890-w,Public transport accessibility indicators to urban and regional services in Great Britain,9,1,2024,https://github.com/urbanbigdatacentre/access_uk_open,All the source codes used for producing this dataset are openly available from the following link: . The versions of the relevant software used are stated for each of the key elements in the main body of the paper.,2024-01-09,,,
235,0,Scientific Data,41597,10.1038/s41597-023-02676-0,Monitoring the West Nile virus outbreaks in Italy using open access data,7,11,2023,https://github.com/fbranda/west-nile,"Refer to the README file accessible at the GitHub repository () for further instructions on how to use the dataset, import it either in R or Python, and carry out some exploratory analysis. The same link also hosts the dynamic version of WNVDB and all source codes to reproduce the results reported in this Data Descriptor.",2023-11-07,,,
236,0,Scientific Data,41597,10.1038/s41597-023-02703-0,Venom-gland transcriptomics and venom proteomics of the Tibellus oblongus spider,22,11,2023,https://github.com/levitsky/identipy,The script code for toxins cDNA analysis can be accessed as supplementary materials to the article “The mining of toxin-like polypeptides from EST database by single residue distribution analysis”. No special constants were used. Identity search engine is available at .,2023-11-22,,,
239,0,Scientific Data,41597,10.1038/s41597-023-02892-8,Changes in oscillatory patterns of microstate sequence in patients with first-episode psychosis,5,1,2024,https://github.com/zddzxxsmile/Chaos-game-representation-of-EEG-microstate,The code used in our analysis is available on GitHub ().,2024-01-05,,,
246,0,Scientific Data,41597,10.1038/s41597-023-02621-1,"IRIDIA-AF, a large paroxysmal atrial fibrillation long-term electrocardiogram monitoring database",18,10,2023,https://github.com/cedricgilon/iridia-af,The code described in the usage notes is available on GitHub (). It includes  tool and example code to start using IRIDIA-AF database.,2023-10-18,,,
250,0,Scientific Data,41597,10.1038/s41597-023-02766-z,Real-time speech MRI datasets with corresponding articulator ground-truth segmentations,2,12,2023,https://github.com/BartsMRIPhysics/Speech_MRI_2D_UNet,"The code that accompanies this article is publicly available in the following GitHub repository:  (software licence: Apache version 2.0). The repository contains already trained versions of a state-of-the-art speech MR image segmentation method that are ready to use immediately. These versions were trained using the datasets described in this article. The repository also contains instructions and Python code to train and evaluate new versions of the method using the datasets described in this article. The code is designed to allow users to choose several important training parameters such as the training and validation dataset split, the number of epochs of training, the learning rate and the mini-batch size. In addition, the code is designed to be compatible with any dataset as long as it is organised and named in a specific way. The repository contains Python code to check that the datasets are not corrupted and are organised and named in the specific way required by the segmentation method, as well as Python code to perform the image pre-processing required by the method, namely normalising the images and saving the normalised images as MAT files.",2023-12-02,,,
251,0,Scientific Data,41597,10.1038/s41597-023-02822-8,Long-term gridded land evapotranspiration reconstruction using Deep Forest with high generalizability,18,12,2023,https://github.com/FQMei/HG-Land-ET.git,"The Python code for dataset generation, validation, and visualization is available at .",2023-12-18,,,
257,0,Scientific Data,41597,10.1038/s41597-023-02682-2,Photos and rendered images of LEGO bricks,18,11,2023,https://github.com/LegoSorter,"Custom tools used to take photos, generate renders, annotate photos, and extract annotated bricks from the complete scene, including the trained neural networks, are publicly available through the Lego Sorter project and its repositories available at .",2023-11-18,,,
258,0,Scientific Data,41597,10.1038/s41597-023-02768-x,Surface oxygen concentration on the Qinghai-Tibet Plateau (2017–2022),15,12,2023,https://github.com/MysteriousBuddha/Surface-oxygen-concentration-on-the-Qinghai-Tibet-Plateau-2017-2022.git,"The code in this study for constructing the oxygen concentration estimation model and estimating the oxygen concentration distribution data on the QTP were based on Python 3.9.2, and the key packages were  and . The code can be found on GitHub ().",2023-12-15,,,
259,0,Scientific Data,41597,10.1038/s41597-023-02654-6,"SM2RAIN-Climate, a monthly global long-term rainfall dataset for climatological studies",31,10,2023,https://github.com/IRPIhydrology/sm2rain,"SM2RAIN algorithm code is available in python, R, and Matlab on GitHub ().",2023-10-31,,,
260,0,Scientific Data,41597,10.1038/s41597-023-02869-7,Constructing a finer-grained representation of clinical trial results from ClinicalTrials.gov,6,1,2024,https://github.com/xuanyshi/Finer-Grained-Clinical-Trial-Results,"The source codes of data collection, processing and analysis are stored at: ().",2024-01-06,,,
261,0,Scientific Data,41597,10.1038/s41597-023-02683-1,A brain MRI dataset and baseline evaluations for tumor recurrence prediction after Gamma Knife radiotherapy,8,11,2023,https://github.com/siolmsstate/brain_mri,"The repository of brain tumor recurrence prediction data can be found on our GitHub (). Pydicom version 2.3.0 and SimpleITK version 2.1.0 have been used in data preprocessing. The baseline model framework is generated using TensorFlow version 2.8.0. We release sample codes for users to get started with raw data, guiding through loading the data and all preprocessing steps. Fundamental data visualization is also available.",2023-11-08,,,
266,0,Scientific Data,41597,10.1038/s41597-023-02627-9,Subjective data models in bioinformatics and how wet lab and computational biologists conceptualise data,2,11,2023,https://github.com/yochannah/subjective-data-models-analysis,Analysis code is deposited on Zenodo and on GitHub at . Code is shared under a permissive MIT licence.,2023-11-02,,,
267,0,Scientific Data,41597,10.1038/s41597-023-02656-4,LESO: A ten-year ensemble of satellite-derived intercontinental hourly surface ozone concentrations,25,10,2023,https://github.com/soonyenju/LESO,"The scripts for processing and reading the LESO datasets are accessible on Github () under the MIT license. The tools and libraries, including Python v3.9, Numpy v1.20.3, Xarray v0.19.0, Pandas v1.3.3, Deep Forest v2021.2.1 (DF21), scigeo v0.0.13, and sciml v0.0.5, were used to build the LESO framework for generating datasets of surface O concentrations. The validation of LESO datasets was processed using scitbx v0.0.42 and scikit-learn v0.24.2.",2023-10-25,,,
271,0,Scientific Data,41597,10.1038/s41597-023-02687-x,"CELLULAR, A Cell Autophagy Imaging Dataset",16,11,2023,https://github.com/simula/cellular,The code and models used to perform the experiments are available online at the following link: .,2023-11-16,,,
273,0,Scientific Data,41597,10.1038/s41597-023-02873-x,"A multilayered urban tree dataset of point clouds, quantitative structure and graph models",4,1,2024,https://github.com/hadi-yazdi/TreeML-Data,"The TreeML-SM script “TreeML-SM.py”, transformation script “point_transformation.py” for location transformation from project coordinate system to global coordinate system, and pre-trained point cloud segmentation model are published in the GitHub repository (). Please refer to the Readme file in the Github repository for further information.",2024-01-04,,,
276,0,Scientific Data,41597,10.1038/s41597-023-02801-z,Towards understanding policy design through text-as-data approaches: The policy design annotations (POLIANNA) dataset,13,12,2023,https://github.com/kueddelmaier/POLIANNA,"Accompanying scripts are made available at . The repository contains scripts to split the raw policy text retrieved from EUR-Lex into articles, to process new data labeled with Inception, and to generate summary statistics.",2023-12-13,,,
277,0,Scientific Data,41597,10.1038/s41597-023-02846-0,An electricity smart meter dataset of Spanish households: insights into consumption patterns,10,1,2024,https://github.com/DeustoTech/GoiEner-dataset,The code files used to process the dataset provided by GoiEner are publicly available on GitHub () and are licensed under the GPL-3.0. The repository includes a comprehensive  file with detailed information and instructions on how to run the code. The code is written in R version 4.2.2 and is compatible with both Windows and Linux operating systems.,2024-01-10,,,
280,0,Scientific Data,41597,10.1038/s41597-023-02802-y,A  heart optical coherence microscopy dataset for automatic video segmentation,9,12,2023,https://github.com/mattfishman/Drosophila-Heart-OCM,"All the models were trained using Python 3.9 and Tensorflow 2.10. Models were trained locally on a 3090 GPU with 24 GB of memory. A GitHub repository which contains the Jupyter notebook for training is available at . To run the code, first download the data and clone the repository. There is a requirements.txt file within the repository that contains all the dependencies. Next, use the utilities to generate a pickle file by changing the path inside create_pickle.py to point to the parent directory that contains all the flies. Insert the pickle file path into the training notebook and run all cells begin training. Our trained model has been provided as a starting place for training, but users may opt to train without this initialization. This notebook will output the model as a.h5 file. The path to the trained model can then be inserted into processing_utils.py to use this model to predict the segmentation on new images. Additional details can be found in the README file within the repository for how to run additional code to produce cardiac parameters.",2023-12-09,,,
281,0,Scientific Data,41597,10.1038/s41597-023-02576-3,A Satellite Imagery Dataset for Long-Term Sustainable Development in United States Cities,4,12,2023,https://github.com/axin1301/Satellite-imagery-dataset,"The Python codes to collect, process, and plot the dataset as well as the supplementary files for this study are publicly available through the GitHub repository (). Detailed instruction for the running environment, file structure, and codes is available in the repository.",2023-12-04,,,
283,0,Scientific Data,41597,10.1038/s41597-023-02690-2,MultiXC-QM9: Large dataset of molecular and reaction energies from multi-level quantum chemical methods,8,11,2023,https://github.com/chemsurajit/largeDFTdata,The energy calculations with the 76 different post-SCF functionals were performed using the SCM software package. The GFN2-xTB energies were computed using the XTB version 6.3.3 software package. The G4MP2 energies were obtained from a previous paper by Kim .. The workflow of the calculations and collection of data are build using the Python3.7.10 and BASH scripts. The atomistic simulation environment (ASE) was used to create the database file in SQLite3 format. The csv files were created using pandas. The plots were generated using the matplotlib library. All scripts are available on GitHub under the MIT license agreement ().,2023-11-08,,,
285,0,Scientific Data,41597,10.1038/s41597-023-02776-x,transcriptome assembly of hyperaccumulating  for gene discovery,1,12,2023,https://github.com/matevzl533/Noccaea_praecox_transcriptome,The specific codes for analyses of RNA-seq data are available at .,2023-12-01,,,
286,0,Scientific Data,41597,10.1038/s41597-023-02662-6,DOES - A multimodal dataset for supervised and unsupervised analysis of steel scrap,8,11,2023,https://github.com/micschaefer/does-utils,The dataset is freely available as described in Data Records. The custom code to generate or process these data can be found in the following GitHub repository: . The rights to the source code of the validation model belong to Saarstahl AG and unfortunately cannot be published.,2023-11-08,,,
288,0,Scientific Data,41597,10.1038/s41597-023-02777-w,High-resolution grids of daily air temperature for Peru - the new PISCOt v1.2 dataset,1,12,2023,https://github.com/adrHuerta/PISCOt_v1-2,The construction of the gridded dataset PISCOt v1.2 was performed using the R (v3.6.3) and Python (v3.8.5) programming languages. The entire code used is freely available at figshare and GitHub () under the GNU General Public License v3.0.,2023-12-01,,,
289,0,Scientific Data,41597,10.1038/s41597-023-02804-w,A database of hourly wind speed and modeled generation for US wind plants based on three meteorological models,8,12,2023,https://github.com/AmosRAncell/PLUSWIND,"Custom scripts were developed in R and python to process, manage, and clean the data. These scripts are available publicly at the repository . Additionally, users may contact the corresponding author with questions about these scripts or about our source data.",2023-12-08,,,
290,0,Scientific Data,41597,10.1038/s41597-023-02549-6,High-resolution (1 km) Köppen-Geiger maps for 1901–2099 based on constrained CMIP6 projections,23,10,2023,https://github.com/hylken/Koppen-Geiger_maps,The new Köppen-Geiger classifications have been produced using Python version 3.10. The code can be accessed at  and is licensed under the GNU General Public License v3.0.,2023-10-23,,,
295,0,Scientific Data,41597,10.1038/s41597-023-02720-z,Antarctic daily mesoscale air temperature dataset derived from MODIS land and ice surface temperature,27,11,2023,https://github.com/evabendix/AntAir-ICE,Python 3.8 was used for conversion of the MODIS products from HDF files to raster and all data handling and processing was thereafter done in R version 4.0.0. All data processing and modelling procedures are available as R scripts on a public Github repository: . Using this code it is possible to download new available MODIS LST and IST scenes and apply the model to continue the near-surface air temperature dataset.,2023-11-27,,,
296,0,Scientific Data,41597,10.1038/s41597-023-02779-8,MultiPro: DDA-PASEF and diaPASEF acquired cell line proteomic datasets with deliberate batch effects,2,12,2023,https://github.com/kaipengl/batcheffectsdataset,The R scripts for reproducing the main figures are available through the GitHub repository at .,2023-12-02,,,
299,0,Scientific Data,41597,10.1038/s41597-023-02721-y,3DSC - a dataset of superconductors including crystal structures,21,11,2023,https://github.com/aimat-lab/3DSC,"Code and data are available free of charge. The code is provided in our Github repository . For reproducibility, the SHA of the final commit for this publication is 2471dd51a298a854cb4f365ebd39e72c7cbf3634. The data is available on figshare.",2023-11-21,,,
300,0,Scientific Data,41597,10.1038/s41597-023-02408-4,Two excited-state datasets for quantum chemical UV-vis spectra of organic molecules,21,8,2023,https://github.com/ORNL/Analysis-of-Large-Scale-Molecular-Datasets-with-Python,The code for calculating the electronic excitation energies and statistical analysis of the dataset is open-source and available at the ORNL-GitHub repository .,2023-08-21,,,
302,0,Scientific Data,41597,10.1038/s41597-023-02495-3,A framework for FAIR robotic datasets,13,9,2023,https://github.com/CorradoMotta/FAIR-Data-in-Marine-Robotics,"Scripts, notebooks and modules to generate metadata in several formats following FAIR principles for marine robotic data is available on GitHub (), under the GNU General Public License v3.0. The dedicated GitHub page of the project supports the understanding and usage of the codes.",2023-09-13,,,
304,0,Scientific Data,41597,10.1038/s41597-023-02608-y,Tryp: a dataset of microscopy images of unstained thick blood smears for trypanosome detection,18,10,2023,https://github.com/esla/trypanosome_parasite_detection,The code and detailed documentation on how to use it to reproduce the results presented in this study is publicly available at  under the permissive Berkeley Software Distribution (BSD) 3-Clause license.,2023-10-18,,,
311,0,Scientific Data,41597,10.1038/s41597-023-02469-5,DiaTrend: A dataset from advanced diabetes technology to enable development of novel analytic solutions,23,8,2023,https://github.com/Augmented-Health-Lab/Diatrend,Python was used for all data processing described in this paper. The Python code used to generate all figures in this paper is available on the Augmented Health Lab’s Github: .,2023-08-23,,,
319,0,Scientific Data,41597,10.1038/s41597-023-02327-4,A comprehensive multi-domain dataset for mitotic figure detection,25,7,2023,https://github.com/DeepMicroscopy/MIDOGpp,We provide the code that we used to run all baseline experiments and all data in our GitHub repository ().,2023-07-25,,,
322,0,Scientific Data,41597,10.1038/s41597-023-02328-3,Home-to-school pedestrian mobility GPS data from a citizen science experiment in the Barcelona area,4,7,2023,https://github.com/ferranlarroyaub/Beepath-Schools,"The  repository holds the Python code and scripts to process the input data and to replicate the statistical analysis and the figures. The 3.8 Python version is used to build the code with the main libraries:  and  to plot the trajectories on OpenStreet maps.  and  to process, clean, and analyze the data in Data-Frame format and perform the basic statistic calculations.  for more advanced calculations such as fitting models to the empirical data and  for plotting purposes. The Python code is built in different Jupyter notebook files which contain a detailed description of the study and the code documentation.",2023-07-04,,,
325,0,Scientific Data,41597,10.1038/s41597-023-02358-x,A dataset on energy efficiency grade of white goods in mainland China at regional and household levels,12,7,2023,https://github.com/CEEGDataset/CEEG-Dataset.git,Examples of the code that we used to produce the datasets presented in this paper (mainly for establishing REPM and HEPM) are provided in GitHub ().,2023-07-12,,,
328,0,Scientific Data,41597,10.1038/s41597-023-02415-5,Continuous observations of the surface energy budget and meteorology over the Arctic sea ice during MOSAiC,4,8,2023,https://github.com/MOSAiC-flux/data-processing,"The code and associated libraries used to create Level 1, Level 2, and Level 3 processed files are based in Python with the following dependencies: Python > = 3.6; netCDF4 > = 1.3.0, NumPy > = 1.13.0, Scipy > = 1.1.0, Pandas > = 0.20, XArray > = 0.11; PVLib > = 0.8.1. Files uploaded to ADC have the following versions: Level 1 are v1.5 (1/8/2020) and Levels 2 and 3 v4.1 (2/1/2023). Code is archived on GitHub, .",2023-08-04,,,
330,0,Scientific Data,41597,10.1038/s41597-023-02559-4,The floodplain inundation history of the Murray-Darling Basin through two-monthly maximum water depth maps,23,9,2023,https://github.com/csiro-hydroinformatics/water-depth-estimation,"Version 1.0 of the water-depth-estimation code used for calculating FwDET is available under GPLv3 licensing at . The repository also contains a Jupyter notebook (notebooks/example_water_depth.ipynb), which is useful for exploring the water depth outputs.",2023-09-23,,,
331,0,Scientific Data,41597,10.1038/s41597-023-02389-4,"AtOM, an ontology model to standardize use of brain atlases in tools, workflows, and data infrastructures",26,7,2023,https://github.com/tgbugs/pyontutils/tree/master/nifstd/nifstd_tools/parcellation,Python code for generating parcellations for the NIF-Ontology is publicly available via GitHub: . Archives of release are available via Zenodo.,2023-07-26,,,
340,0,Scientific Data,41597,10.1038/s41597-023-02503-6,EWELD: A Large-Scale Industrial and Commercial Load Dataset in Extreme Weather Events,11,9,2023,https://github.com/Judy0718/EWELD,The code implementation was done using Python. Source codes that were used to develop and analyze the data are publicly available in the GitHub repository ().,2023-09-11,,,
341,0,Scientific Data,41597,10.1038/s41597-023-02590-5,A comprehensive spectral assay library to quantify the  NRC-1 proteome by DIA/SWATH-MS,13,10,2023,https://github.com/alanlorenzetti/protDynContGenExp_v2,"Code to perform PCA, generate heat maps and volcano plots, and carry out the transcriptome differential expression analysis is publicly available under repository .",2023-10-13,,,
346,0,Scientific Data,41597,10.1038/s41597-023-02393-8,EUSEDcollab: a network of data from European catchments to monitor net soil erosion by water,4,8,2023,https://github.com/matfran/EUSEDcollab.git,"All code can be found at: . We include the R language code to perform the quality control procedure on each time series entry to produce the JSON time series evaluation files for each record. Additionally, a Python language Jupyter notebook is included to demonstrate simple operations that can be undertaken using the database, such as reading and filtering the database, calculating metadata statistics and importing specific time series for analysis.",2023-08-04,,,
347,0,Scientific Data,41597,10.1038/s41597-023-02535-y,A daily high-resolution (1 km) human thermal index collection over the North China Plain from 2003 to 2020,18,9,2023,https://github.com/CSLixiang/HiTIC-NCP.git,"The HiTIC-NCP dataset generation codes are available on GitHub (), and operational under Python 3.8 or JavaScript. In the GitHub repository, we uploaded three code scripts, i.e., “Data preprocessing code.py”, “HiTIC-NCP Code.py” and “Figures code.py”. Additionally, the data samples were uploaded to the “Data Samples” folder.",2023-09-18,,,
353,0,Scientific Data,41597,10.1038/s41597-023-02482-8,Label-free tumor cells classification using deep learning and high-content imaging,26,8,2023,https://github.com/cmb-chula/CancerCellVision-CCA,All code used in this experiment was written in Python3 and could be publicly accessed at . The code is based on PyTorch and MMDetection.,2023-08-26,,,
356,0,Scientific Data,41597,10.1038/s41597-023-02425-3,Hyperlocal environmental data with a mobile platform in urban environments,5,8,2023,https://github.com/MIT-Senseable-City-Lab/OSCS/tree/main,"Other than air quality data stamped with time and location, we also provide a compilation of land use GIS layers that are used in our and NYCCAS’ LUR models for convenient reproduction of the results in our Github repository (). These GIS layers are published by NYC and New York State governments and processed by the authors for modeling, with 2021 as the base year. The audience is encouraged to explore the repository, regarding the details about how we design, build, calibrate, and make use of the CS platform. Python code is available for automatic land use feature extraction, LUR training, and performance evaluation.",2023-08-05,,,
364,0,Scientific Data,41597,10.1038/s41597-023-02511-6,Automated Construction of a Photocatalysis Dataset for Water-Splitting Applications,22,9,2023,https://github.com/CambridgeMolecularEngineering/chemdataextractor2,"ChemDataExtractor 2.2 is available at , and the automatically generated dependency parser, and the files used to specify the knowledge representation have been made open source and are available on Figshare.",2023-09-22,,,
366,0,Scientific Data,41597,10.1038/s41597-023-02341-6,Database of lower limb kinematics and electromyography during gait-related activities in able-bodied subjects,14,7,2023,https://github.com/Rvs94/MyPredict,The scripts that facilitate re-use of the data can be found in the GitHub repository . These scripts were developed and written in Python 3.9. All required software packages are open-source and available online.,2023-07-14,,,
374,0,Scientific Data,41597,10.1038/s41597-023-02487-3,Aci-bench: a Novel Ambient Clinical Intelligence Dataset for Benchmarking Automatic Visit Note Generation,6,9,2023,https://github.com/wyim/aci-bench,"All code used to run data statistics, baseline models, and evaluation to analyze the  corpus is freely available at .",2023-09-06,,,
379,0,Scientific Data,41597,10.1038/s41597-023-02544-x,Points for energy renovation (PointER): A point cloud dataset of a million buildings linked to energy features,20,9,2023,https://github.com/kdmayer/PointER,"The code used for generating building point clouds is available at . The repository includes a detailed description of software and python packages used, as well as their versions.",2023-09-20,,,
380,0,Scientific Data,41597,10.1038/s41597-023-02573-6,A twenty-year dataset of high-resolution maize distribution in China,26,9,2023,https://github.com/Pengqy97/TWDTW_codes,"The classification of maize for each province in this study was performed on the local computer. The codes used is written in Python, Fortran, and Julia which are available from .",2023-09-26,,,
383,0,Scientific Data,41597,10.1038/s41597-023-02630-0,A dataset of skin lesion images collected in Argentina for the evaluation of AI tools in this population,18,10,2023,https://github.com/piashiba/HIBASkinLesionsDataset,"Python scripts for exploratory data analysis and dataset comparison, as well as supplementary data, are publicly available at .",2023-10-18,,,
385,0,Scientific Data,41597,10.1038/s41597-023-02290-0,Ensemble of CMIP6 derived reference and potential evapotranspiration with radiative and advective components,27,6,2023,https://github.com/nelsbjarke/PET,"The Python scripts used to calculate ET, ET, ET components, and VPD can be found within the repository alongside the data described herein. Accompanying the code is a small subset of GCM data that can be used to test run the script. Python scripts utilize a small subset of libraries in the Python3 base and the xarray (v2022.11.0) library to handle calculations of the gridded datasets. The python code used to generate the dataset described above can be found in the GitHub repository using the following link: .",2023-06-27,,,
388,0,Scientific Data,41597,10.1038/s41597-023-02348-z,Single cell transcriptome sequencing of stimulated and frozen human peripheral blood mononuclear cells,6,7,2023,https://github.com/erbon7/sc_pbmc,"All custom R and python scripts for quality control, data integration, figures and analysis are available on our GitHub repository ().",2023-07-06,,,
391,0,Scientific Data,41597,10.1038/s41597-023-02519-y,TIHM: An open dataset for remote healthcare monitoring in dementia,9,9,2023,https://github.com/PBarnaghi/TIHM-Dataset,"The TIHM dataset is available in the corresponding Zenodo repository and consists of five separate tables (Activity, Sleep, Physiology, Labels, and Demographics). For further information on the data records, please refer to the README file. The code for the experiments presented in the manuscript is available on the Github repository (). The libraries and their versions and dependencies that are used in the code are also provided as a separate configuration file in JSON/YAML format.",2023-09-09,,,
405,0,Scientific Data,41597,10.1038/s41597-023-02040-2,EUBUCCO v0.1: European building stock characteristics in a common and open database for 200+ million individual buildings,20,3,2023,https://github.com/ai4up/eubucco/releases/tag/v0.1,All the code used in this study is available on Github as a release: . It is free to re-use and modify with attribution under the .,2023-03-20,,,
406,0,Scientific Data,41597,10.1038/s41597-023-02269-x,PANGAEA - Data Publisher for Earth & Environmental Science,2,6,2023,https://github.com/pangaea-data-publisher,"The code supporting the users with data retrieval and submission is freely available at . PANGAEA as a repository does not generate, test, or process data and metadata, therefore no custom code has been used.",2023-06-02,,,
410,0,Scientific Data,41597,10.1038/s41597-023-02213-z,FOPPA: an open database of French public procurement award notices from 2010–2020,19,5,2023,https://github.com/CompNet/FoppaInit,"Our Python source code is publicly available online as a GitHub repository (). It is designed to be applied to the raw TED tables, and leverages the Hexaposte and SIRENE databases mentioned in the  section. It performs the integrality of the processing described in this section, and produces the FOPPA database. When performed in parallel on 10 NVIDIA GeForce RTX 2080 Ti GPUs, this processing requires approximately 6 days.",2023-05-19,,,
415,0,Scientific Data,41597,10.1038/s41597-023-02272-2,A physiological signal database of children with different special needs for stress recognition,14,6,2023,https://github.com/hiddenslate/aktives-dataset-2022,"The codes include preprocessing of physiological signals, annotation synchronization, facial expression detection, and technical validation available at the Repository for the AKTIVES Dataset 2022 GitHub repository . The Python 3.9 version has been utilized for the development of algorithms. In the requirements.txt file, all necessary packages are mentioned. Uploaded codes can be helpful guidelines to preprocess and analyze the AKTIVES dataset.",2023-06-14,,,
416,0,Scientific Data,41597,10.1038/s41597-023-02073-7,Cholec80-CVS: An open dataset with an evaluation of Strasberg’s critical view of safety for AI,8,4,2023,https://github.com/ManuelRios18/CHOLEC80-CVS-PUBLIC,"We provide scripts to transform our annotations to the frame-wise labels and also the source code of some baseline models that use standard deep learning techniques to detect CVS criteria using our database for interested users. All these scripts were coded using Python 3.8.11 and Pytorch as the machine learning framework. All scripts were tested on Linux Machines. The repository README file contains detailed instructions to ease the use of the repository and brief descriptions of all files. The code is publicly available at , licensed under MIT OpenSource license. Therefore, permission is granted free of charge to copy and use this software and its associated files.",2023-04-08,,,
419,0,Scientific Data,41597,10.1038/s41597-023-02017-1,A multi-sensor dataset with annotated activities of daily living recorded in a residential setting,23,3,2023,https://github.com/IRC-SPHERE/sphere-challenge-sdata/,"A  repository is publicly available at . In this repository a number of scripts for visualisation, bench marking and data processing are available. (All subsequent sensor images were generated using these scripts).",2023-03-23,,,
420,0,Scientific Data,41597,10.1038/s41597-023-02046-w,High-resolution calibrated and validated Synthetic Aperture Radar Ocean surface wind data around Australia,23,3,2023,https://github.com/aodn/imos-user-code-library/blob/master/Python/notebooks/SAR_winds/SAR_winds_getting_started_jupyter_notebook/ausar_winds_getting_started_notebook.ipynb,A Python Jupyter notebook for getting started with reading the data and comparing them with other reanalyses datasets at matching times (as outlined in the Usage Notes Section) is available at the AODN GitHub repository ().,2023-03-23,,,
424,0,Scientific Data,41597,10.1038/s41597-023-02076-4,A collection of read depth profiles at structural variant breakpoints,6,4,2023,https://github.com/latur/SWaveform,"A software suite accompanying the resource is available on . The repository contains scripts for a) database and GUI deployment on the SQLite platform and b) a toolkit for DOC profile and SV data processing and management. The toolkit contains scripts for generation of DOC profiles corresponding to breakpoint loci from alignment files (SAM, BAM or CRAM format) and annotated VCF files, as well as DOC profile conversion into BCOV format. In addition, we provide tools for profile clustering, motif discovery and a script for subsequent motif detection in DOC profiles.",2023-04-06,,,
427,0,Scientific Data,41597,10.1038/s41597-023-02048-8,A dataset of rodent cerebrovasculature from  multiphoton fluorescence microscopy imaging,17,3,2023,https://github.com/ctpn/minivess,"We provide the Python code to separate multichannel and time series 2PFM image volumes into single volumes, which are easier to manipulate. Multichannel XY, XYZ, XYT, and XYZT images are supported. For multichannel images, the user will be asked to select the channel of interest to export. For images with multi-T volumes (XYT and XYZT), the user has the option of exporting each T-stack separately, or as a single file. We also provide sample code for the image pre-processing tools described above. All code can be accessed at the MiniVess Github repository .",2023-03-17,,,
428,0,Scientific Data,41597,10.1038/s41597-023-02104-3,Network Analysis of Academic Medical Center Websites in the United States,28,4,2023,https://github.com/davidchen0420/Academic-Medical-Center-Topology,"The code used to calculate the node-specific metrics, network-wide metrics, as well as static and interactive visualizations of each of the 40 AMC websites can be found at . The Jupyter notebook AMC_Topology_Metrics.ipynb describes the steps used to calculate the metrics as comments. To run the Jupyter notebook, installation of the Anaconda distribution of Python 3.8.0+ and required scientific packages listed in the notebook is needed. Example input data and expected output results are provided in example_data.zip in the GitHub. The example input data is a subset of 3 AMC website nodes and internal edges that can also be found in the Figshare repository (see Data Records).",2023-04-28,,,
436,0,Scientific Data,41597,10.1038/s41597-023-02050-0,High-resolution livestock seasonal distribution data on the Qinghai-Tibet Plateau in 2020,18,3,2023,https://github.com/NingZhan1978/High-resolution-livestock-seasonal-distribution-data-on-the-Qinghai-Tibet-Plateau-in-2020.git,"The code in this study is fully operational under Python 3.8.8, and the key packages were contained in the  and the  toolkit in Python 3.8.8. The code can be found on GitHub ().",2023-03-18,,,
438,0,Scientific Data,41597,10.1038/s41597-023-02166-3,The FAIR Cookbook - the essential resource for and by FAIR doers,19,5,2023,https://github.com/FAIRplus/the-fair-cookbook,The code is open source and available in a dedicated public repository on GitHub: .,2023-05-19,,,
439,0,Scientific Data,41597,10.1038/s41597-023-02195-y,iOBPdb A Database for Experimentally Determined Functional Characterization of Insect Odorant Binding Proteins,19,5,2023,https://github.com/sshuklz/iobpdb_app,iOBPdb GitHub source code can be accessed online here: .,2023-05-19,,,
440,0,Scientific Data,41597,10.1038/s41597-023-02280-2,A large expert-curated cryo-EM image dataset for machine learning protein particle picking,22,6,2023,https://github.com/BioinfoMachineLearning/cryoppp,"The data analysis methods, software and associated parameters used in this study are described in the section of Methods. All the scripts associated with various steps of data curation are available at the GitHub repository: , which includes the instructions about how to download the data.",2023-06-22,,,
445,0,Scientific Data,41597,10.1038/s41597-023-02053-x,A Long-term Consistent Artificial Intelligence and Remote Sensing-based Soil Moisture Dataset,22,3,2023,https://github.com/os2328/CASM-dataset,"All code is written in Python, the analysis is conducted using Columbia University high performance computing clusters (Ginsburg), and is available at .",2023-03-22,,,
446,0,Scientific Data,41597,10.1038/s41597-023-02082-6,Three-Dimensional Motion Capture Data of a Movement Screen from 183 Athletes,24,4,2023,https://github.com/Graham-Lab1/3D_MoCap_Data_of_a_Movement_Screen,Python and Matlab scripts used to de-identify the .c3d and .mat files and validate the selected joint angles are available on Github: . No custom code was used in addition to the Visual3D software to process the dataset.,2023-04-24,,,
449,0,Scientific Data,41597,10.1038/s41597-023-02198-9,SciSciNet: A large-scale open data lake for the science of science research,1,6,2023,https://github.com/kellogg-cssi/SciSciNet,"The source code for data selection and curation, data linkage, and metrics calculation is available at .",2023-06-01,,,
453,0,Scientific Data,41597,10.1038/s41597-023-02111-4,Genome-wide chromatin accessibility and gene expression profiling during flatfish metamorphosis,8,4,2023,https://github.com/GuerreroP/FISHRECAP-ATAC-RNA,We relied on open source tools to perform data analysis. Custom code performed in R used in this analysis have been published in the following repository: .,2023-04-08,,,
456,0,Scientific Data,41597,10.1038/s41597-023-02284-y,Carbon Monitor Europe near-real-time daily CO emissions for 27 EU countries and the United Kingdom,8,6,2023,https://github.com/kepiyu/Carbon-Monitor-Europe/blob/main/CM_EU_v2.py,Python code for producing data for 27 EU countries and the United Kingdom in the dataset is provided at .,2023-06-08,,,
458,0,Scientific Data,41597,10.1038/s41597-023-02227-7,A high spatial resolution dataset of China’s biomass resource potential,15,6,2023,https://github.com/Rui-W-A/biomass-resource-China,"The code used for calculating agricultural, forestry residues, and energy crops is written in Python and available from .",2023-06-15,,,
462,0,Scientific Data,41597,10.1038/s41597-023-02087-1,Big Field of View MRI T1w and FLAIR Template - NMRI225,14,4,2023,https://github.com/barbrakr/NMRI225.git,"We make our code available at  as NMRI225_run.m, NMRI225_run.py and nmri_functions, under a CC BY license. We used MATLAB 2018b to run NMRI225_run.m and Python 3.8 for running NMRI225_run.py. We have summarized the packages of the conda repository in Supplementary Materials.",2023-04-14,,,
464,0,Scientific Data,41597,10.1038/s41597-023-02287-9,EEG-based BCI Dataset of Semantic Concepts for Imagination and Perception Tasks,15,6,2023,https://github.com/hWils/Semantics-EEG-Perception-and-Imagination,"The Psychopy files to compile the experiment are stored on the Github repository . Also on this repository are the Python processing and technical validation scripts. Users can directly use the Python code provided 1) to compute preprocessing as described in this paper, and 2) to reproduce the experimental results presented in the technical validation section.",2023-06-15,,,
470,0,Scientific Data,41597,10.1038/s41597-023-02116-z,CRAFTED: An exploratory database of simulated adsorption isotherms of metal-organic frameworks,20,4,2023,https://github.com/st4sd/nanopore-adsorption-experiment,"The Jupyter notebooks providing the panel visualisation of the isotherm curves, enthalpy of adsorption data, IAST-based multicomponent mixture isotherm, and the t-SNE + DBSCAN analysis of the chemical and geometric properties of MOFs are distributed alongside the database in the Zenodo repository. A fully automated workflow that is capable of recreating the dataset was made available as an open-source project (v1.0.0) on GitHub ().",2023-04-20,,,
472,0,Scientific Data,41597,10.1038/s41597-023-01946-1,Data and Tools Integration in the Canadian Open Neuroscience Platform,6,4,2023,https://github.com/CONP-PCNO/conp-portal,The code used for the portal is available on  and a version of the code is available on Zenodo.,2023-04-06,,,
473,0,Scientific Data,41597,10.1038/s41597-023-02060-y,"Healthy Cities, A comprehensive dataset for environmental determinants of health in England cities",25,3,2023,https://github.com/0oshowero0/HealthyCities,"The Python codes to generate the dataset are publicly available through the GitHub repository (). Detailed instruction for software environment preparation, folder structure and commands to run the provided codes is available in the repository.",2023-03-25,,,
479,0,Scientific Data,41597,10.1038/s41597-023-02203-1,CHQ- SocioEmo: Identifying Social and Emotional Support Needs in Consumer-Health Questions,27,5,2023,https://github.com/Ashwag1/CHQ-SocioEmo-,"The code used to prepare the CHQ-SocioEmo dataset is provided at , and the source code for the benchmarked experiments can be found with the dataset.",2023-05-27,,,
481,0,Scientific Data,41597,10.1038/s41597-023-02062-w,Large scale crowdsourced radiotherapy segmentations across a variety of cancer anatomic sites,22,3,2023,https://github.com/kwahid/C3RO_analysis,"Segmentations were performed using the commercially-available ProKnow (Elekta AB, Stockholm, Sweden) software. The code for NIfTI file conversion of DICOM CT images and corresponding DICOM RTS segmentations, along with code for consensus segmentation generation, was developed using in-house Python scripts and is made publicly available through GitHub: .",2023-03-22,,,
484,0,Scientific Data,41597,10.1038/s41597-023-02262-4,Simulated sulfur K-edge X-ray absorption spectroscopy database of lithium thiophosphate solid electrolytes,2,6,2023,https://github.com/atomisticnet/xas-tools/releases/tag/v0.1.0,"Short scripts used for extracting useful information from the VASP output files, such as the XAS and energies, are provided with the database. The workflow is available on GitHub ().",2023-06-02,,,
486,0,Scientific Data,41597,10.1038/s41597-023-02064-8,ChillsDB: A Gold Standard for Aesthetic Chills Stimuli,20,5,2023,https://github.com/ChillsTV/AffectiveStimuliScraper,The code for parsing YouTube and Reddit networks is available under an MIT license at .,2023-05-20,,,
492,0,Scientific Data,41597,10.1038/s41597-023-02207-x,67 million natural product-like compound database generated via molecular language processing,19,5,2023,https://github.com/SIBERanalytics/Natural-Product-Generator,Code used to train the molecular language model as well as the trained model used for natural product-like molecule generation is available from GitHub at .,2023-05-19,,,
495,0,Scientific Data,41597,10.1038/s41597-023-02208-w,CORE: A Global Aggregation Service for Open Access Papers,7,6,2023,https://github.com/oacore/,"CORE consists of multiple services. Most of our source code is open source and available in our public repository on GitHub (). As of today, we are unfortunately not yet able to provide the source code to our data ingestion module. However, as we want to be as transparent as possible with our community, we have documented in this paper the key algorithms and processes which we apply using pseudocode.",2023-06-07,,,
497,0,Scientific Data,41597,10.1038/s41597-023-02123-0,A comprehensive dataset of annotated brain metastasis MR images with clinical and radiomic data,14,4,2023,https://github.com/ysuter/OpenBTAI-radiomics,"We provide the code used to extract the features with PyRadiomics at . For reproducibility and convenience in case any user wants to customize the extraction, all the.py files needed and a “readme” file are available.",2023-04-14,,,
499,0,Scientific Data,41597,10.1038/s41597-023-02181-4,"M4Raw: A multi-contrast, multi-repetition, multi-channel MRI k-space dataset for low-field MRI research",10,5,2023,https://github.com/mylyu/M4Raw,"To facilitate users of this dataset, we have released the following Github repository: . The repository contains Python examples for data reading and deep learning model training, and the trained model weights to reproduce the results in Figs. –.",2023-05-10,,,
501,0,Scientific Data,41597,10.1038/s41597-023-01954-1,Assessing ternary materials for fluoride-ion batteries,11,2,2023,https://github.com/donmctaggart15/ternary_f_cathodes,"All code used is open source and available at . The datasets are provided on the same repository. We recommend reading the Simmate, Materials Project API, and pymatgen documentation to follow filtering syntax.",2023-02-11,,,
502,0,Scientific Data,41597,10.1038/s41597-022-01892-4,"A Brazilian classified data set for prognosis of tuberculosis, between January 2001 and April 2020",15,12,2022,https://github.com/dotlab-brazil/tuberculosis_preprocessing,The code used to pre-process the data set is publicly available on GitHub and is accessible through the link: .,2022-12-15,,,
504,0,Scientific Data,41597,10.1038/s41597-023-01985-8,A large-scale dataset for end-to-end table recognition in the wild,23,2,2023,https://github.com/MaxKinny/TabRecSet,"A link to the dataset, along with Python codes that are used to create the dataset, statistical analysis and plots, is released and publicly available at .",2023-02-23,,,
505,0,Scientific Data,41597,10.1038/s41597-022-01807-3,Unifying the identification of biomedical entities with the Bioregistry,19,11,2022,https://github.com/biopragmatics/bioregistry,The source code for the Bioregistry is available at  under the MIT License. The source code specific to the version of Bioregistry used in this article (v0.5.132) is archived on Zenodo.,2022-11-19,,,
508,0,Scientific Data,41597,10.1038/s41597-022-01780-x,So2Sat POP - A Curated Benchmark Data Set for Population Estimation from Space on a Continental Scale,19,11,2022,https://github.com/zhu-xlab/So2Sat-POP,Python is used for all the analyses and implementations. The code to create the features for each city and to run the baseline experiments is available on our GitHub project ().,2022-11-19,,,
510,0,Scientific Data,41597,10.1038/s41597-022-01906-1,Image dataset for benchmarking automated fish detection and classification algorithms,3,1,2023,https://github.com/tzutalin/labelImg,The developed Python code for tagging and labelling the images is available through the Zenodo repository. Another device that can be used for tagging fishes is the public Label Image tool ().,2023-01-03,,,
515,0,Scientific Data,41597,10.1038/s41597-023-01959-w,Bioclimatic atlas of the terrestrial Arctic,19,1,2023,https://github.com/fmidev/resiclim-climateatlas,The Python codes needed to reproduce the dataset are available from Github: .,2023-01-19,,,
517,0,Scientific Data,41597,10.1038/s41597-022-01908-z,A large dataset of scientific text reuse in Open-Access publications,26,1,2023,https://github.com/webis-de/scidata22-stereo-scientific-text-reuse,"The complete source code used for candidate retrieval and text alignment is openly accessible and permanently available on GitHub (). The data processing pipeline is written in Python 3.7, utilizing the pyspark framework. The compute cluster on which we carried out the data processing and our experiments run Spark Version 2.4.8. The text alignment component is written in Go 1.16 and can be used as a standalone application. Detailed documentation about each pipeline component, recommendations for compute resources, and suggestions for parameter choices are distributed alongside the code to facilitate code reuse.",2023-01-26,,,
520,0,Scientific Data,41597,10.1038/s41597-023-02016-2,Annotated computed tomography coronary angiogram images and associated data of normal and diseased arteries,10,3,2023,https://github.com/Ramtingh/ASOCADataDescription,"The code for creation of this dataset, usage examples and evaluation code used in the challenge is available on GitHub (). Figures – were created with data included in the dataset. A copy of the raw data used is included in the repository under the corresponding folder to maker recreating these figures easier. 3D Slicer (version 4.3) was used in the preparation of the dataset and Figs.  and . Vascular Modelling Tool Kit (version 1.4) was used to calculate centerlines and generate Fig. .",2023-03-10,,,
525,0,Scientific Data,41597,10.1038/s41597-023-01932-7,A Non-Laboratory Gait Dataset of Full Body Kinematics and Egocentric Vision,12,1,2023,https://github.com/abs711/The-way-of-the-future,"We provide an example python script for loading the processed motion capture and vision data, named ‘main.py’ in the directory ‘data_loading_example’ on the Github repository. In addition we provide scripts for synchronization and frame-dropping, and examples of loading into pytorch machine learning pipeline. All code is available on ().",2023-01-12,,,
529,0,Scientific Data,41597,10.1038/s41597-023-01991-w,A 21-year dataset (2000–2020) of gap-free global daily surface soil moisture at 1-km grid resolution,15,3,2023,https://github.com/zhengchaolei/GlobalSSMGapfillDownscaling.git,The codes used in this study will be available at  after this work is accepted.,2023-03-15,,,
533,0,Scientific Data,41597,10.1038/s41597-023-01963-0,Near-real-time global gridded daily CO2 emissions 2021,2,2,2023,https://github.com/xinyudou820/GRACED2021,"Python code for producing, reading and plotting data in the dataset is provided at .",2023-02-02,,,
535,0,Scientific Data,41597,10.1038/s41597-022-01872-8,"A massive dataset of the NeuroCognitive Performance Test, a web-based cognitive assessment",8,12,2022,https://github.com/pauljaffe/lumos-ncpt-tools/tree/v1.1.0,All of the code used to generate the figures and perform the analyses are included with the public lumos-ncpt-tools repository described above: . See the README file for instructions on how to reproduce the figures and analyses. The software underlying the cognitive tasks themselves is proprietary and consequently cannot be shared at this time.,2022-12-08,,,
541,0,Scientific Data,41597,10.1038/s41597-023-01965-y,An open database on global coal and metal mine production,24,1,2023,www.github.com/fineprint-global/compilation_mining_data,"The code used to derive the final data product from the raw input data file is available under the licence GNU General Public License v3.0 (GPL-v3) from the GitHub repository . All processing scripts were written in R, and geoprocessing was conducted with the R package sf.",2023-01-24,,,
544,0,Scientific Data,41597,10.1038/s41597-023-01966-x,"Eco-ISEA3H, a machine learning ready spatial database for ecometric and species distribution modeling",7,2,2023,https://github.com/mechenich/eco-isea3h,"R and Python code developed for the Eco-ISEA3H database was committed to a public GitHub repository, and may be accessed via the following URL: .",2023-02-07,,,
548,0,Scientific Data,41597,10.1038/s41597-023-01996-5,Genome-wide hydroxymethylation profiles in liver of female Nile tilapia with distinct growth performance,1,3,2023,https://github.com/IoannisKonstantinidis/RRHP_Code,Supplementary files 5 and 6 were deposited in GitHub on 2022/12/20. They can be found at the URL: .,2023-03-01,,,
552,0,Scientific Data,41597,10.1038/s41597-022-01818-0,Building the European Social Innovation Database with Natural Language Processing and Machine Learning,12,11,2022,https://github.com/EuropeanSocialInnovationDatabase/ESID_V2,All the code is freely available in Github at .,2022-11-12,,,
557,0,Scientific Data,41597,10.1038/s41597-022-01819-z,"Evolving collaboration, dependencies, and use in the Rust Open Source Software ecosystem",16,11,2022,https://github.com/wschuell/repo_datasets,Code to recreate the database is included in our Figshare upload and can also be found in a dedicated repository . The software is written in the Python programming language. The database can be created as either PostgreSQL or SQLite database. Version requirements are recorded in the project’s Readme file.,2022-11-16,,,
561,0,Scientific Data,41597,10.1038/s41597-022-01918-x,A benchmark dataset for binary segmentation and quantification of dust emissions from unsealed roads,5,1,2023,https://github.com/RajithaRanasinghe/Automatic_Thresholding,All the Python scripts used to generate the secondary data (binary images by Otsu’s thresholding) are provided at .,2023-01-05,,,
562,0,Scientific Data,41597,10.1038/s41597-022-01878-2,"CloudSEN12, a global dataset for semantic understanding of cloud and cloud shadow in Sentinel-2",24,12,2022,https://github.com/cloudsen12/,"The code to (1) create the raw CloudSEN12 imagery dataset, (2) download assets associated with each ROI, (3) create the manual annotations, (4) build and deploy cloudApp, (5) generate automatic cloud masking, (6) reproduce all the figures, (7) replicate the technical validation, (8) modify  Python package, and (9) train DL models are available in our GitHub organization .",2022-12-24,,,
567,0,Scientific Data,41597,10.1038/s41597-023-01970-1,A high-resolution gridded grazing dataset of grassland ecosystem on the Qinghai–Tibet Plateau in 1982–2015,2,2,2023,https://github.com/nanmeng123456/Grazing-spatilization.git,"The code is fully operational under Python 3.6, and the Python scripts used to implement the gridded grazing dataset can be obtained from . Further questions can be directed to Nan Meng (nanmeng_st@rcees.ac.cn).",2023-02-02,,,
573,0,Scientific Data,41597,10.1038/s41597-022-01881-7,The LUMIERE dataset: Longitudinal Glioblastoma MRI with expert RANO evaluation,15,12,2022,https://github.com/ysuter/gbm-data-longitudinal,The code used for processing this dataset is publicly available in our GitHub repository (). The Python and Bash scripts are available to reproduce and customize the extraction of radiomics features.,2022-12-15,,,
575,0,Scientific Data,41597,10.1038/s41597-023-01975-w,Caravan - A global community dataset for large-sample hydrology,31,1,2023,https://github.com/kratzert/Caravan/,The code that was used to produce the Caravan dataset is available at .,2023-01-31,,,
578,0,Scientific Data,41597,10.1038/s41597-022-01826-0,The PhanSST global database of Phanerozoic sea surface temperature proxy data,6,12,2022,https://github.com/EJJudd/SciDataSupplement,"Figures – were produced in Matlab. Example code and auxiliary functions to (1) reproduce Figs. – and (2) run the automated QC checks on the database are available on GitHub (). The paleocoordinates used to produce Figs. , were estimated using the plate model of Scotese and Wright, implemented in G-Plates (Version 2.2.0).",2022-12-06,,,
579,0,Scientific Data,41597,10.1038/s41597-022-01855-9,A speech corpus of Quechua Collao for automatic dimensional emotion recognition,24,12,2022,https://github.com/qccData/qccCorpus,"Code and data splits for baseline algorithms are available at Github, in .",2022-12-24,,,
584,0,Scientific Data,41597,10.1038/s41597-023-01977-8,Reaction profiles for quantum chemistry-computed [3 + 2] cycloaddition reactions,1,2,2023,https://github.com/coleygroup/dipolar_cycloaddition_dataset,The code described in the previous section is freely available in GitHub under the MIT license (). Further details on how to use it is provided in the associated README.md file.,2023-02-01,,,
585,0,Scientific Data,41597,10.1038/s41597-023-02004-6,Deep learning based atomic defect detection framework for two-dimensional materials,14,2,2023,https://github.com/MeatYuan/MOS2.We,All the code to produce the results of this paper is accessible at:  all use Python and jupyter notebook.,2023-02-14,,,
589,0,Scientific Data,41597,10.1038/s41597-023-02005-5,Mapping the terraces on the Loess Plateau based on a deep learning-based model at 1.89 m resolution,2,3,2023,https://github.com/LYHTTUCAS1/code,"The source code used the Python language. The source code contains five sections: data_loader5_shanxitezhengqu_LP.py, unet_2d.py, data_preprocess.py, train_shanxitezhengqu_LP.py, Config_shanxitezhengqu_LP.py. The source code can be downloaded at .",2023-03-02,,,
594,0,Scientific Data,41597,10.1038/s41597-023-01951-4,A crowdsourced dataset of aerial images with annotated solar photovoltaic arrays and installation metadata,28,1,2023,https://github.com/gabrielkasmi/bdappv,"Our public repository accessible at this URL  contains the code to generate the masks, filter the metadata and analyze our results. Interested users can clone this repository to replicate our results or conduct analyses.",2023-01-28,,,
599,0,Scientific Data,41597,10.1038/s41597-022-01832-2,"RedDB, a computational database of electroactive molecules for aqueous redox flow batteries",28,11,2022,https://github.com/ergroup/RedDB,"All classical and quantum chemical calculations have been performed by using the SMSS, which is a proprietary software package. The solubility predictions have been made by using the AqSolPred, which is a freely accessible tool. In addition, the in-house developed Python scripts that have been used to parse the calculation outputs and to convert them into relational database formats, are openly accessible at .",2022-11-28,,,
601,0,Scientific Data,41597,10.1038/s41597-022-01578-x,A dataset of mentorship in bioscience with semantic and demographic estimations,2,8,2022,https://github.com/sciosci/AFT-MAG,"All the code for generating the dataset and figures is published as IPython notebooks on Github, . All the coding was completed using Python.",2022-08-02,,,
602,0,Scientific Data,41597,10.1038/s41597-022-01692-w,Pan-tumor CAnine cuTaneous Cancer Histology (CATCH) dataset,27,9,2022,https://github.com/DeepPathology/CanineCutaneousTumors,"Code examples for training the segmentation and classification architectures can be found in the form of Jupyter notebooks in our GitHub repository (). Furthermore, we provide exported fastai learners to reproduce the results stated in this work. The  file lists the train, validation, and test split on slide level. For network inference, we provide two Jupyter notebooks for patch-level results ( and ) and one notebook for slide-level results. This  notebook produces segmentation and classification outputs as compressed numpy arrays. After inference, these prediction masks can be visualized as overlays on top of the original images using our custom SlideRunner plugins  and . To integrate these plugins into their local SlideRunner installation, users have to copy the respective plugin from our GitHub repository into their SlideRunner  directory. Additionally, the  notebook provides methods to compute confusion matrices from network predictions and calculate class-wise Jaccard coefficients and the tumor classification recall. As mentioned previously, we provide six python modules to convert annotations back and forth between MS COCO and EXACT, MS COCO and SQLite, and EXACT and SQLite formats. This enables users to extend the annotations by custom classes or polygons in their preferred annotation format. These modules can be found in the  directory of our GitHub repository.",2022-09-27,,,
604,0,Scientific Data,41597,10.1038/s41597-022-01520-1,A long-term reconstructed TROPOMI solar-induced fluorescence dataset using machine learning algorithms,20,7,2022,https://github.com/chen-xingan/Reconstruct-TROPOMI-SIF.git,The code for generating the RTSIF is available at .,2022-07-20,,,
612,0,Scientific Data,41597,10.1038/s41597-022-01636-4,TILES-2019: A longitudinal physiologic and behavioral data set of medical residents in an intensive care unit,1,9,2022,https://github.com/usc-sail/tiles-2019-dataset/,The code is available at .,2022-09-01,,,
613,0,Scientific Data,41597,10.1038/s41597-022-01665-z,A georeferenced rRNA amplicon database of aquatic microbiomes from South America,13,9,2022,https://github.com/microsudaqua/usudaquadb,"The workflow included several custom-made R and python scripts, which are accessible GitHub ().",2022-09-13,,,
615,0,Scientific Data,41597,10.1038/s41597-022-01580-3,A Multi-Modal Gait Database of Natural Everyday-Walk in an Urban Environment,3,8,2022,https://github.com/HRI-EU/multi_modal_gait_database,"To streamline the processing of the data, we provide various tools and scripts that are accessible at . In particular, a Python script is available to join the CSV files into one single pandas data frame, which also supports filtering for specific tasks, participants, and data columns. Furthermore, we provide a visualization tool that jointly displays all three sensor modalities as illustrated by Fig. . The tool allows the adjustment of current labels and the creation of custom labels or tags, enabling the generation of additional machine learning tasks.",2022-08-03,,,
622,0,Scientific Data,41597,10.1038/s41597-022-01696-6,Solar and wind power data from the Chinese State Grid Renewable Energy Generation Forecasting Competition,21,9,2022,https://github.com/Bob05757/Renewable-energy-generation-input-feature-variables-analysis,"All the code and processing scripts used to produce the results of this paper were written in Python, Jupyter lab. Links to scripts and data for analysis can be found in the GitHub repository ().",2022-09-21,,,
623,0,Scientific Data,41597,10.1038/s41597-022-01752-1,A thermoelectric materials database auto-generated from the scientific literature using ChemDataExtractor,22,10,2022,https://github.com/odysie/thermoelectricsdb,"The code used to automatically generate the database is available at , along with examples, code for cleaning and aggregating the database, and supplementary information about the database and the data extraction process.",2022-10-22,,,
625,0,Scientific Data,41597,10.1038/s41597-022-01639-1,"QDataSet, quantum datasets for machine learning",23,9,2022,https://github.com/eperrier/QDataSet,"The datasets are stored in an online repository and are accessible via links on the site. The largest of the datasets is over 500GB (compressed), the smallest being around 1.4GB (compressed). The QDataSet is provided subject to open-access MIT/CC licensing for researchers globally. The code used to generate the QDataSet is contained in the associated repository (see below), together with instructions for reproduction of the dataset. The QDataSet code requires Tensorflow > 2.0 along with a current Anaconda installation of Python 3. The code used to simulate the QDataSet is available via the Github repository (). A Jupyter notebook containing the code used for technical validation and verification of the datasets is available on this QDataSet Github repository.",2022-09-23,,,
626,0,Scientific Data,41597,10.1038/s41597-022-01782-9,Benchmarking emergency department prediction models with machine learning and public electronic health records,27,10,2022,https://github.com/nliulab/mimic4ed-benchmark,The code used to analyze the data in the current study is available at: .,2022-10-27,,,
630,0,Scientific Data,41597,10.1038/s41597-022-01699-3,Optical emissivity dataset of multi-material heterogeneous designs generated with automated figure extraction,29,9,2022,https://github.com/ViktoriiaBaib/curvedataextraction,"The source code (implemented in Python) for performing all the described figure analysis steps and generating the data entries is available at . The axis and legend detection step uses the TensorFlow2 Object Detection API and provides a fine-tuned CNN model. File “object_detection_axes_legend.py” performs object detection of legend, x-axis, and y-axis objects and generates PNG and JSON records for these objects. File “color_decomposition.py” performs clustering by color and produces PNG of color-isolated image, palette, as well as PNG and JSON records of separate color clusters in pixel coordinates. It uses methods from “posterization.py”. File “final-record.py” performs axes scale parsing and applies it to all the clusters, producing cluster records in units of measurement. It utilizes methods from “final_record_func.py”.",2022-09-29,,,
633,0,Scientific Data,41597,10.1038/s41597-022-01670-2,A dataset of hourly sea surface temperature from drifting buoys,14,9,2022,https://github.com/selipot/sst-drift.git,"A Matlab software associated with this manuscript is licensed under MIT and published on GitHub at  and archived on Zenodo. This software allows the user to fit model () to temperature observations and derive the resulting SST estimates and their uncertainties. Input arguments to the model fitting function include an arbitrary order for the background non-diurnal SST model and arbitrary frequencies for the diurnal oscillatory model. A sample of Level-1 data from drifter AOML ID 55366 is provided in order to test the routines and produce figures similar to Figs.  and . Alternatively, the main code can also generate stochastic data for testing purposes.",2022-09-14,,,
634,0,Scientific Data,41597,10.1038/s41597-022-01727-2,Large-scale audio dataset for emergency vehicle sirens and road noises,4,10,2022,https://github.com/tabarkarajab/Large-Scale-Audio-dataset,"Code and the script files used to convert the sounds files into meaningful format are published in (-). We developed this code using Python and Pycharm Community software (Version 2021.3). The large-Scale Audio Dataset relies on the following dependencies: os, logging, traceback, shlex, and subprocess.",2022-10-04,,,
637,0,Scientific Data,41597,10.1038/s41597-022-01786-5,An all-Africa dataset of energy model “supply regions” for solar photovoltaic and wind power,31,10,2022,https://github.com/bhussain89/Model-Supply-Regions-MSR-Toolset,"The Python code used to generate the MSRs along with all their metadata, including hourly profiles, as well as the code to perform screening and clustering, is openly available on .",2022-10-31,,,
643,0,Scientific Data,41597,10.1038/s41597-022-01615-9,"ASL-BIDS, the brain imaging data structure extension for arterial spin labeling",6,9,2022,https://github.com/bids-standard/bids-validator,"The BIDS validator code is available in the BIDS Validator repository on GitHub, .",2022-09-06,,,
644,0,Scientific Data,41597,10.1038/s41597-022-01474-4,Agricultural SandboxNL: A national-scale database of parcel-level processed Sentinel-1 SAR data,13,7,2022,https://github.com/ManuelHuber-Github/Agricultural-SandboxNL,"Python code to access, query, visualize and analyze the Agricultural SandboxNL database is distributed, with the dataset and accompanying documentation. GitHub repository to share all GEE/python scripts and information used to create Agricultural SandboxNL database. .",2022-07-13,,,
646,0,Scientific Data,41597,10.1038/s41597-022-01701-y,A 10 m resolution urban green space map for major Latin American cities from Sentinel-2 remote sensing images and OpenStreetMap,24,9,2022,https://github.com/yangju-90/urban_greenspace_classification,"We used Google Earth Engine via Python to query Sentinel-2 images and to extract spectral indices and texture from the images. We performed all other steps, including image downloading, PCA, sample collection and filtering, and image classification in Python. Code is available at .",2022-09-24,,,
649,0,Scientific Data,41597,10.1038/s41597-022-01561-6,Utility-driven assessment of anonymized data via clustering,30,7,2022,https://github.com/Farmerinpt/clustering-anonymization-utility,The source code of the anonymization tasks is publicly available as open source software. The clustering and cluster validity methods were based on the open source scikit-learn Python library implementation. The clustering & validity pipeline source code is publicly available at .,2022-07-30,,,
655,0,Scientific Data,41597,10.1038/s41597-022-01533-w,"Automatic question answering for multiple stakeholders, the epidemic question answering dataset",21,7,2022,https://github.com/h4ste/epic_qa,"The code used to prepare the EPIC-QA dataset is provided at , and a Python script for computing the evaluation metrics reported in the technical validation section of this manuscript is provided with the dataset.",2022-07-21,,,
656,0,Scientific Data,41597,10.1038/s41597-022-01562-5,Standard metadata for 3D microscopy,27,7,2022,https://github.com/Defining-Our-Research-Methodology-DORy/3D-Microscopy-Metadata-Standards-3D-MMS,"All custom software, including the python code to transfer csv-formatted data to JSON and to validate JSON formatted data, are freely available at .",2022-07-27,,,
663,0,Scientific Data,41597,10.1038/s41597-022-01707-6,Machine actionable metadata models,30,9,2022,https://github.com/FAIRsharing/mircat,"['All the code produced for the present study is available from the following GitHub repositories:', {'ext-link': {'@xlink:href': 'http://github.com/fairsharing/jsonldschema', '@ext-link-type': 'uri', '#text': 'http://github.com/fairsharing/jsonldschema'}, '#text': '-'}, {'ext-link': {'@xlink:href': 'https://github.com/fairsharing/jsonschema-documenter', '@ext-link-type': 'uri', '#text': 'https://github.com/fairsharing/jsonschema-documenter'}, '#text': '-'}, {'ext-link': {'@xlink:href': 'https://github.com/FAIRsharing/JSONschema-compare-and-view', '@ext-link-type': 'uri', '#text': 'https://github.com/FAIRsharing/JSONschema-compare-and-view'}, '#text': '-'}, {'ext-link': {'@xlink:href': 'https://jsonldschema.readthedocs.io', '@ext-link-type': 'uri', '#text': 'https://jsonldschema.readthedocs.io'}, '#text': 'Supporting documentation is available from .'}]",2022-09-30,,,
666,0,Scientific Data,41597,10.1038/s41597-022-01509-w,EEG Dataset for RSVP and P300 Speller Brain-Computer Interfaces,8,7,2022,https://github.com/KyunghoWon-GIST/EEG-dataset-for-RSVP-P300-speller,"Project name: EEG dataset for RSVP and P300 Speller Brain-Computer Interfaces. Project home page: . Operating system(s): Windows, MAC. Programming language: MATLAB, Python. Other requirements: MATLAB r2020a or higher, Python 3.6 or higher. License: MIT License. We note that the results of the article were produced using MATLAB. We provide MATLAB and Python scripts, and users can use Python to extract features and evaluate P300 speller performance as well, but the result may differ slightly from MATLAB.",2022-07-08,,,
667,0,Scientific Data,41597,10.1038/s41597-022-01538-5,A resource for assessing dynamic binary choices in the adult brain using EEG and mouse-tracking,16,7,2022,https://github.com/andlab-um/MT-EEG-dataset,"The code used to preprocess the data and plot results is openly available on GitHub (). For more details about code usage, please refer to the GitHub repository.",2022-07-16,,,
668,0,Scientific Data,41597,10.1038/s41597-022-01623-9,"The NIMH intramural healthy volunteer dataset: A comprehensive MEG, MRI, and behavioral resource",25,8,2022,https://github.com/nih-megcore/hv_protocol,"MEG task paradigms, and scripts used for DICOM to BIDS format conversion and de-identification of structural MRI scans are available in the study git repository: .",2022-08-25,,,
669,0,Scientific Data,41597,10.1038/s41597-022-01796-3,Near-real-time daily estimates of fossil fuel CO emissions from major high-emission cities in China,10,11,2022,https://github.com/dh107/Carbon-Monitor-Cities/,"Python code for producing, reading and plotting data in the dataset is provided at .",2022-11-10,,,
670,0,Scientific Data,41597,10.1038/s41597-022-01624-8,A multi-scale probabilistic atlas of the human connectome,23,8,2022,https://github.com/connectomicslab/probconnatlas,The custom code used to apply the atlas to new subjects is implemented in Python 3.8 and is available at the github repository . This code needs the multi-scale probabilistic atlas files stored on the Zenodo repository. The used and the current version of the software is 1.0. All the parameters employed to process the datasets are provided in the atlas files.,2022-08-23,,,
673,0,Scientific Data,41597,10.1038/s41597-022-01598-7,SeEn: Sequential enriched datasets for sequence-aware recommendations,4,8,2022,https://github.com/lasigeBioTM/SeEn,The code used for creating the datasets is available at: .,2022-08-04,,,
677,0,Scientific Data,41597,10.1038/s41597-022-01570-5,A high spatial resolution land surface phenology dataset for AmeriFlux and NEON sites,27,7,2022,https://github.com/BU-LCSC/PLSP,Python and R source code to download and process the PlanetScope imagery and generate the product can be obtained through a public repository at . R source code for generating the figures in the Technical Validation section is also available on the same repository.,2022-07-27,,,
679,0,Scientific Data,41597,10.1038/s41597-022-01542-9,Dataset of Speech Production in intracranial Electroencephalography,22,7,2022,https://github.com/neuralinterfacinglab/SingleWordProductionDutch,"All Python code to re-run the technical validation described in this report can be found on our Github: . The code relies on the numpy, scipy, pynwb, scikit-learn and pandas packages.",2022-07-22,,,
681,0,Scientific Data,41597,10.1038/s41597-022-01657-z,Carbon Monitor Cities near-real-time daily estimates of CO emissions from 1500 cities worldwide,1,9,2022,https://github.com/dh107/Carbon-Monitor-Cities/,"Python code for producing, reading and plotting data for any city in the dataset is provided at .",2022-09-01,,,
685,0,Scientific Data,41597,10.1038/s41597-022-01659-x,World carbon pricing database: sources and methods,17,9,2022,https://github.com/g-dolphin/WorldCarbonPricingDatabase,"All code is written in the Python 3 programming language. All files, including Python files, necessary to the compilation of the dataset are available on the following GitHub repository: .",2022-09-17,,,
687,0,Scientific Data,41597,10.1038/s41597-022-01545-6,A large collection of real-world pediatric sleep studies,19,7,2022,https://github.com/liboyue/sleep_study,"The code that was used to analyze patient data, read EDF files, run baseline sleep stage classifier, and generate figures and tables in this paper is published at .",2022-07-19,,,
692,0,Scientific Data,41597,10.1038/s41597-022-01746-z,A representation-independent electronic charge density database for crystalline materials,28,10,2022,https://github.com/materialsproject/api,Access to the charge density data provided by the Materials Project API () and grid transforms of the charge density is done using the  python package. See the [sec:usage]Usage Notes section for more information. The scripts used to generate the validation data can be access at along with the direct download of the validation dataset,2022-10-28,,,
694,0,Scientific Data,41597,10.1038/s41597-022-01547-4,An update on global mining land use,22,7,2022,www.github.com/fineprint-global/app-mining-area-polygonization,"All the code and geoprocessing scripts used to produce the results of this paper are distributed under the GNU General Public License v3.0 (GPL-v3) from the repository . The processing scripts were written in R, Python, and GDAL (Geospatial Data Abstraction Library). The web application to delineate the polygons was written in R Shiny using a PostgreSQL database with PostGIS extension for storage. The full app setup uses Docker containers to facilitate management, portability, and reproducibility.",2022-07-22,,,
701,0,Scientific Data,41597,10.1038/s41597-022-01378-3,"A multisource database tracking the impact of the COVID-19 pandemic on the communities of Boston, MA, USA",20,6,2022,https://github.com/BARIBoston,All the codes are published through BARI’s GitHub account (user: @BARIBoston; ).,2022-06-20,,,
703,0,Scientific Data,41597,10.1038/s41597-022-01434-y,An 8-year record of phytoplankton productivity and nutrient distributions from surface waters of Saanich Inlet,4,7,2022,https://github.com/bjmcnabb/Saanich_Inlet,"The majority of data processing was done using Microsoft Excel 2010® version 14.0.4734.100. Python v3.8 was used for the calculations of the percentage size fractions of Chl-a, seasonal averaging (. binning values into a monthly average for each sampled depth) and scaling the figure colormaps. The specific code written for this manuscript can be found within the plotting script at the following open source GitHub repository: .",2022-07-04,,,
704,0,Scientific Data,41597,10.1038/s41597-022-01264-y,"RA-MAP, molecular immunological landscapes in early rheumatoid arthritis and healthy vaccine recipients",9,5,2022,https://github.com/C4TB/RA-MAP,Fully annotated Executable R scripts and R Markdown documents are available in our public RA-MAP GitHub in order to allow complete reproduction of our analysis workflow (). All analyses were conducted in R version 4.0.5.,2022-05-09,,,
708,0,Scientific Data,41597,10.1038/s41597-022-01321-6,"Text-mined dataset of gold nanoparticle synthesis procedures, morphologies, and size entities",26,5,2022,https://github.com/CederGroupHub/text-mined-aunp-synthesis_public,"Scripts developed for the generation of this dataset as well as notebooks for example data analysis are available at , along with an acknowledgement for this paper. The libraries use for this project are: , , , , , , , and .",2022-05-26,,,
709,0,Scientific Data,41597,10.1038/s41597-022-01350-1,Plant phenotype relationship corpus for biomedical relationships between plants and phenotypes,26,5,2022,https://github.com/DMCB-GIST/PPRcorpus,The Python codes for the NER and RE experiments represented in Technical Validation can be accessed from .,2022-05-26,,,
712,0,Scientific Data,41597,10.1038/s41597-022-01380-9,Implementing the reuse of public DIA proteomics datasets: from the PRIDE database to Expression Atlas,14,6,2022,https://github.com/PRIDE-reanalysis/DIA-reanalysis,"The complete open reanalysis pipeline description and documentation, workflows, container recipes, and custom code and visualisation scripts, as well as parameter input files are available through the GitHub repository at .",2022-06-14,,,
715,0,Scientific Data,41597,10.1038/s41597-022-01438-8,Novel inorganic crystal structures predicted using autonomous simulation agents,14,6,2022,http://github.com/TRI-AMDD/CAMD,"The CAMD code used to generate the data described herein is available at . Scripts used to generate and analyze the dataset, as well as reproduce the figures in this manuscript are all included in the above data repository.",2022-06-14,,,
716,0,Scientific Data,41597,10.1038/s41597-022-01467-3,A multi-city urban atmospheric greenhouse gas measurement data synthesis,24,6,2022,https://github.com/uataq/co2usa_data_synthesis,All of the code used to create and extract the CO-USA synthesis data set is maintained in an open access GitHub repository: .,2022-06-24,,,
720,0,Scientific Data,41597,10.1038/s41597-022-01298-2,CPG: A FAIR Knowledge Graph of COVID-19 Publications,8,7,2022,https://github.com/dice-group/COVID19DS,Our source code to generate the new versions of our knowledge graph is publicly available at  and is maintained in parallel with the knowledge graph.,2022-07-08,,,
727,0,Scientific Data,41597,10.1038/s41597-022-01356-9,Spatial and temporal data to study residential heat decarbonisation pathways in England and Wales,27,5,2022,https://github.com/AlexandreLab/UKERC-data,The Python code that we used to produce the datasets presented in this paper is published at .,2022-05-27,,,
729,0,Scientific Data,41597,10.1038/s41597-022-01441-z,"REFLACX, a dataset of reports and eye-tracking data for localization of abnormalities in chest x-rays",18,6,2022,https://github.com/ricbl/eyetracking,"The code used for all automatic processes described in this paper, involving sampling, collection, processing, and validation of data, is available at  The software and versions we used were: MATLAB R2019a, Psychtoolbox 3.0.17, Python 3.7.7, edfapi 3.1, EYELINK II CL v5.15, Eyelink GL Version 1.2 Sensor = AC7, EDF2ASC 3.1, librosa 0.8.0, numpy 1.19.1, pandas 1.1.1, matplotlib 3.5.1, statsmodels 0.12.2, shapely 1.7.1, scikit-image 0.17.2, pyrubberband 0.3.0, pydicom 2.1.2, pydub 0.24.1, soundfile 0.10.3.post1, pyttsx3 2.90, pillow 8.0.1, scikit-learn 0.23.2, nltk 3.5, syllables 1.0.0, moviepy 1.0.3, opencv 3.4.2, Ubuntu 18.04.5 LTS, espeak 1.48.04, joblib 1.1.0, ffmpeg 3.4.8, and rubberband-cli 1.8.1.",2022-06-18,,,
736,0,Scientific Data,41597,10.1038/s41597-022-01415-1,Enhancing the REMBRANDT MRI collection with expert segmentation labels and quantitative radiomic features,14,6,2022,https://github.com/ICBI/rembrandt-mri,The methods and tools applied in this paper use open-source tools detailed in respective publications . publication. The python code for extracting PyRadiomics features from Rembrandt and the TCGA segmented data (Supplementary File  and  respectively) is provided here. .,2022-06-14,,,
738,0,Scientific Data,41597,10.1038/s41597-022-01301-w,Auto-generating databases of Yield Strength and Grain Size using ChemDataExtractor,9,6,2022,https://github.com/gh-PankajKumar/ChemDataExtractorStressEng,"The code used to generate the four databases can be found at . This repository contains the modified ChemDataExtractor 2.0, webscraping scripts and post-processing tools. The repository contains , which was used to automatically extract yield strength and grain size and serves as an example as to how the herein modified version of ChemDataExtractor can be used for extracting engineering-material properties. Also,  is an iPython notebook that walks through the basic steps to extract records from an input article. A static version of the repository is available to download from Figshare",2022-06-09,,,
740,0,Scientific Data,41597,10.1038/s41597-022-01389-0,"Inter-species cell detection - datasets on pulmonary hemosiderophages in equine, human and feline specimens",3,6,2022,https://github.com/ChristianMarzahl/EIPH_WSI/,"All code used in the experiments to generate results, plots and tables was written in Python and is available through our GitHub repository for EIPH analysis [] in the folder SDATA and is referenced on Zenodo.",2022-06-03,,,
742,0,Scientific Data,41597,10.1038/s41597-022-01331-4,"solar energy desalination analysis tool, , with data and models for selecting technologies and regions",20,5,2022,https://github.com/gyetman/DOE_CSP_PROJECT,"All source code for  is made available through github under the academic free license, at the time of writing. The code is hosted at .",2022-05-20,,,
743,0,Scientific Data,41597,10.1038/s41597-022-01360-z,CDCDB: A large and continuously updated drug combination database,2,6,2022,https://github.com/Omer-N/CDCDB,"All of the source code for CDCDB database generation has been uploaded to GitHub: , where it is maintained. We also provide the code for parsing and visualizing the data (see Usage Notes above).",2022-06-02,,,
753,0,Scientific Data,41597,10.1038/s41597-022-01448-6,Real-world sensor dataset for city inbound-outbound critical intersection analysis,21,6,2022,https://github.com/EEM0N/sathorndata.github.io/blob/main/sathorndata.ipynb,"The code implementation was performed in Python using a Jupyter notebook. The Python scripts to perform data preprocessing, visualization and technical validation are available at the sathorndata GitHub repository. (.)",2022-06-21,,,
754,0,Scientific Data,41597,10.1038/s41597-022-01305-6,A dataset of winter wheat aboveground biomass in China during 2007–2015 based on data assimilation,11,5,2022,https://github.com/paperoses/CHN_Winter_Wheat_AGB,"Python scripts that implement model calibration, data assimilation, dataset generation, and mapping are available (). Further questions can be directed towards Hai Huang (haihuang@cau.edu.cn).",2022-05-11,,,
756,0,Scientific Data,41597,10.1038/s41597-022-01505-0,"HistoML, a markup language for representation and exchange of histopathological features in pathology images",8,7,2022,https://github.com/Peiliang/HistoML,The source code of this work can be downloaded from .,2022-07-08,,,
758,0,Scientific Data,41597,10.1038/s41597-022-01364-9,"AnimalTraits - a curated animal trait database for body mass, metabolic rate and brain size",2,6,2022,https://github.com/animaltraits/animaltraits.github.io,"The observations database, all raw CSV files and the R scripts used to standardise and check the observations, as well as a sample script to aggregate the observations database into a species-trait data set are available in the auxiliary material. The auxiliary material also contains README.txt files that describe the structure and usage of the data and scripts. The auxiliary material is managed as a GitHub repository (). GitHub is also used to build and serve the website.",2022-06-02,,,
759,0,Scientific Data,41597,10.1038/s41597-022-01393-4,"NASA Global Daily Downscaled Projections, CMIP6",2,6,2022,https://github.com/bthrasher/daily_BCSD,The NCL code used to generate the downscaled products can be found at .,2022-06-02,,,
764,0,Scientific Data,41597,10.1038/s41597-022-01280-y,AJILE12: Long-term naturalistic human intracranial neural recordings and pose,21,4,2022,https://github.com/BruntonUWBio/ajile12-nwb-data,Code to run our Jupyter Python dashboard and recreate all results in this paper can be found at . We used Python 3.8.5 and PyNWB 1.4.0. A requirements file listing the Python packages and versions necessary to run the code is provided in our code repository. Our code is publicly available without restriction other than attribution.,2022-04-21,,,
767,0,Scientific Data,41597,10.1038/s41597-022-01338-x,"MusMorph, a database of standardized mouse morphology data for morphometric meta-analyses",25,5,2022,https://github.com/jaydevine/MusMorph,"Our code is freely available at . The scripts describe every stage of the MusMorph data acquisition and analysis, including image preprocessing (e.g., file conversion, image resampling and intensity correction), processing (e.g., atlas generation, non-linear registration, label propagation), and postprocessing (e.g., shape optimization, morphometric analysis). We developed and implemented the code with Bash 4.4.20, R 3.6.1, Python 3.6, and Julia 1.2.0 on Ubuntu. To facilitate MusMorph software installations, reproducibility, and data aggregation, we have created a comprehensive Docker image that can be downloaded as follows: . Further information about running the Docker container is available on GitHub. All code is distributed under the GNU General Public License v3.0.",2022-05-25,,,
771,0,Scientific Data,41597,10.1038/s41597-022-01254-0,Deciphering Bitcoin Blockchain Data by Cohort Analysis,7,4,2022,https://github.com/SciEcon/UTXO,"The code used for the cohort analysis is available on GitHub (). The GitHub repository is also archived by Zenodo, with the code available in Python and written in Google Colab Notebook with Markdown. first release created on Github: 22 Apr 2021; license: GPL-3.0 License",2022-04-07,,,
775,0,Scientific Data,41597,10.1038/s41597-022-01284-8,A global record of annual terrestrial Human Footprint dataset from 2000 to 2018,19,4,2022,https://github.com/HaoweiGis/humanFootprintMapping/,The programs used to generate all the results were Python (3.11) and ArcGIS (10.4). Analysis scripts are available on GitHub ().,2022-04-19,,,
777,0,Scientific Data,41597,10.1038/s41597-022-01455-7,A multi-scale time-series dataset with benchmark for machine learning in decarbonized energy grids,22,6,2022,https://github.com/tamu-engineering-research/Open-source-power-dataset,"A step-by-step guidance and the source-code for dataset generation and machine learning benchmarks can be found on GitHub. Specifically, we provide ready-to-use Pytorch data loaders with both data processing and splitting included, and also share the code of evaluators to support fair comparison among different ML-based algorithms, of which the dependencies and usage are also descibed on Github ().",2022-06-22,,,
785,0,Scientific Data,41597,10.1038/s41597-022-01373-8,"PISCOeo_pm, a reference evapotranspiration gridded database based on FAO Penman-Monteith in Peru",17,6,2022,https://github.com/adrHuerta/PISCOeo_pm,"Construction of the gridded data was performed using the R environment for statistical computing version 3.6.3. Python version 3.8.5 was also used. The code that describes the procedures (quality control, gap-filling, homogenization, spatial interpolation, and spatial downscaling) to obtain the gridded data of the meteorological subvariables and PISCOeo_pm is freely available at figshare and GitHub () under GNU public licence version 3.",2022-06-17,,,
790,0,Scientific Data,41597,10.1038/s41597-022-01346-x,"GriddingMachine, a database and software for Earth system modeling at global and regional scales",1,6,2022,https://github.com/CliMA/GriddingMachine.jl,The code can be found at  under the Apache 2.0 License. The exact version of the package used to produce the results presented in this paper is also archived on CaltechDATA along with the datasets.,2022-06-01,,,
792,0,Scientific Data,41597,10.1038/s41597-022-01347-w,"Understanding occupants’ behaviour, engagement, emotion, and comfort indoors with heterogeneous sensors and wearables",2,6,2022,https://github.com/cruiseresearchgroup/InGauge-and-EnGage-Datasets,Python code for prepossessing the data and implementing the segmentation based on different classes are available online .,2022-06-02,,,
798,0,Scientific Data,41597,10.1038/s41597-022-01263-z,COVID-19 Open-Data a global-scale spatially granular meta-dataset for  disease,12,4,2022,github.com/GoogleCloudPlatform/covid-19-open-data,All the code to create the dataset is available at . Jupyter notebooks to reproduce the analyses in this paper are available under the examples folder.,2022-04-12,,,
799,0,Scientific Data,41597,10.1038/s41597-022-01292-8,High-throughput inverse design and Bayesian optimization of functionalities: spin splitting in two-dimensional compounds,29,4,2022,github.com/simcomat/SS_2D_Materials,"The entire computational code employed in the SS analysis within this work is openly available at the GitHub repository . It is intensely built upon tools and methods from Pymatgen and ASE, and provide functions to identify, measure and classify SS effects that appear valence/conduction bands of 2D materials band structure calculations.",2022-04-29,,,
801,0,Scientific Data,41597,10.1038/s41597-021-01094-4,"Mobile BCI dataset of scalp- and ear-EEGs with ERP and SSVEP paradigms while standing, walking, and running",20,12,2021,https://github.com/youngeun1209/MobileBCI_Data,"The MATLAB scripts are available for loading data, for evaluating classification performance or signal quality, and for plotting figures at .",2021-12-20,,,
802,0,Scientific Data,41597,10.1038/s41597-022-01122-x,"ECD-UY, detailed household electricity consumption dataset of Uruguay",20,1,2022,https://github.com/jpchavat/ecd-uy,"Three Jupyter notebooks were implemented to facilitate the handling of the dataset (one notebook for each subset). The notebooks are publicly available to download from . For a correct execution of the notebooks, Python version 3 and the Pandas and Numpy libraries are required.",2022-01-20,,,
810,0,Scientific Data,41597,10.1038/s41597-022-01238-0,Categorized contrast enhanced mammography dataset for diagnostic and artificial intelligence research,30,3,2022,https://github.com/omar-mohamed/CDD-CESM-Dataset,"A Github repository is publicly available () which contains helper scripts to make training a DL model on the dataset easier like reading the annotations, pre-processing the images by resizing and normalizing, training different existing models, augmenting the images while training, and evaluating the different models and plotting the segmentation results. The scripts were written using Python 3.6 with Tensorflow 2.3 for the training process, and OpenCV 4.1 and Pillow 6.1 for the image processing.",2022-03-30,,,
813,0,Scientific Data,41597,10.1038/s41597-022-01154-3,Boosting the predictive performance with aqueous solubility dataset curation,3,3,2022,https://github.com/Mengjintao/SolCuration,"Python and C++ codes used to perform data curation, training workflow, and performance evaluation shown in this manuscript are publicly available on GitHub at  or one can cite our code by.",2022-03-03,,,
817,0,Scientific Data,41597,10.1038/s41597-022-01156-1,Dataset on electrical single-family house and heat pump load profiles in Germany,15,2,2022,https://github.com/ISFH/WPuQ,"The code implementation was done in Python3. The scripts to perform the download, restructuring, validation and visualization of the data are available at the ISFH GitHub repository ().",2022-02-15,,,
821,0,Scientific Data,41597,10.1038/s41597-021-01042-2,Task-evoked simultaneous FDG-PET and fMRI data for measurement of neural metabolism in the human visual cortex,15,10,2021,https://github.com/BioMedAnalysis/petmr-bids,Scripts used to insert required metadata into the published BIDS dataset are freely available at  under Apache License 2.0.,2021-10-15,,,
830,0,Scientific Data,41597,10.1038/s41597-021-01107-2,A kinematic and EMG dataset of online adjustment of reach-to-grasp movements to visual perturbations,21,1,2022,https://github.com/tuniklab/scientific-data,The code used for post-processing of the kinematic data is available at .,2022-01-21,,,
835,0,Scientific Data,41597,10.1038/s41597-022-01245-1,A worldwide epidemiological database for COVID-19 at fine-grained spatial resolution,29,3,2022,https://github.com/covid19datahub/COVID19,All the code used to generate the database is open-source and available at .,2022-03-29,,,
837,0,Scientific Data,41597,10.1038/s41597-021-01074-8,A palaeoclimate proxy database for water security planning in Queensland Australia,2,11,2021,https://github.com/nickmckay/sisal2lipd,Code to reformat the relational database to the LiPD and Rdata formats was adapted from this example () and is available in PalaeoWISE. Code to produce the figures are available in PalaeoWISE. Correlations were all produced using code published within the original publications cited within.,2021-11-02,,,
839,0,Scientific Data,41597,10.1038/s41597-021-01046-y,"Dataset of concurrent EEG, ECG, and behavior with multiple doses of transcranial electrical stimulation",27,10,2021,https://github.com/ngebodh/GX_tES_EEG_Physio_Behavior,"The latest version of all accompanying code for this dataset can be acquired within this repository: . MATLAB, version 2018b and 2019b were utilized with functions from EEGlab, Raincloud plots toolbox, and ANT neuro’s import functions.",2021-10-27,,,
842,0,Scientific Data,41597,10.1038/s41597-022-01218-4,A spatially-explicit harmonized global dataset of critical infrastructure,1,4,2022,https://github.com/snirandjan/CISI,"The code developed to process the OSM data is publicly available through the following GitHub repository: . The procedure for the developed CI dataset can be simulated using the main script, which is divided into three sections: (1) extraction of CI from OSM files in .PBF format, and reclassification; (2) estimation of amount of CI; and (3) calculation of the CISI. We also provide code for the validation procedure, and for the development of the figures and supplementary files. Detailed information per section and on the applied functions can be found on the repository, README file, and throughout the code.",2022-04-01,,,
847,0,Scientific Data,41597,10.1038/s41597-021-01078-4,Short-read and long-read RNA sequencing of mouse hematopoietic stem cells at bulk and single-cell levels,29,11,2021,https://github.com/LuChenLab/hemato,The codes used in this article were deposited in .,2021-11-29,,,
851,0,Scientific Data,41597,10.1038/s41597-021-01113-4,"A 24-hour population distribution dataset based on mobile phone data from Helsinki Metropolitan Area, Finland",4,2,2022,https://github.com/DigitalGeographyLab/mfd-helsinki,The developed codes and tools for generating and validating the population datasets are written in Python and openly available on GitHub: .,2022-02-04,,,
853,0,Scientific Data,41597,10.1038/s41597-022-01165-0,Short-read and long-read full-length transcriptome of mouse neural stem cells across neurodevelopmental stages,2,3,2022,https://github.com/LuChenLab/Neuron,The codes used in this article were deposited in .,2022-03-02,,,
855,0,Scientific Data,41597,10.1038/s41597-021-01050-2,"SMAP-HydroBlocks, a 30-m satellite-based soil moisture dataset for the conterminous US",11,10,2021,https://github.com/chaneyn/HydroBlocks,"Source code for the HydroBlocks land surface model is available at . The Random Forest model used to parameterize the merging scheme was implemented using the RandomForestRegressor class of the scikit-learn Python module. While not written as a portable library or toolset, code is available upon request.",2021-10-11,,,
856,0,Scientific Data,41597,10.1038/s41597-022-01166-z,Southern ocean sea level anomaly in the sea ice-covered sector from multimission satellite observations,2,3,2022,https://github.com/MatthisAuger/SO_SLA,"The codes used to process the along track measurements and for the Optimal Interpolation (OI) are not available for public use as Collecte Localisation Satellite (CLS) and the Centre National des Etudes Spatiales (CNES) are the proprietary owners. However, these codes are extensively described in and. The python code used for the comparison of the product with external sources of data are available at .",2022-03-02,,,
858,0,Scientific Data,41597,10.1038/s41597-022-01251-3,The Mexican magnetic resonance imaging dataset of patients with cocaine use disorder: SUDMEX CONN,31,3,2022,https://github.com/psilantrolab/SUDMEX_CONN,"For the code analysis presented here, please check:",2022-03-31,,,
866,0,Scientific Data,41597,10.1038/s41597-022-01168-x,Vectorized rooftop area data for 90 cities in China,2,3,2022,https://github.com/ChanceQZ/RoofTopSegmatation,"The procedure of spatial sampling is executed in the ArcGIS Pro platform. The code of the deep learning model is available at . The program is described by Python3, packages of which are Pytroch, Numpy, and OpenCV mainly.",2022-03-02,,,
868,0,Scientific Data,41597,10.1038/s41597-022-01169-w,Validation and refinement of cropland data layer using a spatial-temporal decision tree algorithm,2,3,2022,https://github.com/llin-csiss/RCDL,The scripts used to generate the R-CDL dataset are available in this GitHub repository: .,2022-03-02,,,
871,0,Scientific Data,41597,10.1038/s41597-021-01055-x,A high-fidelity residential building occupancy detection dataset,28,10,2021,https://github.com/mhsjacoby/HPDmobile,"All code used to collect, process, and validate the data was written in Python and is available for download (). All image processing was done with the Python Image Library package (PIL) Image module, version 7.2.0. Audio processing was done with SciPy io module, version 1.5.0. Environmental data processing made extensive use of the pandas package, version 1.0.5. The code base that was developed for data collection with the HPDmobile system utilizes a standard client-server model, whereby the sensor hub is the server and the VM is the client. Note that the term “server” in this context refers to the SBC (sensor hub), and not the the on-site server mentioned above, which runs the VMs. All collection code on both the client- and server-side were written in Python to run on Linux systems. Technical validation of the audio and images were done in Python with scikit-learn version 0.24.1, and YOLOv5 version 3.0.",2021-10-28,,,
872,0,Scientific Data,41597,10.1038/s41597-022-01255-z,"fastMRI+, Clinical pathology annotations for knee and brain fully sampled magnetic resonance imaging data",5,4,2022,https://github.com/facebookresearch/fastMRI,"Scripts used to generate the DICOM images for radiologists can be accessed from (‘ExampleScripts/fastmri-to-dicom.py’) in the open-source GitHub repository. The detailed method used has been specified in the Methods section. More open-source tools for reconstructing the original fastMRI dataset, including standardized evaluation criteria, standardized code, and PyTorch data loaders can be found in the fastMRI GitHub repository ().",2022-04-05,,,
879,0,Scientific Data,41597,10.1038/s41597-022-01257-x,A three-year dataset supporting research on building energy management and occupancy analytics,5,4,2022,https://github.com/LBNL-ETA/Data-Cleaning,"The Python code for detecting and filling the data gaps, as well as for modifying outlier values, is available at the dataset’s GitHub page: .",2022-04-05,,,
880,0,Scientific Data,41597,10.1038/s41597-022-01143-6,A large-scale study on research code quality and execution,21,2,2022,https://github.com/a,"To develop and execute the analysis code, we used Python 2.7. The code is released as a single version, which was used for both data collection and analysis. All Python dependencies with their versions are captured in a text file  at the root directory. All code files can be freely accessed on on GitHub at  trisovic/dataverse-r-study. The code is released under MIT license.",2022-02-21,,,
881,0,Scientific Data,41597,10.1038/s41597-021-01058-8,Characterization of hormone-producing cell types in the teleost pituitary gland using single-cell RNA-seq,28,10,2021,https://github.com/sikh09/Medaka-pituitary-scRNA-seq,The R code used in the analysis of the scRNA-seq data is available on GitHub ().,2021-10-28,,,
884,0,Scientific Data,41597,10.1038/s41597-022-01174-z,High spatial resolution dataset of La Mobilière insurance customers,11,3,2022,https://github.com/alibatti/LaMobiliereDatasetCode,The code used to validate our data is available at  in the form of Python scripts.,2022-03-11,,,
885,0,Scientific Data,41597,10.1038/s41597-022-01201-z,An improved daily standardized precipitation index dataset for mainland China from 1961 to 2018,30,3,2022,https://github.com/wangqianfeng23/DailySPI,All calculations of daily SPI are based on the Python language and are available at GitHub: . Any updates will also be published on GitHub.,2022-03-30,,,
888,0,Scientific Data,41597,10.1038/s41597-022-01147-2,"Thinking out loud, an open-access EEG-based BCI dataset for inner speech recognition",14,2,2022,https://github.com/N-Nieto/Inner_Speech_Dataset,"In line with reproducible research philosophy, all codes used in this paper are publicly available and can be accessed at . The stimulation protocol and the auxiliary MatLab functions are also available. The code was run in PC1, and shows the stimulation protocol to the participants while sending the event information to PC2, via parallel port. The processing Python scripts are also available. The repository contains all the auxiliary functions to facilitate the load, use and processing of the data, as described above. By changing a few parameters in the main processing script, a completely different process can be obtained, allowing any interested user to easily build his/her own processing code. Additionally, all scripts for generating the Time-Frequency Representations and the plots here presented, are also available.",2022-02-14,,,
893,0,Scientific Data,41597,10.1038/s41597-022-01177-w,A dataset of 175k stable and metastable materials calculated with the PBEsol and SCAN functionals,2,3,2022,https://github.com/hyllios/utils/tree/main/ht_pd_scan,"All data can be easily processed with publicly available tools such as json and pymatgen. An example usage is provided with the data. The dataset was generated with VASP, the bash and python scripts to generate input files or manage the output files can be downloaded from github repository: .",2022-03-02,,,
896,0,Scientific Data,41597,10.1038/s41597-022-01262-0,"Emognition dataset: emotion recognition with self-reports, facial expressions, and physiology using wearables",7,4,2022,https://github.com/Emognition/Emognition-wearable-dataset-2020,The code used for the technical validation is publicly available at . The code was developed in Python 3.7. The repository contains several  with data manipulations and visualizations. All required packages are listed in  file. The repository may be used as a starting point for further data analyses. It allows you to easily load and preview the Emognition dataset.,2022-04-07,,,
898,0,Scientific Data,41597,10.1038/s41597-022-01205-9,A citizen centred urban network for weather and air quality in Australian schools,30,3,2022,https://github.com/giuliaulpiani/SWAQ,The code used for technical validations is publicly available in the SWAQ repository on Github: .,2022-03-30,,,
900,0,Scientific Data,41597,10.1038/s41597-021-00883-1,Continuous sensorimotor rhythm based brain computer interface learning in a large population,1,4,2021,https://github.com/bfinl/BCI_Data_Paper,The code used to produce the figures in this manuscript is available at .,2021-04-01,,,
905,0,Scientific Data,41597,10.1038/s41597-020-00768-9,Expanded dataset of mechanical properties and observed phases of multi-principal element alloys,8,12,2020,https://github.com/CitrineInformatics/MPEA_dataset,"Data processing, validation and statistical plotting were performed using visualization tools on Citrination and Jupyter notebooks in a Python 3 environment. The code is available on GitHub ().",2020-12-08,,,
908,0,Scientific Data,41597,10.1038/s41597-020-00742-5,The landscape of childhood vaccine exemptions in the United States,18,11,2020,https://github.com/bansallab/exemptions-landscape,The code used to produce the figures included in the manuscript as well as the full cleaned and raw datasets are available on Github at . The code runs in Python 3.6.,2020-11-18,,,
910,0,Scientific Data,41597,10.1038/s41597-021-00849-3,Standardizing human brain parcellations,8,3,2021,https://github.com/neurodata/neuroparc,"[{'ext-link': {'@xlink:href': 'https://github.com/neurodata/neuroparc', '@ext-link-type': 'uri', '#text': 'https://github.com/neurodata/neuroparc'}, '#text': 'Code for processing is publicly available and can be found on GitHub under the scripts folder (). Examples of useful functions include resampling parcellations to a desired voxel size, the ability to register parcellations to any given reference image, and center calculation for regions of interest for 3D parcellations. Jupyter notebook tutorials are also available for learning how to prepare atlases for being added to Neuroparc. All code is provided under the Apache 2.0 License.'}, {'sup': {'xref': [{'@ref-type': 'bibr', '@rid': 'CR50', '#text': '50'}, {'@ref-type': 'bibr', '@rid': 'CR51', '#text': '51'}], '#text': ','}, 'xref': {'@rid': 'Fig1', '@ref-type': 'fig', '#text': '1'}, '#text': 'Visualizations are generated using both MIPAV 8.0.2 and FSLeyes 5.0.10 to view the brain volumes in 2D and 3D spaces. Figure\xa0 can be created using MIPAV triplanar views of each atlases with a striped LUT.'}]",2021-03-08,,,
913,0,Scientific Data,41597,10.1038/s41597-021-00823-z,Collegiate athlete brain data for white matter mapping and network neuroscience,11,2,2021,https://github.com/bacaron/athlete-brain-study,"Table  below reports the links to each web service and github.com URL implementing the processing pipeline. All code not found on brainlife.io, including visualization code, can be found at .",2021-02-11,,,
916,0,Scientific Data,41597,10.1038/s41597-021-00810-4,The human -GlcNAcome database and meta-analysis,21,1,2021,https://github.com/glygener/glygen-backend-integration/blob/master/pipeline/integrator/make-proteoform-dataset.py,Source code for the GlyGen QC and integration can be found in the Github repository:,2021-01-21,,,
917,0,Scientific Data,41597,10.1038/s41597-021-00819-9,A new vector-based global river network dataset accounting for variable drainage density,26,1,2021,https://github.com/peironglinlin/Variable_drainage_density,"The new global vector-based hydrography dataset, consisting of basins, watersheds, and river networks of variable and constant , is produced using Python v3.7.3 and the TauDEM software v5.3.8. All computations are completed using the Della high-performance computing clusters at Princeton University. For geospatial analysis, we use the freely available GeoPandas library in Python; for some figure displaying purposes, we use the ArcPro version 2.4.1. Key Python scripts developed for this work are openly shared with the scientific community at Github: .",2021-01-26,,,
920,0,Scientific Data,41597,10.1038/s41597-021-00806-0,Synthetic skull bone defects for automatic patient-specific craniofacial implant design,29,1,2021,https://github.com/Jianningli/SciData,"We provide the python scripts to inject artificial defects to the healthy skulls on GitHub (), which can serve as a starting point for future development based on our skull dataset for other researchers. We also provide additional python scripts for the extraction of point clouds from 3D image volumes and Matlab scripts to convert the triangular, surface meshes of the skulls back to voxel grids (voxelization). The dependencies and usage of the scripts are described in our GitHub repository.",2021-01-29,,,
922,0,Scientific Data,41597,10.1038/s41597-020-00749-y,A detailed open access model of the PubMed literature,20,11,2020,https://github.com/vtraag/leidenalg,The Leiden algorithm was used for clustering and is freely available at .,2020-11-20,,,
927,0,Scientific Data,41597,10.1038/s41597-020-00764-z,"HuskinDB, a database for skin permeation of xenobiotics",1,12,2020,https://github.com/RhDm/huskinDB_publication,The code which was used to create Figs. – and analyse the data records in huskinDB can be found under the following link:  This repository contains a detailed guide on how to install the requirements and run the code.,2020-12-01,,,
930,0,Scientific Data,41597,10.1038/s41597-021-00890-2,An integrated landscape of protein expression in human cancer,23,4,2021,https://github.com/J-Andy/Protein-expression-in-human-cancer,The scripts used to generate the final quantification values (and selected intermediate files) are available at: .,2021-04-23,,,
932,0,Scientific Data,41597,10.1038/s41597-020-00734-5,Multiscale dynamic human mobility flow dataset in the U.S. during the COVID-19 epidemic,12,11,2020,https://github.com/GeoDS/COVID19USFlows,Data processing and data analysis were performed on a Linux server using the Python version 3.7. All codes used for analysis are available in the public GitHub repository that hosts the data: .,2020-11-12,,,
940,0,Scientific Data,41597,10.1038/s41597-020-00792-9,A gridded establishment dataset as a proxy for economic activity in China,11,1,2021,https://github.com/quanturban/firm,"The preprocess script, validation dataset and the R code that performs the statistical analysis are available through .",2021-01-11,,,
944,0,Scientific Data,41597,10.1038/s41597-021-00897-9,Population cluster data to assess the urban-rural split and electrification in Sub-Saharan Africa,23,4,2021,https://github.com/babakkhavari/Clustering,The latest version of the code is available at  (GNU General Public License v3.0). The code is Python-based and runs in Jupyter Notebook. The code repository includes instructions for how to install and run the algorithm as well as a country example displaying the necessary inputs and expected outputs. The datasets published with this paper were ran using Python 3.6 and the packages listed in the full_project.yml file uploaded to the repository.,2021-04-23,,,
947,0,Scientific Data,41597,10.1038/s41597-021-00915-w,DLBCL-Morph: Morphological features computed using deep learning for an annotated digital DLBCL image set,20,5,2021,https://github.com/stanfordmlgroup/DLBCL-Morph,"The code to compute all geometric features from all tumor nuclei in our dataset, along with notebooks to illustrate usage of our data and reproduce all survival regression results, is publicly available at .",2021-05-20,,,
950,0,Scientific Data,41597,10.1038/s41597-020-00756-z,A completely annotated whole slide image dataset of canine breast cancer to aid human breast cancer research,27,11,2020,https://github.com/DeepPathology/MITOS_WSI_CMC/,All code used in the experiments described in the manuscript was written in Python 3 and is available through our GitHub repository (). We provide all necessary libraries as well as Jupyter Notebooks allowing tracing of our results. The code is based on fast.ai and OpenSlide and provides some custom data loaders for use of the dataset.,2020-11-27,,,
951,0,Scientific Data,41597,10.1038/s41597-021-00895-x,"ODFM, an omics data resource from microorganisms associated with fermented foods",20,4,2021,https://github.com/yang4851/gdkm,The code used to build the systemic architecture of the GDKM is available on GitHub: .,2021-04-20,,,
956,0,Scientific Data,41597,10.1038/s41597-021-00824-y,Computational scanning tunneling microscope image database,11,2,2021,https://github.com/usnistgov/jarvis,Python-language-based codes with examples are given at the JARVIS-Tools page .,2021-02-11,,,
962,0,Scientific Data,41597,10.1038/s41597-021-01024-4,"ValLAI_Crop, a validation dataset for coarse-resolution satellite LAI products over Chinese cropland",20,9,2021,https://github.com/BowenSong123/Code,"In the data repository, the readme files explain the location of the files and folders. All raw measurements records can be found in one Excel sheet. All the field data and satellite images were processed and analysed in IDL and Python. The source codes are available at the Github. .",2021-09-20,,,
963,0,Scientific Data,41597,10.1038/s41597-021-00893-z,"LoDoPaB-CT, a benchmark dataset for low-dose computed tomography reconstruction",16,4,2021,https://github.com/jleuschn/lodopab_tech_ref,"Python scripts for the simulation setup and the creation of the dataset are publicly available on Github (). They make use of the ASTRA Toolbox (version 1.8.3) and the Operator Discretization Library (ODL, version ≥0.7.0). In addition, the ground truth reconstructions from the LIDC/IDRI database are needed for the simulation process. A sample data split into training, validation, test and challenge part is also provided. It differs from the one used for the creation of this dataset in order to keep the ground truth data of the challenge set undisclosed. The random seeds used in the scripts are modified for the same reason. The authors acknowledge the National Cancer Institute and the Foundation for the National Institutes of Health, and their critical role in the creation of the free publicly available LIDC/IDRI database used in this study.",2021-04-16,,,
969,0,Scientific Data,41597,10.1038/s41597-021-00907-w,Time series of useful energy consumption patterns for energy system modeling,31,5,2021,https://github.com/FCN-ESE/JERICHO-E-usage,"The code for compiling the time series of useful energy consumption and energy service profiles is published at  under the open MIT license. Detailed instructions for using the code are included in the repository. All code is implemented in Python. For easy use of the scripts, we have added a Jupyter Notebook with further instructions on the workflow. The required input data, comprising pre-calculated data and data from official reports, are included with references.",2021-05-31,,,
974,0,Scientific Data,41597,10.1038/s41597-020-00735-4,"An fMRI dataset in response to “The Grand Budapest Hotel”, a socially-rich, naturalistic movie",11,11,2020,https://github.com/mvdoc/budapest-fmri-data,"All code is available in the github repository. The code includes scripts to process the stimuli, presentation scripts, and scripts for the analyses presented in this paper. The scripts rely heavily on open source Python packages such as PyMVPA, nilearn, pycortex, scipy, and numpy.",2020-11-11,,,
980,0,Scientific Data,41597,10.1038/s41597-021-00820-2,A compilation of North American tree provenance trials and relevant historical climate data for seven species,26,1,2021,https://github.com/clara-risk/tree-provenance-trials,We used various Python scripts to process the data for input into the databases. These scripts were used to calculate climate value summaries and convert phenology observations to a uniform reference date (January 1). The scripts are available at .,2021-01-26,,,
981,0,Scientific Data,41597,10.1038/s41597-021-00976-x,A multispeaker dataset of raw and reconstructed speech production real-time MRI video and 3D volumetric images,20,7,2021,https://github.com/usc-mrel/usc_speech_mri.git,"This dataset is accompanied by a code repository () that contains examples of software and parameter configurations necessary to load and reconstruct the raw RT-MRI in MRD format. Specifically, the repository contains demonstrations to illustrate and replicate results of Figs. –. Code samples are available in MATLAB and Python programming languages. All software is provided free to use and modify under the MIT license agreement.",2021-07-20,,,
989,0,Scientific Data,41597,10.1038/s41597-021-01031-5,"COVID Border Accountability Project, a hand-coded global database of border closures introduced during 2020",29,9,2021,https://github.com/COBAPteam/COBAP,"Codes for the database, raw data outputs and data visualizations appearing on our website are available on our project GitHub (). The full text of the survey used to code each policy is available in the .",2021-09-29,,,
990,0,Scientific Data,41597,10.1038/s41597-021-00885-z,Database of Wannier tight-binding Hamiltonians using high-throughput density functional theory,13,4,2021,https://github.com/usnistgov/jarvis.,Python-language based scripts for obtaining and analyzing the dataset are available at,2021-04-13,,,
993,0,Scientific Data,41597,10.1038/s41597-021-00974-z,"OPTIMADE, an API for exchanging materials data",12,8,2021,https://github.com/Materials-Consortia,All associated code is hosted under the Materials-Consortia organisation on GitHub ().,2021-08-12,,,
994,0,Scientific Data,41597,10.1038/s41597-021-00916-9,"A 120,000-year long climate record from a NW-Greenland deep ice core at ultra-high resolution",26,5,2021,https://github.com/vgkinis/neem_isotope_data_descriptor_code,"The Python code used for the transfer, organising of the data, estimation of the precision and accuracy metrics as well as the plots included in this manuscript can be found in . In the repository, we also provide auxiliary code with basic routines for post-processing of the PANGAEA data file.",2021-05-26,,,
997,0,Scientific Data,41597,10.1038/s41597-021-01004-8,"A multi-site, multi-disorder resting-state magnetic resonance image database",30,8,2021,https://github.com/bicr-resource/deface,The face-masking code is available on our GitHub project. ().,2021-08-30,,,
999,0,Scientific Data,41597,10.1038/s41597-021-00896-w,Generation of a mouse SWATH-MS spectral library to quantify 10148 proteins involved in cell reprogramming,26,4,2021,https://github.com/M-Russell/Mouse_iPSC_Spectral_Library,"The workflow for spectral library generation was scripted using a gnu-make. The make file and a companion document are included with the data in the pride repository, the make file companion document is also included with this article as supplementary file 1. The make file and the companion document are available on github (). These files should enable precise replication of the library from raw data as presented here and re-use of the raw data through varied processing. The library was created with a series of open source software packages, the precise versions and sources of these programs are given in the documentation. Python scripts are required for the pipeline and instructions are given on how to install the versions used.",2021-04-26,,,
1007,0,Scientific Data,41597,10.1038/s41597-020-00608-w,An annotated fluorescence image dataset for training nuclear segmentation methods,11,8,2020,https://github.com/perlfloccri/NuclearSegmentationPipeline,"We provide code to transform predicted annotation masks in TIFF-format to SVG-files for curation by experts as well as the transformation from SVG-files to TIFF-files. The contour sampling rate when transforming mask objects to SVG-descriptions can be set in accordance to the size of predicted nuclei. Therefore, new nuclei image annotation datasets can easily be created utilizing the proposed framework and a tool to modify SVG-objects, such as Adobe Illustrator. The code is written in python and is publicly available under .",2020-08-11,,,
1010,0,Scientific Data,41597,10.1038/s41597-020-0534-3,The FLUXNET2015 dataset and the ONEFlux processing pipeline for eddy covariance data,9,7,2020,https://github.com/AmeriFlux/ONEFlux,"The ONEFlux collection of codes used to create data intercomparable with FLUXNET2015 has been packaged to be executed as a complete pipeline and is available in both source-code and executable forms under a 3-clause BSD license on GitHub: . The complete environment to run this pipeline requires a GCC compatible C compiler (or capability to run pre-compiled Windows, Linux, and/or Mac executables), a MATLAB Runtime Environment, and a Python interpreter with a few numeric and scientific packages installed. All of these can be obtained at no cost.",2020-07-09,,,
1014,0,Scientific Data,41597,10.1038/s41597-020-00588-x,"Quantum chemical calculations for over 200,000 organic radical species and 40,000 associated closed-shell molecules",21,7,2020,https://github.com/pstjohn/bde,"Code used to perform the high-throughput calculations are available at . The code relies on cclib and RDKit to process molecular information in Python, Gaussian to perform the DFT calculation, and pandas for data processing. Some of the code relating to the PostgreSQL database and NREL’s HPC infrastructure is site-specific and will likely need to altered to run these types of calculations on alternative HPC systems.",2020-07-21,,,
1015,0,Scientific Data,41597,10.1038/s41597-020-00712-x,"The Building Data Genome Project 2, energy meter data from the ASHRAE Great Energy Predictor III competition",27,10,2020,https://github.com/buds-lab/building-data-genome-project-2,The Building Data Genome 2 data set and the custom code used for its creation and analysis is hosted in a public Github repository () and its v1.0 release has been deposited in Zenodo. This codebase includes several Jupyter notebooks with Python and R data analysis workflows that can be easily reproduced.,2020-10-27,,,
1017,0,Scientific Data,41597,10.1038/s41597-020-0473-z,"The ANI-1ccx and ANI-1x data sets, coupled-cluster and density functional theory properties for molecules",1,5,2020,https://github.com/aiqm/torchani,"All electronic structure calculations were computed with the Gaussian 09 [cite] or ORCA electronic structure packages [cite]. All molecular dynamics simulations for sampling were carried out with the atomic simulation environment (ASE). The analysis and active learning scripts are available upon request. The C++/CUDA implementation of our ANI code is available online in binary format [ref], but source code is not publicly released. Alternatively a PyTorch version ANI is available as open source. [].",2020-05-01,,,
1019,0,Scientific Data,41597,10.1038/s41597-020-0415-9,A NWB-based dataset and processing pipeline of human single-neuron activity during a declarative memory task,4,3,2020,https://github.com/rutishauserlab/recogmem-release-NWB,"All code associated with this project is available as open source. The code is available on GitHub under the BSD license (). Both Python and MATLAB scripts are included in this repository along with the matNWB API. We also provide a streamlined workflow as a Jupyter Notebook. Note, we tested our code with the following versions of the Python Packages: numpy (1.17.2), pandas (0.23.0), scipy (1.1.0), matplotlib (2.2.2), pynwb (1.1.0), hdmf (1.2.0), and seaborn (0.9.0). Detailed instructions on installing and running the code in this repository are found in our online documentation on GitHub.",2020-03-04,,,
1021,0,Scientific Data,41597,10.1038/s41597-020-00692-y,A three-dimensional thalamocortical dataset for characterizing brain heterogeneity,20,10,2020,https://github.com/nerdslab/xray-thc,"Code for downloading the data and annotations in bossDB can be found in the ‘data_access_notebooks’ folder here: . A Jupyter notebook for generating the results in Figs. ,  can be found in the ‘analysis_notebooks’ folder in the same repo. Annotations, images, and analysis notebooks used for the inter-rater reliability study, are also provided through figshare to facilitate reproducibility. All of these examples are written in Python 3 and executed using Jupyter notebooks, a cross platform Python solution.",2020-10-20,,,
1024,0,Scientific Data,41597,10.1038/s41597-020-00688-8,A cross-country database of COVID-19 testing,8,10,2020,https://github.com/owid/covid-19-data/tree/master/scripts/scripts/testing,"Code used for the creation of this database is not included in the files uploaded to figshare. Our scripts for data collection, processing, and transformation, are available for inspection in the public GitHub repository that hosts our data ().",2020-10-08,,,
1025,0,Scientific Data,41597,10.1038/s41597-020-00719-4,Paired rRNA-depleted and polyA-selected RNA sequencing data and supporting multi-omics data from human T cells,9,11,2020,https://github.com/LuChenLab/40Tcells,The codes used in this article were deposited in .,2020-11-09,,,
1030,0,Scientific Data,41597,10.1038/s41597-020-0467-x,"Simultaneous human intracerebral stimulation and HD-EEG, ground-truth for source localization methods",28,4,2020,https://github.com/iTCf/mikulan_et_al_2020,"Usage demonstration scripts and the code used for the preparation, pre-processing and technical validation of the Localize-MI dataset are publicly available at .",2020-04-28,,,
1036,0,Scientific Data,41597,10.1038/s41597-019-0346-5,Global karst springs hydrograph dataset for research and management of the world’s fastest-flowing groundwater,20,2,2020,https://github.com/KarstHub/WoKaS,"The R code to download datasets directly from the hydrological databases and to combine them with the spring discharge time series obtained from the other sources (see above) is available at . The code is provided in R programming language version 3.5.0, and commented following a recommended programming comment guidelines. Comprehensive instructions on how to run the code and system requirements are provided by a “README” file included in the GitHub repository.",2020-02-20,,,
1037,0,Scientific Data,41597,10.1038/s41597-020-0411-0,"Very high resolution, altitude-corrected, TMPA-based monthly satellite precipitation product over the CONUS",3,3,2020,https://github.com/JVFayne/HRAC-Precip_v1,"R programming language and Matlab scripts used to produce (Eq. ) and validate (Eq. ) this data as well as the Monte Carlo coefficient analysis are publicly available with a public access license through GitHub: . Due to the simplicity of the correction formula, the scripts can be easily translated to other programming languages; the free to use open source packages ‘raster’, ‘rgdal’, and ‘rgeos’ are required to use the R scripts, although the code functions of these packages that are used in the scripts (such as reading and writing geospatial files) do not change over the course of version updates, and many other programming languages such as Matlab and Python use similar packages to read and write raster files. Additional software packages are not required to produce these data.",2020-03-03,,,
1040,0,Scientific Data,41597,10.1038/s41597-020-0567-7,"GlobalFungi, a global database of fungal occurrences from high-throughput-sequencing metabarcoding studies",13,7,2020,https://github.com/VetrovskyTomas/GlobalFungi,The workflow included several custom made python scripts (labelled by star in the Fig. ) which are accessible here: .,2020-07-13,,,
1044,0,Scientific Data,41597,10.1038/s41597-020-00595-y,A data resource from concurrent intracranial stimulation and functional MRI of the human brain,5,8,2020,https://github.com/wiheto/esfmri_data_descriptor,"See  for code used for: fMRIPrep execution, MRIQC comparision, and confound differences between pre and postop.",2020-08-05,,,
1054,0,Scientific Data,41597,10.1038/s41597-020-00682-0,"ClimActor, harmonized transnational data on climate network participation by city and regional governments",6,11,2020,https://github.com/datadrivenenvirolab/ClimActor,Data and code for the ClimActor R package functions is available on GitHub: .,2020-11-06,,,
1065,0,Scientific Data,41597,10.1038/s41597-020-0498-3,"Simultaneous EEG-fMRI during a neurofeedback task, a brain imaging dataset for multimodal data integration",10,6,2020,https://github.com/glioi/BIDS_fMRI_analysis_nipype,"A detailed description of the bimodal EEG-fMRI NF platform is given in: the platform software package for real-time analysis and visualization is well documented but not publicly available. Python pipelines for the analysis of structural and functional MRI are available on github (), in form of commented jupyter notebooks. Other scripts used for the technical validation in this paper can be provided by the authors upon request.",2020-06-10,,,
1071,0,Scientific Data,41597,10.1038/s41597-020-0574-8,"TAASRAD19, a high-resolution weather radar reflectivity dataset for precipitation nowcasting",13,7,2020,https://github.com/MPBA/TAASRAD19,"All the software described in Technical Validation is available in a public GitHub repository (), along with the Python scripts for sequence pre-processing, installation scripts for the MXNet framework, pre-trained network model weights, and examples of radar prediction output sequences. All the code was written in Python 3.6 and tested on Ubuntu releases 16.04/18.04. Some pre-processing steps (e.g. sequence and outlier mask generation) require a non trivial amount of computing resources and memory. Training the deep learning model with the same parameters described in the paper requires either two GPUs with 8GB of RAM or one GPU with 16GB. Please refer to the  files in the code release for further instructions.",2020-07-13,,,
1073,0,Scientific Data,41597,10.1038/s41597-020-00707-8,Density functional theory-based electric field gradient database,21,10,2020,https://github.com/usnistgov/jarvis,Python-language based codes for carrying out calculations and analyzing the results are provided at the JARVIS-Tools GitHub page ().,2020-10-21,,,
1081,0,Scientific Data,41597,10.1038/s41597-020-0412-z,A multi-omics dataset of heat-shock response in the yeast RNA binding protein Mip6,27,2,2020,https://github.com/ConesaLab/MultiMip6,Preprocessing scripts for each of the omics datasets are available at the Github repository ().,2020-02-27,,,
1083,0,Scientific Data,41597,10.1038/s41597-020-00603-1,A consistent Great Lakes ice cover digital data set for winters 1973–2019,6,8,2020,https://github.com/NOAA-GLERL/icegridresampling,"We developed R scripts to compute the spatial and temporal interpolated ice cover values. Spatial interpolation for Grid-510 is processed by “Resampling_Raster.R”, and temporal interpolation for non-daily data is estimated by “Time_Interp.R”. Both scripts utilize RStudio version 1.1.463, and are available on the NOAA GLERL GitHub Repository at . This repository also contains sample scripts (Python, MATLAB and R) to demonstrate how to load the ASCII data into memory.",2020-08-06,,,
1086,0,Scientific Data,41597,10.1038/s41597-020-0542-3,A rasterized building footprint dataset for the United States,29,6,2020,https://github.com/mehdiheris/RasterizingBuildingFootprints,Our software is available through U.S. Geological Survey code repository (https://doi.org/10.5066/P9XZCPMT). Our serial code is also available in our Github page: .,2020-06-29,,,
1096,0,Scientific Data,41597,10.1038/s41597-020-0508-5,A clinically and genomically annotated nerve sheath tumor biospecimen repository,19,6,2020,http://github.com/sage-bionetworks/JHU-biobank,"A Github repository () con tains the codes required to generate the figures with a versioned repository available at Zenodo. The tutorials are provided in R and Python languages, contained in the r_demos and py_demos directories respectively. All of the analytical code is provided in the directory marked “analysis”. Additionally, we have provided Docker containers and R scripts to facilitate reproducibility of the figures in the paper.",2020-06-19,,,
1097,0,Scientific Data,41597,10.1038/s41597-020-00655-3,"TILES-2018, a longitudinal physiologic and behavioral data set of hospital workers",16,10,2020,https://github.com/usc-sail/tiles-dataset-release,"All code for collecting, formatting, processing, and learning on the data is made freely available at . Information about the code dependencies and package requirements are available in the same Github repository.",2020-10-16,,,
1114,0,Scientific Data,41597,10.1038/s41597-019-0290-4,A large-scale dataset for mitotic figure assessment on whole slide images of canine cutaneous mast cell tumor,21,11,2019,https://github.com/maubreville/MITOS_WSI_CCMCT/,All code used in the experiments described in the manuscript was written in Python 3 and is available through our GitHub repository (). We provide all necessary libraries as well as Jupyter Notebooks allowing tracing of our results. The code is based on fast.ai and OpenSlide and provides some custom data loaders for use of the dataset.,2019-11-21,,,
1118,0,Scientific Data,41597,10.1038/s41597-020-0354-5,United States wildlife and wildlife product imports from 2000–2014,16,1,2020,https://github.com/ecohealthalliance/lemis,"Our custom R package, which provides access to the data described here, is publicly available at . Installation of the package and subsequent download of the data enables efficient, on-disk manipulation of the entire cleaned dataset. Basic package usage is outlined in the main package README file on the GitHub site. The code implementation of the data cleaning process is also available in the package codebase (via the ‘data-raw’ directory) and is outlined in the associated developer README file. These scripts span the entirety of our data processing and cleaning workflow, from importation and collation of the raw USFWS LEMIS data files through to generation of the single, cleaned data file as discussed in this manuscript. Thus, the scripts serve as transparent, reproducible documentation of our data processing in full.",2020-01-16,,,
1124,0,Scientific Data,41597,10.1038/s41597-019-0304-2,"Eyasi Plateau Paleontological Expedition, Laetoli, Tanzania, fossil specimen database 1998–2005",3,12,2019,https://github.com/paleocore/paleocore110/blob/master/eppe/import_1998_2005.py,"Data were imported into the Paleo Core data repository using Python (version 3.6) standard libraries (re, datetime, pytz), the xlrd library (version 1.20) for reading Excel files, and the database API included with the Django web framework (version 1.11.20). All of the original source code used to process the data are freely and publicly available through the Paleo Core github repository at: .",2019-12-03,,,
1142,0,Scientific Data,41597,10.1038/s41597-019-0326-9,A global database of historic and real-time flood events based on social media,9,12,2019,https://github.com/jensdebruijn/Global-Flood-Monitor,"The datasets generated have been created using code for Python 3.6, Elasticsearch 6.6 and PostgreSQL 10.6. The code is available through .",2019-12-09,,,
1153,0,Scientific Data,41597,10.1038/s41597-019-0090-x,"PathoPhenoDB, linking human pathogens to their phenotypes in support of infectious disease research",3,6,2019,https://github.com/bio-ontology-research-group/pathophenodb,The source code for PathoPhenoDB is freely available at .,2019-06-03,,,
1161,0,Scientific Data,41597,10.1038/s41597-020-0355-4,"PIC, a paediatric-specific intensive care database",13,1,2020,https://github.com/Healthink/PIC,"The code that was used to create the PIC database, calculate statistics of this paper, demonstrate a machine learning task and source code which underpins the PIC website and documentation is openly available, and contributions from the research community are encouraged: .",2020-01-13,,,
1173,0,Scientific Data,41597,10.1038/s41597-019-0056-z,A multi-species repository of social networks,29,4,2019,https://github.com/bansallab/asnr/,All code for data characterization has been written in  using the  package. The code is open source at .,2019-04-29,,,
1176,0,Scientific Data,41597,10.1038/s41597-019-0242-z,Integrated open-source software for multiscale electrophysiology,25,10,2019,https://github.com/brainstorm-tools/brainstorm3,The toolbox can be acquired as part of Brainstorm’s GitHub repository: .,2019-10-25,,,
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,"There are several issues with the provided Python script that could impact the quality and viability of its outputs:

1. **Use of deprecated yaml.load() function**: The `yaml.load()` function should be used with a Loader to avoid security risks. It is recommended to specify `Loader=yaml.SafeLoader`.

2. **Potential overwriting of **scen** variable**: The `scen` variable is assigned with `params['rescaler']['scen']` and may be overwritten later (if `len(fsplit) == 10`). This could lead to unintended behavior if `scen` is expected to remain constant.

3. **Dynamic import of all PATH directories**: Appending all directories in `os.environ['PATH']` to `sys.path` could clutter the Python module search path and potentially cause import conflicts or security issues.

4. **Unsafe printing of variables**: The script uses print statements without parentheses, which is not compatible with Python 3. Additionally, `print` is used for error messages but should be replaced by logging or raising exceptions.

5. **Ignored potential warnings**: The usage of `warnings.filterwarnings('ignore')` suppresses all warnings, which can hide important signals that something might be wrong.

6. **Incorrect exit statement with `sys.exit(0)`**: The script uses `sys.exit(0)` in error cases, which indicates a successful termination. It should use a non-zero exit code (e.g., `sys.exit(1)`).

7. **Inconsistent time units handling**: The adjustment of the `time` variable assumes that its units always start with a year. This might not hold true for all datasets and can lead to incorrect time indexing.

8. **Incomplete error handling and validation**: 
   - The script does not handle scenarios where variables or dimensions might be missing from the NetCDF files.
   - If `var` does not have the expected attributes (`units`, `long_name`), defaults are used but may not be sufficient.

9. **Missing pre-check for output file existence**: The script does not check for the existence of the output file prior to writing, potentially leading to accidental overwriting of existing data.

10. **Possible division by zero**: The line `areatot = masked_where(areatot == 0, areatot)` suggests possible zero values in `areatot`. Ensure no division by zero occurs in subsequent calculations involving `areatot`.

11. **Redundant check for `isfile(rffile)`**: There’s no check for whether `rffile` is provided or required. The script assumes its necessity but only conditionally processes it.

12. **Implicit array resizing**: Usage of `resize(area * wir, (nt, nlats, nlons))` and similar resizing methods should be done with caution, ensuring that the intended shape and data align correctly.

In summary, several logical flaws, potential security issues, and insufficient error checks could lead to incorrect, incomplete, or insecure outcomes of the script.

","There are several issues in the provided script that would impact the quality and viability of its outputs:

1. **Deprecation Warning for `yaml.load`**:
   - The code uses `yaml.load(open(args.params, 'r'))`. Using `yaml.load` without specifying a `Loader` is risky as it can execute arbitrary code.
   - The preferred way is to use `yaml.safe_load` instead.

2. **Conflict in Scenario Extraction**:
   - `scen` is set twice: first from the `params` dictionary, and then conditionally based on parsing the `irfile` filename. This may lead to unintended overwrites or inconsistencies if the expectations differ between these two sources.

3. **Unchecked File Existence**:
   - The script does not check if files (`bcfile`, `irfile`, `mkfile`, `rffile`, `wtfile`) actually exist before attempting to open them. This can cause the script to crash with an unhandled exception.

4. **Assumption on Output File Overwriting**:
   - The code does not check if the `outfile` already exists, which could inadvertently overwrite existing data.

5. **Unchecked Dimensions and Variable Existence in NetCDF Files**:
   - When opening NetCDF files, the script assumes the presence of specific dimensions and variables without any check. If a required variable or dimension is missing, it leads to a runtime error.

6. **Incorrect Warning Filter Placement**:
   - The warning filter using `warnings.filterwarnings('ignore')` is globally set and may suppress warnings beyond the intended scope, possibly hiding important issues.

7. **Potential Integer Division in Area Calculation (Python 2 Issue)**:
   - The area calculation `area = 100 * (111.2 / 2) ** 2 * cos(pi * lats / 180)` implicitly uses integer division if run in Python 2, leading to incorrect area values. Ensuring true division by using `from __future__ import division` is necessary for accuracy.

8. **Handling of `nanmean`**:
   - The script uses `nanmean` from numpy assuming it will remove `NaN` values. However, if all values are `NaN`, this results in `NaN` correction, which isn't managed and could propagate through computations.

9. **Hardcoded Indexing and Potential IndexErrors**:
   - The script assumes specific dimensions and ordering in the datasets. If altered or different, the hardcoded indices can lead to `IndexError`.

10. **Syntax Print Statement (Python 2/3 Compatibility)**:
    - The script uses `print` statement syntax tailored to Python 2, which would cause issues in Python 3. For example, `print ""Cannot find variable %s in %s. Exiting"" % (scen, bcfile)` should use parentheses for compatibility with Python 3.

Addressing these issues is crucial to ensure the script functions as intended and produces reliable, accurate outputs.

Thus, it is clear that there are problems in the provided script.","There are several issues with the script that could impact the quality and viability of its outputs:

1. **Mutable Default Argument in YAML load**: The `yaml.load` function without specifying a loader can pose a security risk, leading to arbitrary code execution if the YAML contents are not trusted. Using `yaml.safe_load` would be safer.

2. **String Formatting Syntax**: The script uses old-style string formatting (`%`). This is becoming deprecated in favor of the newer `str.format()` or f-strings. For instance, the exit messages use the older `%` method.

3. **Accessing non-existent keys from YAML**: The script assumes keys like `'rescaler'`, `'dt'`, `'mp'`, `'cr'`, and `'scen'` exist in the YAML file, without any checks. If these keys are missing, it will raise a `KeyError`.

4. **Dataset variable names hardcoded**: Variable names like `'lat'`, `'lon'`, `'irrigated'`, `'rainfed'`, `'yield_detrend'`, `'time'`, `'scen'`, `'dt'`, `'mp'`, `'cr'` are hardcoded. If the NetCDF files have different variable names, it can lead to errors.

5. **Silent overwrite of the output file**: If an existing file with the same name as specified in `outfile` already exists, it will be silently overwritten. It would be better practice to check and possibly warn the user.

6. **Potential division by zero**: There is a potential division by zero when calculating `correction = detrend_ma / detrend_none` if `detrend_none` is zero.

7. **Ignoring warnings**: The script ignores all warnings by setting `warnings.filterwarnings('ignore')`. This may suppress important warnings that could indicate issues in the data or computations.

8. **Inconsistent time variable modifications**: The script modifies the `time` variable twice with the same operation (`int(findall(r'\d+', tunits)[0])`), which could be redundant and potentially erroneous.

9. **Index Errors**: In cases where indices are not found, for instance in the `try` block where it looks for `dtidx`, `dtnone`, `mpidx`, `cridx` and `scenidx`, it will raise a `ValueError`, which may not be handled properly beyond printing a message.

10. **Potential mismatch in variable lengths**: The script doesn't handle cases where the lengths of `time` and `ftime` arrays don't match properly. It may not raise errors but could lead to incorrect slicing or data misalignment.

11. **Possible incorrect usage of `numpy.resize`**: Resizing arrays in this way can lead to unintended results if the new size is not compatible with the original one.

Addressing these issues would improve the robustness and reliability of the script."
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,"There are a few notable issues in the provided script that could impact the quality or viability of its outputs:

1. **Unsafe YAML Loading**: The use of `yaml.load(open(options.params))` without specifying a loader is unsafe as it can lead to security vulnerabilities if the YAML input is not trusted. The recommended safe method is to use `yaml.safe_load(open(options.params))`.

2. **Potential Overwriting of Outfile**: If the `-o` or `--outfile` option is not set, it defaults to an empty string. The `RescaledFile` class is instantiated using this empty string, which may lead to unexpected behavior or overwrite issues if the `outfile` path isn't properly provided.

3. **Inconsistent Print Statements**: The print statements use Python 2 syntax (`print ""message""`) while the shebang line (`#!/usr/bin/env python`) doesn't specify Python 2 or 3. This might cause issues depending on the Python version used to execute the script. This should be updated to use `print(""message"")` for consistency with Python 3.

4. **Hard Exit on Missing Files**: If no files match the pattern in the `fnmatch.filter()`, the script calls `exit(0)` which stops execution without any error. This can lead to silent failures. It's generally better to raise an error or provide a meaningful error message.

5. **Unused Imports**: Several modules (e.g., `os.environ['PATH']`, `sys.path.append(p)`) are added but not used, which clutters the namespace and can be removed to clean up the script.

6. **Assumption of File Naming Convention**: The script assumes specific naming conventions for input files and mask files, which may not always hold true. If the files do not follow the expected naming conventions, the script may fail.

In summary, resolving these issues would be necessary to ensure correct, secure, and expected behavior of the script.","There are several issues in the script that might impact the quality and viability of its outputs:

1. **YAML File Loading**: The `yaml.load` function is called without specifying a loader, which can be unsafe and may raise a deprecation warning in some environments. It should specify `Loader=yaml.SafeLoader`.

2. **Hard-Coded Indices**: The script assumes that certain indices will always exist in the input filename (`infile`). If the filename pattern changes or the expected indices do not match, it will raise an indexing error.

3. **File Overwriting Risk**: The `outfile` parameter might default to an empty string or a constant value. If the outfile isn't specified by the user, it could lead to unintentional overwriting of output files.

4. **Command Injection Risk**: Appending environment paths to `sys.path` without validation can potentially introduce security risks.

5. **File Handling Without Try-Except**: Operations involving file opening (e.g., `open(options.params)`, `nc(mkfile)`, `nc(infile)`, `nc(files[0])`, etc.) do not handle exceptions that might arise from issues like file not found, permission errors, etc.

6. **Incomplete Output Handling**: If a match for the model and scenario file is not found in `indir`, the script will simply exit without any error message or cleanup.

7. **Inconsistent Mask Handling**: The script assumes that all files will have the same masked values, but this might not always be true.

8. **Hardcoded Dimension Naming**: Variables' names (like `vname`, `time`, `lat`, `lon`, `irr`) are hard-coded and don’t account for variations in input files' variable naming conventions.

9. **Potential Variable Overwriting**: Variables such as `time`, `lats`, `lons`, `irr`, etc., are used more than once and could potentially overwrite without clear separation of context.

10. **Lack of Path Validation**: The input directory (`indir`) is not validated for existence prior to accessing files within it.

11. **Ambiguous Exit Codes**: The script uses `exit(0)` for error scenarios, which conventionally means a successful termination.

12. **Compatibility Issues**: Print statements without parentheses (`print ""%s is missing... ""`) are for Python 2.x and can cause a syntax error in Python 3.x.

Due to these issues, the script might not function reliably in all scenarios, and the quality of its outputs can be compromised.

","The provided Python script contains several issues that could impact the quality and viability of its outputs:

1. **Overwriting of Filenames in Directory:**
   - The script collects matching filenames from `indir` and stores them in the `files` list. However, if there are multiple matching files, it will only keep the first match. This behavior might unintentionally exclude relevant data or cause confusion.

2. **Potential Issues with Parsing `params`:**
   - The script uses `yaml.load(open(options.params))`, which can be unsafe as it can execute arbitrary code. For security, it is recommended to use `yaml.safe_load`.

3. **Hard-Coded Indexing:**
   - The script hard-codes indices for extracting `climate` and `crop` from the `infile` name. If the file naming convention varies, this could lead to errors.

4. **Error Handling and Exiting:**
   - When `f` finds that a required dimension is missing, it prints an error message and calls `sys.exit(0)`. Using `exit(0)` should be avoided as it signals a normal termination. Instead, it should use a non-zero status code to indicate an error.

5. **Deprecated Library `optparse`:**
   - The `optparse` library is deprecated and should be replaced by `argparse`. While functioning, using deprecated libraries is not recommended.

6. **Incomplete Error Messages:**
   - The error message constructed for missing dimensions does not reflect the true dimensions being missing, which could be confusing for debugging.

7. **Uninitialized `outfile`:**
   - If the `outfile` option is not provided, it defaults to an empty string, which could cause issues when writing the final output file.

8. **Potential Masking Errors:**
   - The mask handling with `modelorder.mask` assumes that masked values are always relevant for exclusion. Improper mask initialization or handling can lead to incorrect data processing.

9. **Incorrect String Formatting:**
   - Using `%` for string formatting, especially in `except` block for error messages, is outdated. The newer `.format()` method or f-strings provide more clarity and are the recommended approach.

10. **Multiple Matches Handling in fnmatch:**
   - If more than one matching filename is found for a given model and scenario in the directory, only the first match is used (`f[0]`). This may unintentionally exclude relevant data.

These issues suggest that the script may not function as intended in all scenarios and could produce incomplete or erroneous outputs."
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,"There is one significant issue with the script:

- **String Format Syntax for Python 3**:
  The script uses old-style string formatting (`%`), which will raise a `SyntaxError` in Python 3. Specifically, the code sections where prints and formatted strings are used (e.g., `print 'Processing directory', subdir, '. . .'`). This is a compatibility problem if the script is intended to run on Python 3.

Overall, despite this issue, the script does not seem to overwrite filenames unnecessarily, duplicate work, or include incorrect entries. Thus, the output quality and viability of the script are maintained apart from the compatibility issue noted. To make it fully functional on Python 3, you would need to update the string formatting method from old-style to new-style or f-strings.

Since improvements were not asked for, it's best stated that in context to problems:

**There are no problems with the code logic and function, aside from Python 3 string formatting syntax.**","There are several issues with the code that could affect the quality or viability of its outputs:

1. **File Overwriting**: If the `options.sumdir` directory and any existing files match the newly generated filenames, they will be overwritten without warning. This is particularly evident in the line: 
   ```python
   sumfilename = options.sumdir + sep + 'report_' + d + '.txt'
   ```

2. **Unintentional Duplication of Work**: The script assumes that the first directory (`dirs[0]`) contains representative climate models, crop models, and crops. If different directories have different sets of climate models, crop models, or crops, there can be inconsistencies or missed data checks.

3. **Excluding Necessary Entries**: Specific directories (`upload_stats` and `aggregations`) are excluded by default. If these directories contain necessary data for verification, they won't be processed.

4. **File Parsing Issues**: The file parsing logic for scenario index might be flawed. It uses:
   ```python
   scenidx = fi['scen']
   ```
   and then later:
   ```python
   scen = fileparts[scenidx].lower()
   ```
   Since `scenidx` can be a list when using `fileindices2` or `fileindices`, this won't always index correctly, leading to potential errors or ignored files.

5. **Directory Traversal Assumption**: The script assumes a specific directory structure. If there are deviations in the directory naming conventions or structure, the script won't find or process the expected files correctly.

6. **Modifying File Parts**: The script modifies and validates file parts assuming a specific naming convention. If the naming convention varies or has additional identifiers, the script might erroneously modify the filenames or miss crucial validations.

7. **Handling Special Crops**: There's a special case for `winterwheat`, but other crops that might need equivalent handling are not considered, which could lead to incorrect subdirectory paths for such crops:
   ```python
   if crops[cp] == 'winterwheat': 
       subdir = dir + sep + d + sep + climmodels[cm] + sep + 'wheat' 
   ```

In summary, while the script appears to be comprehensive, these issues can impact its robustness, completeness, and reliability. Adjustments should be made to ensure no overwriting of critical files, exhaustive handling of directory structures, accurate parsing, and all potential crops or variables are treated correctly.","There are several issues with the given Python script that could impact the quality or viability of its outputs:

1. **String Formatting (print statement)**: The code uses Python 2.x style print statements without parentheses, which will cause a syntax error in Python 3.x. For example: `print 'Processing directory', subdir, '. . .'`

2. **File Overwriting**: The script writes output to files named `report_<model>.txt` in the directory specified by `options.sumdir`. If multiple models are processed, this could lead to overwriting of the same file if `options.sumdir` is an empty string or the same directory. 

3. **Ignoring Empty Subdirectories**: The script correctly skips subdirectories that don't exist or are empty when checking for the presence of simulations, but it doesn't log or warn about empty subdirectories which might be important for users to know.

4. **Hard-Coded Filename Indexing**: The script assumes specific positions in filenames to determine scenario, variable, crop, etc. If a filename does not match the hard-coded indices (`fileindices`, `fileindices2`, `fileindices3`), it will be skipped. This rigid structure might fail for filenames that are formatted slightly differently but still valid.

5. **No Distinction Between Crop Abbreviation and Full Name in Filename Check**: This could cause issues if there are crops with similar abbreviation patterns or different naming conventions in different files, particularly because `fileparts[cropidx]` and `cropabbr[cp]` are frequently compared for equality without a rigorous check.

6. **Assumption on Time Units Parsing**: When parsing the units attribute of the `time` variable to check date consistency, the code assumes the format ""growing seasons since <date>"". Variations in this format might cause it to fail or raise a `ValueError`.

7. **Undefined `epiic_scens` Variable**: There's a commented section that indicates variable `epiic_scens` should be `epic_scens` but it’s misspelled (`epiic_scens`). If uncommented, this would cause a `NameError`.

8. **Unchecked Directory Separators**: The script does not ensure that paths are correctly joined using `os.path.join`, which makes the script OS-dependent concerning the directory separator.

9. **Assumption on NetCDF Attributes and Dimensions**: The script makes several checks and changes based on attribute presence (`_FillValue`, `units`) or dimension names (`time`, `lat`, `lon`). However, it doesn't account for case sensitivity or some potential variations which might cause it to raise errors or incorrect assumptions about the data structure.

10. **Division with Potential Zero Division Error**: Calculating the percentage coverage by dividing `v.size - v.mask.sum()` by `npts` might raise a `ZeroDivisionError` if `npts` becomes zero:
    ```python
    punmasked = 100. * (v.size - v.mask.sum()) / npts
    ```

11. **Using Literal String Formatting (Not Readable)**: The variable `vlist` and other similar lists are setup for checking file validity but their setup requires constant manual updates to ensure consistency with the filenames being processed. This could be a source of error if the lists are not correctly synchronized.

In summary, the script contains several problematic areas concerning file handling, hard-coded assumptions, error handling, and platform dependency that need to be addressed to ensure robust functionality and accurate outputs."
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,There are no problems,"There are several problems with the given Python script that could impact the quality/viability of its outputs:

1. **Filename Overwriting**:
   - The script generates summarized reports and names them as `report_<model>.txt` and saves them in `options.sumdir`. If the `options.sumdir` is the same for multiple models, each model can overwrite the report of the previous model, leading to loss of information.

2. **Final Separator Removal**:
   - In the line `if dir[-1] == sep: dir = dir[: -1]`, the separator may not always be removed correctly. For example, on Windows, the separator is ""\\"", and if `dir` ends with ""\\"" and `sep` is also ""\\"", this condition will work. However, if there is an extra space or if the path format is different, it might fail or produce incorrect results.

3. **Unintentional Work Duplication**:
   - When climate models or crop names involve wildcards (`*`), and the function `listdir` is used to determine available directories or files, it's possible for the same directories or files to be processed multiple times if the list includes directories named similarly to each other, especially when there could be default directories or files that were included in both wildcard results and listed results.

4. **Hard-coded Lists**:
   - The script contains hard-coded lists for climate years, crop abbreviations, scenarios, variables, ranges, and units. While this may not directly cause issues, any mismatch or omission in these lists could lead to incorrect processing or missing simulations, especially as datasets evolve.

5. **File Handling and Error Reporting**:
   - The script relies heavily on file naming conventions and structures. If any of the files deviate from these conventions, they are ignored or might cause parsing errors. Additionally, there are multiple places where it raises exceptions for unknown climate or crop names. This could halt the process prematurely.

6. **Assumption on Fileparts Length**:
   - The script assumes the specific lengths of `fileparts` (either 10 or 11). If the filenames deviate slightly, such files are skipped by the `continue` statement, which could result in missing data files being unreported.

7. **Search for Variables**:
   - The search for the variable in `fvars` is limited to the provided list `vlist`. Any deviation in variable naming would mean those variables are skipped, potentially missing important checks and inclusions.

8. **Gridcells and Points Calculation**:
   - The calculation of `npts = v.size / 4` as a rough approximation for land points might be too simplistic and not accurately represent the actual number of data points, which could skew the reporting on coverage and errors.

9. **Checking for Array Masking**:
   - The script checks `if not isMaskedArray(v)` and treats it as a fatal error. However, not all variables might necessarily be masked arrays, and treating it as a fatal error might ignore valid data.

10. **Possible Typographic Error**:
   - In the ""COVERAGE"" check, `punmasked` could be calculated incorrectly because `v.mask.sum()` might not exist if `v` is not a masked array, leading to an exception.

11. **Indentation Issues**:
   - Inconsistent indentation in lines such as `fvars = ncf.variables.keys()` could cause syntax errors during execution.

12. **String Representation of Header**:
   - While creating headers for summaries, there is a mix of `+` and `*` for string operations which may lead to inconsistent reporting or format issues in the summary files.

In summary, the script has multiple points where the assumption of strict file and directory structures, the rigid use of lists, and the potential for overwriting files and misinterpreting data could collectively impact the accuracy and reliability of the outputs.","There are a few potential issues with the given code:

1. **File Overwriting**: The script does not check if a file with the new name already exists when it suggests renaming a file. This might lead to unwanted overwrites or errors.

2. **Directory Existence**: When creating the summary report file, the script does not check if the provided `sumdir` directory exists. If it does not, the script will throw an error when trying to save the file.

3. **List Slicing**: The script slices the `dir` variable to remove a trailing separator (if any), but does not check if the resulting directory name is still valid. If it's an empty string or a lone separator after slicing, subsequent operations will fail.

4. **Mixed Use of Variable Types**: While the code attempts to handle both numeric and string representations of data (like years), it might encounter issues if unexpected data is read from the files (e.g., if non-numeric data appears where numbers are expected).

5. **Hard-Coded Values**: The code has a lot of hard-coded values and lists. While not necessarily an error, this can make it harder to maintain and may also lead to issues if unexpected input not fitting these hard-coded values is processed.

Other than these issues, the rest of the script appears to be logically consistent and should work as intended, assuming the input folder structure and file formats are as expected.

There are no problems."
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,"There are a few issues with the script:

1. **Shapefile Attribute Reading:**
   - It's not apparent whether the entries in `shapes` and `records` from the shapefile are matched correctly, as the script depends on the order presented by the `Reader` class. A discrepancy in the order may lead to incorrect association.

2. **Uninitialized Variables:** 
   - The variable `index2` is used within the loop over the shape parts, but if the loop does not iterate, it can lead to a reference before assignment error in `segs.append(data[index2 :])`.

3. **Dimensionality Confusion:**
   - The calculation for `tarea` uses a hardcoded value `111.2 / 2`. This calculation assumes a specific grid size, which might not be appropriate for the dataset used. An incorrect grid size would result in incorrect area calculations.

4. **FPUs Handling:**
   - The script does not check if `fpumap == fpu[i]` results in any elements before attempting operations like `harea[fpumap == fpu[i]].sum()`. If there are no matching elements, operations like summing an empty array might not behave as expected.

5. **Array Broadcasting:**
   - The line `weights[:, :, i] = cals[crops[i]]` assigns scalar values to slices of a higher-dimensional array. Although it might work due to implicit broadcasting, it's generally risky without ensuring the compatibility of dimensions.

6. **File Overwriting:**
   - During the iteration over `mapfiles` and `ncfiles`, if the `splitext` function results in filenames that are not unique for different indexing combinations, some files might get overwritten, losing intermediate results.

Given the script's structure and purpose, these issues can affect both the correctness and comprehensiveness of the results.

There are actual problems.","- The script assumes that the arrays `lats` and `lons` are 1-dimensional, which is typically the case for the lat/lon dimensions of netCDF files, but this should be verified.
- The `where` function is used without a condition guaranteeing there will be a corresponding entry in `cfpu`. If there is not, an `IndexError` will occur.
- The `for` loop that populates the `areas` array assumes that `cfpu` will have a direct mapping for every element of `fpu`, which may not be correct.
- When creating `mapfiles` and `ncfiles`, if the default filenames (""map.png"" and ""map.nc4"", respectively) are not changed, subsequent runs will overwrite these files unintentionally if output already exists.
- The usage of `masked_array` assumes that all elements should start masked, which might not be intended and could result in incorrect masking if initial values are needed.

There are no major critical issues like infinite loops, fundamental logic errors, or misunderstood API calls that would render the script non-functional, but errors or dataset mismatches could lead to runtime problems. Overall, these factors could affect the accuracy or reliability of the output.","There are a few problems with the code that could impact the quality and viability of its outputs:

1. **Incorrect variable extraction**:
   - In the loop where `dyarr` is being filled, the slicing for the variable extraction is reversed. It should be:
     ```python
     dyarr[m, g, c, :, :, co] = f.variables[var][-3:, :].T
     ```
     instead of:
     ```python
     dyarr[m, g, c, :, :, co] = f.variables[var][:, -3:].T
     ```

2. **IndexError possibility**:
   - In the lines:
     ```python
     fpuidx = where(cfpu == fpu[f])[0][0]
     ```
     if `cfpu` does not contain `fpu[f]`, this will raise an `IndexError`.

3. **Possible missing declaration/location of input files**:
   - The script does not confirm the existence or validate the correctness of input files (`metrics.nc4`, `fpu.mask.nc4`, etc.). If any of these files are missing or incorrect, significant failures could occur.

4. **Invalid FPU indices**:
   - In `validfpus` loop, the lack of validation could lead to including invalid FPU indices which might not exist in the shape file or other datasets.

Other than these issues, the code generally appears to address significant aspects of data handling and output generation without potential for unintentional overwriting or duplication of work, assuming the presence of correctly formatted and located input files.

Given the caveats noted above, the response to the script is that there are some problems."
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,There are no problems.,There are no problems.,"The script has the following issues that could potentially impact the quality or viability of its outputs:

1. The variable `decade` is recalculated based on the decade units from `infile`, and if the original units are not in the expected format or if the regex `findall(r'\d+', dunits)` does not find any digits, it may lead to incorrect values or an error.

2. When the `crop` option is set to `all`, the default files `box.png` and `box.nc4` are used. This could cause existing files with the same names to be overwritten unintentionally.

3. In the section where the script replaces values for `pegasus` rice with the median over other crops, the line `notriceidx.pop(riceidx);` should use `notriceidx.pop(riceidx)` instead. The semicolon may cause a syntax error.

4. When creating weighted and area masks using `masked_array` and `masked_where`, if the shapes of the arrays do not match exactly, it may lead to broadcasting errors or incorrect mask applications.

5. In the section where the box plots are created and colored, the positions of the box plots may overlap or be incorrect if `nd` is not properly represented.

6. The `reshape` in `p3 = reshape(wvarr[:, :, :, 0], (nm * ng, nd))` assumes the shape transformation will always be valid, which may not be the case if there is a mismatch in dimensions.

7. The `plt.legend` function relies on dummy lines for plotting the legend, which is an unconventional way to handle the legend entries and might lead to confusing results if the legend markers are not properly represented. Additionally, the corresponding lines are immediately set to invisible.

These issues can lead to errors, incorrect processing, or overwrite important output files unintentionally.

This script should be carefully verified with sample datasets to ensure the validity of each operation and the integrity of the outputs."
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,"There are several issues in the provided Python script that could potentially impact the quality or viability of its outputs:

1. **Potential Index Errors**: In the loop setting `areas` values, there is a potential for an `IndexError`. The code assumes that `cfpu` always contains indices corresponding to `fpu`, which might not be the case. A missing `fpu` value in `cfpu` would cause an error.

2. **Overwriting Output Files**: The script overwrites files with the same name during each iteration of the last loop. This issue happens with `mapfiles[i]` and `ncfiles[i]`. Despite having a sequence (`i`) in the names, if the base filenames are not unique across different script executions or different parameter sets, older files may be unintentionally overwritten.

3. **File Reading Assumptions**: The script assumes that all variables (e.g., `delta_yield_26_fpu_*`) exist in the netCDF file, but this may not always be true. Lack of error handling for missing variables could lead to uncaught exceptions.

4. **Masking Assumption**: The use of `masked_where` assumes that the masks across different variables (`dy26arr`, `dy85arr`) are identical, which might not be true. This could lead to inconsistencies in masking unwanted values.

5. **Unvalidated Inputs**: The script does not validate command-line arguments. Users could provide invalid filenames, non-existent file paths, wrong data types (e.g., a string instead of a float in `percent`), which may lead to runtime errors.

6. **Missing Imports for Used Functions**: The `splitext` and `product` functions are used in the script, but the appropriate modules (`os.path.splitext` and `itertools.product`) need to be imported explicitly.

7. **Unnecessary Loot Iterations**: In the loop calculating `validfpus`, there is an unnecessary iteration through indices. If `fpumap` does not contain `fpu[i]`, `hareafpu` and `tareafpu` will sum to zero, leading to division by zero or other errors.

To clarify, the script generally makes assumptions about the content and structure of the input files without handling potential edge cases or exceptions that could arise from invalid or mismatched data. Without validations or robust error handling, it risks failing or producing incorrect outputs under unforeseen conditions.","There are several issues with the provided Python script that could impact its quality and viability:

1. **Hardcoding File Paths**:
   - The script utilizes hardcoded default file paths in the `OptionParser` setup, which may potentially overwrite important files if the defaults are not changed by the user.

2. **Loss of Overwrite Protection**:
   - When splitting the `mapfile` and `ncfile` filenames to create `mapfiles` and `ncfiles`, there is no explicit check to ensure that files are not being overwritten unintentionally.

3. **Missing Checks for File Existence**:
   - The script doesn't check if the referenced files exist before attempting to read or write them. This could result in runtime errors if the files are missing.

4. **Potential IndexError**:
   - There is a potential `IndexError` in the line `fpuidx = where(cfpu == fpu[f])[0][0]` if the condition `cfpu == fpu[f]` does not find a match, resulting in an empty array being indexed.

5. **Incomplete Plotting Attributes**:
   - The code for plotting the maps uses `vmin` and `vmax` without further customization of the colormap, which might not always be appropriate for the data range.

6. **Duplication of Work**:
   - The code calculates `tarea` and other repetitive tasks within loops that could have been executed outside to improve efficiency.

If these issues are not addressed, it could lead to erroneous outputs, overwriting critical data, or runtime exceptions. However, based on the constraints of the exercise, these specific issues should be considered when evaluating the feasibility of the script.","There are several issues with the script that could impact the quality or viability of its outputs:

1. **Variable Name Conflict**:
   - The variable `careas` is defined as a dictionary to hold harvested area data for different crops but is later overwritten within the loop when accessing weightfile variables. This can cause logical errors and data misalignment, leading to incorrect area calculations.

2. **Potential Key Error**:
   - The dictionary `careas` is accessed using crop keys 'maize', 'wheat', 'soy', 'rice'. If `crop` is not one of these, it assigns `careas` as None, which will lead to errors when trying to use this variable later in computations.

3. **Missing Variables in NetCDF File**:
   - The script assumes that all variables it attempts to access in NetCDF files are present. If any variables such as 'lat', 'lon', 'fpu', 'sum', or 'area_crop' are missing, it will throw a KeyError.

4. **Overwriting Files**:
   - The script overwrites output files if they already exist. Files `map.png` and `map.nc4` are split and modified into multiple outputs (`mapfiles` and `ncfiles`). Previous iterations of the loop can get overwritten if the script is run multiple times without changing the output filename patterns.

5. **Hardcoded Index Access**:
   - The script accesses specific indices of arrays without checks. This is risky because the assumption about array shapes & sizes may not always hold true, leading to IndexErrors.

6. **Possible Misalignment in Aggregated Areas**:
   - The assignment for `areas` assumes indexed matching between FPUs. Misalignment risks are present when fetching data via direct index access since not all FPU indices might exist in the given shape arrays, leading to inaccurate area assignments.

7. **Handling Shape Parts**:
   - The handling of shape parts seems potentially wrong since `index2` is last defined within the loop for parts but later used outside. This might raise undefined variable errors unless the shape object always populates `index2` within its length.

To summarize the issues:
- Variable name conflict and potential key error with `careas`
- Absence of checks for variables before accessing them in NetCDF files
- Overwriting output files without user consent
- Hardcoded and potentially dangerous index access
- Possible misalignment in aggregated area indexing
- Incorrect handling of shape parts which may lead to undefined variables

There are indeed problem areas that need to be resolved to ensure the script runs reliably and produces accurate outputs."
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,There are no problems.,There are no problems.,"There is an issue in the code that could impact the quality/viability of its outputs. Specifically, the `plt.show()` function is used just before `plt.savefig()` in each section. This results in the current figure being displayed interactively, potentially requiring user interaction before the script can continue, and it might also be cleared before saving the file. This could prevent the figures from being saved as intended.

Correcting this would involve removing the `plt.show()` calls or ensuring that the `plt.savefig()` calls happen before `plt.show()`. Without this correction, the saved files might not contain the expected visualizations.

So, the problem is the inclusion of `plt.show()` before `plt.savefig()` in each section."
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,"There are several issues with the code that would impact the quality/viability of its outputs:

1. **Invalid Variable Check**: The line where it checks if `var` is in `f.variables`: 
   - It uses the form `if var in f.variables` but `f.variables` is likely an attribute of the netCDF4 Dataset. This should be `var in f.variables.keys()` to ensure proper membership checking.

2. **Potential for Index Out of Bound in Areas Allocation**:
   - The code assumes `cfpu` contains `fpu`. There's no check whether `fpuidx = where(cfpu == fpu[f])[0][0]` could generate an empty array, which can lead to an IndexError if a match is not found.

3. **Potential Overwriting**:
   - The output file names for the maps and netCDF files can be overwritten unintentionally if these files already exist. There's no safeguard to prevent this, such as by checking for existing files beforehand.

4. **Incorrect Data Shape Handling**:
   - There's potential for index errors or mismatches, particularly in the reshaping and indexing processes. For example, `reshape(dyarr[:, :, :, 0], (nm * ng, nfpu))` assumes `dyarr` has more than two dimensions. If the shape is not as expected, this will cause runtime errors.

5. **Hardcoded Variable**:
   - There’s an assumption throughout the code that the spatial dimensions (`lat` and `lon`) and other variables in the netCDF files match the expected formats and names (`fpu`, `area_maize`, etc.). If the netCDF files deviate from this structure, it will cause issues.

In summary, these issues need handling to ensure the program runs correctly and reliably under various input conditions.",There are no problems.,"The script contains several issues that may affect the quality and viability of its outputs:

1. **Index Out of Range Error**:
    - When iterating over the validfpus to fill `dymap`, the code assumes that the `validfpus` indices align perfectly with `fpu`. This can lead to `IndexError` if they do not align.
   
2. **Potential Errors in Indexing**:
    - In the section where indices of weights and areas are assigned, it assumes `fpuidx = where(cfpu == fpu[f])[0][0]` will always return an index. If `fpu[f]` is not found in `cfpu`, this will result in an `IndexError`.

3. **Hardcoded Variable Name (with assumption)**:
    - The variable name construction `%s_fpu_%s_%s_%s_%s` assumes that the netCDF files have variables named exactly in this format. Any deviation in this naming convention will cause a `KeyError`.

4. **Inconsistent Use of Masking**:
    - The masks applied to `dyarr`, `weights`, and `areas` arrays may not consistently carry through all computations leading to masked or invalid values being used later in calculations.

5. **Plotting Issues**:
    - If `validfpus` is empty or certain FPU indices are missing, the `dymap` may remain entirely masked, leading to issues in plotting and saving files.

6. **Overwriting Output Files**:
    - The output filenames for both the map and netCDF files are constructed by appending specific suffixes. This is fine unless `mapfile` or `ncfile` provided as inputs already have those suffixes, causing potential overwriting or name collision issues.

7. **Possibility of Infinite Loop or Long Computation Time**:
    - Iterating over large combinations in `product(range(nm), range(ng), range(ncr), range(nco2))` on large datasets could lead to significant computation time or exhaustion of resources.

Addressing these issues is crucial for ensuring that the script runs correctly and provides valid outputs."
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,"There are a few issues with the code that could impact the quality/viability of its outputs:

1. **Overwriting Filenames**: The script does not appear to check if `outfile` already exists before creating `RescaledFile(outfile, ...)`. This could result in unintentionally overwriting existing files.

2. **Hardcoded Dimensions (`nirr`)**: The variable `nirr` is hardcoded as 3. If the shape of `aggmap` changes, this could lead to indexing errors or unexpected results.

3. **Time Range Calculation**: There could be an issue with how the time ranges `tref` and `tin` are calculated using regular expressions. If the units string format changes, the extraction of numbers could fail or yield incorrect results.

4. **resizing `area`**: The script resizes the `area` array using `resize(area, (nlons, nlats)).T`, which could result in incorrect area calculations if `nlons` and `nlats` dimensions do not match the expected input dimensions.

5. **Checking ncattrs() Existence**: In the portion where units and long name are obtained for the variable, it checks for their existence using `var.ncattrs()`. However, the code does not handle the case where they might be absent appropriately (the script assumes that these attributes exist).

6. **Silent Failures in Logical Indexing**: The script performs logical indexing (e.g., `logical_and(tref >= time[0], tref <= time[-1])`) and implicitly assumes these operations are valid for all parts of the script. If the arrays are not aligned properly, this could lead to silent errors and misalignment without producing obvious exceptions or errors.

7. **Common Time and Aggregations**: After obtaining common times (`time = intersect1d(tref, tin)`) and common aggregations (`aggs = intersect1d(aref, ain)`), the script does not check if these arrays are empty. An empty common time or aggregation would lead to errors later in the script without proper handling.

8. **Potential Misalignment of `datetime`**: Calculating `y1, y2` using regular expressions might not always yield the correct timeframe if the naming convention of `irfile` changes. If the parsed years (`y1`, `y2`) are incorrect, the script would handle an incorrect time range.

9. **Missing zz Variable**: Calculation and directly using `varr[:, :, :, 1]` without checking if `rffile` actually had a successful read might introduce `nan` if `rffile` is missing or contains different length `ftime`.

Overall, the presence of multiple potential points of failure suggests that there are indeed significant issues that would impact the quality and viability of the script's outputs.","There are several problems with the provided Python script that could impact the quality and viability of its outputs:

1. **Appending PATHs to `sys.path`:** Appending the system paths to `sys.path` can lead to name conflicts if any of the directories in the system PATH contain modules that have the same name as standard Python modules or any other imported modules. This can cause issues in module resolution and lead to unexpected behavior.

2. **Silent failures in finding scenarios and crops:** The script will exit silently without performing any cleanup or providing more context if it cannot find the specified scenarios or crops in the aggregated and reference files. This might leave the user uncertain about what went wrong and could result in partial or no output files being written.

3. **Hard-coded assumptions about filenames:** The script makes assumptions about the structure of the filenames. If the filenames of the input files do not conform to the expected pattern, the script will not function correctly (e.g., the variables `scen` and `crop` might be wrongly assigned or exceptions might occur).

4. **Potential problem with time array adjustments:** The time arrays `tref` and `tin` are manipulated based on the assumption that the units contain a recognizable year format. If this assumption fails, the time arrays may not be correctly calculated, leading to time mismatches.

5. **Handling of NaN values:** NaNs in the data are masked using `numpy.ma.masked_where`, but when they are replaced by zeros in the variable `varr`, masked values are not preserved, which might lead to incorrect calculations later on.

6. **Incomplete check for rainfed file:** The script checks `isfile(rffile)`, but it does not consider if this file might be unreadable or otherwise problematic. If the file does exist but is corrupted or misspecified, this could lead to unidentified errors later in the script.

7. **Possible integer overflow in area calculations:** The area is calculated as `100 * (111.2 / 2) ** 2 * cos(pi * lat / 180)`, which might be quite large and could potentially lead to overflow or precision issues.

8. **Filename extraction for range might fail:** The code extracts `y1` and `y2` using regular expressions from the filenames, assuming certain positions. If filenames do not follow expected conventions, this can fail and lead to range issues.

9. **RescaledFile expectation:** The code heavily relies on the `RescaledFile` implementation details (from `filespecs`), which might not have been provided. Any misalignment in the expected interface or functionality might lead to file writing problems or errors.

In conclusion, the script contains several issues that could impact its reliability and robustness, potentially leading to incorrect outputs or script failures under various scenarios.","There are several issues in the script that could impact the quality or viability of its outputs:

1. **Adding Paths to `sys.path`**:
    - Modifying `sys.path` by appending paths from the environment variable `PATH` is unconventional and likely unnecessary. This might unintentionally cause conflicts with module imports.

2. **Scenario Identification**:
    - The script slices the `fsplit` list based on an assumption about its length. If the input file name structure changes, `scen` and `crop` might be incorrectly identified.

3. **Coordinate overwriting problem**:
    - Variable `aggmap.shape` returns the dimensions `(nlats, nlons, nirr)`, but `areair` and `arearf` are resized to `(nt, nlats, nlons)` and used in subsequent calculations assuming they correspond to specific times, which might cause misinterpretation or mismatch.

4. **Handling of Conditional Variables (Rainfed)**
    - The variable `varr` is always set to zero when there’s no `rffile`. This potentially makes the results invalid if the rainfed file isn't always present.

5. **Hardcoded String Indices**:
    - Hardcoding the indexing of file name components (`fsplit[3]` and friends) can lead to future errors if the file naming convention changes.

6. **Aggregation and Reference Time Calculation**:
    - The relative time calculations for `tref` and `tin` assume consistent formatting of time units. Differences in formatting might cause misalignment.

7. **Masked Arrays Initialized with Zeros**:
    - The `varr` variable is initialized with zeros and masked values, which could potentially lead to incorrect data interpretations where zero might be valid data.

8. **Lack of Exception Handling**:
    - The script assumes that files exist and variables within the files are present without any error handling, potentially leading to runtime errors if the files are missing or malformed.

9. **Appending Variables to `RescaledFile`**:
    - The script makes an assumption about the order and presence of attributes `units` and `long_name` which may not always be present in `irfile`.

10. **Redundant Work**:
    - The innermost loop reads and writes the same `yield_ref` and `yield_in` values multiple times if there are multiple `latidx` and `lonidx` for a given aggregation, which leads to redundant computation. 

These issues can lead to incorrect paths, indexing errors, potential division by zero, and misinterpretation of data, thus affecting the overall output quality and viability. There are no problems."
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,"There are several issues with the script that would impact the quality and viability of its outputs:

1. **Python 2 Print Statements**: The script uses Python 2 style print statements without parentheses. If run with Python 3, this will cause a syntax error. The script appears to use Python 2 but doesn't specify this explicitly.

2. **File Removal at the End**: The script removes all input files at the end with `os.remove(f)`. This might not be the intended behavior as it permanently deletes the input files, which could be needed later and might cause data loss.

3. **Overwriting Temporary File**: The line `os.system(""ncks -h -O -x -v %s %s %s.tmp"" % ("","".join(variables_to_delete), output, output))` could potentially create an inconsistent state if there is any issue while copying or moving the temporary file.

4. **Data Duplication**: If there are variables with the same name as some dimension names, they will be excluded from the new variable list because of `variables_to_delete`. This might unintentionally exclude necessary variables, leading to incomplete data aggregation.

5. **Potential Index Errors**: The script assumes that specific indices in the `splits` list will correspond to `carbon_levels`, `temp_levels`, `water_levels`, and `nitrogen_levels`. This makes the script brittle; if the filenames do not follow the exact expected format, it could raise `IndexError`.

6. **Script Assumption on Filename Format**: The script assumes that filenames contain specific parts that match exactly with the crop, model, and adaptation level. If the filenames do not conform exactly to this pattern, files will either be ignored or incorrectly processed.

7. **Hard-coded Filename Extensions**: The script assumes a `.nc4` file extension. If the files have a different extension, they will not be processed.

There are no indications that the script handles potential inconsistencies in the dataset structures, like varying number of dimensions across input files, but these are not explicit errors depending on the consistency of the input datasets.

In summary, there are several problems with the script that could lead to errors in execution and unintended consequences such as irreversible file deletions and erroneous data processing.

","There are several issues with the provided script that could impact the quality and viability of its outputs:

1. **Print Statements (Python 2 Syntax)**: The `print` statements are using Python 2 syntax (without parentheses), which will cause an error if the script is run with Python 3.

2. **Native NC4 Handling Inconsistency**: The use of `os.system(""ncks -h -O -x -v %s %s %s.tmp"" % ("","".join(variables_to_delete), output, output))` for manipulating netCDF files assumes that the `ncks` tool is installed and available in the system's PATH. This adds a dependency external to the Python script which may not always be available.

3. **Removing Necessary Files**: The last loop removes all files listed in `files`, which means the original input files will be deleted after processing. This could be undesirable if these files need to be preserved for any reason.

4. **Assumption of File Format**: The script makes a lot of assumptions about the naming structure of the files and relies on specific delimiters (underscores) and order of segments (e.g., `carbon_levels`, `temp_levels`, `water_levels`, `nitrogen_levels`). Any deviation or error in the file naming conventions will cause files to be skipped or ignored without a strong error handling mechanism.

5. **Static Dimension Sizes**: Dimensions of variables in the output file (`dim_names[0]`, `dim_names[1]`) are assumed to be consistent across all input files, but the script does not verify this consistency explicitly, which might lead to mismatches or corrupt data if they differ.

6. **Variable Inclusion/Exclusion Mismanagement**: There's logic here to exclude some variables (`global`, `gadm0`, `time`), but the interaction between the `variables`, `new_variables`, and `variables_to_delete` lists is complex and not entirely clear. This might lead to unintended variables being excluded or included.

7. **Potential Data Overwrites**: The script copies a sample file to the output path early on (`shutil.copyfile(files[0], output)`), then removes variables and regenerates dimensions later, which might lead to issues if interruptions occur in between or if there's inconsistency in the sample file used.

8. **Hard-coded Fill Value**: The script uses a hard-coded fill value (`1.e+20`), which may not be suitable for all datasets.

Given these observations, there are significant issues that could affect the quality/viability of the script's output.","There are several issues with the code that would impact the quality and viability of its outputs:

1. **Python 2 Print Statements**: The code uses Python 2 style print statements without parentheses (e.g., `print ""No files to process""`) which would cause syntax errors if run with Python 3 since the script uses Python 3 imports and methods (e.g., `argparse`, `from netCDF4 import Dataset` which are compatible with Python 3).

2. **Overwriting Output File**: The code copies a sample file to the output file and immediately modifies it (e.g., `shutil.copyfile(files[0], output)`). This might cause data corruption if not carefully handled, and if the script is run multiple times without ensuring unique output filenames, it might create unintended overwrites.

3. **Inadequate Handling of File Deletion**: The part of the code `os.system(""ncks -h -O -x -v %s %s %s.tmp"" % ("","".join(variables_to_delete), output, output))` followed by `shutil.move(""%s.tmp"" % output, output)` works but it depends on the external command `ncks`. There is an inconsistency in removing the intermediate file only after complete execution; if an error occurs, the intermediate file will not be removed, potentially leading to file clutter.

4. **Clearing Original Data**: At the end of the script, it removes the original intermediate files (e.g., `os.remove(f)`). This behavior could be problematic because if the process needs to be rerun or if there is an error in the processing stage, the original data will be lost.

5. **Assumption on Filenames**: The script assumes a certain naming structure of the input files to extract indices for carbon, temperature, water, and nitrogen levels, as seen in `splits = bname.split('_')`. If the filenames do not conform exactly to this assumed structure, the script may silently ignore valid files.

6. **KeyError Handling**: Only a warning is given if a variable is missing in a file. This might lead to incomplete data in the output without breaking the operation or ensuring the dataset's integrity.

7. **Lack of Error Handling**: The code generally lacks comprehensive error handling which can cause it to fail silently or crash, making debugging difficult.

These issues can disrupt the script's execution and the integrity of its output data."
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,"There are several issues with the code that would impact the quality and viability of its outputs:

1. **Python 2 Print Statements**: The script uses Python 2 style print statements (for example, `print ""No files found""`). If this script is run with Python 3 (which is more current), it will cause a syntax error. The correct way to print in Python 3 is `print(""No files found"")`.

2. **Logical Error in Index Calculation**: The line in the `shiftdata` function `latidx, lonidx = where(logical_and(pl_date >= harv_date, harv_date >= year))` seems suspicious. The `year` variable is an integer and comparing it with `harv_date` may or may not be intended behavior. 

3. **Mismatch in Indentation for Averaging Units**: In the final loop for variable processing, `if not i:` should use the current index `j` instead (`if not j:`) to correctly update units for each variable.

4. **Potential Index Out of Range**: The script refers to `outfiles[iidx]` indiscriminately based on `nirrs`, assuming that the length of `outfiles` matches `nirrs`. If they don't match, it will raise an `IndexError`.

5. **Printing Statements Inside Loops**: The script uses multiple `print` statements (for example, `print ymsk.sum(), ymsk.shape, adata.shape, weights[iidx].shape`) which will clutter the output if processing large data.

6. **Possible File Output Overwrite**: In the final block where output files are written, the variables `fout` are created in loops, and they could potentially overwrite files if the `outfiles` list contains duplicate entries.

7. **Use of Mask Array Without Proper Handling**: The script uses masked arrays for aggregating data, but doesn't handle cases explicitly where masks might affect the logical operations. This might cause unintended behavior when working with masks.

8. **Implicit Assumptions Without Error Handling**: There is no error handling for the existence and validity of required files (e.g., `lufile`, `gsfile`). If these are missing or corrupted, the script will fail.

There are no problems explicitly related to reading and processing the netCDF4 files (assuming inputs are correctly formatted), and no duplicate effort seems to be happening in loops after correcting the index issues. Also, the initial setup takes care of missing variables by skipping, which prevents crashes. 

However, the above issues need to be addressed to ensure the quality and reliability of the script's outputs.","1. **Argument Parsing Issue:** The `help` flag for `--outfiles` indicates it should be a list of output files, however, the script assumes each output file name is separated by a comma. This inconsistency could lead to a misinterpretation of the flag's usage and potential issues with output file handling.

2. **Writing Output Files Issue:** If the number of specified `outfiles` does not match the expected number of irrigation levels (`nirrs`), the script will likely fail when accessing an index of `outfiles` that does not exist.

3. **Mask Issue in `shiftdata` Function:** In the `shiftdata` function, `shiftd[0].mask = True` should be `shiftd[0].mask = False` instead, otherwise, this could lead to issues where data that should not be masked is incorrectly masked.

4. **Error Handling in File Reading:** Scatter error handling exists for when files (`plantingfile`, `harvestfile`, `yieldfile`) are not found, but not for when `varfile` is not found. Attempting to read a `varfile` that does not exist will lead to a crash. 

5. **Variable Assignment Inside Loop:** The assignment `var = var[:]` should be corrected to `var = var[:].filled(0)` to handle any NaNs properly as done in a later line.

6. **Logical Error in Area Assignment:** In the section where areas are combined, assigning combined values directly through masked arrays with different conditions might lead to logical errors or unintended values.

7. **Warning Filtered Globally:** Global usage of `filterwarnings('ignore')` without specificity can suppress important warnings unintentionally.","There are several problems in the given code that could impact its quality and viability:

1. **String Formatting Syntax in Print Statements**:
    - In Python 3, `print` is a function, and the given syntax `print ""No files found""` should be `print(""No files found"")`. This will cause a `SyntaxError` because the code is using Python 3's `print` function incorrectly.

2. **Overwrite Risk with Output Files**:
    - The `outfiles` argument is expected to be a list of output filenames. However, there is no check to ensure these filenames are unique or that they don't already exist, which could lead to overwriting files unintentionally.

3. **Loop Over Files Without Indices**:
    - The `files` variable is a list of file names, and a loop iterates over them to extract variables. However, it accesses `file_split[3]` directly, which may not be a reliable method if the file naming convention is slightly different or not consistent. This could lead to `IndexError` if the split list is shorter than expected.

4. **Potential Unhandled Cases in `findfile` Function**:
    - The `findfile` function searches for a file containing the given variable. If no matching files are found, the function returns `None`. This return value isn't always checked before being used, which could lead to `TypeError` when attempting operations on `None`.

5. **Incorrect Indexing**:
    - There are indexing mistakes, such as using a variable `i` instead of `j` within the nested loops: `if not i:` should be `if not j:`. This can lead to incorrect assumptions and logic errors in the code.

6. **Areas Calculation in Aggregation Loop**:
    - The calculation and assignment to `areas[:, :, iidx]` is not properly protected against cases where `yvar` might be masked entirely, as it doesn't validate if `yvar` has valid (unmasked) data before using it for aggregation.

7. **Incorrect Mask Handling**:
    - There are potential mask handling issues, where the masks for different variables (e.g., `area1`, `area2`) are mixed without clear checks on their consistency. This could lead to incorrect aggregate area calculations.

8. **Redundant Handling of NaNs**:
    - Changing NaNs to zeros without properly assessing the impact on data quality could lead to potentially misleading averages in the aggregated results, as NaNs might imply missing or invalid data rather than zeros.

9. **Misleading Log Messages**:
    - The use of `print` statements for debugging or logging might be misleading and aren't contextually formatted, which could hinder understanding the actual flow and state of the computation.

Given these issues, the script would produce incorrect or misleading outputs due to a combination of incorrect indexing, unhandled null cases, potential overwriting of files, and improper mask management."
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,"There are problems:

1. **Appending PATH to sys.path**: The script incorrectly appends each path from `os.environ['PATH']` to `sys.path`. This is unusual and might lead to conflicts or security issues by potentially importing unintended modules.

2. **Dimension Misalignment**: In `createAggFile`, the time dimension and the scenario dimension are set conditionally, but there's an overlap in how `scen` is handled, potentially leading to errors if `scen` is not present.

3. **Incorrect Dimension Length for `irr`**: The 'irr' dimension in `createAggFile` adds 1 to `len(irr)`, potentially leading to an off-by-one error in slicing and data manipulation.

4. **Potentially Overwriting Files**: The file `outputf` is opened in write mode (`'w'`) in `createAggFile` without checking if the file already exists, potentially overwriting important data.

5. **Incorrect Handling of Slices**: 
    - In the loops where `slicer` is manipulated and `dims[i]` and `dimslist` are used, dimensions are being removed or changed, but then used again.
    - The `timeidx`, `latidx`, and `lonidx` are used based on `dimslist` after dimensions have been removed, which might not align.

6. **Hard-Coded Thresholds**: The yield thresholds are defined directly in the script (`yieldthr1, yieldthr2 = 0.1, 33`). This inflexibility can lead to issues if different thresholds are required.

7. **No Error Handling for Missing Variables**: The script assumes all necessary variables (`lat`, `lon`, `time`, `irr`, the ones specified in `--input`, etc.) exist in the input files. If any are missing, it will result in an error.

8. **Potential Unclosed Files**: The file `f` opened in `f = nc(outputf, 'a')` near the end of the script is not guaranteed to be closed using a `with` statement. This can be problematic if the script is interrupted before the `f.close()` call.

These issues might cause runtime errors, data corruption, or unintended overwriting of files.",There are no problems.,"1. **Appending system paths from environment PATH:** Appending paths from the `PATH` environment variable to `sys.path` is problematic because it could introduce conflicting binaries or libraries into the module search path. This can cause unintended behavior if different programs have modules with the same names.

2. **Parsing the input file**: The input format for the input file (`inputf`) is expected to be `file:var1,var2,...,varM`, but if `inputf` is not provided correctly or is empty, it will cause an error.

3. **Overwriting existing files**: When calling `createAggFile`, if `outputf` corresponds to an existing file, it will be silently overwritten.

4. **Variable assignments with masked arrays**: In the `getyieldmask` function, `masked_where(yieldvar < yieldthr1, yieldvar)` can result in a masked array due to the double masking condition. This might not be intended.

5. **Index handling in scenario and irrigation processing**: There is potential ambiguity and risk of error with scenarios and irrigation indices from the input variables, particularly with nested loops and index manipulations.

6. **Handling dimensions in NetCDF variables**: The code assumes certain dimensions and structures for the NetCDF input variables (like 'time', 'lat', 'lon', and 'irr'). If the input data does not match these expectations, there will be runtime errors.

7. **Closing the file handle explicitly**: `f.close()` is correctly called at the end of the script, but if an exception occurs before this point, the file may not be closed properly, leading to potential data corruption or resource leaks.

8. **Handling of yield variable**: If `yieldvar` is provided but the variable is not found or has shape mismatches, the code will fail. In particular, index and slicing handling need to be robust.

In summary, the script has several potential pitfalls related to environmental dependencies, file handling, and specific assumptions about the data structure. Careful validation and error handling would be required to make this script robust."
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,There are no problems.,"The script has several issues that could impact the quality/viability of its outputs:
1. The script does not check if the `rcp26file` or `rcp85file` paths are empty strings or `None` before attempting to open them. This could lead to a `FileNotFoundError`.
2. If `variable` is an empty string, the code assumes the variable exists in the netCDF files and constructs variable names using the empty string, which could lead to a `KeyError`.
3. The `outfile` could be an empty string, leading to potential issues when trying to create the output netCDF file.
4. The variable `nd` is computed as `nt / 10`, which results in a float, but it is later used as a range in `f.createDimension('decade', nd)`, which expects an integer. This would raise a `TypeError`.
5. The `tidx1` and `tidx2` are calculated based on the assumption that the value `1980` exists in the `time` variable, which may not be the case and would result in an `IndexError` from `where`.

These issues need to be addressed to ensure the script runs correctly and produces valid outputs.","There are definite problems in the provided script that would impact the quality and viability of its outputs:

1. **Division Result in Float**: `nd = nt / 10` will result in a float in Python 3.x. Since `nd` is later used as a dimension in reshaping arrays, this should be an integer. This can cause an error or unexpected behavior.

2. **Mismatch Between Array Shapes**: The calculation `resize(fpu26[:, tidx1 : tidx2].mean(axis = 1), (nd, nf)).T` might not produce the intended results if `nd` is not an integer or if there's a mismatch in the shapes of arrays being subtracted.

3. **Ambiguity in Time Range Calculation**: `tidx1, tidx2 = where(time == 1980)[0][0], where(time == 2009)[0][0] + 1` assumes that `time` contains the exact values 1980 and 2009. If these values do not exist in the `time` array, indexing will fail.

4. **Fixed Dimension Sizes Without Guarding for Variability**: The script assumes fixed sizes for certain dimensions (in the way it slices and reshapes arrays). If the input data does not match these assumed dimensions, this will cause errors.

5. **Default Values for Command Line Arguments**: The script sets default values for the input files to empty strings. If these arguments are not provided, the script would attempt to open files with empty names, causing it to fail.

6. **Implicit Overwriting of Output File**: The script will overwrite the output file if it exists. This might not always be a problem, but it can lead to data loss if not intended.

These issues collectively would impact the proper execution and the correctness of the script's outputs."
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,"There are a few problems with the script that could impact the quality or viability of its outputs:

1. **Appending PATH to sys.path**:
   Appending each component of the system's PATH environment variable to `sys.path` is unnecessary for module imports and could potentially cause unintended import behavior by introducing non-Python paths.

2. **Incorrect handling of masked array**:
   The default `masked_array` function creates a mask filled with zeros (which means all elements are valid data), not ones. Therefore, `mask = ones(sh)` would result in no values being masked, which might not be the intended behavior.

3. **Data type mismatch for dimensions**:
   The dimensions 'dt' and 'mp' are created as `i4` (integer 32-bit) variables, and their values are set with `range(1, ndt + 1)` and `range(1, nmp + 1)` respectively. This might suffice if the values are small integers, but without knowing the exact range of `ndt` and `nmp`, it could potentially cause compatibility issues.

There are no issues with file overwriting, unintentional duplication of work, or inclusion of unintended entries. 

Overall, these issues need to be addressed to ensure the quality of the output:

- Safeguard imports by not adding system paths unnecessarily.
- Correctly handle the creation and masking of masked arrays.
- Ensure that data types match expected ranges and constraints for NetCDF variables.

Given these considerations, there are problems that need to be addressed in the script.",There are no problems,"There is one main problem in the code that will impact the quality and viability of the outputs:

1. **Appending system paths to `sys.path`:**
   The script adds each path in the system `$PATH` environment variable to the `sys.path` list. This is not only unnecessary but also potentially dangerous, as it can lead to unintentional overriding of standard library modules or third-party modules with executables in the system path, introducing unexpected behavior or security vulnerabilities.

2. **Strong assumption about input variables:**
   The code assumes that certain variables (like `'time'`, the crop yields, and the aggregation level) are present and correctly named in the input NetCDF file (`inputfile`). If these variables are missing or misnamed, the script will raise an error.

With these identified, ensure these points are corrected to avoid potential bugs and security risks."
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,"The script has several issues that could impact the quality or viability of its outputs:

1. **Appending Paths to `sys.path`**: The script appends each path in the system's `PATH` environment variable to `sys.path`. This is generally unnecessary and could lead to unintended consequences, such as importing incorrect or unsafe modules.

2. **Hardcoded Indices and Variables**: There are multiple instances of hardcoded indices and variables (e.g., `['none']`, `['true']`, `['mean-scale']`, `['default']`, etc.). These should be dynamically derived or verified to ensure compatibility with input files.

3. **File Overwriting**: The output filename (`fn`) is created by concatenating `outdir` and the input filename. If the input filename is not unique within `outdir`, this could lead to overwriting existing files. The script does not check or warn about this possibility.

4. **Error Handling**: The script prints an error message and exits when a crop is unavailable, but it does not provide detailed error messages for other potential issues, such as missing variables or dimensions in the NetCDF files.

5. **Uncritical Usage of Indexing on Arrays**: The script obtains indices using `index` on lists of names (e.g., `aggs`, `aref`). If an expected name is not found, an exception will be raised, terminating the script abruptly.

6. **Assumption of Non-Empty Intersection**: The script assumes that there will be non-zero common aggregate indices (`naggs`). If there are no common indices, the script raises an exception without handling this scenario gracefully or providing context-specific information.

7. **Potential for Overwriting Variables**: Variables such as `yield_detr` and `yield_retr` are initialized with zeros and then overwritten. If the `BiasCorrecter` doesn't yield valid outputs, the results may be misleading.

8. **Lack of Logging**: There is minimal logging throughout the script, making it difficult to debug issues based on script output.

Overall, while there is potential for improvement, the identified issues directly affect the reliability and robustness of the script's outputs.","There are several issues with the provided Python script that could impact the quality and viability of its outputs:

1. **Appending to `sys.path`**: The code appends all paths from the `PATH` environment variable to `sys.path`. This could potentially introduce module conflicts or unintended imports.

2. **Filename extraction for crop**: The code assumes the crop name is always the fourth last element when splitting the filename by underscores. If the input filename format changes, this could extract an incorrect crop name.

3. **Incorrect format string**: The `print` statement in Python 3 should use parentheses (or the script should specify using Python 2 explicitly).

4. **Index Retrieval**: The code retrieves indices for `'none'` and `'true'` from long names in `reffile` without checking if those values exist. It assumes that those values will always be present, which may not necessarily be the case.

5. **Reference time and input time adjustment**: Adding integers extracted from `tref_units` and `tin_units` directly to `tref` and `tin` could lead to incorrect time values due to the assumption that the units contain only one integer value representing the starting point.

6. **Find common aggregates**: The code raises an exception if no common aggregates are found, but this case is not handled gracefully, and it immediately stops execution.

7. **File Overwriting**: The output file name is generated by concatenating `outdir` with the input file's name. If this file exists in the output directory, it will be overwritten without warning, potentially leading to data loss.

8. **Bias Correction Logic**: The script attempts to perform bias correction assuming that the `BiasCorrecter` class and its `correct` method work properly, but there’s no error handling if they do not.

Given these observations, the statement ""There are no problems"" would be incorrect. The script has several issues that could impact its execution and the quality of its outputs.","There are a few issues with the code that could impact the quality or viability of its outputs:

1. **Environment PATH Misuse**: The script adds each directory in the `PATH` environment variable to the `sys.path`. This is generally not a good practice because those directories are meant for executable binaries, not Python modules. It could potentially lead to unpredictable behavior if a directory that is not meant for Python contains a Python module of the same name as something else you're trying to import.
   
2. **Missing `os.makedirs` for Output Directory**: There is no check or creation of the `outdir` directory. If the output directory specified does not exist, the script will fail when trying to write the output file.

3. **Potential Filename Overwriting**: The script constructs the output filename by appending the input filename to the output directory. If `outdir` and `infile` combinations are not well managed, it could result in unintentional overwriting of output files.

4. **Print Function Syntax Error**: The script uses a print statement compatible with Python 2 (`print 'Crop %s unavailable in reference file %s. Exiting . . .' % (crop, reffile)`). As the other parts of the script suggest Python 3 compatibility, this would raise a syntax error. It should be updated to a print function to align with Python 3 (`print('Crop {} unavailable in reference file {}. Exiting . . .'.format(crop, reffile))`).

5. **List Index Lookup**: The code assumes that the `fin.variables['irr'].long_name.split(', ')` and `fref.variables['dt'].long_name.split(', ')` will have an index that matches `'sum'` and `'none'`. If these substrings are not in the `long_name`, it will raise a `ValueError`.

6. **Static Values in List Index Lookup**: Indexed values (like `dtidx`, `mpidx`, `sum_idx`) are statically managed which means if the variables in the netCDF files change or do not contain these strings, the script will fail.

Due to these issues, caution is necessary when using this script in production or for critical analysis, and adjustments should be made to address these problems.

Thus, ""there are problems""."
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,"The script has a few notable issues:

1. **Modifying `sys.path` Unnecessarily**: The script appends all directories in the system's `$PATH` environment variable to Python's `sys.path`. This can lead to unintended behavior or module conflicts since it unnecessarily modifies the module search path.

2. **Hard-Coded File Pattern**: The `fnmatch` pattern `*_%s_*_%s_*` used for matching files is hard-coded with the assumption of a specific format. While this is not inherently problematic, any deviation in file naming conventions could lead to misidentification or exclusion of files that should be included.

3. **Overwriting Output Files**: If the `outfile` path generated (`%s_%s_hist_%s_annual_%d_%d.ensemble.nc4`) already exists, it will be overwritten without warning. This can lead to loss of previous data if the script is run multiple times with the same parameters.

4. **Silent Exit When No Files Matched**: If no files match the criteria in `indir`, the script prints a message and exits. Depending on the context, this may be acceptable, but it may be better to log this event or handle it differently depending on user expectations.

There are no problems in terms of duplicated work or issues with including entries that should be excluded, given the specified exclusion of `'epic-test'` files in the `bcfiles` list comprehension. If there are no files matching the specified pattern, the script will exit gracefully after printing a message, which is appropriate behavior.

Therefore, these concerns primarily affect the functionality and reliability of the script, but there are no further problems detected.","There are a few issues with the code that could impact its quality or viability:

1. **Dynamic Modification of `sys.path`**:
   - The script modifies `sys.path` by appending all directories in the system `PATH`, which is unconventional and risky because it can lead to importing the wrong modules if there are name clashes.

2. **Potential for Overwriting Output Files**:
   - The script creates an output file named using variables including metric, weather, crop, tmin, and tmax. If the script is run multiple times with the same parameters, it will overwrite the existing file without warning.

3. **Vague Error Handling for Missing Files**:
   - If no appropriate files are found in the input directory (`bcfiles` is empty), the script simply prints a message and exits. More explicit error handling and user feedback would improve clarity.

4. **Hard-Coded Strings in `fnmatch`**:
   - The code uses a specific pattern to match files. If the pattern changes or is not followed correctly in the input files, valid files might be missed.

There are no indications in the script of unintentional duplication of work, unnecessary inclusions, or similar issues.

Therefore, the identified issues are limited to the dynamic modification of `sys.path`, potential overwriting of output files, vague error handling for missing files, and rigidity in file name pattern matching.","There are a few issues with the provided code that could impact the quality and viability of its outputs:

1. **Appending to `sys.path`**: The script appends each path in the `PATH` environment variable to the `sys.path`. This can cause unexpected behavior if any of these paths contain modules or scripts with the same names as those being imported or used within the script.

2. **String formatting**: The use of string formatting without safeguarding against potential special characters in variables like `weather`, `crop`, or other inputs might lead to unexpected file names or formats that can break the file operations or subsequent processing steps.

3. **File concatenation for `mmfiles`**: When constructing `mmfiles`, the script uses 
   ```
   mmfiles = [basename(f).replace('biascorr', 'multimetrics') for f in bcfiles]
   mmfiles = [metricsdir + sep + f for f in mmfiles]
   ```
   If `metricsdir` is an empty string (which is the default), then `sep` will appear at the beginning of each file path in `mmfiles`, which might cause issues with file path resolution.

4. **Directory checks**: The script does not check whether `indir` or `metricsdir` directories actually exist before attempting to list files or access them, which could lead to errors if these directories are missing or incorrectly specified.

5. **Output file name collision**: The script constructs the output file name as:
   ```
   outfile = outdir + sep + '%s_%s_hist_%s_annual_%d_%d.ensemble.nc4' % (metric, weather, crop, tmin, tmax)
   ```
   If `outdir` is not provided or is empty, this could lead to the file being created in an unintended directory or overwriting an existing file with the same name. Additionally, no check is performed to see if a file with the same name already exists, which could result in overwriting existing files.

6. **Error handling**: There is no error handling for the file operations or for the methods of the `Ensembler` class, which might cause the script to terminate abruptly without clear error messages if any step fails.

In summary, the script could encounter several issues related to path manipulations, potential file name collisions, the handling of directories, and lack of error handling."
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,"There are a few issues with the provided script that can impact the quality or viability of its outputs:

1. **Environment PATH Addition**: The line that adds each component of `os.environ['PATH']` to `sys.path` modifies the module search path. This could lead to unintended consequences, such as importing the wrong version of a module if a different one is found in the specified paths.

2. **Variable `dtvar` and `mpvar` Data Type**: The script uses `'i4'` as the data type for `dtvar` and `mpvar` dimensions. These variables appear to represent categories and might be better represented as a simpler integer without explicit typecasting to `'i4'`.

3. **Latitude and Longitude Variable Definitions**: In the creation of latitude and longitude dimensions, the script uses the variable name as the single string to specify the dimension. To be precise, it should use a tuple containing the variable name (i.e., `(latname,)` and `(lonname,)`) to avoid potential future deprecations or confusions.

4. **Error Message Typo**: The error message in the exception raised when latitude and longitude variables are not found contains a typo: `'Uncannot find latitude, longitude variables'`. This should be corrected to `'Cannot find latitude, longitude variables'`.

5. **Default Mask Value in `vardt`**: The script creates `vardt` with a mask of ones across the whole array. If `varmat` is not masked and contains valid data, this may result in unwanted masked entries, potentially excluding valid data points unintentionally.

6. **Compression Level**: The compression level is set to the maximum (`9`) for netCDF variable creation. While this isn't an error, it may significantly slow down the I/O operations. Lower compression levels might strike a better balance between file size and performance.

7. **Potential Overwrite of Existing Output File**: The script does not check for the existence of the output file before creating/overwriting it. This could lead to unintended data overwriting if the script is run multiple times.

There are no problems.","There are several issues with the given script that could impact the quality and viability of its outputs:

1. **Overwriting Output File**: The script directly writes the output to a file specified by the option `-o` or the default `""out.nc4""`. If this file already exists, it will be overwritten without warning.

2. **Environment Dependencies**: Adding paths from the environment variable `PATH` to `sys.path` is unconventional and risky. It can lead to unintended consequences, such as importing the wrong versions of modules if there are similarly named modules in different directories.

3. **Inconsistency in `lat` and `lon` Variable Names**: The script assumes that the latitude and longitude variables will only be named `lat`, `lon`, `latitude`, or `longitude`. If the input file uses different names for these variables, the script will fail with a non-descriptive exception message.

4. **Inappropriate Exception Message**: The exception message `'Uncannot find latitude, longitude variables'` contains a typo and is not very informative. If the variables are not found, it could be more descriptive about what variables it expected to find.

5. **Potential Typo in Exception Handling**: The error message `raise Exception('Uncannot find latitude, longitude variables')` contains a typo (`'Uncannot'` should be `'Cannot'`).

6. **Improper Handling of Time Units**: The script assumes that the `units` attribute always exists for the `time` variable. If it does not, the script assigns an empty string, which might not be appropriate in all contexts.

7. **Unit and Long Name Extraction**: Similar issues exist when extracting `units` and `long_name` attributes for the variables being processed. If these attributes do not exist, they are assigned empty strings, which may not be correct.

8. **Masked Arrays Check**: The check for masked arrays only verifies the first element of the mask using `not varmat.mask[0]`. This can lead to incorrect handling if other elements are masked but the first is not.

9. **Hardcoded Fill Value**: The script uses a hardcoded fill value of `1e20` when creating new variables. This might not be suitable for all datasets.

10. **Noisy Compression Settings**: The script applies compression settings (`zlib = True`, `shuffle = False`, `complevel = 9`) repeatedly with each call to `createVariable`. While not incorrect, this repetition could make the code more cumbersome to maintain.

Considering these points, there are indeed several issues that could impact the viability of the script’s output.","The script has several issues that would impact the quality and viability of its outputs:

1. **Appending PATH to sys.path**:
   - The script appends every directory in the PATH environment variable to `sys.path`. This could lead to unintended behavior if any of these directories contain modules that shadow the standard or installed modules.

2. **Inconsistent Naming Conventions**:
   - The script checks for both ""lat""/""lon"" and ""latitude""/""longitude"" variables in the input file but only creates variables named ""lat"" and ""lon"" in the output file. If the input file uses ""latitude"" and ""longitude"" and any later code refers to them, it could cause confusion or errors.

3. **Inconsistent Use of Quotes**:
   - The script uses both single and double quotes inconsistently. While this is not a major problem, it is a good practice to be consistent.

4. **Potential Overwrite of Output File**:
   - The script will overwrite the output file if it already exists without any warning.

5. **Possibility of Missing Units Attribute Handling**:
   - For several variable attributes (e.g., units, long_name), the script assigns an empty string if the attribute is missing. This could be acceptable, but better handling may involve logging a warning or providing default values. This isn't strictly an error, but it could affect the usability of the output.

6. **Closing Files**:
   - The `fi.close()` and `fo.close()` are called at the end. If an exception occurs anywhere before these calls, the files may not be closed properly. While not a script-breaking issue, it's recommended to use `with` statements to ensure proper closure of files.

7. **Hardcoded Defaults**:
   - The script uses hardcoded default values for `inputfile`, `variables`, and `outputfile`. If these defaults are not appropriate, it could lead to unintended behavior or errors.

8. **Handling of Masked Arrays**:
   - If `varmat` is a masked array and `varmat.mask[0]` is `False`, the script skips the detrending process for that (lat, lon) coordinate but does not explicitly handle cases where only parts of the `varmat` might be masked.

9. **Datatype Mismatch for Time Variable**:
   - In the netCDF creation code, `time` is defined as having a 'i4' (integer) type. If `fi.variables['time'][:]` actually contains floating-point numbers (common for time variables), this could result in data loss or errors.

Taking these points into account, there are several issues that need to be addressed to ensure the script functions correctly and produces reliable outputs."
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,"There are a couple of issues with the code that would impact the quality/viability of its outputs:

1. **File Not Closed Properly in Case of Exception:**
   - If an exception occurs during the parsing of the YAML file, the opened file would not be properly closed, leading to a potential resource leak.

2. **Print Statement Syntax Error:**
   - The print statements lack parentheses, which would cause a syntax error in Python 3.x. The correct syntax should be `print(get_value(params[k], v))` and `print(get_value(params, options.key))`.

Therefore, the identified issues would need to be resolved to avoid negative impacts on the script's outputs.

Note: Additional issues could potentially exist based on the contents of the YAML or other edge cases, but these are the clear issues based on the provided script.","There are several issues with the provided Python script that could impact the quality or viability of its outputs:

1. **Potential Syntax Error in Print Statement**: The print statement should use parentheses, which is necessary for compatibility with Python 3.x.
   
2. **File Opening Without a Context Manager**: The input file is opened without using a context manager (`with` statement). This can lead to resource leaks if an exception occurs before the file is closed.

3. **Loading YAML Without Exception Handling**: There is no exception handling around the YAML loading. If the YAML file is malformed, the script will raise an unhandled exception and crash.

4. **Unchecked Key Splitting**: When splitting `key` using `key.split(':')`, it assumes there will always be exactly one colon. If there are more or fewer colons, this will raise a `ValueError`.

5. **Inconsistent Key Checking and Function Call**: In the `if ':' in key:` block, it uses `key.split(':')` but does not account for potential index errors when accessing the split parts. This could cause issues if the format isn't exactly ""k:v"".

6. **Unnecessary Assignment**: `inputfile = open(inputfile)` reassigns `inputfile`. A new variable should be used to avoid confusion.

Given these issues, it is clear that the script needs several adjustments to ensure it operates reliably and as intended. Therefore, the statement ""there are no problems"" does not apply.","There are several issues with the script that could impact the quality or viability of its outputs:

1. **File Not Being Closed in Exception Handling**: If the `open(inputfile)` statement fails, the file won't be closed because it's only explicitly closed after successful loading. Using a `with` statement for opening and closing the file would be better, but this is not part of the requested improvements.

2. **Print Statement Syntax**: The script uses `print` statements like `print get_value(params[k], v)` and `print get_value(params, options.key)` which is incorrect syntax for Python 3 as it should be `print(` ... `)`. However, if this script was intended for Python 2, then it is fine, but the shebang suggests it might be used in a more flexible environment where Python 3 could be the default.

3. **Handling Missing Arguments**: If the `--key` argument is missing and `key` is `None`, the script will not handle this gracefully because there are no checks for the presence of `key` before use. This can cause an `TypeError` when trying to split a `NoneType`.

4. **Unused Imports**: The script imports `re` and `sys` modules, but neither is used anywhere in the code, which is unnecessary.

5. **Invalid Indexing**: In the `if ':' in key` block, if the key doesn't exist in `params`, trying to access `params[k]` will raise a `KeyError`, which is not caught by the `except KeyError` block meant for `get_value` function.

Except for the potential print statement version mismatch depending on Python version, these would all impact the quality or viability of the output.

Hence, the appropriate conclusion is: there are problems."
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,"The script has several issues that might impact the quality and viability of its outputs:

1. **File Overwriting**: The filenames generated in the `plot` method of `MaskPlotter` class do not ensure unique output filenames for different variables, masks, and plots. This can lead to overwriting of files if multiple runs produce plots with the same name.

2. **Index Error in Plot Labels**: The `plot` method has a logic to populate `regionlabels` that does not ensure that the `str(self.indices[si])` conversion is correct for the type of `self.indices`. This could potentially raise an error or generate incorrect plot labels.

3. **Delimiter Issue in Meta Data**: The meta data files are read using `csv.reader` with a tab delimiter, but later the first element is split using a comma which assumes a certain structure in the content. This discrepancy can cause errors or incorrect meta information being read.

4. **Hard-Coded Newline Character**: When setting `filename1` and `filename2`, newline characters are inadvertently included in the filenames (`\n`). This is a serious issue as it results in invalid file paths and will prevent files from being saved correctly.

5. **Unicode Warning Mode**: The use of `'rU'` mode in the `open` function call is deprecated and could cause a warning or error in future versions of Python.

6. **Inconsistency in Filename Generation**: In the `MaskPlotter.plot` method, there is an inconsistent splitting logic to extract `yr0` and `yr1` from the filename which expects a specific structure. Any deviation in the input filenames' structure will lead to an incorrect assignment of these variables.

7. **Print Statements for Strings**: The print statements in the script are written for Python 2, which requires parentheses in Python 3 and later. This will cause the script to fail in Python 3.

8. **Meta file handling**: The `meta` files are expected to correspond directly to each mask with a one-to-one relationship. If the lengths of `meta` and `masks` differ, or if a `meta` file is not valid, it could lead to incorrect behavior or a crash.

Based on these points, there are several issues with the code that need to be addressed to ensure correct functionality and prevent potential file overwriting, errors or crashes.

","There are a few issues with the provided script that could impact the quality/viability of its outputs:

1. **Hardcoded `.split('_')` Indexes**:
   The script assumes a specific structure for filenames when splitting by underscores:
   ```python
   filesplit = basename(filename).split('_')
   self.model = filesplit[0]
   self.climate = filesplit[1]
   ...
   ```
   If filenames do not follow this exact structure, it could lead to an `IndexError`.

2. **CSV File Reading Mode**:
   The script uses `'rU'` as the mode for opening CSV files, which is deprecated in Python 3:
   ```python
   with open(meta, 'rU') as f:
   ```
   This should be updated to just `'r'`.

3. **Inconsistent Indentation**:
   There is a mixture of tabs and spaces for indentation, which may cause `IndentationError`. For example:
   ```python
       self.yr1 = filesplit[6].split('.')[0]
	self.yr1 = filesplit[6].split('.')[0]
   ```
   This should be consistently formatted using either spaces or tabs.

4. **Use of Non-existent `sep` Parameter in `os.path.join`**:
   The script uses `sep.join([self.outdir, ...])` to construct file paths:
   ```python
   filehead = sep.join([self.outdir, self.crop, var, self.mask, 'reg', self.prefix])
   ```
   This should use `os.path.join`.

5. **Non-breaking String Literals**:
   Strings split across lines should use proper continuation with a backslash or parentheses:
   ```python
   parser.add_option(""-o"", ""--outdir"", dest = ""outdir"", default = """", type = ""string"",
                     help = ""Output directory to save plots"")
   ```
   Python allows multi-line strings with triple quotes or by wrapping the content in parentheses (for improved readability).

6. **Filename Extension Handling**:
   The script assumes a single file extension after the filename period (`.`), which might not be the case:
   ```python
   self.yr1 = filesplit[6].split('.')[0]
   ```

7. **String Formatting for Print Statements**:
   The script uses old-style formatting for `print` statements:
   ```python
   print 'No jobs for processor to perform. Exiting . . .'
   print 'Plotting file', f, '. . .'
   ```
   It should use `print()` function for compatibility with Python 3.

There are no issues related to overwriting filenames, duplicating work unintentionally, or including entries that should be excluded based on the given information.

With all the above points considered, the script has several issues that could affect its operation and the quality of its output.","There are several issues with the provided script that could impact the quality or viability of its outputs:

1. **File Handling in `MaskPlotter.__init__`**:
   - The mode used for opening metadata files is `'rU'`, which is deprecated in Python 3. It should use `'r'`.
   - `basename(filename).split('_')`: This method assumes a specific filename structure. If the filenames don't follow this exact structure, it could throw an error or misbehave.
   
2. **Filename Overwriting**:
   - The script saves plots with filenames created by concatenating various parameters. If any of these parameters have the same values across different runs or iterations, it could result in overwriting previously saved files.

3. **Indentation Errors**:
   - There are some indentation inconsistencies, particularly with:
     - `self.yr1 = filesplit[6].split('.')[0]`
     - `self.prefix = '_'.join([self.model, self.climate, self.yr0, self.yr1]) + '_'`
     Correct indentation is essential for proper execution of the script.

4. **String Formatting in Print Statements**:
   - The script uses the old-style print statement without parentheses and placeholders, which is incompatible with Python 3.
     - Example: `print 'No jobs for processor to perform. Exiting . . .'`
     - It should be updated to: `print('No jobs for processor to perform. Exiting...')`
   
5. **Hard-Coded File Separators**:
   - The script uses `sep.join` in several places for constructing file paths. This assumes that `sep` is a file path separator, which might not be robust across different operating systems. It is better to use `os.path.join`.

6. **Data Initialization in `MaskPlotter.plot`**:
   - `pdat` is initialized with zeros but may be partially filled depending on the value of `n`. This could lead to plots with incorrect or incomplete data.

7. **Handling of Numpy Arrays**:
   - Directly using `dat[dat.mask] = nan` assumes that the array `dat` is a masked array from the netCDF data. If this is not the case, it would raise an error.

8. **Implicit Assumptions on File Contents**:
   - The script makes several implicit assumptions about the structure and availability of variables within the netCDF files (e.g., specific variable names and dimensions). If any of these assumptions are not met, it would result in runtime errors.

9. **Explicit Type Conversion**:
   - The usage of `int(ceil(double(nfiles) / numbatches))` seems like an overly complicated way to perform the operation. Since Python 3 automatically handles integer division and type conversions better, using `int(ceil(nfiles / numbatches))` would be more appropriate.

Considering these points, ""there are no problems"" would not be an appropriate conclusion. The script has issues that would need to be addressed to ensure it runs as intended and produces reliable output."
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,"There are a few issues with the script that could impact the quality/viability of its outputs:

1. **Meta File Handling:**
   - The script expects a single meta file whereas, in reality, there could be multiple meta files corresponding to different masks. This could lead to incorrect metadata being associated with the respective mask.

2. **File Path Construction:**
   - The script constructs file paths using string concatenation with `sep`. If `options.outdir` does not end with the appropriate separator or handle edge cases well, it could lead to incorrect file paths.

3. **Filename Overwriting:**
   - The filenames for plots are not unique enough if the script is run multiple times with different parameters. This could lead to overwriting of files if the output filenames are not sufficiently differentiated.

4. **Opening Files with Universal Newline Mode:**
   - The meta file is opened with the universal newline mode (`'rU'`), which is deprecated in Python 3 and could potentially cause issues.

5. **Mask Handling:**
   - The code assumes that certain variables (`area`, `indices`, etc.) exist for each mask. If the mask does not have these variables, it will result in a key error.

Given these issues, modifications would be needed to properly handle multiple meta files, construct file paths robustly, ensure unique output filenames, avoid deprecated file handling practices, and verify the existence of mask-specific variables before attempting to access them.

Thus, the overall quality and viability of the outputs could be adversely impacted by these issues in the script.","1. **Metadata Parsing:**
   The script assumes that the metadata file specified by `meta` contains rows with comma-separated values. However, when reading the file, it uses the tab delimiter (`\t`) to split rows. This could lead to incorrect parsing of the metadata file if the file does not use tabs as a delimiter.

2. **Overwriting Filenames:**
   The script constructs output filenames based on variables, aggregation masks, scenarios, and irrigation names. However, it does not check if combinations of these parameters could lead to duplicate filenames being generated. This means that it is possible for files to be overwritten if multiple plots share identical parameters.

3. **Handling of Time-Series Plot Limitations:**
   When creating time-series plots, the length of `scenlines` is determined based on the product of the number of scenarios and irrigation names. If this product exceeds the length of `colors` (which is 5), it could cause index-errors or unintended plotting behavior because the `colors` list is not long enough to handle a larger number of combinations.

4. **File Reading Mode:**
   The file reading mode `'rU'` used for opening the metadata file is deprecated in Python 3. This could raise a warning or error in environments where strict compatibility with Python 3 conventions is enforced.

Overall, these issues can potentially impact the quality and consistency of the outputs generated by the script.","The script contains several issues that could impact the quality and viability of its outputs:

1. **Opening Meta File with Mode 'rU'**: The mode `'rU'` for reading files with universal newline support is deprecated and may not function properly in Python 3. It should be replaced with `'r'`.

2. **Hard-Coded Delimiters and Assumptions**: There are hard-coded delimiters and assumptions about input data formats, such as assuming comma delimiters in the metadata splitting.

3. **File Overwriting Risk**: The way filenames are generated for the plots potentially allows for overwriting:
    - Multiple masks or variables could lead to filenames resolving to the same string, leading to one plot file being overwritten by another.

4. **Potential Index Out of Range**: The script does not validate the length of `filesplit` before accessing indices, which might lead to index out-of-range errors if the filename structure is not as expected.

5. **Inconsistent Handling of Missing Data**: Different techniques are used to handle missing data, with NaNs being assigned to masked data but not considered elsewhere explicitly.

6. **Global Variables and Hardcoded Values**: The use of magic numbers and hardcoded defaults, such as column counts in plots, without validation or flexibility based on actual data size.

7. **No Input Validation**: The script assumes that all input files and variables exist and can be read without checking for potential errors or existence, which could lead to runtime errors.

Given these observations, the script has several potential problems that should be addressed to improve reliability. Therefore, the statement ""there are no problems"" does not apply."
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,"There are several issues in the script which could impact the quality and viability of its outputs:

1. **Hardcoded Assumption for `split`**: The script assumes that splitting the `options.input` string will always produce a list with at least 4 elements (``split(options.input)[1].split('_')[3]``). If the file name convention changes or if the input filename does not follow this format, it will cause a `IndexError`.

2. **Unused Variable (`lat1`, `lon1`)**: The variables `lat1` and `lon1` are defined but not used when `options.maplimits` is not set. This might lead to incorrect plotting if `options.maplimits` is specified, and it can cause confusion regarding what the map limits would be otherwise.

3. **Implicit Dependencies on File Content**: The code implicitly assumes certain structures in the netCDF input file, such as the existence of dimensions and attributes like `'lat'`, `'lon'`, `'scen'`, `'irr'`, `'time'`, etc. If any of those are missing or named differently, the script will fail.

4. **Hardcoded Projection (`'cyl'`)**: The script uses the cylindrical projection (`'cyl'`) but it does not provide an option to choose other projections. While this may not be a bug, it limits the script's usability in varied geospatial contexts.

5. **Color Map Handling**: The script sets the colormap to `matplotlib.cm.YlGn` without an option for the user to change it, which could impact the usefulness of the visual output depending on different datasets and user preferences.

6. **Basemap Boundaries and Grid Checking**: The map boundaries (`llcrnrlon`, `llcrnrlat`, etc.) should be checked, as the script does not guarantee that `lon0`, `lon1`, `lat0`, `lat1` are always defined when they should be. This can cause the map plotting to fail.

7. **File Overwriting Without Warning**: The script directly saves the figure to `options.output` without checking if the file already exists. This can lead to unintentional overwriting of existing files.

8. **`time` Adjustment Logic**: The script adjusts the `time` variable directly, altering the original data, which can cause issues if the time units are not properly parsed or if further computations are required on the original `time` values.

9. **Hardcoded Parallel and Meridian Intervals**: The intervals for drawing parallels and meridians are hardcoded (`arange(90, -110, -30)` and `arange(-180, 180, 60)`). This may not be suitable for all datasets and can make maps confusing or cluttered.

10. **Unnecessary Imports**: The script imports some modules and functions which are not used, like `arange` and `double` from `numpy` and `re`. This does not impact functionality but is not efficient.

11. **Potential Non-handled Exceptions**: The script does not handle exceptions that could arise from missing or malformed input files or other runtime errors (except for partial handling in the time processing section), leading to possible crashes without informative errors.

With these points in mind, the script has several issues that could affect its execution reliability and the accuracy of its output.","There are a few issues that would impact the quality and viability of the outputs in this script:

1. **IndexError in `crop` extraction**:
   - The line `crop = split(options.input)[1].split('_')[3]` assumes that the input filename structure will always be such that the fourth underscore-separated component exists. If the file name does not match this pattern, the script will throw an `IndexError`.

2. **Variable Dimension Mismatch**:
   - The line `var = var[:, :, :, scen_idx, irr_idx]` assumes that the `var` array has at least five dimensions. If it does not, this line will throw an `IndexError`.

3. **Incorrect use of `+` operator with `time` that is a numpy array**:
   - The line `time += int(re.findall(r'\d+', tunits)[0])` modifies `time` in-place, which could cause unintended issues since `time` is a numpy array and might be modified outside the intended context.

4. **Handling time strings**:
   - The script does not validate the format of the time strings properly. If `options.time` contains improperly formatted time ranges or non-integer time values, the script would raise an exception without a clear explanation.

5. **Longitude default assignment missing in map limits**:
   - If `options.maplimits` is empty, `lon0` and `lon1` are not initialized. This will cause an `UnboundLocalError` when `Basemap` is instantiated.

6. **Handling non-existent variable attributes (e.g., units)**:
   - If `var.ncattrs()` does not contain 'units', an empty string is assigned to `varunits`. This logic is sound, but if the variable has other missing or unexpected attributes, it might affect plot labeling or interpretation.

7. **Possible overwriting of output file**:
   - The script does not check if a file with the same name as `options.output` already exists before saving the plot. This might lead to unintended overwriting of files.

8. **Handling of `--maplimits` and `--cblimits` options**:
   - The script does not validate if `options.maplimits` and `options.cblimits` contain the appropriate number of comma-separated values. Incorrect numbers of values will raise a `ValueError`.

These issues should be addressed to ensure robust and intended behavior of the script.","There are several problems with the code that would impact the quality/viability of its outputs:

1. The script does not check if the input file specified by the `-i` or `--input` option exists or can be opened. If the file cannot be opened, the script will fail without providing a user-friendly error message.
2. The `time += int(re.findall(r'\d+', tunits)[0])` line assumes that `tunits` always contains a number, which might not be the case. If it doesn’t, the script will throw an error.
3. The colorbar limits (`pmin` and `pmax`) are derived from `var.min()` and `var.max()` if not specified. This assumes that `var` is not empty. If `var` is empty, the script will throw an error.
4. If the user provides invalid (non-numeric or incomplete) `-t` (time) or `-l` (maplimits) options, no validation is done, and this might cause the script to fail.
5. There is potential for the output file specified by `-o` or `--output` to be overwritten if it already exists. There is no check or warning about this.
6. The script imports modules that it does not use (`re`, and `matplotlib`). While this doesn't impact the functionality directly, it is unnecessary and could be considered as an issue in terms of code quality.
7. The `options.scen` and `options.irr` values are compared against `scen` and `irr` lists but it does not check for their existence in these lists before attempting to get their indices using `index()`. If either `options.scen` or `options.irr` are not found, the script will throw an error.
8. The map limits `options.maplimits` does not set `lon0` and `lon1` if it is an empty string, which will cause an error when these variables are used in `Basemap()`.

Addressing these issues would enhance the robustness and usability of the code, ensuring it handles various edge cases gracefully and provides meaningful feedback to the user."
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,"There are a few issues with the provided Python script that could impact the quality or viability of its outputs:

1. **Output Filename Collisions**: The script saves output files with the following naming pattern: `options.outputdir + sep + splitext(files[i])[0] + '.' + options.var + '.png'`. If the `options.var` is the same for different files within the same batch, it will overwrite the previous output and only the last one will be saved.

2. **Potential IndexError**: The code extracts `crop` from the filename using `split(filein)[1].split('_')[3]`. If the file name does not follow this expected format, it will raise an `IndexError`.

3. **Misleading Console Output**: The print statement `print 'No jobs for processor to perform. Exiting . . .'` uses unsupported Python 2 syntax in Python 3. It should be updated to use parentheses, otherwise an error will occur.

4. **Assumption of Variables**: The script assumes that the netCDF files have certain variables (`lat`, `lon`, `scen`, `irr`, `time`). If any file does not contain one of these variables, the script will fail.

5. **Uncaught Exceptions**: There is no error handling for reading or accessing variables in the netCDF files. If the file is corrupted or the variable names do not match what's expected, the script will raise an uncaught exception and terminate.

6. **Mixed Data Types for `time` and `options.time`**: The `time` data is updated by adding an integer value (`time += int(re.findall(r'\d+', tunits)[0])`). If `time` is not initially an integer array but another data type like float or datetime, this operation could behave incorrectly.

7. **Static Longitude Limits**: The script does not have a mechanism to set `lon0` and `lon1` dynamically if `options.maplimits` is not provided, which can lead to incomplete or incorrect map plotting.

Addressing these issues would be crucial to ensure the script generates correct and comprehensive outputs.","There are a few problems with the code that could impact the quality or viability of its outputs:

1. **Incorrect Output Directory Separator**: The code uses `sep` from the `os` module to concatenate directory paths. While this is generally correct for cross-platform compatibility, it should ensure that `options.dir` and `options.outputdir` do not end with an unnecessary separator, which could result in double separators in the final path (e.g., `dir//file`). This could cause issues in file paths.

2. **Missing `lon0` and `lon1` Initialization**: If the `options.maplimits` is not provided, `lon0` and `lon1` variables are not initialized. This will cause an `UnboundLocalError` when they are used in the `Basemap` initialization.

3. **String Conversion Issue**: The code tries to concatenate a string with integers in `years_str` in a non-consistent manner. The `years_str` variable is defined in different places, sometimes concatenating integers directly with strings without converting them to strings first.

4. **Print Statement Syntax**: The print statement `print 'No jobs for processor to perform. Exiting . . .'` uses the Python 2 syntax. If this script is intended to be executed with Python 3, it should be updated to use parentheses.

5. **Hard-coded Filename Index Assumption**: The code assumes that the file names are structured in a way that the fourth part (index `3`) separated by underscores (`'_'`) contains the crop information. If files do not follow this naming convention, it will cause an error.

6. **Ambiguous Error Handling for Time Option**: If the `options.time` contains more than two parts when split by `'-'`, it raises a generic `Exception` without providing a useful message that specifies exactly what went wrong or what the expected format is.

7. **Unsanitized User Inputs**: User inputs for the `dir`, `outputdir`, `scen`, `irr`, and `var` options are passed directly without any validation or sanitization, which could lead to unexpected behavior if the inputs are not as expected.

8. **File Overwriting**: There is a possibility of file overwriting in the output directory if files generated by different batches have the same filename. This can happen if the netCDF files in different batches share the same base filenames.

In summary:
- Incorrect handling or potential issues with path separators.
- Missing initialization for `lon0` and `lon1`.
- Concatenation of strings with integers without conversion.
- Usage of Python 2 print syntax which will fail in Python 3.
- Filename parsing assumptions that may not be met.
- Generic error handling in time option parsing.
- Lack of validation for user inputs.
- Potential for file overwriting in the output directory.

Overall, these issues need to be addressed to ensure that the script functions properly and produces viable outputs.","There are a few problems that could impact the quality or viability of the outputs:

1. The script does not handle cases where the `options.dir` or `options.outputdir` directories do not exist, which could lead to file not found or directory not found errors.
2. The script uses a static separator (`sep`, which usually is `/` or `\` depending on the OS) to join paths. If either `options.dir` or `options.outputdir` ends with a trailing separator, this could lead to incorrect paths.
3. There is a potential issue with naming the output files, as it uses `splitext(files[i])[0]` to name the output files. If the base names of the input files are not unique, it could lead to overwriting output files unintentionally.
4. The script does not check if the specified variable options (`scen`, `irr`, `var`, etc.) exist in the netCDF file, which could lead to an error if they do not.
5. The script does not catch exceptions or handle errors during file reading and processing, which means any error would cause the script to terminate abruptly.
6. In the exception for the `-t` option, the raised `Exception` might lead to abrupt termination without a helpful error message for the user.

There are other potential improvements that could be made, but these points outline actual problems that would occur regardless of external files or inputs.

"
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,"There are several problems with the code that could impact the quality and viability of its outputs:

1. **Variable Shape Mismatch**:
    - The `areasum1`, `areair2`, and `arearf2` arrays are resized using the `resize` function of `numpy.ma`, which can reorder elements to fit the new shape. This might not preserve the correct arrangement of data corresponding to lat/lon dimensions, leading to incorrect computations.

2. **Incorrect Mask Handling**:
    - The mask is handled incorrectly in various places which might not be aligned with the actual masked data of the input variables. For example, the code does not account for the mask of `areair2` and `arearf2` before `summing`, which might lead to incorrect rainfed and irrigated areas.

3. **Dimension Order Misalignment**:
    - There might be an issue with dimension order alignment when resizing arrays. The latitude and longitude steps follow differently between different datasets, leading to inconsistent matrix operations.

4. **Wrongly Indexed numpy `where()` Results**:
    - `numpy.where` returns a tuple of arrays representing indices, but the code unpacks these into three separate variables (`idx1, idx2, idx3`) and attempts to use these for indexing 2D arrays causing dimension mismatch and potential `IndexError`.

5. **Improper Unit Handling**:
    - The unit conversion for area from percent to area appears overly simple. The formula assumes a uniformly distributed grid which may not be accurate depending on the actual format of latitude and longitude values and how they correspond to real-world distances.

6. **Ignoring Fill Values**:
    - NetCDF `createVariable` calls set `fill_value` to 1e20, but there is no guarantee this fill value is suitable for the dataset being handled. The fill value for the input datasets might need to be considered and aligned.

Based on these points, the script has multiple issues that would affect its output quality and reliability.","The script has a few issues that might impact the quality and viability of its outputs:

1. **Improper Handling of the 'area' Calculation Dimensions**:
   - The script calculates the 'area' variable using latitude values. The dimensions are then resized twice, which may not be appropriate for the intended use. The double resize operation can lead to incorrect dimension alignment or mismatched data.

2. **Dealing with Time Coordinate**:
   - There is an implicit assumption that the time coordinate starts from 0 and increments by 1 for each time step. This might not align with the actual time data in the input NetCDF file, leading to potential inconsistency in the time dimension.

3. **Incorrect Index Arrays**:
   - The use of `where` to extract indices is problematic because it returns a tuple of arrays. Additionally, the use of `idx1, idx2, idx3 = where(...)` assumes that the mask operations would always result in a 3D structure. Any variations in dimensions can cause index mismatch and possibly incorrect data assignment.

4. **Incorrect Mask Handling**:
   - The script uses `masked_array` and `masked_where` methods from `numpy.ma`, but the logic for masking and reshaping masks can propagate inconsistencies. Specifically, the mask `mk` is applied to areasum1, and later it is used for areair1 and arearf1 without ensuring consistency.

5. **Output File Overwriting**:
   - The script writes the output to a file named ""out.nc4"" by default. If this script is run multiple times without changing the output filename, it will overwrite the existing file each time, potentially resulting in loss of previous data.

6. **Mandatory Assumption about Units**:
   - The script assumes that the units of the variable in the input file can only be `%`. If there are other units, the script misses handling those cases, which could lead to incorrect area calculations.

Therefore, the script has issues that would affect the quality and viability of its outputs.","There are multiple issues with the given Python script:

1. **Overwriting Variable Data**:
   - After `areasum1` is initially assigned from the input file, it is potentially resized and masked further down the script. This re-assignment might overwrite the original data in a non-reversible manner.

2. **Improper Mask Handling**:
   - The way masks are being manipulated might lead to inconsistent results. `masked_where()` and logical operations might not handle all edge cases properly, especially if `areasum1` and `areasum2` have different shapes or masks applied.

3. **Assuming Units**:
   - The script assumes that if `aunits` is not `%`, no conversion is needed. This might be an oversimplification if other unit types need conversion or scaling.

4. **Resizing with `resize`**:
   - The script uses `resize` to adjust array shapes, which duplicates data to fit the new shape. This might not be the intended behavior and could lead to incorrect data manipulation.

5. **I/O Errors**:
   - The script does not handle I/O errors, such as failing to open files or write to them. This could result in uninformative crashes.

6. **Incorrect Dimension Sizes**:
   - When creating dimensions in the output NetCDF file, the dimensions (`time`, `lat`, `lon`) are assumed to be determined correctly by the shape `sh`. However, without proper validation, any mismatch can lead to incorrect outputs or crashes.

7. **Inconsistent Treatment of Missing Data**:
   - The script assumes `areasum2.mask` or `areasum2 == 0`, but does not account for other possible missing data indicators. This can lead to logical errors in the mask application.

8. **Using Deprecated Library**:
   - `optparse` is deprecated and should be replaced by `argparse`. Even though you asked not to consider potential improvements, using a deprecated library could cause issues with maintainability or compatibility.

Overall, the script does have multiple issues that would impact the quality/viability of its outputs."
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,"The script has several issues that could impact the quality or viability of its outputs:

1. **Overwriting `nm` Variable**: The variable `nm`, which was initially defined as the number of models, is being overwritten inside the nested loop when accessing `f.variables['nm'][:]`. This can lead to unexpected behavior later in the script.

2. **Hardcoded Strings for Checking Metrics**: The script checks if `models[m]` is `'rmse'` or `'tscorr'` to determine specific handling. However, these strings are also part of the list of models, leading to a potential conflict. The script assumes that `'rmse'` and `'tscorr'` are the only names used that could appear within `models`, and this can lead to errors or unintended behavior if there's a name collision or if these special model names are not present.

3. **Checking Directory for Files Starting with ftag**: If more than one file starts with the same `ftag`, the script does not handle this case and simply proceeds with the first match. This can cause confusion and errors if unrelated or duplicated files are present in the directory.

4. **Changing Ensemble Model Names Assumes Existence**: The script changes the names of models `rmse` and `tscorr` in `dims['model']`. However, if these models do not exist, it will result in a `ValueError` since the `index` function will fail.

5. **Handling of `options.nm` and `options.wt`**: The script has the potential to select `nmidx` and `wtidx` based on the maximum values of `nm` and `wt`. However, if `options.nm` is greater than `nm.max()`, it assigns `nmidx` to be `len(nm) - 1`, which is an ambiguous fallback and does not provide clear behavior or warnings to the user.

There are more subtle issues, but these are the primary problems that would impact the quality or viability of the script's outputs.","There are a few issues with the code that would affect the quality or viability of its outputs:

1. **PATH Pollution:**
   - The code appends all directories in the system `PATH` to `sys.path`. This can cause imported modules to be ambiguous and may lead to importing the wrong module if there are naming conflicts.

2. **Incorrect Handling of 'rmse' and 'tscorr':**
   - When determining if the `model` should be treated as 'rmse' or 'tscorr', it uses `models[m] in ['rmse', 'tscorr']`. However, without knowing the exact contents of `models`, it's unclear if this behavior is consistent. It seems the intent is to select specific model types for special treatment.

3. **Assuming `gadm0` Exists in the First File:**
   - The code assumes that `gadm0` and other specific variables will be present in the first file found in the directory. If the files do not have consistent variables, this could cause the script to fail.

4. **Overwriting Loop Variables `nm`, `nw`, `ng`:**
   - The loop variable `nm` is overwritten within the loop when reading the file, as in `nm = f.variables['nm'][:]`. This overwrites the `nm` which was originally the number of models. This can lead to confusion and potential errors in subsequent code.

5. **Hardcoded Assumptions for File Naming Conventions:**
   - The code heavily relies on specific naming conventions when looking for files (`ftag`). If the filenames do not follow the assumed pattern, the script will fail to locate the required files.

6. **Modifications on `dims['model']`:**
   - Changing the names in the `dims['model']` list (from `rmse` to `rmse-weighted` and `tscorr` to `tscorr-weighted`) directly may lead to unexpected issues since other parts of the code may still rely on the original naming.

7. **File Directory Not Guaranteed to Have Files:**
   - If the provided directory (`options.dir`) doesn't contain any files that match the expected patterns, the script may not handle this case gracefully, which could lead to runtime errors.

These issues could significantly impact the reliability and robustness of the script. The code makes several assumptions that may not always hold, leading to potential failures or incorrect outputs if the data or environment doesn't match the assumptions.","There are a few problems in the code that could impact the quality and viability of its outputs:

1. **Appending all system paths to `sys.path`:** The code appends every directory in the system's PATH environment variable to `sys.path`. This could lead to potential conflicts and importing issues if there are files with the same names in different directories.

2. **Improper handling of file existence:** The script assumes that a specific file (first file from `options.dir`) will always exist. If the directory is empty or if the expected file naming convention is not followed, it will throw an error.

3. **Inconsistent handling for certain conditions:**
   - When `models[m]` is in `['rmse', 'tscorr']`, it relies on variables like `nm` and `wt` from the netCDF file, which might not always be present or match the expected conditions, leading to possible index errors.
   - The code tries to find indices using the condition `options.nm <= nm.max()` but does not consider the case when `nm` might be an empty array, which will raise an error.

4. **Potential issues in `'model'` renaming:**
   - If `'rmse'` or `'tscorr'` are not present in `dims['model']`, trying to find their index will raise a `ValueError`.

5. **Safety checks for variable existence in netCDF files:** 
   - The script accesses specific variables (`options.metric`, `gadm0`, `scen`, etc.) from netCDF files without checking their existence. If these variables are missing, the script will raise an error.

Overall, these issues can lead to errors or incorrect results being generated, and they should be addressed to ensure the script runs correctly under various conditions."
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,"There are a few issues with the code that may impact the quality or viability of its outputs:

1. **Inconsistent Directory Checks:**
   - The code relies on the structure of directories and file existence without thorough validation. If a directory structure slightly deviates from the assumed format, it might result in missed files or directories.

2. **Handling of Multiple Matching Files:**
   - The code assumes that the directory containing yield files will always have exactly one match (`if len(fs) == 1`). If there are no match or multiple matches, those scenarios are not handled, and it might skip valid files or cause unexpected behavior.

3. **Exit Statement:**
   - The `exit` statement meant to exit the program when no files are matched is incorrect. It should be `exit()`, which is a function call, otherwise, it will not work as intended.

4. **Hardcoded File Pattern:**
   - The script looks for files matching the pattern ""*yield*"". This hardcoding may lead to missing relevant files if the naming convention changes slightly.

5. **Wildcard Exception Catching:**
   - The code uses a general exception handler (`except:`), which catches all exceptions indiscriminately. This can mask the actual error and make debugging harder.

6. **Division Operations:**
   - There is a potential risk of division by zero if `yld[htidx0 : htidx1, :, 2].mean(axis = 0)` results in zero, which may not be handled properly.

Aside from these issues, other parts of the code seem structurally sound. Therefore, based on the points mentioned above, the code has several problems that need to be addressed.","There are multiple issues with the provided Python script:

1. **Exit Statement:**
   The `exit` statement should be `exit()`. Without the parentheses, it does not actually exit the script.

2. **Deprecation of `OptionParser`:**
   The `optparse` module used is deprecated and replaced by `argparse`. This, however, doesn't impact the immediate execution but is worth noting for future uses.

3. **Loading Time Units Inconsistency:**
   The way `time` units are extracted and used may lead to inconsistent results. Specifically, it assumes the `time` units string will always have the year as the first sequence of digits.

4. **Directory Handling:**
   If `len(fs) != 1`, the corresponding directory is ignored without warning. This might hide potential issues if no yield file is found or if more than one yield file exists.

5. **Ambiguous Time Index Handling:**
   The time index handling, especially in the block:
   ```python
   htidx0, htidx1 = where(time == 1980)[0][0], where(time == 2009)[0][0] + 1
   ftidx0, ftidx1 = where(time == decade)[0][0], where(time == decade + 9)[0][0] + 1
   ```
   assumes that time values exactly match the given years, which might not always be the case. If no match is found, this will cause the script to fail.

6. **Potential Data Overwrite:**
   If `outputfile` already exists in the specified output directory, it will be overwritten by default, leading to potential data loss.

7. **Mean Calculation:**
   The script uses hard-coded and potentially arbitrary axis indices:
   ```python
   data[:, i] = yld[ftidx0 : ftidx1, :, 2].mean(axis = 0) / yld[htidx0 : htidx1, :, 2].mean(axis = 0) # sum
   ```
   This could lead to incorrect calculations if the dataset's structure does not follow the assumed dimensionality.

8. **Exception Handling:**
   Broadly catching all exceptions with `except` and printing a general message without handling specific exceptions can make debugging difficult and may hide underlying issues.

In conclusion, there are several issues with the code that would impact the quality and viability of its outputs.","The provided Python script has several issues that could impact the quality and viability of its outputs:

1. **File Overwriting**:
   - The script assumes that there is only one file matching `*yield*` per directory. If there are multiple files, only the first match will be processed, and the rest will be ignored. This could lead to incomplete data processing.
   - The output file (`outputfile`) is named based on the crop, decade, and CO2 setting. If you run the script with the same parameters multiple times, the output file will be overwritten, potentially leading to loss of previous results.

2. **Exit Handling**:
   - The script uses `exit` without parentheses. This will not terminate the script as intended. It should be `exit()` to exit the script properly.

3. **Exception Handling**:
   - The try-except block prints a warning when an exception occurs but does not halt or otherwise handle the error condition cleanly. This means data for the failing file will be incomplete or missing without a robust error notification or corrective action.

4. **String Formatting for Print Statements**:
   - The print statements are using the older `%` string formatting method, which is deprecated. While this won't cause the script to fail, it is recommended to use the newer `str.format()` method or f-strings for improved readability and future compatibility.

5. **Timezone Adjustments**:
   - The script adjusts the time variable by extracting a number from the time units string ('int(findall(r'\d+', f.variables['time'].units)[0])'). If this extraction fails (e.g., if the time units string does not contain any digits), it will cause an error. There is no fallback or error handling for this situation.

6. **Import Optimization**:
   - The import of unused modules, such as `os.isFile`, unnecessarily clutters the script. This isn't a critical error but can be misleading.

7. **Data Integrity**:
   - When writing the output, the `append` function from numpy is used, which is generally less efficient and less clear than directly creating a list or using concatenation. This could impact performance, especially with large datasets.

8. **Hard-Coded Dictionary**:
   - The script uses a hard-coded dictionary `cf2cs` for crop codes, which may not be comprehensive or flexible for additional crops that might be used in the future.

There are several issues that impact the script's operation. Therefore, the statement ""there are no problems"" does not apply."
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,"There are several issues with the provided Python script that could impact the quality or viability of its outputs:

1. **Appending to `sys.path`:**
   Adding all directories from the system `PATH` environment variable to `sys.path` is unnecessary and might introduce unintended side effects by potentially shadowing standard modules or importing unwanted ones.

2. **Option Parsing:**
   If `options.agg`, `options.input`, or `options.output` are not provided, the script will attempt to unpack and use empty strings, which would cause errors.

3. **Overwriting Files:**
   The `createAggFile` function creates and overwrites the file specified by `options.output` without checking whether the file already exists, potentially leading to data loss if the file already contains valuable information.

4. **Use of Global Variables:**
   Variables like `weights`, `ivars`, `var`, etc., are defined and used in a global scope, making the script harder to debug and maintain. While not an immediate issue, this could lead to bugs if the script is modified or expanded.

5. **Indexing Errors:**
   Splitting and unpacking `options.input` and `options.agg` assumes they are well-formed and contain exactly one colon. If these strings do not meet expectations, the unpacking will throw a `ValueError`.

6. **Resizing Masks:**
   The `resize` function is used on masks with the assumption that resizing is necessary. This might lead to data integrity issues if the original and resized masks differ.

7. **Opening `output` Twice:**
   The script opens the `options.output` file twice: initially in `createAggFile` and later in append mode. This unnecessary duplication could be streamlined to avoid opening the same file twice.

8. **File Closing:**
   The script manually closes the output file using `f.close()`. It is better to use a context manager (`with` statement) to ensure the file is properly closed even if an error occurs during processing.

These issues, combined, could lead to errors, data loss, or integrity issues in the outputs.","There are a few issues with the script that could potentially impact the quality or viability of its outputs:

1. **Appending System Paths**: Adding all system paths (`os.environ['PATH'].split(':')`) to `sys.path` is unnecessary and may lead to unintended consequences, such as importing the wrong module if there are naming conflicts. 

2. **File Overwriting**: When calling `createAggFile`, if a file with the same name as `options.output` already exists, the code will overwrite it without warning. This could lead to the loss of important data.

3. **Parsing Input Files**: The script assumes that the options `input`, `weights`, and `agg` will be provided in the format `file:var`. If the input strings are not in this format, it will likely raise an error. No validation is performed to ensure the correct format.

4. **Handling Weights**: The code tries to load weights if the `weights` option is not 'none', 'None', or an empty string. If there is a problem reading the weights file or the specified variable does not exist, the script will fail without a helpful error message.

5. **Opening NetCDF Files**: The script opens the output NetCDF file twice, once in ""write"" mode in `createAggFile` and again in ""append"" mode using `nc(options.output, 'a')`. This is inefficient and error-prone. Ideally, the file should be opened once to perform all necessary operations.

6. **Averager Object**: The `MeanAverager` or `SumAverager` instance is created based on the `type` option after the output file is opened and passed to `createVariable`. If there's an issue with the `avobj` creation, it may lead to unwanted behavior or crashes without proper handling.

7. **Masks and Resizing**: `resize(var[i].mask, var[i].shape)` is not a recommended way to handle the resizing of masks, as it can introduce unintended consequences if the sizes change unexpectedly. It may lead to incorrect masking and, subsequently, incorrect calculations for averaging.

8. **Missing Attribute Check**: The script accesses certain attributes of NetCDF variables without checking for their existence, which might raise errors if these attributes do not exist (e.g., `vunits[i] = v.units if 'units' in v.ncattrs() else ''`).

Each of these issues could lead to errors, crashes, or incorrect data.","There are indeed several issues in this script that would impact the quality or viability of its output:

1. **Appending Paths to `sys.path`**: Directly appending directories from `os.environ['PATH']` to `sys.path` could lead to unexpected behavior or conflicts with different versions of libraries.

2. **Filename Overwriting**: The script opens the output NetCDF file for writing (`'w'` mode) and then later reopens it for appending (`'a'` mode). This approach works, but if the file exists, it would be overwritten when first opened (`'w'` mode). There should be a check to prevent unintentional overwriting of existing files.

3. **Weights Loading**: The `weights` variable is set to `None` and only loaded if `options.weights` is not 'none', 'None', or an empty string. If the weights file or variable within the file does not exist or is improperly formatted, the program could fail.

4. **Dimension Resizing**: Resizing the mask for the variables read from the input file using `resize` may alter the data structure, leading to potential mismatches when applying masks.

5. **Variable Initialization**: Variables `var` and `vunits` are initialized as arrays of zeroes, which is not idiomatic and could lead to confusion or errors in other parts of the code.

6. **Flag Consistency**: The `calcarea` option is a boolean flag, but its usage in the call to `avobj.av` is inconsistent. Ensure the flag's logic is applied correctly in the `avobj.av` function call.

7. **Exception Handling**: There should be more robust exception handling, especially when working with file I/O operations, to capture issues such as missing files, incorrect formats, or failed reads/writes, which currently could crash the script without useful feedback.

In conclusion, there are several issues with the code that need addressing to ensure the script's reliability and correctness."
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,"There are several issues in the provided script that can impact the quality and viability of its outputs:

1. **Path Appending to `sys.path`:** 
    By appending all the paths from the environment `PATH` variable to `sys.path`, there is a risk of including unintended or insecure paths in the module search path, potentially leading to importing wrong or malicious modules.

2. **Error Printing Syntax:**
    The `print` statement used in the block where the script checks if the crop variable exists in the reference file uses Python 2 syntax. Given that the shebang line does not specify Python 2, this will result in a syntax error in Python 3.

3. **Potential Index Error in `where` Condition:**
    The line `aidx = where(aref == ain[a])[0][0]` assumes that `where` will always return a non-empty array. If `ain[a]` is not found in `aref`, this will cause an IndexError due to attempting to access the first element of an empty array.

4. **Modification of `tref` and `tin`:**
    The script modifies `tref` and `tin` directly by adding an integer value to them. This can be problematic if these variables are needed in their original form later in the script or if they are used in other parts of the code.

5. **No Check for Infile or Reffile Existence:**
    The script does not check if the `infile` or `reffile` provided actually exist before attempting to open and read from them. This can result in runtime errors if the files do not exist.

6. **Hardcoding Metric Names and Assumptions:**
    The script assumes that metric calculations and their labels (""dt"", ""mp"", ""cr"", etc.) are consistent across different files and models. If this is not the case, it can lead to incorrect indexing and data misalignment.

7. **Appending Data to Output File:**
    There is a risk of inconsistency or duplication when using the `append` method to add data to the output file if the file already exists. The script should handle such cases to avoid unintentional duplication of data.

8. **Unscoped Variables in `product` Loops:**
    Variables like `refidx`, `aidx` within nested loops do not have explicit checks or bounds, which might lead to logical errors or crashes in edge cases.

Considering these points, there are definite issues in the script that could impact the output quality and overall reliability.","There are problems with the code that could impact the quality and viability of its outputs:

1. **Overwriting Output File**:
   - The `MultimetricsEnsembleFile(outfile, ...)` is used without any checks to see if the `outfile` already exists. This means that running the script multiple times can overwrite the existing output file, potentially losing valuable information.

2. **Potential Indexing Error**:
   - In the loop that iterates over `product(range(ndt), range(nmp), range(ncr))`, the code retrieves the index of `dt[d]` from `dtref` using `refidx = dtref.index(dt[d])`. There is no check to ensure that `dt[d]` actually exists in `dtref`, which can cause the script to throw an error if it doesn't find the element.

3. **Unintentional Duplicated Metric Calculation**:
   - The script calculates metrics for each combination in the nested loop without checking if such a combination is redundant or has been computed before. This could lead to duplicated work, especially if the combinations of `dt`, `mp`, and `cr` result in repeated calculations.

Apart from these specific issues, the provided code appears to be generally sound.","There are several issues with the script that would impact the quality/viability of its outputs:

1. **Appending to `sys.path`**: The way paths are appended from `os.environ['PATH']` is unusual and can lead to potential issues, as these are typically directories containing binaries, not Python modules.

2. **Parsing Filename Logic**: The script assumes that the input filename split by underscores will have at least 4 parts and that the 4th part is the crop name. If the filename format does not follow this exact pattern, it could lead to an IndexError.

3. **Hardcoded Variable Name Assumption**: The script assumes that the variable names in the reference and simulation files follow a specific pattern (`yield_<crop>`). If the variable names deviate from this, the script will not find the required data.

4. **Potential for IndexError in `where` Statement**: The `where(aref == ain[a])[0][0]` assumes that there is always an index where `aref == ain[a]`. If such a condition is not met, it will cause an IndexError.

5. **Concatenation of Time Units**: The way the script adds integers extracted from the time unit string (using `findall`) directly to the time arrays (`tref` and `tin`) can lead to incorrect time processing if the units are not properly parsed or contain unexpected values.

6. **Proper Handling of Aggregation Levels**: The script assumes that both reference and input data files have the same aggregation levels and corresponding dimensions. Any mismatch in the structure or the presence of additional/sparse data can lead to misaligned data and subsequent errors.

7. **Potential Overwriting of Output File**: There is no check to prevent the existing output file from being overwritten. If the `outfile` option is provided with a name that already exists, it will be overwritten without warning.

8. **Use of Deprecation Libraries (`optparse`)**: The script uses `optparse` which is deprecated. Although this is more of a stylistic issue, it might lead to problems in maintainability and future-proofing the code.

9. **Print Statement for Error Handling**: The error message in case of an unavailable crop in the reference file is printed using the Python 2 `print` statement instead of Python 3, which is inconsistent with the rest of the script and can cause syntax errors in Python 3 environments.

10. **Missing Error Handling for File Operations**: The script does not handle potential errors or exceptions that can occur during file read/write operations. This can lead to unexpected termination of the script without providing useful feedback to the user.

These issues must be addressed to ensure the script runs correctly and produces reliable output."
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,"There are some problems with the given script that could impact the quality/viability of its outputs:

1. **Default Option Values**: The default values for `inputfile`, `plantingfile`, `maturityfile`, and `outputfile` are set to `64`, which is not a valid file path. If the user does not provide these options, the script will fail when it tries to open these files.

2. **Indexing without Validation**: The script assumes that `var[idx][0]` will always have at least one element. If `idx` happens to filter out all variables, this will throw an `IndexError`.

3. **Mask Modification**: The line `shiftd[-1].mask = True` assumes that `shiftd` is a masked array. If it is not, this will raise an `AttributeError`.

4. **NetCDF File Handling**: The script opens the NetCDF file in 'r+' mode and modifies `v[:]` with the data from `inputd`. However, if `inputd`'s shape does not match the shape of `v`, it will raise a ValueError or another type of exception.

5. **File Overwriting Risk**: If the `outputfile` is the same as `inputfile`, the script will copy the file to itself and then attempt to modify it. This can result in unintended overwriting of the original file.

6. **Unmasked Input Data**: There is no guarantee that `inputd` or `shiftd` are masked arrays. If they are not, invoking `.mask` on them will fail.

Given these issues, the script may not operate correctly under the assumptions that valid files and data formats are always provided.","The script has several issues that could impact the quality and viability of its outputs:

1. **Default Values of Options**: The default values for `inputfile`, `plantingfile`, `maturityfile`, and `outputfile` are all set to `64`, which is not a valid file path. This will cause the program to fail if the user does not provide these options.

2. **Variable Selection in `loadData`**: The script assumes there is only one variable other than 'lon', 'lat', and 'time' in the file and selects the first one found. If there are multiple such variables, it will not handle them correctly.

3. **Indexing in `loadData`**: The script assumes `var[idx][0]` exists without checking if `var[idx]` is empty. This could lead to an `IndexError` if no variables other than 'lon', 'lat', and 'time' are found.

4. **Selective Year Extraction**: The code `pdates = pdates[0]` and `mdates = mdates[0]` assumes that the data has more than one year and that it is safe to select the first year without checking bounds or the structure.

5. **Mask Treatment**: The line `shiftd[-1].mask = True` assumes that `shiftd` is a masked array, but this might not always be the case. This could cause an AttributeError if `shiftd` is not a masked array.

6. **Data Shifting**: The shifting operation `shiftd[: -1] = shiftd[1 :]` does not properly handle dimensions, potentially causing data misalignment or loss. It overwrites data without a clear structure, which might lead to inaccuracies.

7. **Copying and Modifying File**: The copy operation (`copyfile(inputfile, outputfile)`) happens before checking if `inputfile` exists, and there is no error handling if the copy operation fails.

8. **Opening Output File**: The script opens the `outputfile` in 'r+' mode, but it assumes that this file format supports the required read and write operations without verification.

In summary, the script has issues related to invalid default values, improper variable handling, unchecked assumptions, and possibly incorrect file operations. These could lead to errors during execution, incorrect data manipulations, or file operations failing silently.","There are several issues with the provided Python script that may impact the quality or viability of its outputs:

1. **Default Values for OptionParser:**
   - The default value for the options (`inputfile`, `plantingfile`, `maturityfile`, `outputfile`) is set to `64`, which is not a valid string for a file path. This would likely lead to errors if the user fails to provide these arguments.

2. **Hardcoding Dimension Index:**
   - The script extracts `pdates = pdates[0]` and `mdates = mdates[0]` without verifying the dimensions of these arrays. If these variables do not have an expected shape, this could cause issues.

3. **Potential for Overwriting Output File:**
   - The use of `copyfile(inputfile, outputfile)` risks overwriting existing files without warning. This may lead to data loss if the output file already exists.

4. **Assertion on `shifted[-1].mask`:**
   - The assignment `shifted[-1].mask = True` requires the `shifted` array to be a masked array. There is no guarantee that `inputd[:, latidx, lonidx]` is a masked array, potentially causing an AttributeError.

5. **Direct Array Manipulation Without Masking:**
   - The script does not handle data masking consistently, especially with the line `shiftd[-1].mask = True`. If `shiftd` is not a masked array, this assignment would fail.

6. **No Check for Valid Variable Names:**
   - The script assumes at least one variable in the dataset is neither `'lon'`, `'lat'`, nor `'time'`. If all variables are among these, the script will fail with an IndexError when accessing `varname = var[idx][0]`.

There are no problems."
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,"The Python script has a significant issue related to its usage of default values in the `OptionParser` configuration. Specifically, it sets default values for `inputfile`, `growingfile`, and `outputfile` to `64`, which will conflict with the expected string type for these options. This could cause runtime errors when the script is executed without proper command-line arguments. 

Here is a detailed list of the problems:

1. **Default Values for OptionParser:**
   The default values for `inputfile`, `growingfile`, and `outputfile` are set to `64` (an integer), which is not appropriate since the script expects file paths (strings).

There are no other structural problems with the code regarding overwriting filenames, duplicating work unintentionally, or including unintended entries according to the given context. 

Therefore, the primary issue is the inappropriate default values in the `OptionParser` configuration. If corrected, the script should work as expected, assuming correct inputs and the existence of necessary files.

Solely focusing on the provided code snippet, the conclusion is:
There is a problem with the default values for `inputfile`, `growingfile`, and `outputfile` in the `OptionParser` configuration.","There are multiple issues with the provided Python script that would impact the quality and viability of its outputs:

1. **Default Values for File Options**:
   The default values for `--inputfile`, `--growingfile`, and `--outputfile` options are set to an integer (`64`) instead of an empty string or a more appropriate value. This will cause problems if these options are not provided by the user, as the script will try to open a file with a name `64`.

2. **Hardcoded Variable Names**:
   The script assumes the existence of variables named `'planting day'` and `'growing season length'` in the netCDF file referenced by `growingfile`. If these variables do not exist, the script will raise a `KeyError`.

3. **Indexing Logic**:
   When extracting `varname`, the script uses a condition that excludes `'lon'`, `'lat'`, and `'time'`. If none of the remaining variables satisfy the condition, trying to access `var[idx][0]` will raise an `IndexError`.

4. **Data Shifting Logic**:
   The shifting of `shiftd` data may introduce unintended consequences. The code `shiftd[: -1] = shiftd[1 :]` effectively shifts the data by one time step but does not properly handle edge cases or data integrity after the shift. It also masks the last entry with `shiftd[-1].mask = True`, but it does not provide sufficient context on the correctness of this operation.

5. **Overwriting Files**:
   The script copies the input file to the output file location and then opens the output file in write mode to modify it. If `inputfile` and `outputfile` are the same, this will result in overwriting the original input file.

6. **Potential Data Loss**:
   When modifying the variable data and saving it to the `outputfile`, if there are any issues with the write operations (like file permission issues or write failures), data integrity might be compromised.

Due to these issues, the script is likely to encounter runtime errors, data integrity problems, and might not produce valid or expected outputs.","There are a few problems in the provided Python script that could impact the quality and viability of its outputs:

1. **Default Argument Types in OptionParser:**
   The `default` values for the `inputfile`, `growingfile`, and `outputfile` options are incorrectly set to an integer (64) instead of a string. This will likely cause issues when accessing file paths as strings.

2. **Unnecessary Array Conversion:**
   The `var = array(f.variables.keys())` line forces keys into a NumPy array, which is unnecessary as `f.variables.keys()` returns a dictionary view object that supports iteration and membership tests directly.

3. **Error-prone Variable Selection:**
   The variable selection logic `varname = var[idx][0]` assumes that there will always be at least one valid variable that isn't 'lon', 'lat', or 'time'. If no such variable is present, this will result in an `IndexError`.

4. **Mask Attribute Setting:**
   The line `shiftd[-1].mask = True` directly sets the `mask` attribute of a masked array, which is generally not advised. Instead, it should use `ma.masked_where` or similar functions to properly handle masking.

5. **Hardcoding Strings when Accessing Variables:**
   The script directly accesses the 'planting day' and 'growing season length' variables with strings. If the variable names in the NetCDF file differ (e.g., have different capitalization or spacing), this will cause a `KeyError`.

6. **Overwriting Files:**
   The `copyfile(inputfile, outputfile)` step will overwrite the contents of `outputfile` if it already exists, which may not be the desired behavior.

These issues need to be corrected to ensure that the script functions properly and produces viable and accurate outputs.

Therefore, there are problems."
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,"There are a few problems with the code that could impact the quality or viability of its outputs:

1. **Overwriting Filenames**: If the file specified by `outfile` already exists, it might be overwritten without any warning.

2. **Missing Key in `params`**: If the `params` file does not contain the 'time_ranges' key, the code will raise a KeyError.

3. **Potential `IndexError` for `ain`**: The line `aidx = where(aref == ain[a])[0][0]` assumes that `where(aref == ain[a])` returns an array with at least one element. If no elements match, this will raise an IndexError.

4. **Printing Syntax (Python 2.x style)**: The print statement `print 'Crop %s unavailable in reference file %s. Exiting . . .' % (crop, reffile)` uses Python 2.x syntax. If this script is run with Python 3.x, this will cause a SyntaxError.

5. **Unclosed File**: If the `params` file specified by `options.params` does not exist or cannot be opened, a FileNotFoundError will occur and the program will not handle it gracefully.

These issues would need to be addressed to ensure the script runs correctly and reliably in all scenarios.

","There are several issues with the script that could impact the quality or viability of its outputs:

1. **Appending the PATH Environment Variable**: The script adds all directories listed in the PATH environment variable to `sys.path`. This can lead to importing unexpected or incorrect modules if there are naming conflicts.

2. **Unsafe YAML Loading**: The YAML file is loaded using `ruamel.yaml.load`, which is not safe and can execute arbitrary code if the YAML input is malicious. It is better to use `ruamel.yaml.safe_load`.

3. **Filename Checking**: Both `infile` and `reffile` are read from the command line options but are not checked for correctness. If the files do not exist or are incorrect, it could cause the script to fail.

4. **Undefined Behavior for Missing Crop Data**: If the crop does not exist in the reference file, the script prints an error message and exits without closing the file properly, which could potentially lead to unclosed file handles.

5. **Ambiguous Time Addition**: The script uses `findall` to extract numbers from `tref_units` and `tin_units` and directly adds these to `tref` and `tin`. If these units are not as expected, it could lead to incorrect time calculations.

6. **Potential IndexError**: When accessing `yield_refc[aidx, :, refidx, m]`, `aidx`, `refidx`, and `m` must be valid indices. If `ain[a]` is not found in `aref`, `aidx` could be out of range, leading to an `IndexError`.

7. **Multiple Nested Loops**: The script uses multiple nested loops especially over `naggs`, `nscen`, `ndt`, `nmp`, and `ncr`. If these dimensions are large, the computational cost can be extremely high, impacting efficiency.

8. **Data Type Compatibility**: The script assumes that the operations between NumPy arrays `tref` and `tin` will be valid after adding extracted time integers. This may not hold true if the arrays do not have compatible data types.

9. **Output File Overwriting**: The script does not check if the `outfile` already exists before writing to it. This may lead to unintentional overwriting of existing files.

10. **Implicit Dependency on `metrics` and `filespecs` Modules**: The code requires `MetricsWrapper` from `metrics` and `MultimetricsFile` from `filespecs`. If these modules are missing or have been modified, they could cause the script to fail.

11. **Printing in Python 2 Syntax**: The script uses a print statement designed for Python 2 rather than the Python 3 print function (`print 'Crop %s unavailable...'` vs. `print('Crop %s unavailable...')`).

Based on the issues listed, the script has several potential problems impacting its reliability and output quality.","There are several issues with the code that would impact the quality or viability of its outputs:

1. **Adding PATH directories to `sys.path`**: The script adds all directories in the system's PATH to `sys.path`, which is not a recommended practice. This can lead to unintended consequences by importing the wrong modules if there are modules with the same names in those directories.

2. **Opening files without exception handling**: The script opens files without any attempt to catch exceptions. If the files specified by the user do not exist or cannot be opened for some other reason (like permissions), the script will crash.

3. **Non-compatible print statement**: The script uses a `print` statement without parentheses (`print 'Crop %s unavailable in reference file %s. Exiting . . .' % (crop, reffile)`), which is not compatible with Python 3.

4. **Mutating Time Variables**: The lines `tref += int(findall(r'\d+', tref_units)[0])` and `tin += int(findall(r'\d+', tin_units)[0])` modify the `tref` and `tin` variables in place. This could lead to unexpected outcomes if these variables are used elsewhere in the program.

5. **Uninitialized Variable `times`**: The variable `times` is initialized with `[tin]`, but its length is assumed to be equal to `ntimes` in the for loop `for t in range(ntimes):`. If `ntimes` is greater than 1, it will cause an index error.

6. **Assuming Variable Presence and Structure**: The script assumes specific variables and structure in both input NetCDF files without checks, potentially leading to KeyError or IndexError if the files do not conform to these expectations.

7. **Finding Indices with Assumed Matching**: The script uses statements like `dtref.index('none')` to find the index of certain elements. This can lead to ValueError if 'none' or the other specified strings are not found in those lists.

8. **Array Out of Bounds**: The script uses `aref` and `ain` arrays, assuming that `ain` values will have corresponding indices in `aref`. If any value in `ain` does not exist in `aref`, this will cause an IndexError.

Each of these issues could impact the reliability and accuracy of the script's output, potentially causing crashes or incorrect data processing.

"
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,"There are some problems in the given code that may impact its quality and viability:

1. **Iteration Over Dictionary Values**: The line `firstfile = filedic.values()[0].values()[0]` will raise a `TypeError` in Python 3, as `filedic.values()` does not return a list but a view object. This can be resolved by converting the view to a list.

2. **Division Issues in `winave` Function**: In the function `winave`, the division operations are performed with integers which will result in integer division instead of floating-point division, potentially leading to incorrect averages. This issue can be fixed by ensuring that the division results in a float.

3. **Slicing with Floats in `winave` Function**: The slicing indices `idx0` and `idx1` in the `winave` function are computed as floats due to the division. Only integers should be used for list slicing.

4. **File Overwriting**: The resultant netCDF file `filename = 'faostat.{:d}-{:d}.nc4'.format(years[0], years[-1])` might get overwritten if the script is run multiple times with different input data involving the same years. There is no mechanism to handle existing files or create unique filenames.

5. **Uninitialized Mask in Complex Array**: The line `yielddt.mask = ones((yielddt.shape))` assumes all data needs to be masked initially. This might not be appropriate and masks all data initially.

6. **String Formatting**: The string formatting within the `print` statements uses old-style string formatting (e.g., `%d`). While it is not problematic in terms of functionality, it is common to use the newer `format` method or f-strings in newer Python versions for consistency and readability.

7. **Handling of Mutually Exclusive Options**: The script does not handle situations where detrending methods might be mutually exclusive or where a suitable default behavior should be defined if none are specified.

There is a mixture of Python 2 and Python 3 syntax and style. Using a consistent Python version syntax is essential to avoid issues. If the code is intended for Python 3, some older idioms (like `print` statements) and behaviors (like dictionary `values()` method handling) should be updated.

Overall, these issues need to be addressed to ensure the correctness and robustness of the script.","There are indeed several issues with the provided code that could impact the quality/viability of its outputs:

1. **Indexing Issues in `winave` Function**:
   - The indexing logic for determining the start and end of the averaging window might not be correctly handled, potentially causing array indexing errors or returning incorrect window averages.

2. **Slice-over Index Error in ` winave` Function**:
   - The line `dave = dave[n / 2 : -(n / 2)]` may result in unexpected behavior or errors if `n / 2` is not an integer. This could affect the final averaged data returned by the function.

3. **Accessing Values in Dictionary**:
   - The line `firstfile = filedic.values()[0].values()[0]` is problematic because `filedic.values()` returns a view object, and it should be converted to a list for direct indexing. This can cause a `TypeError`.

4. **Combining Paths Using `+` Instead of `os.path.join()`**:
   - Combining paths using `options.input + sep + f` is not robust and platform-independent. The `os.path.join()` function should be used to construct file paths correctly.

5. **Iteritems**:
   - The use of `iteritems()` in the line `for crop, dic in filedic.iteritems():` is specific to Python 2.x. In Python 3.x, `items()` should be used instead.

6. **Unclear and Incorrect Mask Handling**:
   - The handling of masks and fill values in the line `ma.zeros((yieldarr.shape + (len(detrend),)))` is not clear and may lead to incorrect masking of data values. Proper initialization of masks and fill values needs to be carefully handled.

7. **Logging Warnings**:
   - The warnings about mismatched years and GADM codes (`print 'Warning: Years not the same in file', yieldfile`) are printed to the console. It might be better to log these warnings using a logging framework for better traceability.

In summary, there are multiple issues such as indexing errors, improper dictionary access, non-robust path construction, and deprecated methods that need to be addressed to ensure the quality and viability of the script's outputs.","There are several issues in the provided code that could impact the quality and viability of its outputs:

1. **Integer Division Issues**:
   - In the `winave` function, the expressions `n / 2` and `(nend - nstart) / 2` perform integer division in Python 2. If this script is run with Python 3, it will perform floating-point division, causing unexpected results.

2. **File Overwriting**:
   - The filename for the output netCDF files is set to `'faostat.{:d}-{:d}.nc4'`. If this script is run multiple times with the same input years, it will overwrite the existing file.

3. **Iteration over Dictionary Items**:
   - The line `firstfile = filedic.values()[0].values()[0]` will throw an error. In Python 3, `filedic.values()` returns a view object, not a list. Moreover, using `values()` directly to index dictionaries is error-prone.

4. **Incorrect Mask Setting**:
   - In the nested `for` loops under the `yielddt` logic, the line `yielddt.mask = ones((yielddt.shape))` sets the mask incorrectly. This will mask all values, meaning that subsequent operations may not perform as expected.

5. **Misalignment of Indices**:
   - The `yielddt[i, 1, :, j]` and similar slicing used in the detrending process for 'ffd' and 'ffdtr' methods appear to be incorrect and should be reviewed for proper array assignment.

6. **Unnecessary Nested Dictionary Check**:
   - The check `if not 'yield' in dic or not 'area' in dic` could skip some crops without warning or logging, making it difficult to debug which crops were omitted and why.

7. **Division by 10000 without Units Check**:
   - When dividing the yield data by 10000, the script assumes the units require this transformation but does not check the `units` attribute to confirm it is necessary.

8. **Unsorted GADM Codes**:
   - Sorting `self.gadm` directly could impact the alignment of yield and area data if indices no longer correspond post-sorting.

9. **Deprecated OptionParser**:
   - `optparse.OptionParser` is deprecated and should ideally be replaced with `argparse.ArgumentParser`. While not causing an error, it indicates the script is not using more modern Python practices.

Correcting these issues involves proper handling of integer and floating-point division, ensuring files are not unintentionally overwritten, correctly iterating over dictionary items, revising array masking practices, and refining the detrending methods and checks."
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,There are no problems.,"There are issues with the given script that would impact the quality/viability of its outputs:

1. **Appending Paths to `sys.path`**:
   Adding each path from `os.environ['PATH']` to `sys.path` may lead to unexpected behavior and conflicts with existing modules. This approach is unnecessary and can cause the script to import the wrong modules.

2. **Output File Overwriting**:
   The script constructs an output file path using `outdir` and `infile`, but it does not check if a file with the same name already exists in the `outdir`. This can result in unintended overwriting of files.

3. **Exiting Without Cleanup**:
   The use of `sys.exit()` within the context of file handling (`with nc(infile) as fin`) can cause unflushed data and resource leaks. The script should allow for proper cleanup before exiting.

4. **Hardcoded Splitting Logic for File Name**:
   The script assumes that the crop name is always at the 4th position when splitting `infile` by underscores. This is a fragile assumption and can lead to incorrect behavior if the filename format changes.

5. **Unconditional Path Join with `sep`**:
   Constructing the output file path using `sep` might introduce portability issues across different operating systems. Using `os.path.join` would be more robust.

Apart from these issues, there doesn't seem to be any duplicating of work unintentionally or incorrect inclusion of entries.

Therefore, the statement ""there are no problems"" is not applicable here. Specific flaws have been identified that would indeed impact the script's outputs.","There are a few issues with the code that could impact the quality or viability of its outputs:

1. **Appending to System Path**:
   Adding each directory from the system's `PATH` environment variable to Python's `sys.path` can unintentionally include directories that contain unwanted or malicious code. It can also lead to name conflicts if there are scripts or modules with the same names in different directories within the `PATH`.

2. **Usage of `OptionParser`**:
   The `optparse` module is deprecated and has been replaced by `argparse`. While this does not directly impact the functionality, it may cause issues with future compatibility and maintenance.

3. **File Reference and Input Variables**:
   If the `params` file or the `infile` or `reffile` paths are incorrect or missing, the script will fail. However, the script does not include comprehensive error handling for cases where these files cannot be opened or parsed correctly, which could lead to unhandled exceptions.

4. **Hard-coded Indices**:
   The script assumes specific structures within the input files, such as hardcoding indices (e.g., `dtidx`, `mpidx`, and `sum_idx`). If the structure of the input files changes, this can lead to index errors or incorrect data being processed.

5. **Implicit `sys.exit`**:
   The use of `print` followed by `sys.exit` does not provide any meaningful exit status codes, which could be useful for debugging or automating the script within other systems. 

6. **Potential Overwriting of Output Files**:
   The variable `fn` is constructed without checking if a file with the same name already exists in the output directory. This could lead to unintended overwriting of files.

7. **Incrementing Time Values**:
   The script modifies `tref` and `tin` by adding values extracted from their respective units. If these values are not correctly formatted or if the units do not contain the expected strings, it could lead to incorrect time values.

8. **Mask Handling**:
   The way masked arrays are handled can be problematic. When populating the `yield_sim_common` and `yield_ref_common`, the mask from the input files is not preserved but rather a new mask is created based on the common aggregates. This could lead to loss of information about which values were originally masked.

Given these points, it's clear there are several problems that could affect the quality or viability of the outputs."
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,"There are a few issues with the provided Python script that could impact the quality and viability of its outputs:

1. **Overwriting Files:**
   - In the `aggregate` function, the `outputfile` is overwritten multiple times consecutively without backup. This could potentially lead to loss of data if an intermediate step fails or if the file is accessed elsewhere simultaneously.
   - Temporary files, such as `tmpfile1`, `tmpfile2`, and `tmpfile3`, are reused with the same names, and there is a potential risk that an error or an unexpected halt would leave the files in an inconsistent state. 

2. **Duplicating Work:**
   - During aggregation, the script removes and recreates temporary directories. If a previous script execution is interrupted or an error occurs, it could potentially remove necessary files in the next run.
   - The presence of many intermediate steps that read and write to files suggests there might be redundant data processing, especially if the same region is processed multiple times in different contexts.

3. **Handling of Fill Values:**
   - The script appears to handle fill values inconsistently. It sets them in some places but may not always check or clean them up properly in subsequent steps. This can lead to masked or erroneous data being processed.

4. **Potential Inclusion of NaNs:**
   - There is code to replace NaNs with zeros, which can be risky depending on what those NaNs represent. If the NaNs are legitimate, converting them to zero might distort the aggregated results.

5. **Printing Diagnostic Messages During Execution:**
   - Printing statements like `""Processing %s '%s'""` and `""Aggregation %s region %d/%d""` could clutter the output and may not be suitable for long-running batch processes. More structured logging might be appropriate, especially if this script is run in a production environment.

6. **Command Line Misinterpretation:**
   - It is possible for user parameters to affect command lines constructed with `run_nco()`. For example, if user input for file names or options contains unexpected characters, it could lead to command injection vulnerabilities or misinterpreted NCO commands.

7. **Hardcoded Paths and Parameters:**
   - Hardcoded paths, such as `mask_path`, directly reference the script's location and assume specific directory structures. If the script is moved or run in different environments, this might cause errors.

8. **Potential for Unsanitized Inputs:**
   - The `main()` function does not perform extensive sanitization on user inputs. There is the potential for invalid values or file paths being supplied, leading to runtime errors that are not gracefully handled.

Given these issues, the script would need revision to ensure reliability, robustness to invalid inputs, consistent data handling, and appropriate logging or error messages. Therefore, there are indeed problems that could impact its quality and viability.","Yes, there are several issues with the code that could impact the quality or viability of its outputs:

1. **Filename Overwrites**:
   - The code frequently writes to the same `outputfile` for various operations. This can cause potential data loss or incorrect results if the `outputfile` is overwritten before previous data is properly used or finalized.

2. **Command Injection Vulnerability**:
   - The `run_nco` function constructs command strings using string formatting, which can be manipulated if any of the input parameters are not properly validated or sanitized.

3. **Unintended Temporary File Overwrites**:
   - Temporary files (`tmpfile1`, `tmpfile2`, `tmpfile3`, etc.) are created with predictable filenames based on `region` and `level`, which means concurrent runs or the same region/level in loops could accidentally overwrite each other.

4. **Potential Unhandled Errors**:
   - There is potential for unhandled errors when working with files or datasets, such as `Dataset` might fail but are not caught, which would halt the script.

5. **Close File Descriptors**:
   - The file descriptor `devnull` used for redirecting output in the `run_nco` function is opened but never explicitly closed, which could lead to resource leakage.

6. **Removing Temporary Files Post Aggregation**:
   - The `mergedtmp` handling within the temporary files block has logic but might leave behind files if errors occur. The removal of these files seems inconsistent and could lead to leftover temporary files.

7. **Improper Handling of Aggregation Arguments**:
   - If either `args.agg` or `args.aggweight` is not provided, the script does not fail gracefully or provide informative messages thereafter.

8. **Invalid Function References**:
   - The code references `fitnc.variable`, `fitnc.long_name`, and `fitnc.units` which are not standard Dataset attributes. It should check if these exist before usage.

Addressing these problems would require revising parts of the script to handle file operations more robustly, ensuring that temporary and output files are not overwritten unintentionally, managing concurrent runs properly, sanitizing command inputs, and handling exceptions gracefully.","There are several issues with the given Python script that would impact the quality and viability of its outputs:

1. **Hardcoded Fill Value**:
   - In the `aggregate` function, the `prodvar` variable is created using `fill` value which is retrieved from the input NetCDF file, but later in `aggregate` and `process_var` functions, hardcoded fill value `1e20` is used which can lead to inconsistency in results.

2. **Outdated Print Statements**:
   - The script uses print statements without parentheses, which is not compatible with Python 3. E.g., `print ""Processing %s '%s'"" % (desc, var)`. This will cause a syntax error.

3. **Temporary File Management**:
   - The script creates various temporary files and directories such as `tempfile1`, `tempfile2`, `tempfile3`, and `agg.temp`. However, in some parts of the script, it appears to overwrite temporary files without checking their current state. This can lead to race conditions or overwrite issues if the function is called concurrently.

4. **Output File Overwriting**:
   - The output file `outputfile` in the `aggregate` function is repeatedly overwritten in subsequent `run_nco` calls, potentially leading to data loss or incorrect results. There should be checks or different filenames used for intermediate processing to avoid multiple overwrites.

5. **Inconsistent Handling of NaNs and Fill Values**:
   - In the `aggregate` function, the NaN values are replaced twice — once for the `variable` read from `inputfile` and then again for the `variable` read from `outputfile` later in the function. This redundant processing is unnecessary and could be optimized.

6. **Missing Data Masking**:
   - In the `process_var` function, the data is masked where all values are set to NaN or below 0 for `result` variable without appropriate context which might lead to loss of potentially valid data.

7. **Incorrect Conditional**:
   - Incorrect check in `main` function: `fit = False` is set before checking the equations. Even if `rfequation` or `irrequation` are valid, `fit` is set to False afterward, defeating the purpose of the initial fit check.

8. **Dead Code and Unused Imports**:
   - The import statement `import shutil` and command `shutil.copyfile(args.o, merged)` in `main` function is intended functionality but never properly utilized as `run_nco` directly operates after it. Additionally, the import and declaration of `mask_path` is not utilized correctly.

9. **Lack of Error Handling**:
   - Functions like `run_nco` execute shell commands without thorough error handling besides checking the exit code. Any exception raised during these commands would cause the program to terminate without useful debugging information.

Due to these issues, the script will encounter multiple problems during execution leading to potential data loss, incorrect results, or failing runs. These concerns must be addressed to ensure reliable output and correct functionality.
"
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,"There are a few issues with the provided Python script that could impact the quality and viability of its outputs:

1. **File overwrite without warning**: The script uses `copyfile` to copy the input file to the output file. If `outputfile` already exists, it will be overwritten without any warning, potentially causing data loss.

2. **IndexError if latitude or longitude differences are not consistent**: The code assumes that `diff(lats)[0]` and `diff(lons)[0]` will always succeed and be available. If the `lat` or `lon` arrays only contain a single value or their differences are not consistent, this will cause an `IndexError`.

3. **Incorrect assumption about the sizes of `lat` and `lon` arrays**: Using `resize` to generate `latd` and `lond` assumes that `lats` and `lons` arrays can be resized to a grid of the given dimensions without repetition or truncation, which might not always be the case depending on the input data.

4. **Hardcoded dimensions and variable attributes**: The script creates a dimension `'bnds'` and associated variable `bndvar` with fixed values and attributes, which may not be appropriate for all netCDF files or use cases. This hardcoding could cause compatibility issues.

5. **Handling masked arrays**: The script checks if `levvals` is a masked array and removes masks. However, if `levvals` is a masked array with all values masked, this could lead to issues where `len(levvals)` becomes 0, causing subsequent operations to fail.

6. **Inconsistent variable definition**: The `createVariable` method's parameters assume certain attributes, such as `zlib=True`, `shuffle=False`, and `complevel=9`. These assumptions may not always align with the needs of different netCDF files or data compression requirements.

There are no issues in terms of duplicating work unintentionally, including entries that should be excluded, or other aspects mentioned in the instructions.",There are no problems.,"There are several issues with the code that would impact the quality/viability of its outputs:

1. **Improper Default Argument Handling**: The `default` value for `--inputfile` and `--outputfile` is set to an empty string. This can potentially cause issues if the arguments are not provided since an empty string is not a valid filepath.

2. **Overwriting Existing Files**: The `copyfile(inputfile, outputfile)` function call will overwrite any existing `outputfile` without warning. This could unintentionally destroy existing data.

3. **Creating Dimensions with Existing Names**: If `lev` values for the dataset variables contain names that could conflict with existing dimensions, variables with the same names could inadvertently overwrite each other or cause conflicts.

4. **Handling Masked Arrays**: The code attempts to handle masked arrays but doesn't always guarantee that all possible masked cases are accounted for.

5. **Assumptions On Lat/Lon Variables**: The code assumes specific dimensions and naming conventions for `lat` and `lon`, which might not hold true for all netCDF files. This would cause the script to fail or produce incorrect results for files that do not adhere to these assumed constraints.

6. **No Error Handling on File Operations**: There are no try-except blocks to handle potential errors when opening files, reading variables, or writing to the output file, which can lead to unhandled exceptions and script termination.

These issues should be addressed to ensure the script produces reliable and correct outputs."
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,"There are a few potential issues in the provided Python script that could impact the quality or viability of its outputs:

1. **Incorrect Check for Zero in `VarRatio`**:
   The expression `if sim_mean * obs_std:` in `VarRatio.metric` does not correctly check for zeros in a numerical sense. Specifically, `if sim_mean * obs_std` will evaluate to `False` if `sim_mean * obs_std == 0.0`, but it may not handle small floating-point errors effectively.

2. **Partial and Ineffective Mask Checking**:
   The script often checks masks using `not sim.mask.all()`. However, `sim.mask` may not exist if the `sim` array is not a masked array. This would throw an error if `sim` or `obs` are regular numpy arrays without masks. It would be more robust to convert arrays into masked arrays by default or handle such conditions explicitly.

3. **Type Inconsistency for Division**:
   There is an implicit assumption that division will always yield a float (e.g., in `VarRatio.metric` and `HitRate.metric` methods). NumPy allows integer arrays, and Python 2 performs integer division by default, which can lead to incorrect calculations. Specifically:
   - In `VarRatio`, if both `sim.mean()` and `obs.std()` are integers, `sim.std() / obs_mean` could yield incorrect results.
   - In `HitRate`, if `ot.size` is zero (acting as a denominator), the division should be handled to avoid division-by-zero issues.

4. **Inadequate Error Handling for Empty `tc` in Extreme Metrics**:
   The metrics that deal with extremes (e.g., `RMSEExtreme`, `BiasExtreme`) calculate variables based on intersections (`tc`). If `tc` is empty but other calculations proceed, this might lead to silent errors or warnings, particularly in subsequent array operations.

5. **Code Duplication and Complexity**:
   Multiple metrics reimplement the logic to identify `st` and `ot` arrays, leading to duplicated code. While this is not an error per se, it does mean that any change to this logic needs to be made in multiple places, which increases the risk of inconsistencies.

In summary, the script has potential issues with how it handles masked arrays, zero checks, type consistency in division, and duplicated logic for extreme metrics. There are improvements needed to ensure robustness and avoid unexpected errors during execution.",There are no problems.,There are no problems.
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,There are no problems.,There are no problems.,There are no problems.
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,"The script appears to be well-structured, and there are no evident issues regarding overwriting filenames, duplicating work unintentionally, or inappropriately including entries that should be excluded. Each section of the code is clear in its purpose and operations.

Thus, there are no problems.","There is one potential issue with the code:

1. The line `rT = dT.copy()` creates a copy of the masked array `dT`. This means that `rT` initially has the same mask as `dT`, which is all `True` (since it's initialized with `ones(len(tsim))`). When `rT_c` is assigned values within the overlap period, their corresponding mask entries in `rT` are not updated to `False`, so they remain masked. As a result, `rT` may not correctly reflect the retrended values outside of the overlap period. This could lead to incorrect or incomplete outputs.

Other than that, there are no problems.",There are no problems.
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,"There are a few issues in the provided Python script that would impact the quality or viability of its outputs:

1. **Division by Zero:**
   - In the `MeanAverager` class, the `av` method performs division by `areas`. If `areas` has any zero values, this will result in a `ZeroDivisionError`.
   
2. **Mismatched Array Sizes in `sum`:**
   - In the `sum` method of the `Averager` class, the `chunksize` can be a float because of the division `sz / numchunks`. Later on, this value is used as an index range which requires an integer. This can lead to unexpected behavior or errors.
   
3. **Potential Mask Misalignment:**
   - In the `areas` method, the mask is recalculated and re-sized, but there could still be mismatches between the mask and the data size leading to incorrect masking.

4. **Index Handling in `for` Loops:**
   - In the `sum` method, the calculation of `endidx` does not guarantee that the index range will always cover all elements. Specifically, if `chunksize` is not an integer, this indexing might not cover the entire `aggvals` array.

5. **Logical Operations on Masks:**
   - The `combine` method of the `Averager` class uses logical operations on masked arrays. It's crucial to ensure that these operations are applied correctly; otherwise, the result might be incorrect due to masked elements being processed improperly.

6. **Unused Variables and Functions:**
   - The variables such as `nt, nlats, nlons` in some methods might not be used directly after being set which might cause confusion of `nt` representing `number of time steps` or some other meaning in different contexts.
   - The function `areas` in `SumAverager` might not be utilized in the intended manner or handle mask appropriately, causing unexpected results in calculations.

As a result, these issues could lead to runtime errors or incorrect results from the script.","There are several issues with the provided Python script that could impact the quality and viability of its outputs:

1. **Integer Division in `chunksize` Calculation**:
    - In the `sum` method, the line `chunksize = sz / numchunks` performs integer division in Python 2.x. In Python 3.x, this should be `chunksize = sz // numchunks` to ensure proper integer division. Dividing the size of `aggvals` by `numchunks` without integer division could result in non-integer values causing runtime errors or unexpected behavior in subsequent array indexing.

2. **Potential Mismatched Array Sizes**:
    - In the `sum` method, `maxchunksize` is calculated using `max(chunksize, chunksize + sz - chunksize * numchunks)`. Depending on the values of `sz` and `numchunks`, `maxchunksize` might not be evenly divisible by `chunksize`, which could lead to mismatches in array sizes and potentially out-of-bound index errors during array manipulation.

3. **Array Initialization and Masking**:
    - In the `sum` method, `vartmp.mask = ones(vartmp.shape)` implicitly assumes assignments of masks with arrays of integers rather than boolean values. Proper masking for `ma.masked_array` should ideally use boolean arrays.
   
4. **Manual Clearing of Arrays**:
    - There are multiple instances where arrays are manually cleared using array slicing and assignments. This might not be efficient for large arrays, and there is potential for errors if the shapes do not match perfectly. For instance, `aselect[:] = 0` and `vartmp[:] = 0` could be prone to shape mismatch issues.

5. **Mask Handling in `areas` Method**:
    - In the `areas` method, there is an attempt to mask zero-valued areas using `areas = ma.masked_where(areas == 0, areas)`, but there is no guarantee that zero values correctly represent invalid data. This could mask valid data inadvertently if zero is a valid result.

6. **Ensuring Shapes for Mask Arrays**:
    - In the same `areas` method, the line `areas.mask = resize(areas.mask, areas.shape)` modifies the mask array's shape without explicitly handling cases where resizing the mask may corrupt the intended mask structure. Modifying masks explicitly should be done with care to preserve data integrity.

7. **Combination of Aggregated Variables**:
    - In the `combineVar` method of `MeanAverager`, when summing and weighting `var1` and `var2` with their corresponding areas, there is potential for division by zero or incorrect masking if `totarea` has elements equal to zero. This is partially handled by masking `totarea`, but it implies silent masking of all zero entries without detailed logging or error messages.

Overall, these issues indicate that the script needs careful inspection for its handling of array operations, especially concerning dimensionality and masking to ensure output consistency.

Thus, the statement is, ""there are several problems"".","There are several issues in the provided Python script that could impact the quality and viability of its outputs:

1. **Integer Division in Python 3**: The line `chunksize = sz / numchunks` conducts a floating-point division, which might not be intended if the size of `chunksize` should be an integer. It can lead to inappropriate indexing later. Instead, `//` should be used for integer division if that is what is intended.

2. **Logical Operators on Arrays**: The use of logical operators such as `logical_and` and `logical_not` assume that the variables involved are already compatible for element-wise operations. If there are shape mismatches, it could lead to runtime errors.

3. **Handling of Masks and Masked Arrays**: The script is assuming that applying masks and manipulating them will always result in the expected shapes and non-empty masks. These assumptions are risky and may fail if input conditions slightly deviate.

4. **Incorrect Masking**: In the `areas` method, `resize` is used on the mask which might not produce the correct masking for a 2D or 3D array because `resize` reshapes the array to the specified shape which can reorder the elements incorrectly.

5. **Lack of Input Validation**: There's no validation check for the input arguments. Invalid inputs might lead to unexpected behaviors or errors.

6. **Namespace Pollution**: Importing too much into the namespace, such as all functions from `numpy`, can lead to naming conflicts and make it difficult to identify the source of certain functions.

7. **Potential Array Overwrites**: Instances where arrays are being re-used (e.g., `sumv`, `vartmp`) without proper re-initialization in each iteration might lead to unintentional data carry-over between iterations.

8. **Inefficient Use of Memory**: The methods, especially in their handling of masked arrays and weights, might not be memory-efficient for large data, even though the intention of chunking was to save memory.

Therefore, the script contains multiple issues that could affect the quality and correctness of its output."
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,"The provided script contains several issues that would impact the quality and viability of its outputs:

1. **Indexing Errors in `MovingAveDetrender`**:
   - In the `__winave` method, the calculation of `idx0` and `idx1` uses integer division. Since the resulting indices could be non-integers, this would raise an error. Additionally, integer division in cases like `(n / 2)` might lead to unexpected results depending on the Python version used.
   - When slicing `dave` with `dave[n / 2 : -(n / 2)]`, the indices are again computed using integer division which might cause incorrect slicing of the array.

2. **Edge Handling in `MovingAveDetrender`**:
   - The `line[2 : -2]` assignment makes an assumption about the size of the array `y`, which can lead to issues if `y` has fewer than 5 elements, potentially leading to out-of-bounds errors.

3. **Potential Division by Zero in `FFDDetrender`**:
   - In the `detrend` method of `FFDDetrender`, the computation `dy[1 :] = diff(y) / y[: -1]` might cause division-by-zero errors if any elements in `y` are zero.

4. **Missing Values Handling in `FFDRemoveDetrender`**:
   - Similar to the `FFDDetrender`, the `FFDRemoveDetrender` class could encounter division-by-zero errors in the line `ffd = diff(y) / y[: -1]` if any previous elements in `y` are zero.

5. **Initialization Error in `DetrenderWrapper`**:
   - The `mp` parameter in `DetrenderWrapper` only accepts the strings `'true'` or `'false'`, but the resulting `self.mp` is still a string, leading to a logical error when checking conditions. The script doesn't verify whether `self.mp` has those exact string values when later used to determine behavior, thus possibly leading to incorrect operation.

6. **Inconsistent Mean-Preserving Logic**:
   - In the `DetrenderWrapper` class's `detrend` method, both branches of the `self.mp` check use `dy.mean()` for adjustments, leading to subtly different behaviors that might not be consistent with the intended logic for mean-preserving detrending.

7. **Lack of Data Validation**:
   - The script assumes that the input `y` will always be a compatible numeric array and does not handle cases where `y` may contain non-numeric or inconsistent data types, potentially leading to crashes or incorrect results.

Given these points, there are indeed several problems with the script that need addressing to ensure reliable and correct operation.","The provided script contains a few issues that could impact the quality or viability of its outputs:

1. In `MovingAveDetrender` class, the `__winave` method:
    - The calculation of `numramp` should use integer division `//` instead of floating-point division `/`.
    - The range calculations `idx0` and `idx1` use integer division `//` for the number of points in the window to ensure idx0 and idx1 are integers.
    - Slicing `d[idx0: idx1]` should use integers, but `idx0` and `idx1` are calculated with floating-point division if `//` is not used.
    - The removal of left and right edges `dave[n / 2 : -(n / 2)]` should be integer slices, so again `//` should be used.

2. In `FFDDetrender` class:
    - Division by zero can occur in the `dy[1 :] = diff(y) / y[: -1]` line where differences are calculated. If any `y[:-1]` element is zero, it will cause a divide-by-zero error.

3. In `FFDRemoveDetrender` class:
    - Similarly, there's a potential for division by zero in the line `ffd = diff(y) / y[: -1]`.

4. In the `DetrenderWrapper` class:
    - The attribute `dt` should be set as an instance attribute (`self.dt = dt`) if it's intended to be used outside the constructor. Currently, it is only checked, but not stored.

These issues need to be addressed to ensure the script functions correctly under all possible input conditions.

So, there are indeed problems.","There are several issues with the script that could impact the quality and viability of its outputs:

1. **Potential division by zero**: In the FFDDetrender and FFDRemoveDetrender classes, the line `dy[1 :] = diff(y) / y[: -1]` may lead to a division by zero if any element in `y` is zero.

2. **Inconsistent index handling in MovingAveDetrender**: 
   - The length calculation and subsequent assignment can lead to an inconsistent state if `len(y) < 5`, potentially causing an index error or improper output.
   - `__winave` function, the line `dave = dave[n / 2 : -(n / 2)]` may result in unexpected behavior due to improper handling of the boundary conditions.

3. **Mask handling in polyfit**:
   - In `polytrend`, if `x2` or `y2` are completely masked, the function should handle this case gracefully. Currently, it assumes that `line[~mask]` will always have some elements, which might not be true.

4. **Removal of left and right edges in `__winave`**:
   - The expression `dave[n / 2 : -(n / 2)]` incorrectly slices the edges of the array `dave`. This could lead to an unexpected shape and could exclude valid data points.

5. **Non-integer indices for array slicing**:
   - Within the `__winave` method, slicing uses division which can result in floating-point numbers. Indices for array slicing must be integers.

6. **Incorrect handling in indices calculation within the window average**:
   - When calculating `idx0` and `idx1`, the use of integer division may be incorrect as they require manual rounding or handling integer conversion properly.

These issues will force the script to throw errors or produce incorrect results, affecting reliability and accuracy.

Therefore, the statement ""there are no problems"" does not apply here."
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,There are no problems.,There are no problems,"There are several potential issues with the code:

1. **Error Handling:** The code does not handle cases where variables might result in division by zero, except partially in `MSTransformer`. Not checking for zero variance in `VSTransformer ` and zero values while calculating percentiles in `QMTransformer` can lead to runtime exceptions.
   
2. **Percentile Calculation on Empty Arrays:** The method `__percentile` in `QMTransformer` does not handle cases where the input arrays can be empty. This can result in errors when calculating the scoreatpercentile.

3. **Masked Arrays Mismanagement:** In `MSTransformer`, the use of `isMaskedArray` on the result of `p.mean()` can be incorrect because `p.mean()` will return a scalar value, not an array, which can lead to logical errors.

4. **Handling of Arrays with Only Non-Numeric Values:** If any input array contains only non-numeric values such as `NaN` or `Inf`, some operations like mean or variance calculations would result in incorrect transformations or runtime warnings and errors.

5. **Modification of Copies:** The `transform` functions often copy arrays before modifying them; however, in some contexts, these copied arrays are not effectively used, such as in `pfull2 -= median(pfull2)` in `QMTransformer`. Depending on the behavior of these functions, this might lead to inappropriate results.

Overall, these issues can impact the quality and viability of the outputs by leading to errors, unexpected results, or invalid transformations when special cases are met in the inputs."
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,"The script has the following issues that could impact the quality or viability of its outputs:

1. **File Opening and Closing Error**: The netCDF file is opened but only closed later after processing. If an error occurs before the file closure (`f.close()`), this could potentially lead to a file handle leak.

2. **Modification of Input Lists**: The `varnames` list may be modified inside the constructor. If a mutable object is passed and is later used outside of this class, the changes made within the class will be reflected outside of it, which could lead to unexpected behavior.

3. **Variable Existence for Global Mask**: The code checks if `global` is in `varnames` and creates a synthetic global variable if it is specified but not present in the file. This logic is somewhat hidden and may not be immediately apparent to the user, leading to possible confusion about the contents of `self.dat`.

4. **Masked Array Handling**: When creating the global masked array, the `mask` parameter is set to zeros. If the intention is to have a fully valid mask, it should be all ones instead.

5. **Redundant Appending for 'global' Variable**: In the loop over `varnames`, there's redundant conditional logic checking if the variable is `global` again when much of this was already handled.

There are no problems with overwriting filenames, duplicating work unintentionally, or including entries that should be excluded based on the given script.

Therefore, the conclusion is that there are specific potential issues centered around file handling, variable handling, and logical conditions, which might impact the script more subtly but fundamentally in certain scenarios.",There are no problems.,There are no problems.
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,There are no problems.,"There are a few issues with the script that could impact the quality and viability of its outputs:

1. **Division by zero in `ScaleRetrender`**:
   If `R.mean()` is zero, there would be a division by zero error in the `ScaleRetrender` class.

2. **Initial condition assignment in `FFDRightRetrender`**:
   In the `FFDRightRetrender` class, there is no handling for the case where the rightmost point `o[-1]` might be masked or invalid, which might lead to unintended results.

3. **Incomplete masking logic in `FFDRightRetrender`**:
   The line `r.mask[i - 1] = True` assumes that `r` has a `mask` attribute, but there is no verification or creation of a masked array.

4. **Off-by-one error in `FFDRightRetrender`**:
   The loop `for i in range(-1, -len(r), -1)` in `FFDRightRetrender` does not include the first element when iterating. The correct range should be `for i in range(-1, -len(r) - 1, -1)`.

There are no other significant issues with overwriting filenames, duplicating work unintentionally, or including entries that should be excluded.",There are no problems.
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,"There are indeed some issues in the code that could impact the quality/viability of its outputs.

1. **Overwriting Filenames**:
   - Every time an instance of any of the classes (`AggregationFile`, `BiasCorrectFile`, `MultimetricsFile`, `ModelEnsembleFile`, `MultimetricsEnsembleFile`, `RescaledFile`) is created with the same filename, the content of the existing file (if any) will be overwritten. This could lead to loss of data if multiple instances are created with the same filename.

2. **Inconsistent Data Types for `time` Variable**:
   - In some classes, the `time` variable is written with the data type `'i4'` (32-bit integer). If the `time` data contains non-integer values or if the range exceeds the limit of a 32-bit integer, this might lead to incorrect data representation or overflow errors. This issue appears in `BiasCorrectFile`, `ModelEnsembleFile`, and `RescaledFile`.

3. **Inconsistent Use of `[:]` Assignment for Variables**:
   - The code uses the `[:]` notation to assign values to variables. However, if the dimensions or lengths of the variables do not match, this might lead to errors. Proper checks or handling should be implemented to ensure that the shapes of assigned data match the dimensions of the variables.

4. **Potential Conflicts with Variable Names**:
   - Given that these classes create and name variables in the netCDF files based on input parameters, there is a risk of naming conflicts if different instances or variables use the same names unintentionally. This can affect the integrity of the data within the files.

5. **Missing Error Handling**:
   - There is no error handling in place for file writing operations. Issues like file permission errors, insufficient disk space, or invalid data types are not accounted for, which could lead to unhandled exceptions and data loss.

To summarize, there are issues with potential overwriting of filenames, inconsistent data types for the `time` variable, potential conflicts with variable names, inconsistent use of data assignments, and missing error handling. Addressing these could improve reliability and data integrity.","There are several issues in the provided script that could impact the quality and viability of its outputs.

1. **Overwriting Files:**
   Each time an instance of any of the classes is created, the file specified by `filename` is opened in write mode (`'w'`). This will overwrite any existing file with the same name, potentially leading to loss of data.

2. **Duplication of Dimensions:**
   There are potential duplications of dimension names and variables that could result in conflicts within a single file. For instance, if multiple variables are added with the same dimension names but different lengths or attributes, this could cause data integrity issues.

3. **Fill Value Usage:**
   In the `append` method of `FileSpec`, a fill value of `1e20` is used arbitrarily for new variables. While this is not necessarily problematic, it might be too high and could conflict with the valid range of the actual data.

4. **Implicit Data Types and Handling:**
   The data type `'i4'` (32-bit integer) is used for variables like `aggs`, `time`, `scen`, `dt`, `mp`, `cr`, `irr`, etc. without checking if the input data (e.g., `aggs`, `time`, etc.) appropriately fits within this type. If floating-point data or data outside the 32-bit integer range is provided, it could lead to incorrect values or overflow errors.

5. **Unconventional Metadata Definitions:**
   The attributes `long_name` and `units` for some dimensions and variables might be unconventional and uninformative (e.g., using `mapping` as a unit). This might not comply with netCDF conventions, potentially causing issues when these files are used in standard tools or pipelines.

6. **Assumption on Input Arrays:**
   Code assumes that inputs like `aggs`, `time`, etc., are provided in a format that can be directly assigned to a netCDF variable. If the input arrays are not correctly formatted or of a different type (e.g., list instead of NumPy array or vice versa), this could lead to runtime errors.

7. **Descriptors and Notes:**
   Custom additions like `dtvar.note = 'detrend method'`, while allowed, are non-standard and could be misinterpreted if the netCDF files are used outside of the context of this script.

8. **Missing Error Handling:**
   There is no error handling for file operations or data assignments, which means if any error occurs (e.g., due to incorrect data type or file I/O issues), it could lead to an abrupt termination without graceful recovery or meaningful reporting.

There could be other smaller semantic issues, but the above points highlight the main potential problems in the script.","There are a few issues with the code that could impact the quality or viability of its outputs:

1. In `BiasCorrectFile`, `ModelEnsembleFile`, and `RescaledFile` classes, the time variable is adjusted (`timevar[:] = time - time[0]`), and its units attribute is set. This adjustment might be intended, but it changes the original time values.

2. The `dtype` (data type) for the `time` variable is set to `'i4'` (32-bit integer) in `BiasCorrectFile`, `ModelEnsembleFile`, `RescaledFile`, and other classes. If the `time` values are not integers or large (e.g., in floating-point years), this could result in loss of precision or incorrect values.

3. In `MultimetricsFile` and `MultimetricsEnsembleFile`, the `time_range` variable likely represents time ranges or periods. Using `'i4'` as the data type may not be appropriate, especially if the time ranges are not integers. Additionally, `'mapping'` is used as units, which might not be informative or accurate.

4. The classes `BiasCorrectFile`, `MultimetricsFile`, `ModelEnsembleFile`, and `MultimetricsEnsembleFile` create similar dimensions and variables (`dt`, `mp`, `cr`), with some shared attributes. Any inherent issues in the attribute assignments (e.g., coercing lists into strings) or overwriting files could propagate across multiple files.

5. The `units` attribute for the `scen`, `dt`, `mp`, and `cr` variables is set to `'mapping'`. The term `'mapping'` might not be clear, and accurate units should be specified if they're known, or omit them if there are none.

6. There is no checking for the existence of dimensions or variables before attempting to create them. This could lead to errors if other processes or scripts modify these files concurrently.

There are no problems with filename overwriting or including unintended entries. The file creation/append mechanisms seem correct within the described use cases.

Overall, while these issues might not always result in errors, they could lead to data precision loss, vague metadata, or subtle bugs, impacting data quality and usability."
1179,0,Scientific Data,41597,10.1038/s41597-019-0023-8,The Global Gridded Crop Model Intercomparison phase 1 simulation dataset,8,5,2019,https://github.com/RDCEP/ggcmi/tree/phase1,The data of the GGCMI phase 1 simulation set were produced by the individual modeling groups using different GGCMs. The source code of these models is subject to different distribution policies and needs to be requested from the individual groups. The source code of the central quality check as well as some general aggregation and data-processing scripts are available at .,2019-05-08,"There are several issues with the code that could impact the quality and viability of its outputs:

1. **Use of `iteritems()` and `items()` Method on Dictionaries**:
    - The `iteritems()` method is used in Python 2, while in Python 3 it has been replaced with `items()`. This code appears to be a mix of both Python 2 and Python 3 syntax.
    - `dict.keys()` and `dict.values()` return iterables in Python 3, not lists. Thus, expressions like `x.keys()[0]` will result in an error in Python 3.

2. **Filename Overwriting**:
    - There’s a possibility that filenames could be overwritten if the filenames generated in the `__plotHeatmap` method are not unique. The code might generate the same filename for different slices, especially when multiple slices produce the same `title` or `dim_val`.

3. **Unconditional Array Masking**:
    - The `__condition` method uses `masked_array` to mask all entries initially and only unmask specific slices. If the conditions are not met accurately, it might result in an incorrectly masked output array where more elements are masked than necessary.

4. **Presence of Placeholder Comments**:
    - There are TODO comments like `# TODO: make more generic` in the `__optimize` method that may indicate the code is incomplete and might not handle all metrics optimally or generically.

5. **Potential Index Errors with Dynamic List Indexing**:
    - In several places, the code dynamically accesses indices via list comprehensions and indexing logic (e.g., `varidx`, `dimidx`). If the index values are incorrect or out of range, this could lead to runtime exceptions.

6. **Mix of Formatting Methods in Strings**:
    - The code mixes old-style formatting (`%`) with new-style `.format()`, making it inconsistent and potentially error-prone.

Given these identified problems, it’s clear that the code has various issues that need to be addressed to ensure robust and reliable functionality.","There are several issues in the code that could impact the quality and viability of its outputs:

1. **Python 2 vs Python 3 Compatibility**:
   - Methods `.keys()`, `.values()`, `.items()`, and `.iteritems()` are used in a way that is compatible with Python 2, but not Python 3. In Python 3, `.keys()`, `.items()`, and `.values()` return views rather than lists, so explicit conversion might be necessary.
   - Specifically `dims.keys()[0]`, `c.iteritems()`, `a.iteritems()`, `o.iteritems()`, `cross.items()[0]`, `dims_out.itervalues()`, `cross.keys()[0]`, `dims_out.keys().index(var)` will cause compatibility issues with Python 3.

2. **Potential Variable Overwriting**:
   - The variable `arr` is reused within the condition: `if x_idx > y_idx: arr = data.T` in the `__plotHeatmap` method, which means it may not be always clear from where the data is coming in subsequent lines where `arr` is used. This isn't technically an error, but poor practice as it can lead to code that's harder to debug and maintain.

3. **Missing Initialization of `arr`**:
   - In the line `if x_idx > y_idx: arr = data.T` within the method `__plotHeatmap`, if `x_idx <= y_idx`, `arr` is not initialized, leading to a potential `UnboundLocalError` when `__ploth` is called with `arr`.

4. **Filename Construction for Saving Plots**:
   - In `__plotHeatmap`, generated filenames do not ensure uniqueness if plot dimensions and variable names are the same across different calls to the method, which could lead to overwriting of image files: `filename = '%s_x.%s_y.%s.%s' % (self.metric, x, y, self.fmt)`.

5. **Metric Condition Error Handling**:
   - If an unsupported metric is passed that isn't handled by `__ploth`'s `if self.metric` conditions, `zmin` and `zmax` will not be initialized, potentially causing an error.

6. **File Path Construction**:
   - Using `self.outdir + sep + ...` for constructing file paths may not work correctly on all platforms (e.g., it assumes that `sep` is the correct separator which might not be true on all OS).

Fixing these issues would require making several changes, including compatibility adjustments for Python 3, ensuring all variables are initialized properly, and handling file paths in a cross-platform manner.","There are several issues with the provided code that could impact the quality and viability of its outputs:

1. **Use of `iteritems()` and `keys()`**: The code uses `iteritems()` and `keys()`, which are available in Python 2 but not in Python 3. If run in a Python 3 environment, the code will raise an `AttributeError`. These should be replaced with `items()` and `list(dims.keys()).index(...)` respectively for compatibility with Python 3.

2. **Fixed Filename Construction**: The filenames are constructed using fixed patterns that could lead to filename collisions if the same `metric`, `x`, and `y` values are reused with different conditions, averages, or optimizations. This would overwrite previously generated files.

3. **`arr` Variable Not Defined in `__plotHeatmap`**: In the `else` block of `__plotHeatmap`, the `arr` variable is used without being defined, which will cause a `NameError`.

4. **Slice Assignment in `__condition` and `__average`**: There's a potential issue in the `__condition` and `__average` methods with using slices. These slices may not work as intended due to modifying list elements, which could lead to incorrect slicing behavior.

5. **Hardcoded Color Mapping**: In the `__ploth` method, the color map (`plt.pcolor`) is hardcoded, and it doesn't allow for different types of data visualization, which might lead to suboptimal visualization for different `self.metric` values.

6. **Potential Masking Logic Errors**: In the `__condition` and other methods, there is extensive use of masked arrays. There could be logic errors or inefficiencies in handling masked values, especially during operations that modify the shape or slice the masked array.

In summary, the code contains issues related to Python version compatibility, potential filename collisions, undefined variable usage, slicing logic, and hardcoded visualization parameters. These would lead to errors or unintended behavior in various situations."
1184,0,Scientific Data,41597,10.1038/s41597-019-0266-4,Travel times to hospitals in Australia,1,11,2019,https://github.com/sebbarb/times_to_hospitals_AU,The Python 3 code related to this project is available for download at .,2019-11-01,,,
