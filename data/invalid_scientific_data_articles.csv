0,0,0.1,0.1.1,0.1.1.1,0.1.1.1.1,0.1.1.1.1.1,0.1.1.1.1.1.1,Unnamed: 0
,,,,,,,0,Unnamed: 0
,,,,,,,Scientific Data,journalName
,,,,,,,41597,journalId
,,,,,,,10.1038/s41597-023-02842-4,doi
,,,,,,,"MarFERReT, an open-source, version-controlled reference library of marine microbial eukaryote functional genes",title
,,,,,,,21,pubDay
,,,,,,,12,pubMonth
,,,,,,,2023,pubYear
,,,,,,,"['https://github.com/armbrustlab/marferret', 'https://zenodo.org/records/10278540']",codeLink
,,,,,,,"Code, documentation, and tutorials for this project are available on the MarFERReT repository: . This repository has also been archived for the v.1.1 release and the archived code is available on Zenodo here: . Information on the software versions and parameters used in this publication are included in the MarFERReT containerized build on the repository and in the archived code.",codeText
,,,,,,,2023-12-21,pubDate
,,,,,,0,,
,,,,,,Scientific Data,,
,,,,,,41597,,
,,,,,,10.1038/s41597-023-02808-6,,
,,,,,,Labelled dataset for Ultra-Low Temperature Freezer to aid dynamic modelling & fault detection and diagnostics,,
,,,,,,9,,
,,,,,,12,,
,,,,,,2023,,
,,,,,,['https://lab.compute.dtu.dk/taohu/ult-freezers-labelled-dataset-sci-data.git'],,
,,,,,,The Python code for ETL pipelines is available on the open-access GitLab at .,,
,,,,,,2023-12-09,,
,,,,,0,,,
,,,,,Scientific Data,,,
,,,,,41597,,,
,,,,,10.1038/s41597-023-02808-6,,,
,,,,,Labelled dataset for Ultra-Low Temperature Freezer to aid dynamic modelling & fault detection and diagnostics,,,
,,,,,9,,,
,,,,,12,,,
,,,,,2023,,,
,,,,,['https://lab.compute.dtu.dk/taohu/ult-freezers-labelled-dataset-sci-data.git'],,,
,,,,,The Python code for ETL pipelines is available on the open-access GitLab at .,,,
,,,,,2023-12-09,,,
,,,,,0,,,
,,,,,Scientific Data,,,
,,,,,41597,,,
,,,,,10.1038/s41597-023-02706-x,,,
,,,,,Georeferenced dataset of maritime piracy in the Gulf of Guinea from 2010 to 2021,,,
,,,,,7,,,
,,,,,12,,,
,,,,,2023,,,
,,,,,"['https://github.com/ricardomourarpm/Gulf_of_Guinea_Piracy', 'https://colab.research.google.com/', 'https://jupyter.org/install.html']",,,
,,,,,"The code used to generate the interactive visualization of the attacks shown in Fig.  is provided in the convert.py (). To run the provided code, it is possible to run it locally using Python in a Jupyter Notebook or even use the Anaconda Distribution, or you can use it directly online using, e.g., the Google Colab (). The Anaconda Distribution () is usually a good choice since it includes Python, the Jupyter Notebook, and other commonly used scientific computing and data science packages.",,,
,,,,,2023-12-07,,,
,,,,,0,,,
,,,,,Scientific Data,,,
,,,,,41597,,,
,,,,,10.1038/s41597-023-02706-x,,,
,,,,,Georeferenced dataset of maritime piracy in the Gulf of Guinea from 2010 to 2021,,,
,,,,,7,,,
,,,,,12,,,
,,,,,2023,,,
,,,,,"['https://github.com/ricardomourarpm/Gulf_of_Guinea_Piracy', 'https://colab.research.google.com/', 'https://jupyter.org/install.html']",,,
,,,,,"The code used to generate the interactive visualization of the attacks shown in Fig.  is provided in the convert.py (). To run the provided code, it is possible to run it locally using Python in a Jupyter Notebook or even use the Anaconda Distribution, or you can use it directly online using, e.g., the Google Colab (). The Anaconda Distribution () is usually a good choice since it includes Python, the Jupyter Notebook, and other commonly used scientific computing and data science packages.",,,
,,,,,2023-12-07,,,
,,,,,0,,,
,,,,,Scientific Data,,,
,,,,,41597,,,
,,,,,10.1038/s41597-023-02809-5,,,
,,,,,A dataset of proteomic changes during human heat stress and heat acclimation,,,
,,,,,7,,,
,,,,,12,,,
,,,,,2023,,,
,,,,,['https://acclimation.statgen.org/'],,,
,,,,,The source code for this dataset is publicly available through the Figshare repository. The data may also be visualized with an interactive web visualization tool:,,,
,,,,,2023-12-07,,,
,,,,,0,,,
,,,,,Scientific Data,,,
,,,,,41597,,,
,,,,,10.1038/s41597-023-02798-5,,,
,,,,,A global land cover training dataset from 1984 to 2020,,,
,,,,,7,,,
,,,,,12,,,
,,,,,2023,,,
,,,,,"['https://github.com/parevalo/measures_collector', 'https://github.com/ma-friedl/GlanceFiltering', 'https://measures-glance.github.io/glance-grids/params', 'https://glance.earthengine.app/view/fulltstools', 'https://github.com/repository-preservation/lcmap-pyccd']",,,
,,,,,"We used open-source tools to ensure transparency and reproducibility of our research, including R (4.3.0), Python 3.6.7, and Google Earth Engine. Time series tools for training data collection are available on GitHub () as is the repository for filtering training data (). Custom continental definitions can be found at this repository: . Continuous Change Detection and Classification (CCDC) tools and applications can be found on Google Earth Engine () and python ().",,,
,,,,,2023-12-07,,,
,,,,,0,,,
,,,,,Scientific Data,,,
,,,,,41597,,,
,,,,,10.1038/s41597-023-02798-5,,,
,,,,,A global land cover training dataset from 1984 to 2020,,,
,,,,,7,,,
,,,,,12,,,
,,,,,2023,,,
,,,,,"['https://github.com/parevalo/measures_collector', 'https://github.com/ma-friedl/GlanceFiltering', 'https://measures-glance.github.io/glance-grids/params', 'https://glance.earthengine.app/view/fulltstools', 'https://github.com/repository-preservation/lcmap-pyccd']",,,
,,,,,"We used open-source tools to ensure transparency and reproducibility of our research, including R (4.3.0), Python 3.6.7, and Google Earth Engine. Time series tools for training data collection are available on GitHub () as is the repository for filtering training data (). Custom continental definitions can be found at this repository: . Continuous Change Detection and Classification (CCDC) tools and applications can be found on Google Earth Engine () and python ().",,,
,,,,,2023-12-07,,,
,,,,,0,,,
,,,,,Scientific Data,,,
,,,,,41597,,,
,,,,,10.1038/s41597-023-02757-0,,,
,,,,,The OREGANO knowledge graph for computational drug repurposing,,,
,,,,,6,,,
,,,,,12,,,
,,,,,2023,,,
,,,,,['https://gitub.u-bordeaux.fr/erias/oregano'],,,
,,,,,The code for the integration and the knowledge graph are available on the GitHub of the OREGANO project in the  folder ().,,,
,,,,,2023-12-06,,,
,,,,,0,,,
,,,,,Scientific Data,,,
,,,,,41597,,,
,,,,,10.1038/s41597-023-02659-1,,,
,,,,,Grammars Across Time Analyzed (GATA): a dataset of 52 languages,,,
,,,,,28,,,
,,,,,11,,,
,,,,,2023,,,
,,,,,"['https://doi.org/10.5281/zenodo.8250217', 'https://github.com/cldf-datasets/gata']",,,
,,,,,"The dataset is stored on Zenodo () and curated on Github (). The current release of the repository is Version 1.0.0 and was peer-reviewed in 2023. All data is available under a CC-BY 4.0 license. All scripts that have been used during the pre-processing of the data are made available within the repository. Specifically, Python-scripts were used after the manual annotation of the data for the standardization of all annotations, as well as for the aggregation of the individual spreadsheets. The script that was used for the conversion into CLDF is also part of this repository.",,,
,,,,,2023-11-28,,,
,,,,0,,,,
,,,,Scientific Data,,,,
,,,,41597,,,,
,,,,10.1038/s41597-023-02808-6,,,,
,,,,Labelled dataset for Ultra-Low Temperature Freezer to aid dynamic modelling & fault detection and diagnostics,,,,
,,,,9,,,,
,,,,12,,,,
,,,,2023,,,,
,,,,['https://lab.compute.dtu.dk/taohu/ult-freezers-labelled-dataset-sci-data.git'],,,,
,,,,The Python code for ETL pipelines is available on the open-access GitLab at .,,,,
,,,,2023-12-09,,,,
,,,,0,,,,
,,,,Scientific Data,,,,
,,,,41597,,,,
,,,,10.1038/s41597-023-02809-5,,,,
,,,,A dataset of proteomic changes during human heat stress and heat acclimation,,,,
,,,,7,,,,
,,,,12,,,,
,,,,2023,,,,
,,,,['https://acclimation.statgen.org/'],,,,
,,,,The source code for this dataset is publicly available through the Figshare repository. The data may also be visualized with an interactive web visualization tool:,,,,
,,,,2023-12-07,,,,
,,,,0,,,,
,,,,Scientific Data,,,,
,,,,41597,,,,
,,,,10.1038/s41597-023-02757-0,,,,
,,,,The OREGANO knowledge graph for computational drug repurposing,,,,
,,,,6,,,,
,,,,12,,,,
,,,,2023,,,,
,,,,['https://gitub.u-bordeaux.fr/erias/oregano'],,,,
,,,,The code for the integration and the knowledge graph are available on the GitHub of the OREGANO project in the  folder ().,,,,
,,,,2023-12-06,,,,
,,,,0,,,,
,,,,Scientific Data,,,,
,,,,41597,,,,
,,,,10.1038/s41597-023-02755-2,,,,
,,,,High-throughput density functional theory screening of double transition metal MXene precursors,,,,
,,,,25,,,,
,,,,11,,,,
,,,,2023,,,,
,,,,"['https://nanohub.org/tools/vaspingestor', 'https://github.com/katnykiel/vasp_ingestor']",,,,
,,,,"The DFT results database, code used to generate figures, and tool for ingesting new data into this database are available at . This code is also available at .",,,,
,,,,2023-11-25,,,,
,,,,0,,,,
,,,,Scientific Data,,,,
,,,,41597,,,,
,,,,10.1038/s41597-023-02756-1,,,,
,,,,A global daily evapotranspiration deficit index dataset for quantifying drought severity from 1979 to 2022,,,,
,,,,24,,,,
,,,,11,,,,
,,,,2023,,,,
,,,,"['https://github.com/XiaZhang1113/Daily-drought-index–DEDI', 'https://www.python.org/', 'https://code.mpimet.mpg.de/']",,,,
,,,,The code used to calculate the DEDI dataset is available via GitHub () under the MIT license. The scripts are written with the open-source Python language version 3.8.6 () and the Climate Data Operators (CDO) version 1.9.10 (). Any updates will be published on GitHub.,,,,
,,,,2023-11-24,,,,
,,,,0,,,,
,,,,Scientific Data,,,,
,,,,41597,,,,
,,,,10.1038/s41597-023-02756-1,,,,
,,,,A global daily evapotranspiration deficit index dataset for quantifying drought severity from 1979 to 2022,,,,
,,,,24,,,,
,,,,11,,,,
,,,,2023,,,,
,,,,"['https://github.com/XiaZhang1113/Daily-drought-index–DEDI', 'https://www.python.org/', 'https://code.mpimet.mpg.de/']",,,,
,,,,The code used to calculate the DEDI dataset is available via GitHub () under the MIT license. The scripts are written with the open-source Python language version 3.8.6 () and the Climate Data Operators (CDO) version 1.9.10 (). Any updates will be published on GitHub.,,,,
,,,,2023-11-24,,,,
,,,,0,,,,
,,,,Scientific Data,,,,
,,,,41597,,,,
,,,,10.1038/s41597-023-02682-2,,,,
,,,,Photos and rendered images of LEGO bricks,,,,
,,,,18,,,,
,,,,11,,,,
,,,,2023,,,,
,,,,['https://github.com/LegoSorter'],,,,
,,,,"Custom tools used to take photos, generate renders, annotate photos, and extract annotated bricks from the complete scene, including the trained neural networks, are publicly available through the Lego Sorter project and its repositories available at .",,,,
,,,,2023-11-18,,,,
,,,,0,,,,
,,,,Scientific Data,,,,
,,,,41597,,,,
,,,,10.1038/s41597-023-02673-3,,,,
,,,,Daylong acoustic recordings of grazing and rumination activities in dairy cows,,,,
,,,,8,,,,
,,,,11,,,,
,,,,2023,,,,
,,,,['https://gitlab.com/luciano.mrau/acoustic_dairy_cow_dataset'],,,,
,,,,"The code for automatically adjusting the timesteps of JM labels, computing the JM labels of the audio recordings and for technical validation is available at Gitlab (). All code was written in Python 3.8.10 and distributed under the MIT license. Small changes should be made to the scripts by specifying the path of the audio files of the execution environment.",,,,
,,,,2023-11-08,,,,
,,,,0,,,,
,,,,Scientific Data,,,,
,,,,41597,,,,
,,,,10.1038/s41597-023-02698-8,,,,
,,,,FIKElectricity: A Electricity Consumption Dataset from Three Restaurant Kitchens in Portugal,,,,
,,,,8,,,,
,,,,11,,,,
,,,,2023,,,,
,,,,"['https://github.com/feelab-info/eGaugeDataAcquisition', 'http://www.raspbian.org/', 'https://doi.org/10.17605/OSF.IO/K3G8N']",,,,
,,,,The code used to collect and store the individual consumption data is available at . The code was developed using Python3.6 and deployed on a Linux machine (Raspbian see )). The Python3 code to reproduce the examples presented in this paper is available on the dataset repository at .,,,,
,,,,2023-11-08,,,,
,,,,0,,,,
,,,,Scientific Data,,,,
,,,,41597,,,,
,,,,10.1038/s41597-023-02698-8,,,,
,,,,FIKElectricity: A Electricity Consumption Dataset from Three Restaurant Kitchens in Portugal,,,,
,,,,8,,,,
,,,,11,,,,
,,,,2023,,,,
,,,,"['https://github.com/feelab-info/eGaugeDataAcquisition', 'http://www.raspbian.org/', 'https://doi.org/10.17605/OSF.IO/K3G8N']",,,,
,,,,The code used to collect and store the individual consumption data is available at . The code was developed using Python3.6 and deployed on a Linux machine (Raspbian see )). The Python3 code to reproduce the examples presented in this paper is available on the dataset repository at .,,,,
,,,,2023-11-08,,,,
,,,,0,,,,
,,,,Scientific Data,,,,
,,,,41597,,,,
,,,,10.1038/s41597-023-02697-9,,,,
,,,,The EPOS multi-disciplinary Data Portal for integrated access to solid Earth science datasets,,,,
,,,,8,,,,
,,,,11,,,,
,,,,2023,,,,
,,,,"['https://epos-eu.github.io/epos-open-source/', 'https://github.com/epos-eu/SHAPEness-Metadata-Editor', 'https://github.com/epos-eu/EPOS-DCAT-AP']",,,,
,,,,"The code developed for the EPOS Data Portal system is available on the GitHub repository of EPOS . Other code that contributed to the development of the system is already available on such repository, as in the case of the SHAPEness metadata Editor () and the specifications for EPOS-DCAT-AP extension ().",,,,
,,,,2023-11-08,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02808-6,,,,,
,,,Labelled dataset for Ultra-Low Temperature Freezer to aid dynamic modelling & fault detection and diagnostics,,,,,
,,,9,,,,,
,,,12,,,,,
,,,2023,,,,,
,,,['https://lab.compute.dtu.dk/taohu/ult-freezers-labelled-dataset-sci-data.git'],,,,,
,,,The Python code for ETL pipelines is available on the open-access GitLab at .,,,,,
,,,2023-12-09,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02809-5,,,,,
,,,A dataset of proteomic changes during human heat stress and heat acclimation,,,,,
,,,7,,,,,
,,,12,,,,,
,,,2023,,,,,
,,,['https://acclimation.statgen.org/'],,,,,
,,,The source code for this dataset is publicly available through the Figshare repository. The data may also be visualized with an interactive web visualization tool:,,,,,
,,,2023-12-07,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02757-0,,,,,
,,,The OREGANO knowledge graph for computational drug repurposing,,,,,
,,,6,,,,,
,,,12,,,,,
,,,2023,,,,,
,,,['https://gitub.u-bordeaux.fr/erias/oregano'],,,,,
,,,The code for the integration and the knowledge graph are available on the GitHub of the OREGANO project in the  folder ().,,,,,
,,,2023-12-06,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02755-2,,,,,
,,,High-throughput density functional theory screening of double transition metal MXene precursors,,,,,
,,,25,,,,,
,,,11,,,,,
,,,2023,,,,,
,,,"['https://nanohub.org/tools/vaspingestor', 'https://github.com/katnykiel/vasp_ingestor']",,,,,
,,,"The DFT results database, code used to generate figures, and tool for ingesting new data into this database are available at . This code is also available at .",,,,,
,,,2023-11-25,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02756-1,,,,,
,,,A global daily evapotranspiration deficit index dataset for quantifying drought severity from 1979 to 2022,,,,,
,,,24,,,,,
,,,11,,,,,
,,,2023,,,,,
,,,"['https://github.com/XiaZhang1113/Daily-drought-index–DEDI', 'https://www.python.org/', 'https://code.mpimet.mpg.de/']",,,,,
,,,The code used to calculate the DEDI dataset is available via GitHub () under the MIT license. The scripts are written with the open-source Python language version 3.8.6 () and the Climate Data Operators (CDO) version 1.9.10 (). Any updates will be published on GitHub.,,,,,
,,,2023-11-24,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02756-1,,,,,
,,,A global daily evapotranspiration deficit index dataset for quantifying drought severity from 1979 to 2022,,,,,
,,,24,,,,,
,,,11,,,,,
,,,2023,,,,,
,,,"['https://github.com/XiaZhang1113/Daily-drought-index–DEDI', 'https://www.python.org/', 'https://code.mpimet.mpg.de/']",,,,,
,,,The code used to calculate the DEDI dataset is available via GitHub () under the MIT license. The scripts are written with the open-source Python language version 3.8.6 () and the Climate Data Operators (CDO) version 1.9.10 (). Any updates will be published on GitHub.,,,,,
,,,2023-11-24,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02682-2,,,,,
,,,Photos and rendered images of LEGO bricks,,,,,
,,,18,,,,,
,,,11,,,,,
,,,2023,,,,,
,,,['https://github.com/LegoSorter'],,,,,
,,,"Custom tools used to take photos, generate renders, annotate photos, and extract annotated bricks from the complete scene, including the trained neural networks, are publicly available through the Lego Sorter project and its repositories available at .",,,,,
,,,2023-11-18,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02673-3,,,,,
,,,Daylong acoustic recordings of grazing and rumination activities in dairy cows,,,,,
,,,8,,,,,
,,,11,,,,,
,,,2023,,,,,
,,,['https://gitlab.com/luciano.mrau/acoustic_dairy_cow_dataset'],,,,,
,,,"The code for automatically adjusting the timesteps of JM labels, computing the JM labels of the audio recordings and for technical validation is available at Gitlab (). All code was written in Python 3.8.10 and distributed under the MIT license. Small changes should be made to the scripts by specifying the path of the audio files of the execution environment.",,,,,
,,,2023-11-08,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02697-9,,,,,
,,,The EPOS multi-disciplinary Data Portal for integrated access to solid Earth science datasets,,,,,
,,,8,,,,,
,,,11,,,,,
,,,2023,,,,,
,,,"['https://epos-eu.github.io/epos-open-source/', 'https://github.com/epos-eu/SHAPEness-Metadata-Editor', 'https://github.com/epos-eu/EPOS-DCAT-AP']",,,,,
,,,"The code developed for the EPOS Data Portal system is available on the GitHub repository of EPOS . Other code that contributed to the development of the system is already available on such repository, as in the case of the SHAPEness metadata Editor () and the specifications for EPOS-DCAT-AP extension ().",,,,,
,,,2023-11-08,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02666-2,,,,,
,,,A dataset for benchmarking Neotropical anuran calls identification in passive acoustic monitoring,,,,,
,,,6,,,,,
,,,11,,,,,
,,,2023,,,,,
,,,"['https://doi.org/10.5281/zenodo.8342596', 'https://github.com/soundclim/anuraset']",,,,,
,,,"The dataset and the raw data are hosted in Zenodo  under the CC0 license. All the code for reproducing the experimental protocol, the building and preprocessing of the dataset, and the use of the baseline model are available in the repository  under the MIT license. We open the Python code to fast development of new deep learning models and experiments in Pytorch.",,,,,
,,,2023-11-06,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02652-8,,,,,
,,,FAIR EVA: Bringing institutional multidisciplinary repositories into the FAIR picture,,,,,
,,,4,,,,,
,,,11,,,,,
,,,2023,,,,,
,,,"['https://github.com/IFCA-Advanced-Computing/FAIR_eva', 'https://doi.org/10.20350/digitalCSIC/14559']",,,,,
,,,"The FAIR EVA source code developed in Python is fully open access (), a running instance can be found in fair.csic.es and in full detail at DIGITAL.CSIC [].",,,,,
,,,2023-11-04,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02669-z,,,,,
,,,Database of segmentations and surface models of bones of the entire lower body created from cadaver CT scans,,,,,
,,,3,,,,,
,,,11,,,,,
,,,2023,,,,,
,,,"['https://github.com/MCM-Fischer/VSDFullBodyBoneModels', 'https://doi.org/10.5281/zenodo.8316730']",,,,,
,,,The code used to create and analyze the datasets is openly accessible via  and versioned at Zenondo: .,,,,,
,,,2023-11-03,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02634-w,,,,,
,,,Land cover and forest health indicator datasets for central India using very-high resolution satellite data,,,,,
,,,25,,,,,
,,,10,,,,,
,,,2023,,,,,
,,,['https://doi.org/10.57760/sciencedb.10422/'],,,,,
,,,The code classifying land cover from PlanetScope imagery and deriving the BGI was written in Google Earth Engine. The JavaScript language to classify land covers from Planetscope imagery and derive the BGI from the land cover is available as the ‘Code’ text file from Science Data Bank at .,,,,,
,,,2023-10-25,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02650-w,,,,,
,,,A Large Finer-grained Affective Computing EEG Dataset,,,,,
,,,25,,,,,
,,,10,,,,,
,,,2023,,,,,
,,,['https://doi.org/10.7303/syn50614194'],,,,,
,,,All the codes used for the data pre-processing and the technical validation are publicly available together with the FACED datasets in Synapse (). The codes were developed in Python 3.10. These codes can be executed on Linux and Windows. All required packages are listed in the torch_ubuntu.yml and torch_win.yml files. The README file under the Code file provides a detailed explanation of the procedure to reproduce the validation results using the codes and data.,,,,,
,,,2023-10-25,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02316-7,,,,,
,,,Restructuring and serving web-accessible streamflow data from the NOAA National Water Model historic simulations,,,,,
,,,20,,,,,
,,,10,,,,,
,,,2023,,,,,
,,,['https://www.hydroshare.org/resource/84c2b029f97343a59d0739115d4087f1/'],,,,,
,,,"All code for data download and reformatting can be found in the appropriate USGS repository. The  R package is available on GitHub and the dataset is documented and published via HydroShare. All the data are currently open, and publicly available at this URL: .",,,,,
,,,2023-10-20,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02612-2,,,,,
,,,A benchmark dataset for machine learning in ecotoxicology,,,,,
,,,18,,,,,
,,,10,,,,,
,,,2023,,,,,
,,,['https://renkulab.io/gitlab/mltox/adore'],,,,,
,,,"The code used to load and process the input data and generate the output dataset was created and run in Python 3.9 and is made available on . The repository contains code on how to load the data, prepare it for modeling, ., create one-hot and multi-hot-encodings for categorical features, and apply the train-test-split for 5-fold cross-validation. A good starting point are the files in the folder  for random forests ( and ).",,,,,
,,,2023-10-18,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02599-w,,,,,
,,,Global database of cement production assets and upstream suppliers,,,,,
,,,13,,,,,
,,,10,,,,,
,,,2023,,,,,
,,,"['Spatial macrolocalisation model', 'Cement production types and capacity estimation models', 'Sentinel-2 cement assets deployment models']",,,,,
,,,The deployment phase code notebooks are available from the GitHub Spatial Finance repository: [1] ; [2] ; [3] .,,,,,
,,,2023-10-13,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02599-w,,,,,
,,,Global database of cement production assets and upstream suppliers,,,,,
,,,13,,,,,
,,,10,,,,,
,,,2023,,,,,
,,,"['Spatial macrolocalisation model', 'Cement production types and capacity estimation models', 'Sentinel-2 cement assets deployment models']",,,,,
,,,The deployment phase code notebooks are available from the GitHub Spatial Finance repository: [1] ; [2] ; [3] .,,,,,
,,,2023-10-13,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02599-w,,,,,
,,,Global database of cement production assets and upstream suppliers,,,,,
,,,13,,,,,
,,,10,,,,,
,,,2023,,,,,
,,,"['Spatial macrolocalisation model', 'Cement production types and capacity estimation models', 'Sentinel-2 cement assets deployment models']",,,,,
,,,The deployment phase code notebooks are available from the GitHub Spatial Finance repository: [1] ; [2] ; [3] .,,,,,
,,,2023-10-13,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02555-8,,,,,
,,,CherryChèvre: A fine-grained dataset for goat detection in natural environments,,,,,
,,,11,,,,,
,,,10,,,,,
,,,2023,,,,,
,,,['https://doi.org/10.57745/QEZBNA'],,,,,
,,,"The Python 3.10 scripts used for converting the VGG VIA csv format to YOLO format, as well as other scripts used for generating statistics presented in the article, are available at .",,,,,
,,,2023-10-11,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02583-4,,,,,
,,,The R package for DICOM to brain imaging data structure conversion,,,,,
,,,4,,,,,
,,,10,,,,,
,,,2023,,,,,
,,,"['https://github.com/bidsconvertr/bidsconvertr', 'https://bidsconvertr.github.io']",,,,,
,,,"All code is hosted freely and open-source on a GitHub repository, accessible via . The documentation is hosted on the GitHub page: .",,,,,
,,,2023-10-04,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02569-2,,,,,
,,,A synthetic data set to benchmark anti-money laundering methods,,,,,
,,,28,,,,,
,,,9,,,,,
,,,2023,,,,,
,,,"['https://sdv.dev', 'https://sdv.dev/SDV/user_guides/relational/hma1', 'https://scikit-learn.org', 'https://lightgbm.readthedocs.io/en/stable']",,,,,
,,,"All our simulations are made using version 0.14.1 of the SDV library (). We specifically employ the HMA1 model class using two tables as inputs: a primary table with alerts (see Table ) and a secondary table with transactions (see Table ). A demonstration by the SDV developers is available online (; using data different from ours). Due to confidentiality, we do not share our code that (i) transforms the raw data so that it can be fed to the HMA1 model class and (ii) re-transforms and adds noise to the simulated data. The data-providing bank felt that providing this code would reveal sensitive information about its internal setup and the real data. All our transformations are, however, described in detail in our subsection “Implementation: Pre- and Postprocessing.” Our machine learning experiments were conducted with version 1.1.3 of the Scikit-learn library () and version 3.3.3 of the LightGBM library ().",,,,,
,,,2023-09-28,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02569-2,,,,,
,,,A synthetic data set to benchmark anti-money laundering methods,,,,,
,,,28,,,,,
,,,9,,,,,
,,,2023,,,,,
,,,"['https://sdv.dev', 'https://sdv.dev/SDV/user_guides/relational/hma1', 'https://scikit-learn.org', 'https://lightgbm.readthedocs.io/en/stable']",,,,,
,,,"All our simulations are made using version 0.14.1 of the SDV library (). We specifically employ the HMA1 model class using two tables as inputs: a primary table with alerts (see Table ) and a secondary table with transactions (see Table ). A demonstration by the SDV developers is available online (; using data different from ours). Due to confidentiality, we do not share our code that (i) transforms the raw data so that it can be fed to the HMA1 model class and (ii) re-transforms and adds noise to the simulated data. The data-providing bank felt that providing this code would reveal sensitive information about its internal setup and the real data. All our transformations are, however, described in detail in our subsection “Implementation: Pre- and Postprocessing.” Our machine learning experiments were conducted with version 1.1.3 of the Scikit-learn library () and version 3.3.3 of the LightGBM library ().",,,,,
,,,2023-09-28,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02569-2,,,,,
,,,A synthetic data set to benchmark anti-money laundering methods,,,,,
,,,28,,,,,
,,,9,,,,,
,,,2023,,,,,
,,,"['https://sdv.dev', 'https://sdv.dev/SDV/user_guides/relational/hma1', 'https://scikit-learn.org', 'https://lightgbm.readthedocs.io/en/stable']",,,,,
,,,"All our simulations are made using version 0.14.1 of the SDV library (). We specifically employ the HMA1 model class using two tables as inputs: a primary table with alerts (see Table ) and a secondary table with transactions (see Table ). A demonstration by the SDV developers is available online (; using data different from ours). Due to confidentiality, we do not share our code that (i) transforms the raw data so that it can be fed to the HMA1 model class and (ii) re-transforms and adds noise to the simulated data. The data-providing bank felt that providing this code would reveal sensitive information about its internal setup and the real data. All our transformations are, however, described in detail in our subsection “Implementation: Pre- and Postprocessing.” Our machine learning experiments were conducted with version 1.1.3 of the Scikit-learn library () and version 3.3.3 of the LightGBM library ().",,,,,
,,,2023-09-28,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02569-2,,,,,
,,,A synthetic data set to benchmark anti-money laundering methods,,,,,
,,,28,,,,,
,,,9,,,,,
,,,2023,,,,,
,,,"['https://sdv.dev', 'https://sdv.dev/SDV/user_guides/relational/hma1', 'https://scikit-learn.org', 'https://lightgbm.readthedocs.io/en/stable']",,,,,
,,,"All our simulations are made using version 0.14.1 of the SDV library (). We specifically employ the HMA1 model class using two tables as inputs: a primary table with alerts (see Table ) and a secondary table with transactions (see Table ). A demonstration by the SDV developers is available online (; using data different from ours). Due to confidentiality, we do not share our code that (i) transforms the raw data so that it can be fed to the HMA1 model class and (ii) re-transforms and adds noise to the simulated data. The data-providing bank felt that providing this code would reveal sensitive information about its internal setup and the real data. All our transformations are, however, described in detail in our subsection “Implementation: Pre- and Postprocessing.” Our machine learning experiments were conducted with version 1.1.3 of the Scikit-learn library () and version 3.3.3 of the LightGBM library ().",,,,,
,,,2023-09-28,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02554-9,,,,,
,,,Multimodal video and IMU kinematic dataset on daily life activities using affordable devices,,,,,
,,,22,,,,,
,,,9,,,,,
,,,2023,,,,,
,,,"['https://doi.org/10.5281/zenodo.7681316', 'https://doi.org/10.5281/zenodo.7693096', 'https://github.com/twyncoder/vidimu-tools']",,,,,
,,,"The VIDIMU dataset () was built using the free tools  (v0.8) and  (v4.4). The VIDIMU-TOOLS code contains the Jupyter notebooks and Python scripts used for data conversion, data synchronization and checking the contents of the dataset to ensure its integrity. A first release of the VIDIMU-TOOLS project is accessible in Zenodo () and the latest version of the code can be found in GitHub ().",,,,,
,,,2023-09-22,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02554-9,,,,,
,,,Multimodal video and IMU kinematic dataset on daily life activities using affordable devices,,,,,
,,,22,,,,,
,,,9,,,,,
,,,2023,,,,,
,,,"['https://doi.org/10.5281/zenodo.7681316', 'https://doi.org/10.5281/zenodo.7693096', 'https://github.com/twyncoder/vidimu-tools']",,,,,
,,,"The VIDIMU dataset () was built using the free tools  (v0.8) and  (v4.4). The VIDIMU-TOOLS code contains the Jupyter notebooks and Python scripts used for data conversion, data synchronization and checking the contents of the dataset to ensure its integrity. A first release of the VIDIMU-TOOLS project is accessible in Zenodo () and the latest version of the code can be found in GitHub ().",,,,,
,,,2023-09-22,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02551-y,,,,,
,,,Multi-view emotional expressions dataset using 2D pose estimation,,,,,
,,,22,,,,,
,,,9,,,,,
,,,2023,,,,,
,,,['https://doi.org/10.5281/zenodo.8185369'],,,,,
,,,The MATLAB code for parsing the JSON file and processing the coordinates can be found at .,,,,,
,,,2023-09-22,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02489-1,,,,,
,,,A pseudoproxy emulation of the PAGES 2k database using a hierarchy of proxy system models,,,,,
,,,14,,,,,
,,,9,,,,,
,,,2023,,,,,
,,,"['https://doi.org/10.5281/zenodo.7652533', 'https://github.com/fzhu2e/paper-pseudoPAGES2k']",,,,,
,,,The Jupyter notebooks illustrating the usage of the pseudoPAGES2k dataset can be accessed at  or .,,,,,
,,,2023-09-14,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02524-1,,,,,
,,,Open access dataset integrating EEG and fNIRS during Stroop tasks,,,,,
,,,12,,,,,
,,,9,,,,,
,,,2023,,,,,
,,,"['https://github.com/Yaaaaaaaaabby/fNIRS-data-pre-processing-from-Zemeng-Chen.git', 'https://gitee.com/chen-zemeng/f-nirs-data-pre-processing-from-zemeng-chen.git']",,,,,
,,,Scripts to import the fNIRS raw data (.tdms file format) into MATLAB and fNIRS data processing code used above are available at  or . A user guide describing the basic situation and usage of the dataset is uploaded together with the code. There are two files in the zip file. The MATLAB code file named “process_fNIRS_EEG_Stroop” is used to pre-process the fNIRS data of a subject. The folder used for MATLAB to load the .tdms file format is named “Matlab_read_tdms_file”. Please add the folder to the MATLAB search path before loading the .tdms file format.,,,,,
,,,2023-09-12,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02477-5,,,,,
,,,A Quantum-Chemical Bonding Database for Solid-State Materials,,,,,
,,,11,,,,,
,,,9,,,,,
,,,2023,,,,,
,,,"['https://github.com/naik-aakash/lobster-database-paper-analysis-scripts', 'https://doi.org/10.5281/zenodo.8172527']",,,,,
,,,"The following program versions have been used in this study:  2022.11.7,  2023.3.10,  1.0.3,  4.1.0, and  5.4.4 for  and  computations using the workflow. For data validation and processing, we have used  2023.6.23 and  0.2.9. All the scripts used in this study, from starting the workflow, generating data records, reproducing technical validation plots, and ML model evaluations, can be accessed here:  ().",,,,,
,,,2023-09-11,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02531-2,,,,,
,,,AmeriFlux BASE data pipeline to support network growth and data sharing,,,,,
,,,11,,,,,
,,,9,,,,,
,,,2023,,,,,
,,,"['https://github.com/AMF-FLX/AMF-BASE-QAQC', 'https://doi.org/10.5281/zenodo.8250754']",,,,,
,,,The core Python-based BASE data-processing pipeline code is available under a modified BSD license at . The R-based code for generating the article’s figures is available at .,,,,,
,,,2023-09-11,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02514-3,,,,,
,,,Ontology for the Avida digital evolution platform,,,,,
,,,9,,,,,
,,,9,,,,,
,,,2023,,,,,
,,,['https://graphdb.fortunalab.org'],,,,,
,,,"[{'ext-link': [{'@xlink:href': 'https://gitlab.com/fortunalab/ontoavida', '@ext-link-type': 'uri', '#text': 'https://gitlab.com/fortunalab/ontoavida'}, {'@xlink:href': 'https://gitlab.com/fortunalab/ontoavida/-/blob/master/ontoavida.obo', '@ext-link-type': 'uri', '#text': 'https://gitlab.com/fortunalab/ontoavida/-/blob/master/ontoavida.obo'}, {'@xlink:href': 'https://gitlab.com/fortunalab/ontoavida/-/blob/master/ontoavida.owl', '@ext-link-type': 'uri', '#text': 'https://gitlab.com/fortunalab/ontoavida/-/blob/master/ontoavida.owl'}, {'@xlink:href': 'http://www.obofoundry.org/ontology/ontoavida.html', '@ext-link-type': 'uri', '#text': 'http://www.obofoundry.org/ontology/ontoavida.html'}, {'@xlink:href': 'http://creativecommons.org/licenses/by/4.0/', '@ext-link-type': 'uri', '#text': 'http://creativecommons.org/licenses/by/4.0/'}], '#text': 'The ontology is available in both OBO and OWL format from the GitLab repository () and can be found at  and . OntoAvida OBO and OWL files are also available from the OBO Foundry (). All files are available under the Creative Commons Attribution 4.0 International License () which allows for the copying, redistribution and adaption of the ontology for any purpose.'}, {'ext-link': [{'@xlink:href': 'https://github.com/rdflib/pyLODE', '@ext-link-type': 'uri', '#text': 'https://github.com/rdflib/pyLODE'}, {'@xlink:href': 'https://gitlab.com/fortunalab/pyLODE', '@ext-link-type': 'uri', '#text': 'https://gitlab.com/fortunalab/pyLODE'}, {'@xlink:href': 'https://owl.fortunalab.org/ontoavida', '@ext-link-type': 'uri', '#text': 'https://owl.fortunalab.org/ontoavida'}], 'xref': {'@rid': 'Fig4', '@ref-type': 'fig', '#text': '4'}, '#text': 'pyLODE () was used to obtain a user-friendly visualization of the ontology. pyLODE is based on the OWL Documentation Environment tool (LODE), implemented in Python, and used to generate human-readable HTML documents for OWL and RDF ontologies. We have customized the original pyLODE templates () to convert a scheme of OntoAvida, in a HTML file so that its classes, object properties, and datatype properties can be easily visualized (Fig.\xa0). The pyLODE file of OntoAvida is available at .'}]",,,,,
,,,2023-09-09,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02499-z,,,,,
,,,An RFI-suppressed SMOS L-band multi-angular brightness temperature dataset spanning over a decade (since 2010),,,,,
,,,8,,,,,
,,,9,,,,,
,,,2023,,,,,
,,,"['https://doi.org/10.11888/Terre.tpdc.300406', 'https://cstr.cn/18406.11.Terre.tpdc.300406', 'https://github.com/thimpeng/RFI-Suppressed_SMOS_L-band_multi-angular_TB_Refinement']",,,,,
,,,The software and codes for processing the collected data and for plotting the figures are conducted in Python 3.7 and included in the dataset at  or . And they are also available on GitHub: .,,,,,
,,,2023-09-08,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02499-z,,,,,
,,,An RFI-suppressed SMOS L-band multi-angular brightness temperature dataset spanning over a decade (since 2010),,,,,
,,,8,,,,,
,,,9,,,,,
,,,2023,,,,,
,,,"['https://doi.org/10.11888/Terre.tpdc.300406', 'https://cstr.cn/18406.11.Terre.tpdc.300406', 'https://github.com/thimpeng/RFI-Suppressed_SMOS_L-band_multi-angular_TB_Refinement']",,,,,
,,,The software and codes for processing the collected data and for plotting the figures are conducted in Python 3.7 and included in the dataset at  or . And they are also available on GitHub: .,,,,,
,,,2023-09-08,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02470-y,,,,,
,,,Applying FAIR4RS principles to develop an integrated modeling environment for the magnetic confinement fusion,,,,,
,,,7,,,,,
,,,9,,,,,
,,,2023,,,,,
,,,['https://doi.org/10.5281/zenodo.8098117'],,,,,
,,,"We use the ZENODO service to keep a persistent archive of FyDev code, with .",,,,,
,,,2023-09-07,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02473-9,,,,,
,,,Monitoring of carbon-water fluxes at Eurasian meteorological stations using random forest and remote sensing,,,,,
,,,7,,,,,
,,,9,,,,,
,,,2023,,,,,
,,,['https://doi.org/10.6084/m9.figshare.21510183.v2'],,,,,
,,,The code to generate the carbon-water flux datasets is available at figshare ().,,,,,
,,,2023-09-07,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02494-4,,,,,
,,,SECURES-Met: A European meteorological data set suitable for electricity modelling applications,,,,,
,,,7,,,,,
,,,9,,,,,
,,,2023,,,,,
,,,['https://doi.org/10.5281/zenodo.8108927'],,,,,
,,,The data are stored as ASCII text (csv) and no specific software is necessary to access the data. The production of the data was done with Python. These scripts are mainly data manipulation routines and do not contribute in processing the data further. To assure repeatability all scripts are available at Zenodo ().,,,,,
,,,2023-09-07,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02442-2,,,,,
,,,"Development of an integrated and inferenceable RDF database of glycan, pathogen and disease resources",,,,,
,,,6,,,,,
,,,9,,,,,
,,,2023,,,,,
,,,"['https://github.com/glycoinfo/GlycanBind/releases/tag/v1.0.2', 'https://zenodo.org/record/8072786']",,,,,
,,,All code to generate the SugarBind RDF resources and the generated RDF data files are available from the GitHub repository as a v1.0.2 release at . The entire Github repository for the v1.0.2 release is archived on Zenodo: .,,,,,
,,,2023-09-06,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02484-6,,,,,
,,,"2DeteCT - A large 2D expandable, trainable, experimental Computed Tomography dataset for machine learning",,,,,
,,,4,,,,,
,,,9,,,,,
,,,2023,,,,,
,,,"['https://github.com/mbkiss/2DeteCTcodes', 'www.astra-toolbox.com', 'www.astra-toolbox.com']",,,,,
,,,"Python scripts for loading, pre-processing and reconstructing the projection data in the way described above are published on GitHub: . They make use of the ASTRA toolbox, which is openly available on () or accessible as a conda package (). ASTRA is currently only fully supported for Windows and Linux. Installing it on Mac OS is possible but in the current state very involved and version-dependent. All reference reconstructions provided have been computed with the Python scripts. Furthermore, while the scripts allow for angular sub-sampling the projections and the reference reconstructions were computed with all projections as mentioned in the subsection “Reconstruction production” above.",,,,,
,,,2023-09-04,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02484-6,,,,,
,,,"2DeteCT - A large 2D expandable, trainable, experimental Computed Tomography dataset for machine learning",,,,,
,,,4,,,,,
,,,9,,,,,
,,,2023,,,,,
,,,"['https://github.com/mbkiss/2DeteCTcodes', 'www.astra-toolbox.com', 'www.astra-toolbox.com']",,,,,
,,,"Python scripts for loading, pre-processing and reconstructing the projection data in the way described above are published on GitHub: . They make use of the ASTRA toolbox, which is openly available on () or accessible as a conda package (). ASTRA is currently only fully supported for Windows and Linux. Installing it on Mac OS is possible but in the current state very involved and version-dependent. All reference reconstructions provided have been computed with the Python scripts. Furthermore, while the scripts allow for angular sub-sampling the projections and the reference reconstructions were computed with all projections as mentioned in the subsection “Reconstruction production” above.",,,,,
,,,2023-09-04,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02483-7,,,,,
,,,Standardised images of novel objects created with generative adversarial networks,,,,,
,,,2,,,,,
,,,9,,,,,
,,,2023,,,,,
,,,['https://artbreeder.com'],,,,,
,,,"Software to generate novel objects is available at . Code to perform data collection (i.e., run the online experiment) was created using jsPsych (version 7.2.1), and a modified version of the  extension (all available at). Code to extract the objective properties of each object, and to compile the subjective ratings from our online study, was written in Python, MATLAB and Julia respectively (available at).",,,,,
,,,2023-09-02,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02475-7,,,,,
,,,Development of global monthly dataset of CMIP6 climate variables for estimating evapotranspiration,,,,,
,,,26,,,,,
,,,8,,,,,
,,,2023,,,,,
,,,['https://pyeto.readthedocs.io/en/latest/'],,,,,
,,,"The code to produce the data was written using Python, PyCharm 2022.2.2. The code is available in pyeto ().",,,,,
,,,2023-08-26,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02417-3,,,,,
,,,Geometrical digital twins of the as-built microstructure of three-leaf stone masonry walls with laser scanning,,,,,
,,,10,,,,,
,,,8,,,,,
,,,2023,,,,,
,,,"['https://ibois-epfl.github.io/Cockroach-documentation/', 'https://github.com/ibois-epfl/collide', 'https://doi.org/10.5281/zenodo.7093710']",,,,,
,,,"The pipeline is based on the use of Rhinoceros3D and the Cockroach plugin which is available at . A tutorial for the implementation of the pipeline is freely available online. In this work, we used Rhinoceros3D Version 7 SR29. The collision analysis algorithm is available at: . The Python script used for the analysis of the collision data is available at: .",,,,,
,,,2023-08-10,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02417-3,,,,,
,,,Geometrical digital twins of the as-built microstructure of three-leaf stone masonry walls with laser scanning,,,,,
,,,10,,,,,
,,,8,,,,,
,,,2023,,,,,
,,,"['https://ibois-epfl.github.io/Cockroach-documentation/', 'https://github.com/ibois-epfl/collide', 'https://doi.org/10.5281/zenodo.7093710']",,,,,
,,,"The pipeline is based on the use of Rhinoceros3D and the Cockroach plugin which is available at . A tutorial for the implementation of the pipeline is freely available online. In this work, we used Rhinoceros3D Version 7 SR29. The collision analysis algorithm is available at: . The Python script used for the analysis of the collision data is available at: .",,,,,
,,,2023-08-10,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02433-3,,,,,
,,,ThoughtSource: A central hub for large language model reasoning data,,,,,
,,,8,,,,,
,,,8,,,,,
,,,2023,,,,,
,,,"['https://github.com/OpenBioLink/ThoughtSource', 'https://doi.org/10.5281/zenodo.8199390', 'https://doi.org/10.5281/zenodo.8199538']",,,,,
,,,"All code, data and tools are openly available at , a snapshot of the GitHub repository is archived at , and a snapshot of dataset contents is archived at . Our code and data are licensed under an MIT license, while data adapted from existing datasets are available under the licenses of their respective sources.",,,,,
,,,2023-08-08,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02433-3,,,,,
,,,ThoughtSource: A central hub for large language model reasoning data,,,,,
,,,8,,,,,
,,,8,,,,,
,,,2023,,,,,
,,,"['https://github.com/OpenBioLink/ThoughtSource', 'https://doi.org/10.5281/zenodo.8199390', 'https://doi.org/10.5281/zenodo.8199538']",,,,,
,,,"All code, data and tools are openly available at , a snapshot of the GitHub repository is archived at , and a snapshot of dataset contents is archived at . Our code and data are licensed under an MIT license, while data adapted from existing datasets are available under the licenses of their respective sources.",,,,,
,,,2023-08-08,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02339-0,,,,,
,,,Three-dimensional topology dataset of folded radar stratigraphy in northern Greenland,,,,,
,,,7,,,,,
,,,8,,,,,
,,,2023,,,,,
,,,"['https://gitlab.com/openpolarradar/opr', 'https://gitlab.com/openpolarradar/opr/-/wikis/home']",,,,,
,,,"The CReSIS toolbox used to process the MCoRDS RES data is available at , and the main documentation can be found at .",,,,,
,,,2023-08-07,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02339-0,,,,,
,,,Three-dimensional topology dataset of folded radar stratigraphy in northern Greenland,,,,,
,,,7,,,,,
,,,8,,,,,
,,,2023,,,,,
,,,"['https://gitlab.com/openpolarradar/opr', 'https://gitlab.com/openpolarradar/opr/-/wikis/home']",,,,,
,,,"The CReSIS toolbox used to process the MCoRDS RES data is available at , and the main documentation can be found at .",,,,,
,,,2023-08-07,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02432-4,,,,,
,,,"FracAtlas: A Dataset for Fracture Classification, Localization and Segmentation of Musculoskeletal Radiographs",,,,,
,,,5,,,,,
,,,8,,,,,
,,,2023,,,,,
,,,"['https://doi.org/10.6084/m9.figshare.22363012', 'https://github.com/XLR8-07/FracAtlas']",,,,,
,,,"The conversion of DICOM to JPEG image format was done using proprietary software of the X-ray machines from brands like Fujifilm and Philips hence they could not be made available. The mask annotations for segmentation were done using an open-source web tool named makedsense.ai. It was also used for generating VGG annotations from COCO format. As explained in the Methods section, the annotation conversion procedures from COCO to YOLO and YOLO to PASCAL VOC were performed using Python 3.10.1 on a Windows 11 operating system using ‘coco2yolo.ipynb’ and ‘yolo2voc.ipynb’. Both the Jupyter notebooks can be found inside the ‘Utility’ folder along with the dataset at Figshare (). The code used for technical validation can be accessed from (). There are 2 notebooks inside ‘notebooks’ under the root folder called ‘Train_8s.ipynb’ and ‘Prediction_8s.ipynb’. The ‘Train_8s.ipynb’ is used to train 2 models of ‘YOLO8s_seg’ and ‘YOLO8s’ variants targeted toward segmentation and localization tasks respectively. ‘Prediction_8s.ipynb’ is used to generate predictions out of the 2 aforementioned models and view the results.",,,,,
,,,2023-08-05,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02405-7,,,,,
,,,ReCANVo: A database of real-world communicative and affective nonverbal vocalizations,,,,,
,,,5,,,,,
,,,8,,,,,
,,,2023,,,,,
,,,['https://doi.org/10.5281/zenodo.5786859'],,,,,
,,,We used the Python programming language for the data processing described above. Volume segmentation was implemented using the  libary. The label assignment algorithm is summarized in Fig. . The code is available as part of our dataset in Zenodo: .,,,,,
,,,2023-08-05,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02409-3,,,,,
,,,DIPS-Plus: The enhanced database of interacting protein structures for interface prediction,,,,,
,,,3,,,,,
,,,8,,,,,
,,,2023,,,,,
,,,"['Zenodo', 'GitHub', 'GitHub', 'DOI']",,,,,
,,,"Preprocessed data for DIPS-Plus as well as its associated source code and instructions for data processing and reproducibility can be found on  and , respectively. The GitHub instructions illustrate how users can install the Python programming language and build an Anaconda virtual environment containing the software dependencies required to preprocess and analyze DIPS-Plus using the provided Python scripts. Lastly, the GitHub instructions show users how to run such scripts and the order in which to do so to successfully rebuild DIPS-Plus from scratch, to featurize a given PDB file, or to train new machine learning models (e.g., NeiA) for protein interface prediction. For provenance, the original DIPS dataset’s source code can also be found on , along with a corresponding .",,,,,
,,,2023-08-03,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02409-3,,,,,
,,,DIPS-Plus: The enhanced database of interacting protein structures for interface prediction,,,,,
,,,3,,,,,
,,,8,,,,,
,,,2023,,,,,
,,,"['Zenodo', 'GitHub', 'GitHub', 'DOI']",,,,,
,,,"Preprocessed data for DIPS-Plus as well as its associated source code and instructions for data processing and reproducibility can be found on  and , respectively. The GitHub instructions illustrate how users can install the Python programming language and build an Anaconda virtual environment containing the software dependencies required to preprocess and analyze DIPS-Plus using the provided Python scripts. Lastly, the GitHub instructions show users how to run such scripts and the order in which to do so to successfully rebuild DIPS-Plus from scratch, to featurize a given PDB file, or to train new machine learning models (e.g., NeiA) for protein interface prediction. For provenance, the original DIPS dataset’s source code can also be found on , along with a corresponding .",,,,,
,,,2023-08-03,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02409-3,,,,,
,,,DIPS-Plus: The enhanced database of interacting protein structures for interface prediction,,,,,
,,,3,,,,,
,,,8,,,,,
,,,2023,,,,,
,,,"['Zenodo', 'GitHub', 'GitHub', 'DOI']",,,,,
,,,"Preprocessed data for DIPS-Plus as well as its associated source code and instructions for data processing and reproducibility can be found on  and , respectively. The GitHub instructions illustrate how users can install the Python programming language and build an Anaconda virtual environment containing the software dependencies required to preprocess and analyze DIPS-Plus using the provided Python scripts. Lastly, the GitHub instructions show users how to run such scripts and the order in which to do so to successfully rebuild DIPS-Plus from scratch, to featurize a given PDB file, or to train new machine learning models (e.g., NeiA) for protein interface prediction. For provenance, the original DIPS dataset’s source code can also be found on , along with a corresponding .",,,,,
,,,2023-08-03,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02409-3,,,,,
,,,DIPS-Plus: The enhanced database of interacting protein structures for interface prediction,,,,,
,,,3,,,,,
,,,8,,,,,
,,,2023,,,,,
,,,"['Zenodo', 'GitHub', 'GitHub', 'DOI']",,,,,
,,,"Preprocessed data for DIPS-Plus as well as its associated source code and instructions for data processing and reproducibility can be found on  and , respectively. The GitHub instructions illustrate how users can install the Python programming language and build an Anaconda virtual environment containing the software dependencies required to preprocess and analyze DIPS-Plus using the provided Python scripts. Lastly, the GitHub instructions show users how to run such scripts and the order in which to do so to successfully rebuild DIPS-Plus from scratch, to featurize a given PDB file, or to train new machine learning models (e.g., NeiA) for protein interface prediction. For provenance, the original DIPS dataset’s source code can also be found on , along with a corresponding .",,,,,
,,,2023-08-03,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02340-7,,,,,
,,,"A pipeline to further enhance quality, integrity and reusability of the NCCID clinical data",,,,,
,,,27,,,,,
,,,7,,,,,
,,,2023,,,,,
,,,['https://gitlab.developers.cam.ac.uk/maths/cia/covid-19-projects/nccidxclean'],,,,,
,,,"Our extended cleaning pipeline  is publicly available and can be accessed on GitLab:  with accompanying documentation available on the project website (package version at the time of publication: v1.0). The package will automatically download the necessary packages and requirements during installation, including the original NCCID cleaning pipeline upon which our pipeline builds. The python package is independent of the operating system and allows replication of our results using the command line interface, python scripts, or the included Jupyter notebooks. For full analysis, the  parameter can be set to ‘all_with_original’, returning all possible data features. Additional code is provided to allow for the original NHSx pipeline to be run using a single command in the command line. The data from the NHSx pipeline may be then used in the analysis subpackage to generate all figures and numerical results found in this work. An additional subpackage  is included for exploratory data analysis of the final cleaned data. Full step-by-step guidance and further details of the package are provided on the project’s website. .",,,,,
,,,2023-07-27,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02258-0,,,,,
,,,The Translational Data Catalog - discoverable biomedical datasets,,,,,
,,,20,,,,,
,,,7,,,,,
,,,2023,,,,,
,,,['https://datacatalog.elixir-luxembourg.org/'],,,,,
,,,"[{'ext-link': {'@xlink:href': 'https://github.com/FAIRplus/imi-data-catalogue', '@ext-link-type': 'uri', '#text': 'https://github.com/FAIRplus/imi-data-catalogue'}, '#text': 'All Data Catalog code is available in a dedicated repository of the FAIRplus Github organisation, at . The repository includes full documentation on how to deploy a stand-alone version of the Data Catalog.'}, {'ext-link': {'@xlink:href': 'https://github.com/datatagsuite/schema', '@ext-link-type': 'uri', '#text': 'https://github.com/datatagsuite/schema'}, '#text': 'The DATS model is available on Github at .'}]",,,,,
,,,2023-07-20,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02373-y,,,,,
,,,Single-nucleus chromatin landscapes during zebrafish early embryogenesis,,,,,
,,,19,,,,,
,,,7,,,,,
,,,2023,,,,,
,,,['https://figshare.com/articles/dataset/Code/22121171'],,,,,
,,,The codes used to analyze the data in this study were available online ().,,,,,
,,,2023-07-19,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02366-x,,,,,
,,,"Atomic structures, conformers and thermodynamic properties of 32k atmospheric molecules",,,,,
,,,12,,,,,
,,,7,,,,,
,,,2023,,,,,
,,,"['https://github.com/Supervitux/COSMO_on_Merlin', 'http://geckoa.lisa.u-pec.fr/', 'https://www.3ds.com/', 'https://merlin.readthedocs.io/en/latest/index.html#']",,,,,
,,,Custom code written for data generation mainly consists of scripts for pre- and postprocessing steps linking together the software mentioned below. These scripts are executed through a Merlin workflow. All these scripts are publicly available in a GitHub repository: . GECKO-A is available at their website . COSMO 4.3 and COSMO 2021 and their licenses were purchased from Dassault Systemes (). We provide our custom COSMO jobtemplate (( in the repository. Merlin version 1.7.5 is freely available from .,,,,,
,,,2023-07-12,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02366-x,,,,,
,,,"Atomic structures, conformers and thermodynamic properties of 32k atmospheric molecules",,,,,
,,,12,,,,,
,,,7,,,,,
,,,2023,,,,,
,,,"['https://github.com/Supervitux/COSMO_on_Merlin', 'http://geckoa.lisa.u-pec.fr/', 'https://www.3ds.com/', 'https://merlin.readthedocs.io/en/latest/index.html#']",,,,,
,,,Custom code written for data generation mainly consists of scripts for pre- and postprocessing steps linking together the software mentioned below. These scripts are executed through a Merlin workflow. All these scripts are publicly available in a GitHub repository: . GECKO-A is available at their website . COSMO 4.3 and COSMO 2021 and their licenses were purchased from Dassault Systemes (). We provide our custom COSMO jobtemplate (( in the repository. Merlin version 1.7.5 is freely available from .,,,,,
,,,2023-07-12,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02366-x,,,,,
,,,"Atomic structures, conformers and thermodynamic properties of 32k atmospheric molecules",,,,,
,,,12,,,,,
,,,7,,,,,
,,,2023,,,,,
,,,"['https://github.com/Supervitux/COSMO_on_Merlin', 'http://geckoa.lisa.u-pec.fr/', 'https://www.3ds.com/', 'https://merlin.readthedocs.io/en/latest/index.html#']",,,,,
,,,Custom code written for data generation mainly consists of scripts for pre- and postprocessing steps linking together the software mentioned below. These scripts are executed through a Merlin workflow. All these scripts are publicly available in a GitHub repository: . GECKO-A is available at their website . COSMO 4.3 and COSMO 2021 and their licenses were purchased from Dassault Systemes (). We provide our custom COSMO jobtemplate (( in the repository. Merlin version 1.7.5 is freely available from .,,,,,
,,,2023-07-12,,,,,
,,,0,,,,,
,,,Scientific Data,,,,,
,,,41597,,,,,
,,,10.1038/s41597-023-02318-5,,,,,
,,,Helicopter-borne RGB orthomosaics and photogrammetric digital elevation models from the MOSAiC Expedition,,,,,
,,,3,,,,,
,,,7,,,,,
,,,2023,,,,,
,,,['https://gitlab.com/mosaic12/orthomosaics'],,,,,
,,,All processing developed within this study is wrapped in a python environment and is available at . For calculating image footprint locations we used the  python package. Note that part of the code is based on the commercial Agisoft Metashape software requiring licensing.,,,,,
,,,2023-07-03,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02808-6,,,,,,
,,Labelled dataset for Ultra-Low Temperature Freezer to aid dynamic modelling & fault detection and diagnostics,,,,,,
,,9,,,,,,
,,12,,,,,,
,,2023,,,,,,
,,['https://lab.compute.dtu.dk/taohu/ult-freezers-labelled-dataset-sci-data.git'],,,,,,
,,The Python code for ETL pipelines is available on the open-access GitLab at .,,,,,,
,,2023-12-09,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02809-5,,,,,,
,,A dataset of proteomic changes during human heat stress and heat acclimation,,,,,,
,,7,,,,,,
,,12,,,,,,
,,2023,,,,,,
,,['https://acclimation.statgen.org/'],,,,,,
,,The source code for this dataset is publicly available through the Figshare repository. The data may also be visualized with an interactive web visualization tool:,,,,,,
,,2023-12-07,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02757-0,,,,,,
,,The OREGANO knowledge graph for computational drug repurposing,,,,,,
,,6,,,,,,
,,12,,,,,,
,,2023,,,,,,
,,['https://gitub.u-bordeaux.fr/erias/oregano'],,,,,,
,,The code for the integration and the knowledge graph are available on the GitHub of the OREGANO project in the  folder ().,,,,,,
,,2023-12-06,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02755-2,,,,,,
,,High-throughput density functional theory screening of double transition metal MXene precursors,,,,,,
,,25,,,,,,
,,11,,,,,,
,,2023,,,,,,
,,"['https://nanohub.org/tools/vaspingestor', 'https://github.com/katnykiel/vasp_ingestor']",,,,,,
,,"The DFT results database, code used to generate figures, and tool for ingesting new data into this database are available at . This code is also available at .",,,,,,
,,2023-11-25,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02756-1,,,,,,
,,A global daily evapotranspiration deficit index dataset for quantifying drought severity from 1979 to 2022,,,,,,
,,24,,,,,,
,,11,,,,,,
,,2023,,,,,,
,,"['https://github.com/XiaZhang1113/Daily-drought-index–DEDI', 'https://www.python.org/', 'https://code.mpimet.mpg.de/']",,,,,,
,,The code used to calculate the DEDI dataset is available via GitHub () under the MIT license. The scripts are written with the open-source Python language version 3.8.6 () and the Climate Data Operators (CDO) version 1.9.10 (). Any updates will be published on GitHub.,,,,,,
,,2023-11-24,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02756-1,,,,,,
,,A global daily evapotranspiration deficit index dataset for quantifying drought severity from 1979 to 2022,,,,,,
,,24,,,,,,
,,11,,,,,,
,,2023,,,,,,
,,"['https://github.com/XiaZhang1113/Daily-drought-index–DEDI', 'https://www.python.org/', 'https://code.mpimet.mpg.de/']",,,,,,
,,The code used to calculate the DEDI dataset is available via GitHub () under the MIT license. The scripts are written with the open-source Python language version 3.8.6 () and the Climate Data Operators (CDO) version 1.9.10 (). Any updates will be published on GitHub.,,,,,,
,,2023-11-24,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02682-2,,,,,,
,,Photos and rendered images of LEGO bricks,,,,,,
,,18,,,,,,
,,11,,,,,,
,,2023,,,,,,
,,['https://github.com/LegoSorter'],,,,,,
,,"Custom tools used to take photos, generate renders, annotate photos, and extract annotated bricks from the complete scene, including the trained neural networks, are publicly available through the Lego Sorter project and its repositories available at .",,,,,,
,,2023-11-18,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02673-3,,,,,,
,,Daylong acoustic recordings of grazing and rumination activities in dairy cows,,,,,,
,,8,,,,,,
,,11,,,,,,
,,2023,,,,,,
,,['https://gitlab.com/luciano.mrau/acoustic_dairy_cow_dataset'],,,,,,
,,"The code for automatically adjusting the timesteps of JM labels, computing the JM labels of the audio recordings and for technical validation is available at Gitlab (). All code was written in Python 3.8.10 and distributed under the MIT license. Small changes should be made to the scripts by specifying the path of the audio files of the execution environment.",,,,,,
,,2023-11-08,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02697-9,,,,,,
,,The EPOS multi-disciplinary Data Portal for integrated access to solid Earth science datasets,,,,,,
,,8,,,,,,
,,11,,,,,,
,,2023,,,,,,
,,"['https://epos-eu.github.io/epos-open-source/', 'https://github.com/epos-eu/SHAPEness-Metadata-Editor', 'https://github.com/epos-eu/EPOS-DCAT-AP']",,,,,,
,,"The code developed for the EPOS Data Portal system is available on the GitHub repository of EPOS . Other code that contributed to the development of the system is already available on such repository, as in the case of the SHAPEness metadata Editor () and the specifications for EPOS-DCAT-AP extension ().",,,,,,
,,2023-11-08,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02634-w,,,,,,
,,Land cover and forest health indicator datasets for central India using very-high resolution satellite data,,,,,,
,,25,,,,,,
,,10,,,,,,
,,2023,,,,,,
,,['https://doi.org/10.57760/sciencedb.10422/'],,,,,,
,,The code classifying land cover from PlanetScope imagery and deriving the BGI was written in Google Earth Engine. The JavaScript language to classify land covers from Planetscope imagery and derive the BGI from the land cover is available as the ‘Code’ text file from Science Data Bank at .,,,,,,
,,2023-10-25,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02650-w,,,,,,
,,A Large Finer-grained Affective Computing EEG Dataset,,,,,,
,,25,,,,,,
,,10,,,,,,
,,2023,,,,,,
,,['https://doi.org/10.7303/syn50614194'],,,,,,
,,All the codes used for the data pre-processing and the technical validation are publicly available together with the FACED datasets in Synapse (). The codes were developed in Python 3.10. These codes can be executed on Linux and Windows. All required packages are listed in the torch_ubuntu.yml and torch_win.yml files. The README file under the Code file provides a detailed explanation of the procedure to reproduce the validation results using the codes and data.,,,,,,
,,2023-10-25,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02316-7,,,,,,
,,Restructuring and serving web-accessible streamflow data from the NOAA National Water Model historic simulations,,,,,,
,,20,,,,,,
,,10,,,,,,
,,2023,,,,,,
,,['https://www.hydroshare.org/resource/84c2b029f97343a59d0739115d4087f1/'],,,,,,
,,"All code for data download and reformatting can be found in the appropriate USGS repository. The  R package is available on GitHub and the dataset is documented and published via HydroShare. All the data are currently open, and publicly available at this URL: .",,,,,,
,,2023-10-20,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02612-2,,,,,,
,,A benchmark dataset for machine learning in ecotoxicology,,,,,,
,,18,,,,,,
,,10,,,,,,
,,2023,,,,,,
,,['https://renkulab.io/gitlab/mltox/adore'],,,,,,
,,"The code used to load and process the input data and generate the output dataset was created and run in Python 3.9 and is made available on . The repository contains code on how to load the data, prepare it for modeling, ., create one-hot and multi-hot-encodings for categorical features, and apply the train-test-split for 5-fold cross-validation. A good starting point are the files in the folder  for random forests ( and ).",,,,,,
,,2023-10-18,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02599-w,,,,,,
,,Global database of cement production assets and upstream suppliers,,,,,,
,,13,,,,,,
,,10,,,,,,
,,2023,,,,,,
,,"['Spatial macrolocalisation model', 'Cement production types and capacity estimation models', 'Sentinel-2 cement assets deployment models']",,,,,,
,,The deployment phase code notebooks are available from the GitHub Spatial Finance repository: [1] ; [2] ; [3] .,,,,,,
,,2023-10-13,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02599-w,,,,,,
,,Global database of cement production assets and upstream suppliers,,,,,,
,,13,,,,,,
,,10,,,,,,
,,2023,,,,,,
,,"['Spatial macrolocalisation model', 'Cement production types and capacity estimation models', 'Sentinel-2 cement assets deployment models']",,,,,,
,,The deployment phase code notebooks are available from the GitHub Spatial Finance repository: [1] ; [2] ; [3] .,,,,,,
,,2023-10-13,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02599-w,,,,,,
,,Global database of cement production assets and upstream suppliers,,,,,,
,,13,,,,,,
,,10,,,,,,
,,2023,,,,,,
,,"['Spatial macrolocalisation model', 'Cement production types and capacity estimation models', 'Sentinel-2 cement assets deployment models']",,,,,,
,,The deployment phase code notebooks are available from the GitHub Spatial Finance repository: [1] ; [2] ; [3] .,,,,,,
,,2023-10-13,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02555-8,,,,,,
,,CherryChèvre: A fine-grained dataset for goat detection in natural environments,,,,,,
,,11,,,,,,
,,10,,,,,,
,,2023,,,,,,
,,['https://doi.org/10.57745/QEZBNA'],,,,,,
,,"The Python 3.10 scripts used for converting the VGG VIA csv format to YOLO format, as well as other scripts used for generating statistics presented in the article, are available at .",,,,,,
,,2023-10-11,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02583-4,,,,,,
,,The R package for DICOM to brain imaging data structure conversion,,,,,,
,,4,,,,,,
,,10,,,,,,
,,2023,,,,,,
,,"['https://github.com/bidsconvertr/bidsconvertr', 'https://bidsconvertr.github.io']",,,,,,
,,"All code is hosted freely and open-source on a GitHub repository, accessible via . The documentation is hosted on the GitHub page: .",,,,,,
,,2023-10-04,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02569-2,,,,,,
,,A synthetic data set to benchmark anti-money laundering methods,,,,,,
,,28,,,,,,
,,9,,,,,,
,,2023,,,,,,
,,"['https://sdv.dev', 'https://sdv.dev/SDV/user_guides/relational/hma1', 'https://scikit-learn.org', 'https://lightgbm.readthedocs.io/en/stable']",,,,,,
,,"All our simulations are made using version 0.14.1 of the SDV library (). We specifically employ the HMA1 model class using two tables as inputs: a primary table with alerts (see Table ) and a secondary table with transactions (see Table ). A demonstration by the SDV developers is available online (; using data different from ours). Due to confidentiality, we do not share our code that (i) transforms the raw data so that it can be fed to the HMA1 model class and (ii) re-transforms and adds noise to the simulated data. The data-providing bank felt that providing this code would reveal sensitive information about its internal setup and the real data. All our transformations are, however, described in detail in our subsection “Implementation: Pre- and Postprocessing.” Our machine learning experiments were conducted with version 1.1.3 of the Scikit-learn library () and version 3.3.3 of the LightGBM library ().",,,,,,
,,2023-09-28,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02569-2,,,,,,
,,A synthetic data set to benchmark anti-money laundering methods,,,,,,
,,28,,,,,,
,,9,,,,,,
,,2023,,,,,,
,,"['https://sdv.dev', 'https://sdv.dev/SDV/user_guides/relational/hma1', 'https://scikit-learn.org', 'https://lightgbm.readthedocs.io/en/stable']",,,,,,
,,"All our simulations are made using version 0.14.1 of the SDV library (). We specifically employ the HMA1 model class using two tables as inputs: a primary table with alerts (see Table ) and a secondary table with transactions (see Table ). A demonstration by the SDV developers is available online (; using data different from ours). Due to confidentiality, we do not share our code that (i) transforms the raw data so that it can be fed to the HMA1 model class and (ii) re-transforms and adds noise to the simulated data. The data-providing bank felt that providing this code would reveal sensitive information about its internal setup and the real data. All our transformations are, however, described in detail in our subsection “Implementation: Pre- and Postprocessing.” Our machine learning experiments were conducted with version 1.1.3 of the Scikit-learn library () and version 3.3.3 of the LightGBM library ().",,,,,,
,,2023-09-28,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02569-2,,,,,,
,,A synthetic data set to benchmark anti-money laundering methods,,,,,,
,,28,,,,,,
,,9,,,,,,
,,2023,,,,,,
,,"['https://sdv.dev', 'https://sdv.dev/SDV/user_guides/relational/hma1', 'https://scikit-learn.org', 'https://lightgbm.readthedocs.io/en/stable']",,,,,,
,,"All our simulations are made using version 0.14.1 of the SDV library (). We specifically employ the HMA1 model class using two tables as inputs: a primary table with alerts (see Table ) and a secondary table with transactions (see Table ). A demonstration by the SDV developers is available online (; using data different from ours). Due to confidentiality, we do not share our code that (i) transforms the raw data so that it can be fed to the HMA1 model class and (ii) re-transforms and adds noise to the simulated data. The data-providing bank felt that providing this code would reveal sensitive information about its internal setup and the real data. All our transformations are, however, described in detail in our subsection “Implementation: Pre- and Postprocessing.” Our machine learning experiments were conducted with version 1.1.3 of the Scikit-learn library () and version 3.3.3 of the LightGBM library ().",,,,,,
,,2023-09-28,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02569-2,,,,,,
,,A synthetic data set to benchmark anti-money laundering methods,,,,,,
,,28,,,,,,
,,9,,,,,,
,,2023,,,,,,
,,"['https://sdv.dev', 'https://sdv.dev/SDV/user_guides/relational/hma1', 'https://scikit-learn.org', 'https://lightgbm.readthedocs.io/en/stable']",,,,,,
,,"All our simulations are made using version 0.14.1 of the SDV library (). We specifically employ the HMA1 model class using two tables as inputs: a primary table with alerts (see Table ) and a secondary table with transactions (see Table ). A demonstration by the SDV developers is available online (; using data different from ours). Due to confidentiality, we do not share our code that (i) transforms the raw data so that it can be fed to the HMA1 model class and (ii) re-transforms and adds noise to the simulated data. The data-providing bank felt that providing this code would reveal sensitive information about its internal setup and the real data. All our transformations are, however, described in detail in our subsection “Implementation: Pre- and Postprocessing.” Our machine learning experiments were conducted with version 1.1.3 of the Scikit-learn library () and version 3.3.3 of the LightGBM library ().",,,,,,
,,2023-09-28,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02551-y,,,,,,
,,Multi-view emotional expressions dataset using 2D pose estimation,,,,,,
,,22,,,,,,
,,9,,,,,,
,,2023,,,,,,
,,['https://doi.org/10.5281/zenodo.8185369'],,,,,,
,,The MATLAB code for parsing the JSON file and processing the coordinates can be found at .,,,,,,
,,2023-09-22,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02489-1,,,,,,
,,A pseudoproxy emulation of the PAGES 2k database using a hierarchy of proxy system models,,,,,,
,,14,,,,,,
,,9,,,,,,
,,2023,,,,,,
,,"['https://doi.org/10.5281/zenodo.7652533', 'https://github.com/fzhu2e/paper-pseudoPAGES2k']",,,,,,
,,The Jupyter notebooks illustrating the usage of the pseudoPAGES2k dataset can be accessed at  or .,,,,,,
,,2023-09-14,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02524-1,,,,,,
,,Open access dataset integrating EEG and fNIRS during Stroop tasks,,,,,,
,,12,,,,,,
,,9,,,,,,
,,2023,,,,,,
,,"['https://github.com/Yaaaaaaaaabby/fNIRS-data-pre-processing-from-Zemeng-Chen.git', 'https://gitee.com/chen-zemeng/f-nirs-data-pre-processing-from-zemeng-chen.git']",,,,,,
,,Scripts to import the fNIRS raw data (.tdms file format) into MATLAB and fNIRS data processing code used above are available at  or . A user guide describing the basic situation and usage of the dataset is uploaded together with the code. There are two files in the zip file. The MATLAB code file named “process_fNIRS_EEG_Stroop” is used to pre-process the fNIRS data of a subject. The folder used for MATLAB to load the .tdms file format is named “Matlab_read_tdms_file”. Please add the folder to the MATLAB search path before loading the .tdms file format.,,,,,,
,,2023-09-12,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02514-3,,,,,,
,,Ontology for the Avida digital evolution platform,,,,,,
,,9,,,,,,
,,9,,,,,,
,,2023,,,,,,
,,['https://graphdb.fortunalab.org'],,,,,,
,,"[{'ext-link': [{'@xlink:href': 'https://gitlab.com/fortunalab/ontoavida', '@ext-link-type': 'uri', '#text': 'https://gitlab.com/fortunalab/ontoavida'}, {'@xlink:href': 'https://gitlab.com/fortunalab/ontoavida/-/blob/master/ontoavida.obo', '@ext-link-type': 'uri', '#text': 'https://gitlab.com/fortunalab/ontoavida/-/blob/master/ontoavida.obo'}, {'@xlink:href': 'https://gitlab.com/fortunalab/ontoavida/-/blob/master/ontoavida.owl', '@ext-link-type': 'uri', '#text': 'https://gitlab.com/fortunalab/ontoavida/-/blob/master/ontoavida.owl'}, {'@xlink:href': 'http://www.obofoundry.org/ontology/ontoavida.html', '@ext-link-type': 'uri', '#text': 'http://www.obofoundry.org/ontology/ontoavida.html'}, {'@xlink:href': 'http://creativecommons.org/licenses/by/4.0/', '@ext-link-type': 'uri', '#text': 'http://creativecommons.org/licenses/by/4.0/'}], '#text': 'The ontology is available in both OBO and OWL format from the GitLab repository () and can be found at  and . OntoAvida OBO and OWL files are also available from the OBO Foundry (). All files are available under the Creative Commons Attribution 4.0 International License () which allows for the copying, redistribution and adaption of the ontology for any purpose.'}, {'ext-link': [{'@xlink:href': 'https://github.com/rdflib/pyLODE', '@ext-link-type': 'uri', '#text': 'https://github.com/rdflib/pyLODE'}, {'@xlink:href': 'https://gitlab.com/fortunalab/pyLODE', '@ext-link-type': 'uri', '#text': 'https://gitlab.com/fortunalab/pyLODE'}, {'@xlink:href': 'https://owl.fortunalab.org/ontoavida', '@ext-link-type': 'uri', '#text': 'https://owl.fortunalab.org/ontoavida'}], 'xref': {'@rid': 'Fig4', '@ref-type': 'fig', '#text': '4'}, '#text': 'pyLODE () was used to obtain a user-friendly visualization of the ontology. pyLODE is based on the OWL Documentation Environment tool (LODE), implemented in Python, and used to generate human-readable HTML documents for OWL and RDF ontologies. We have customized the original pyLODE templates () to convert a scheme of OntoAvida, in a HTML file so that its classes, object properties, and datatype properties can be easily visualized (Fig.\xa0). The pyLODE file of OntoAvida is available at .'}]",,,,,,
,,2023-09-09,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02470-y,,,,,,
,,Applying FAIR4RS principles to develop an integrated modeling environment for the magnetic confinement fusion,,,,,,
,,7,,,,,,
,,9,,,,,,
,,2023,,,,,,
,,['https://doi.org/10.5281/zenodo.8098117'],,,,,,
,,"We use the ZENODO service to keep a persistent archive of FyDev code, with .",,,,,,
,,2023-09-07,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02473-9,,,,,,
,,Monitoring of carbon-water fluxes at Eurasian meteorological stations using random forest and remote sensing,,,,,,
,,7,,,,,,
,,9,,,,,,
,,2023,,,,,,
,,['https://doi.org/10.6084/m9.figshare.21510183.v2'],,,,,,
,,The code to generate the carbon-water flux datasets is available at figshare ().,,,,,,
,,2023-09-07,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02494-4,,,,,,
,,SECURES-Met: A European meteorological data set suitable for electricity modelling applications,,,,,,
,,7,,,,,,
,,9,,,,,,
,,2023,,,,,,
,,['https://doi.org/10.5281/zenodo.8108927'],,,,,,
,,The data are stored as ASCII text (csv) and no specific software is necessary to access the data. The production of the data was done with Python. These scripts are mainly data manipulation routines and do not contribute in processing the data further. To assure repeatability all scripts are available at Zenodo ().,,,,,,
,,2023-09-07,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02483-7,,,,,,
,,Standardised images of novel objects created with generative adversarial networks,,,,,,
,,2,,,,,,
,,9,,,,,,
,,2023,,,,,,
,,['https://artbreeder.com'],,,,,,
,,"Software to generate novel objects is available at . Code to perform data collection (i.e., run the online experiment) was created using jsPsych (version 7.2.1), and a modified version of the  extension (all available at). Code to extract the objective properties of each object, and to compile the subjective ratings from our online study, was written in Python, MATLAB and Julia respectively (available at).",,,,,,
,,2023-09-02,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02475-7,,,,,,
,,Development of global monthly dataset of CMIP6 climate variables for estimating evapotranspiration,,,,,,
,,26,,,,,,
,,8,,,,,,
,,2023,,,,,,
,,['https://pyeto.readthedocs.io/en/latest/'],,,,,,
,,"The code to produce the data was written using Python, PyCharm 2022.2.2. The code is available in pyeto ().",,,,,,
,,2023-08-26,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02339-0,,,,,,
,,Three-dimensional topology dataset of folded radar stratigraphy in northern Greenland,,,,,,
,,7,,,,,,
,,8,,,,,,
,,2023,,,,,,
,,"['https://gitlab.com/openpolarradar/opr', 'https://gitlab.com/openpolarradar/opr/-/wikis/home']",,,,,,
,,"The CReSIS toolbox used to process the MCoRDS RES data is available at , and the main documentation can be found at .",,,,,,
,,2023-08-07,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02339-0,,,,,,
,,Three-dimensional topology dataset of folded radar stratigraphy in northern Greenland,,,,,,
,,7,,,,,,
,,8,,,,,,
,,2023,,,,,,
,,"['https://gitlab.com/openpolarradar/opr', 'https://gitlab.com/openpolarradar/opr/-/wikis/home']",,,,,,
,,"The CReSIS toolbox used to process the MCoRDS RES data is available at , and the main documentation can be found at .",,,,,,
,,2023-08-07,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02432-4,,,,,,
,,"FracAtlas: A Dataset for Fracture Classification, Localization and Segmentation of Musculoskeletal Radiographs",,,,,,
,,5,,,,,,
,,8,,,,,,
,,2023,,,,,,
,,"['https://doi.org/10.6084/m9.figshare.22363012', 'https://github.com/XLR8-07/FracAtlas']",,,,,,
,,"The conversion of DICOM to JPEG image format was done using proprietary software of the X-ray machines from brands like Fujifilm and Philips hence they could not be made available. The mask annotations for segmentation were done using an open-source web tool named makedsense.ai. It was also used for generating VGG annotations from COCO format. As explained in the Methods section, the annotation conversion procedures from COCO to YOLO and YOLO to PASCAL VOC were performed using Python 3.10.1 on a Windows 11 operating system using ‘coco2yolo.ipynb’ and ‘yolo2voc.ipynb’. Both the Jupyter notebooks can be found inside the ‘Utility’ folder along with the dataset at Figshare (). The code used for technical validation can be accessed from (). There are 2 notebooks inside ‘notebooks’ under the root folder called ‘Train_8s.ipynb’ and ‘Prediction_8s.ipynb’. The ‘Train_8s.ipynb’ is used to train 2 models of ‘YOLO8s_seg’ and ‘YOLO8s’ variants targeted toward segmentation and localization tasks respectively. ‘Prediction_8s.ipynb’ is used to generate predictions out of the 2 aforementioned models and view the results.",,,,,,
,,2023-08-05,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02405-7,,,,,,
,,ReCANVo: A database of real-world communicative and affective nonverbal vocalizations,,,,,,
,,5,,,,,,
,,8,,,,,,
,,2023,,,,,,
,,['https://doi.org/10.5281/zenodo.5786859'],,,,,,
,,We used the Python programming language for the data processing described above. Volume segmentation was implemented using the  libary. The label assignment algorithm is summarized in Fig. . The code is available as part of our dataset in Zenodo: .,,,,,,
,,2023-08-05,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02409-3,,,,,,
,,DIPS-Plus: The enhanced database of interacting protein structures for interface prediction,,,,,,
,,3,,,,,,
,,8,,,,,,
,,2023,,,,,,
,,"['Zenodo', 'GitHub', 'GitHub', 'DOI']",,,,,,
,,"Preprocessed data for DIPS-Plus as well as its associated source code and instructions for data processing and reproducibility can be found on  and , respectively. The GitHub instructions illustrate how users can install the Python programming language and build an Anaconda virtual environment containing the software dependencies required to preprocess and analyze DIPS-Plus using the provided Python scripts. Lastly, the GitHub instructions show users how to run such scripts and the order in which to do so to successfully rebuild DIPS-Plus from scratch, to featurize a given PDB file, or to train new machine learning models (e.g., NeiA) for protein interface prediction. For provenance, the original DIPS dataset’s source code can also be found on , along with a corresponding .",,,,,,
,,2023-08-03,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02409-3,,,,,,
,,DIPS-Plus: The enhanced database of interacting protein structures for interface prediction,,,,,,
,,3,,,,,,
,,8,,,,,,
,,2023,,,,,,
,,"['Zenodo', 'GitHub', 'GitHub', 'DOI']",,,,,,
,,"Preprocessed data for DIPS-Plus as well as its associated source code and instructions for data processing and reproducibility can be found on  and , respectively. The GitHub instructions illustrate how users can install the Python programming language and build an Anaconda virtual environment containing the software dependencies required to preprocess and analyze DIPS-Plus using the provided Python scripts. Lastly, the GitHub instructions show users how to run such scripts and the order in which to do so to successfully rebuild DIPS-Plus from scratch, to featurize a given PDB file, or to train new machine learning models (e.g., NeiA) for protein interface prediction. For provenance, the original DIPS dataset’s source code can also be found on , along with a corresponding .",,,,,,
,,2023-08-03,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02409-3,,,,,,
,,DIPS-Plus: The enhanced database of interacting protein structures for interface prediction,,,,,,
,,3,,,,,,
,,8,,,,,,
,,2023,,,,,,
,,"['Zenodo', 'GitHub', 'GitHub', 'DOI']",,,,,,
,,"Preprocessed data for DIPS-Plus as well as its associated source code and instructions for data processing and reproducibility can be found on  and , respectively. The GitHub instructions illustrate how users can install the Python programming language and build an Anaconda virtual environment containing the software dependencies required to preprocess and analyze DIPS-Plus using the provided Python scripts. Lastly, the GitHub instructions show users how to run such scripts and the order in which to do so to successfully rebuild DIPS-Plus from scratch, to featurize a given PDB file, or to train new machine learning models (e.g., NeiA) for protein interface prediction. For provenance, the original DIPS dataset’s source code can also be found on , along with a corresponding .",,,,,,
,,2023-08-03,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02409-3,,,,,,
,,DIPS-Plus: The enhanced database of interacting protein structures for interface prediction,,,,,,
,,3,,,,,,
,,8,,,,,,
,,2023,,,,,,
,,"['Zenodo', 'GitHub', 'GitHub', 'DOI']",,,,,,
,,"Preprocessed data for DIPS-Plus as well as its associated source code and instructions for data processing and reproducibility can be found on  and , respectively. The GitHub instructions illustrate how users can install the Python programming language and build an Anaconda virtual environment containing the software dependencies required to preprocess and analyze DIPS-Plus using the provided Python scripts. Lastly, the GitHub instructions show users how to run such scripts and the order in which to do so to successfully rebuild DIPS-Plus from scratch, to featurize a given PDB file, or to train new machine learning models (e.g., NeiA) for protein interface prediction. For provenance, the original DIPS dataset’s source code can also be found on , along with a corresponding .",,,,,,
,,2023-08-03,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02340-7,,,,,,
,,"A pipeline to further enhance quality, integrity and reusability of the NCCID clinical data",,,,,,
,,27,,,,,,
,,7,,,,,,
,,2023,,,,,,
,,['https://gitlab.developers.cam.ac.uk/maths/cia/covid-19-projects/nccidxclean'],,,,,,
,,"Our extended cleaning pipeline  is publicly available and can be accessed on GitLab:  with accompanying documentation available on the project website (package version at the time of publication: v1.0). The package will automatically download the necessary packages and requirements during installation, including the original NCCID cleaning pipeline upon which our pipeline builds. The python package is independent of the operating system and allows replication of our results using the command line interface, python scripts, or the included Jupyter notebooks. For full analysis, the  parameter can be set to ‘all_with_original’, returning all possible data features. Additional code is provided to allow for the original NHSx pipeline to be run using a single command in the command line. The data from the NHSx pipeline may be then used in the analysis subpackage to generate all figures and numerical results found in this work. An additional subpackage  is included for exploratory data analysis of the final cleaned data. Full step-by-step guidance and further details of the package are provided on the project’s website. .",,,,,,
,,2023-07-27,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02258-0,,,,,,
,,The Translational Data Catalog - discoverable biomedical datasets,,,,,,
,,20,,,,,,
,,7,,,,,,
,,2023,,,,,,
,,['https://datacatalog.elixir-luxembourg.org/'],,,,,,
,,"[{'ext-link': {'@xlink:href': 'https://github.com/FAIRplus/imi-data-catalogue', '@ext-link-type': 'uri', '#text': 'https://github.com/FAIRplus/imi-data-catalogue'}, '#text': 'All Data Catalog code is available in a dedicated repository of the FAIRplus Github organisation, at . The repository includes full documentation on how to deploy a stand-alone version of the Data Catalog.'}, {'ext-link': {'@xlink:href': 'https://github.com/datatagsuite/schema', '@ext-link-type': 'uri', '#text': 'https://github.com/datatagsuite/schema'}, '#text': 'The DATS model is available on Github at .'}]",,,,,,
,,2023-07-20,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02373-y,,,,,,
,,Single-nucleus chromatin landscapes during zebrafish early embryogenesis,,,,,,
,,19,,,,,,
,,7,,,,,,
,,2023,,,,,,
,,['https://figshare.com/articles/dataset/Code/22121171'],,,,,,
,,The codes used to analyze the data in this study were available online ().,,,,,,
,,2023-07-19,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02318-5,,,,,,
,,Helicopter-borne RGB orthomosaics and photogrammetric digital elevation models from the MOSAiC Expedition,,,,,,
,,3,,,,,,
,,7,,,,,,
,,2023,,,,,,
,,['https://gitlab.com/mosaic12/orthomosaics'],,,,,,
,,All processing developed within this study is wrapped in a python environment and is available at . For calculating image footprint locations we used the  python package. Note that part of the code is based on the commercial Agisoft Metashape software requiring licensing.,,,,,,
,,2023-07-03,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02230-y,,,,,,
,,Signing data citations enables data verification and citation persistence,,,,,,
,,27,,,,,,
,,6,,,,,,
,,2023,,,,,,
,,"['https://github.com/bio-guoda/preston', 'https://doi.org/10.5281/zenodo.1410543', 'https://doi.org/10.5281/zenodo.7005141']",,,,,,
,,The source code for the Preston software is available in GitHub at  and in Zenodo at . Preston version 4.4 has been assigned the . The MD5 content signature for the zip archive of the source code is .,,,,,,
,,2023-06-27,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02230-y,,,,,,
,,Signing data citations enables data verification and citation persistence,,,,,,
,,27,,,,,,
,,6,,,,,,
,,2023,,,,,,
,,"['https://github.com/bio-guoda/preston', 'https://doi.org/10.5281/zenodo.1410543', 'https://doi.org/10.5281/zenodo.7005141']",,,,,,
,,The source code for the Preston software is available in GitHub at  and in Zenodo at . Preston version 4.4 has been assigned the . The MD5 content signature for the zip archive of the source code is .,,,,,,
,,2023-06-27,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02315-8,,,,,,
,,HeiPorSPECTRAL - the Heidelberg Porcine HyperSPECTRAL Imaging Dataset of 20 Physiological Organs,,,,,,
,,24,,,,,,
,,6,,,,,,
,,2023,,,,,,
,,"['https://github.com/MIC-Surgery-Heidelberg', 'https://github.com/IMSY-DKFZ/htc']",,,,,,
,,Data acquisition was performed with the TIVITA® Suite (version 1.6.0.1. The polygon annotations were created with a software developed in-house which is available on GitHub: . All data analyses and visualizations in this manuscript were performed using Python and the corresponding code is available at .,,,,,,
,,2023-06-24,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02321-w,,,,,,
,,A curated gene and biological system annotation of adverse outcome pathways related to human health,,,,,,
,,24,,,,,,
,,6,,,,,,
,,2023,,,,,,
,,['https://doi.org/10.5281/zenodo.7980953'],,,,,,
,,Custom code and data used in the NLP-based prioritisation of the gene set annotations is available in the data repository on Zenodo at  (file aop_mapping_nlp.tar.gz).,,,,,,
,,2023-06-24,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02256-2,,,,,,
,,A guide to sharing open healthcare data under the General Data Protection Regulation,,,,,,
,,24,,,,,,
,,6,,,,,,
,,2023,,,,,,
,,['https://doi.org/10.6084/m9.figshare.22643419'],,,,,,
,,No code was written or used for this paper.,,,,,,
,,2023-06-24,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02238-4,,,,,,
,,An Observation-Based Dataset of Global Sub-Daily Precipitation Indices (GSDR-I),,,,,,
,,22,,,,,,
,,6,,,,,,
,,2023,,,,,,
,,['https://doi.org/10.5281/zenodo.7492877'],,,,,,
,,The Python code for indices calculation based on gauge records and subsequent gridding is available in the code repository here: .,,,,,,
,,2023-06-22,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02282-0,,,,,,
,,"Population, land use and economic exposure estimates for Europe at 100 m resolution from 1870 to 2020",,,,,,
,,8,,,,,,
,,6,,,,,,
,,2023,,,,,,
,,"['https://doi.org/10.5281/zenodo.7556953', 'https://doi.org/10.5281/zenodo.6783023', 'https://doi.org/10.4121/collection:HANZE']",,,,,,
,,"The source code of HANZE v2.0 (implemented in Python 3.9) presented in the paper is archived at . All necessary input data are archived at . The flood impact data shown in Usage Notes, with a description of sources of the data, are available in the HANZE v1.0 repository, .",,,,,,
,,2023-06-08,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02282-0,,,,,,
,,"Population, land use and economic exposure estimates for Europe at 100 m resolution from 1870 to 2020",,,,,,
,,8,,,,,,
,,6,,,,,,
,,2023,,,,,,
,,"['https://doi.org/10.5281/zenodo.7556953', 'https://doi.org/10.5281/zenodo.6783023', 'https://doi.org/10.4121/collection:HANZE']",,,,,,
,,"The source code of HANZE v2.0 (implemented in Python 3.9) presented in the paper is archived at . All necessary input data are archived at . The flood impact data shown in Usage Notes, with a description of sources of the data, are available in the HANZE v1.0 repository, .",,,,,,
,,2023-06-08,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02282-0,,,,,,
,,"Population, land use and economic exposure estimates for Europe at 100 m resolution from 1870 to 2020",,,,,,
,,8,,,,,,
,,6,,,,,,
,,2023,,,,,,
,,"['https://doi.org/10.5281/zenodo.7556953', 'https://doi.org/10.5281/zenodo.6783023', 'https://doi.org/10.4121/collection:HANZE']",,,,,,
,,"The source code of HANZE v2.0 (implemented in Python 3.9) presented in the paper is archived at . All necessary input data are archived at . The flood impact data shown in Usage Notes, with a description of sources of the data, are available in the HANZE v1.0 repository, .",,,,,,
,,2023-06-08,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02208-w,,,,,,
,,CORE: A Global Aggregation Service for Open Access Papers,,,,,,
,,7,,,,,,
,,6,,,,,,
,,2023,,,,,,
,,['https://github.com/oacore/'],,,,,,
,,"CORE consists of multiple services. Most of our source code is open source and available in our public repository on GitHub (). As of today, we are unfortunately not yet able to provide the source code to our data ingestion module. However, as we want to be as transparent as possible with our community, we have documented in this paper the key algorithms and processes which we apply using pseudocode.",,,,,,
,,2023-06-07,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02242-8,,,,,,
,,How to establish and maintain a multimodal animal research dataset using DataLad,,,,,,
,,5,,,,,,
,,6,,,,,,
,,2023,,,,,,
,,['https://doi.org/10.12751/g-node.3yl5qi'],,,,,,
,,DataLad and GIN are freely available. The manuscript contains all code to reproduce the workflow.,,,,,,
,,2023-06-05,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02269-x,,,,,,
,,PANGAEA - Data Publisher for Earth & Environmental Science,,,,,,
,,2,,,,,,
,,6,,,,,,
,,2023,,,,,,
,,['https://github.com/pangaea-data-publisher'],,,,,,
,,"The code supporting the users with data retrieval and submission is freely available at . PANGAEA as a repository does not generate, test, or process data and metadata, therefore no custom code has been used.",,,,,,
,,2023-06-02,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02225-9,,,,,,
,,SRAM-Based PUF Readouts,,,,,,
,,27,,,,,,
,,5,,,,,,
,,2023,,,,,,
,,"['https://github.com/servinagrero/SRAMPlatform', 'https://servinagrero.github.io/SRAMPlatform', 'https://www.postgresql.org/', 'https://www.rabbitmq.com/']",,,,,,
,,"The source code of the platform and the STM32 devices are available under the GPL-2.0 license at . Online documentation on the platform and guidance on custom station set up can be found at . The full list of python dependencies is available in the  file in GitHub repository. PostgreSQL () is needed to store data, a message broker is necessary to communicate with the station, RabbitMQ ()in our case, and Grafana is used to monitory metrics and sensors in a dashboard.",,,,,,
,,2023-05-27,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02225-9,,,,,,
,,SRAM-Based PUF Readouts,,,,,,
,,27,,,,,,
,,5,,,,,,
,,2023,,,,,,
,,"['https://github.com/servinagrero/SRAMPlatform', 'https://servinagrero.github.io/SRAMPlatform', 'https://www.postgresql.org/', 'https://www.rabbitmq.com/']",,,,,,
,,"The source code of the platform and the STM32 devices are available under the GPL-2.0 license at . Online documentation on the platform and guidance on custom station set up can be found at . The full list of python dependencies is available in the  file in GitHub repository. PostgreSQL () is needed to store data, a message broker is necessary to communicate with the station, RabbitMQ ()in our case, and Grafana is used to monitory metrics and sensors in a dashboard.",,,,,,
,,2023-05-27,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02225-9,,,,,,
,,SRAM-Based PUF Readouts,,,,,,
,,27,,,,,,
,,5,,,,,,
,,2023,,,,,,
,,"['https://github.com/servinagrero/SRAMPlatform', 'https://servinagrero.github.io/SRAMPlatform', 'https://www.postgresql.org/', 'https://www.rabbitmq.com/']",,,,,,
,,"The source code of the platform and the STM32 devices are available under the GPL-2.0 license at . Online documentation on the platform and guidance on custom station set up can be found at . The full list of python dependencies is available in the  file in GitHub repository. PostgreSQL () is needed to store data, a message broker is necessary to communicate with the station, RabbitMQ ()in our case, and Grafana is used to monitory metrics and sensors in a dashboard.",,,,,,
,,2023-05-27,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02199-8,,,,,,
,,KG: A FAIR Knowledge Graph of Graffiti,,,,,,
,,25,,,,,,
,,5,,,,,,
,,2023,,,,,,
,,"['https://github.com/dice-group/Ingrid', 'https://www.gnu.org/licenses/gpl-3.0']",,,,,,
,,Our source code to generate the new versions of KG is publicly available at () and will be maintained in parallel with the knowledge graph. We provide our source code under the software license of GPL 3.0 ().,,,,,,
,,2023-05-25,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02152-9,,,,,,
,,A continent-wide detailed geological map dataset of Antarctica,,,,,,
,,18,,,,,,
,,5,,,,,,
,,2023,,,,,,
,,"['https://gis.gns.cri.nz/server/rest/services/SCAR_GeoMAP/ATA_SCAR_GeoMAP_Geology/MapServer', 'https://data.gns.cri.nz/ata_geomap/index.html', 'https://geomap.readthedocs.io/en/latest/', 'https://github.com/selkind/GeoMap']",,,,,,
,,"GeoMAP v.2022-08 has been generated for ArcGIS (10.8.1) and QGIS (3.4) as geodatabase and geopackage material, using a GCS WGS 1984 geographic coordinate reference and WGS 1984 Antarctic Polar Stereographic projection. Data were developed manually, then stored in a GIS database developed, web-delivered and maintained by GNS Science in New Zealand. Software ArcGIS® has been used to create the GIS database, but data can be exported in a variety of formats and compatible with most other GIS software. ArcGIS data are available from the PANGAEA data archive, an ArcGIS REST service (), or viewed through a webmap (). A series of QGIS and Google Earth KMZ files, exported from the ArcGIS geodatabase layers, are also available from the archive. The original data have been segmented into ten regions to keep KMZ files at a reasonable (<25 Mb) and useable size. GeoMAP documentation () has been generated using code deposited on GitHub ().",,,,,,
,,2023-05-18,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02152-9,,,,,,
,,A continent-wide detailed geological map dataset of Antarctica,,,,,,
,,18,,,,,,
,,5,,,,,,
,,2023,,,,,,
,,"['https://gis.gns.cri.nz/server/rest/services/SCAR_GeoMAP/ATA_SCAR_GeoMAP_Geology/MapServer', 'https://data.gns.cri.nz/ata_geomap/index.html', 'https://geomap.readthedocs.io/en/latest/', 'https://github.com/selkind/GeoMap']",,,,,,
,,"GeoMAP v.2022-08 has been generated for ArcGIS (10.8.1) and QGIS (3.4) as geodatabase and geopackage material, using a GCS WGS 1984 geographic coordinate reference and WGS 1984 Antarctic Polar Stereographic projection. Data were developed manually, then stored in a GIS database developed, web-delivered and maintained by GNS Science in New Zealand. Software ArcGIS® has been used to create the GIS database, but data can be exported in a variety of formats and compatible with most other GIS software. ArcGIS data are available from the PANGAEA data archive, an ArcGIS REST service (), or viewed through a webmap (). A series of QGIS and Google Earth KMZ files, exported from the ArcGIS geodatabase layers, are also available from the archive. The original data have been segmented into ten regions to keep KMZ files at a reasonable (<25 Mb) and useable size. GeoMAP documentation () has been generated using code deposited on GitHub ().",,,,,,
,,2023-05-18,,,,,,
,,0,,,,,,
,,Scientific Data,,,,,,
,,41597,,,,,,
,,10.1038/s41597-023-02152-9,,,,,,
,,A continent-wide detailed geological map dataset of Antarctica,,,,,,
,,18,,,,,,
,,5,,,,,,
,,2023,,,,,,
,,"['https://gis.gns.cri.nz/server/rest/services/SCAR_GeoMAP/ATA_SCAR_GeoMAP_Geology/MapServer', 'https://data.gns.cri.nz/ata_geomap/index.html', 'https://geomap.readthedocs.io/en/latest/', 'https://github.com/selkind/GeoMap']",,,,,,
,,"GeoMAP v.2022-08 has been generated for ArcGIS (10.8.1) and QGIS (3.4) as geodatabase and geopackage material, using a GCS WGS 1984 geographic coordinate reference and WGS 1984 Antarctic Polar Stereographic projection. Data were developed manually, then stored in a GIS database developed, web-delivered and maintained by GNS Science in New Zealand. Software ArcGIS® has been used to create the GIS database, but data can be exported in a variety of formats and compatible with most other GIS software. ArcGIS data are available from the PANGAEA data archive, an ArcGIS REST service (), or viewed through a webmap (). A series of QGIS and Google Earth KMZ files, exported from the ArcGIS geodatabase layers, are also available from the archive. The original data have been segmented into ten regions to keep KMZ files at a reasonable (<25 Mb) and useable size. GeoMAP documentation () has been generated using code deposited on GitHub ().",,,,,,
,,2023-05-18,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02808-6,,,,,,,
,Labelled dataset for Ultra-Low Temperature Freezer to aid dynamic modelling & fault detection and diagnostics,,,,,,,
,9,,,,,,,
,12,,,,,,,
,2023,,,,,,,
,['https://lab.compute.dtu.dk/taohu/ult-freezers-labelled-dataset-sci-data.git'],,,,,,,
,The Python code for ETL pipelines is available on the open-access GitLab at .,,,,,,,
,2023-12-09,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02809-5,,,,,,,
,A dataset of proteomic changes during human heat stress and heat acclimation,,,,,,,
,7,,,,,,,
,12,,,,,,,
,2023,,,,,,,
,['https://acclimation.statgen.org/'],,,,,,,
,The source code for this dataset is publicly available through the Figshare repository. The data may also be visualized with an interactive web visualization tool:,,,,,,,
,2023-12-07,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02757-0,,,,,,,
,The OREGANO knowledge graph for computational drug repurposing,,,,,,,
,6,,,,,,,
,12,,,,,,,
,2023,,,,,,,
,['https://gitub.u-bordeaux.fr/erias/oregano'],,,,,,,
,The code for the integration and the knowledge graph are available on the GitHub of the OREGANO project in the  folder ().,,,,,,,
,2023-12-06,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02755-2,,,,,,,
,High-throughput density functional theory screening of double transition metal MXene precursors,,,,,,,
,25,,,,,,,
,11,,,,,,,
,2023,,,,,,,
,"['https://nanohub.org/tools/vaspingestor', 'https://github.com/katnykiel/vasp_ingestor']",,,,,,,
,"The DFT results database, code used to generate figures, and tool for ingesting new data into this database are available at . This code is also available at .",,,,,,,
,2023-11-25,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02756-1,,,,,,,
,A global daily evapotranspiration deficit index dataset for quantifying drought severity from 1979 to 2022,,,,,,,
,24,,,,,,,
,11,,,,,,,
,2023,,,,,,,
,"['https://github.com/XiaZhang1113/Daily-drought-index–DEDI', 'https://www.python.org/', 'https://code.mpimet.mpg.de/']",,,,,,,
,The code used to calculate the DEDI dataset is available via GitHub () under the MIT license. The scripts are written with the open-source Python language version 3.8.6 () and the Climate Data Operators (CDO) version 1.9.10 (). Any updates will be published on GitHub.,,,,,,,
,2023-11-24,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02756-1,,,,,,,
,A global daily evapotranspiration deficit index dataset for quantifying drought severity from 1979 to 2022,,,,,,,
,24,,,,,,,
,11,,,,,,,
,2023,,,,,,,
,"['https://github.com/XiaZhang1113/Daily-drought-index–DEDI', 'https://www.python.org/', 'https://code.mpimet.mpg.de/']",,,,,,,
,The code used to calculate the DEDI dataset is available via GitHub () under the MIT license. The scripts are written with the open-source Python language version 3.8.6 () and the Climate Data Operators (CDO) version 1.9.10 (). Any updates will be published on GitHub.,,,,,,,
,2023-11-24,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02682-2,,,,,,,
,Photos and rendered images of LEGO bricks,,,,,,,
,18,,,,,,,
,11,,,,,,,
,2023,,,,,,,
,['https://github.com/LegoSorter'],,,,,,,
,"Custom tools used to take photos, generate renders, annotate photos, and extract annotated bricks from the complete scene, including the trained neural networks, are publicly available through the Lego Sorter project and its repositories available at .",,,,,,,
,2023-11-18,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02673-3,,,,,,,
,Daylong acoustic recordings of grazing and rumination activities in dairy cows,,,,,,,
,8,,,,,,,
,11,,,,,,,
,2023,,,,,,,
,['https://gitlab.com/luciano.mrau/acoustic_dairy_cow_dataset'],,,,,,,
,"The code for automatically adjusting the timesteps of JM labels, computing the JM labels of the audio recordings and for technical validation is available at Gitlab (). All code was written in Python 3.8.10 and distributed under the MIT license. Small changes should be made to the scripts by specifying the path of the audio files of the execution environment.",,,,,,,
,2023-11-08,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02697-9,,,,,,,
,The EPOS multi-disciplinary Data Portal for integrated access to solid Earth science datasets,,,,,,,
,8,,,,,,,
,11,,,,,,,
,2023,,,,,,,
,"['https://epos-eu.github.io/epos-open-source/', 'https://github.com/epos-eu/SHAPEness-Metadata-Editor', 'https://github.com/epos-eu/EPOS-DCAT-AP']",,,,,,,
,"The code developed for the EPOS Data Portal system is available on the GitHub repository of EPOS . Other code that contributed to the development of the system is already available on such repository, as in the case of the SHAPEness metadata Editor () and the specifications for EPOS-DCAT-AP extension ().",,,,,,,
,2023-11-08,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02634-w,,,,,,,
,Land cover and forest health indicator datasets for central India using very-high resolution satellite data,,,,,,,
,25,,,,,,,
,10,,,,,,,
,2023,,,,,,,
,['https://doi.org/10.57760/sciencedb.10422/'],,,,,,,
,The code classifying land cover from PlanetScope imagery and deriving the BGI was written in Google Earth Engine. The JavaScript language to classify land covers from Planetscope imagery and derive the BGI from the land cover is available as the ‘Code’ text file from Science Data Bank at .,,,,,,,
,2023-10-25,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02650-w,,,,,,,
,A Large Finer-grained Affective Computing EEG Dataset,,,,,,,
,25,,,,,,,
,10,,,,,,,
,2023,,,,,,,
,['https://doi.org/10.7303/syn50614194'],,,,,,,
,All the codes used for the data pre-processing and the technical validation are publicly available together with the FACED datasets in Synapse (). The codes were developed in Python 3.10. These codes can be executed on Linux and Windows. All required packages are listed in the torch_ubuntu.yml and torch_win.yml files. The README file under the Code file provides a detailed explanation of the procedure to reproduce the validation results using the codes and data.,,,,,,,
,2023-10-25,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02316-7,,,,,,,
,Restructuring and serving web-accessible streamflow data from the NOAA National Water Model historic simulations,,,,,,,
,20,,,,,,,
,10,,,,,,,
,2023,,,,,,,
,['https://www.hydroshare.org/resource/84c2b029f97343a59d0739115d4087f1/'],,,,,,,
,"All code for data download and reformatting can be found in the appropriate USGS repository. The  R package is available on GitHub and the dataset is documented and published via HydroShare. All the data are currently open, and publicly available at this URL: .",,,,,,,
,2023-10-20,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02612-2,,,,,,,
,A benchmark dataset for machine learning in ecotoxicology,,,,,,,
,18,,,,,,,
,10,,,,,,,
,2023,,,,,,,
,['https://renkulab.io/gitlab/mltox/adore'],,,,,,,
,"The code used to load and process the input data and generate the output dataset was created and run in Python 3.9 and is made available on . The repository contains code on how to load the data, prepare it for modeling, ., create one-hot and multi-hot-encodings for categorical features, and apply the train-test-split for 5-fold cross-validation. A good starting point are the files in the folder  for random forests ( and ).",,,,,,,
,2023-10-18,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02599-w,,,,,,,
,Global database of cement production assets and upstream suppliers,,,,,,,
,13,,,,,,,
,10,,,,,,,
,2023,,,,,,,
,"['Spatial macrolocalisation model', 'Cement production types and capacity estimation models', 'Sentinel-2 cement assets deployment models']",,,,,,,
,The deployment phase code notebooks are available from the GitHub Spatial Finance repository: [1] ; [2] ; [3] .,,,,,,,
,2023-10-13,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02599-w,,,,,,,
,Global database of cement production assets and upstream suppliers,,,,,,,
,13,,,,,,,
,10,,,,,,,
,2023,,,,,,,
,"['Spatial macrolocalisation model', 'Cement production types and capacity estimation models', 'Sentinel-2 cement assets deployment models']",,,,,,,
,The deployment phase code notebooks are available from the GitHub Spatial Finance repository: [1] ; [2] ; [3] .,,,,,,,
,2023-10-13,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02599-w,,,,,,,
,Global database of cement production assets and upstream suppliers,,,,,,,
,13,,,,,,,
,10,,,,,,,
,2023,,,,,,,
,"['Spatial macrolocalisation model', 'Cement production types and capacity estimation models', 'Sentinel-2 cement assets deployment models']",,,,,,,
,The deployment phase code notebooks are available from the GitHub Spatial Finance repository: [1] ; [2] ; [3] .,,,,,,,
,2023-10-13,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02555-8,,,,,,,
,CherryChèvre: A fine-grained dataset for goat detection in natural environments,,,,,,,
,11,,,,,,,
,10,,,,,,,
,2023,,,,,,,
,['https://doi.org/10.57745/QEZBNA'],,,,,,,
,"The Python 3.10 scripts used for converting the VGG VIA csv format to YOLO format, as well as other scripts used for generating statistics presented in the article, are available at .",,,,,,,
,2023-10-11,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02583-4,,,,,,,
,The R package for DICOM to brain imaging data structure conversion,,,,,,,
,4,,,,,,,
,10,,,,,,,
,2023,,,,,,,
,"['https://github.com/bidsconvertr/bidsconvertr', 'https://bidsconvertr.github.io']",,,,,,,
,"All code is hosted freely and open-source on a GitHub repository, accessible via . The documentation is hosted on the GitHub page: .",,,,,,,
,2023-10-04,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02569-2,,,,,,,
,A synthetic data set to benchmark anti-money laundering methods,,,,,,,
,28,,,,,,,
,9,,,,,,,
,2023,,,,,,,
,"['https://sdv.dev', 'https://sdv.dev/SDV/user_guides/relational/hma1', 'https://scikit-learn.org', 'https://lightgbm.readthedocs.io/en/stable']",,,,,,,
,"All our simulations are made using version 0.14.1 of the SDV library (). We specifically employ the HMA1 model class using two tables as inputs: a primary table with alerts (see Table ) and a secondary table with transactions (see Table ). A demonstration by the SDV developers is available online (; using data different from ours). Due to confidentiality, we do not share our code that (i) transforms the raw data so that it can be fed to the HMA1 model class and (ii) re-transforms and adds noise to the simulated data. The data-providing bank felt that providing this code would reveal sensitive information about its internal setup and the real data. All our transformations are, however, described in detail in our subsection “Implementation: Pre- and Postprocessing.” Our machine learning experiments were conducted with version 1.1.3 of the Scikit-learn library () and version 3.3.3 of the LightGBM library ().",,,,,,,
,2023-09-28,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02569-2,,,,,,,
,A synthetic data set to benchmark anti-money laundering methods,,,,,,,
,28,,,,,,,
,9,,,,,,,
,2023,,,,,,,
,"['https://sdv.dev', 'https://sdv.dev/SDV/user_guides/relational/hma1', 'https://scikit-learn.org', 'https://lightgbm.readthedocs.io/en/stable']",,,,,,,
,"All our simulations are made using version 0.14.1 of the SDV library (). We specifically employ the HMA1 model class using two tables as inputs: a primary table with alerts (see Table ) and a secondary table with transactions (see Table ). A demonstration by the SDV developers is available online (; using data different from ours). Due to confidentiality, we do not share our code that (i) transforms the raw data so that it can be fed to the HMA1 model class and (ii) re-transforms and adds noise to the simulated data. The data-providing bank felt that providing this code would reveal sensitive information about its internal setup and the real data. All our transformations are, however, described in detail in our subsection “Implementation: Pre- and Postprocessing.” Our machine learning experiments were conducted with version 1.1.3 of the Scikit-learn library () and version 3.3.3 of the LightGBM library ().",,,,,,,
,2023-09-28,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02569-2,,,,,,,
,A synthetic data set to benchmark anti-money laundering methods,,,,,,,
,28,,,,,,,
,9,,,,,,,
,2023,,,,,,,
,"['https://sdv.dev', 'https://sdv.dev/SDV/user_guides/relational/hma1', 'https://scikit-learn.org', 'https://lightgbm.readthedocs.io/en/stable']",,,,,,,
,"All our simulations are made using version 0.14.1 of the SDV library (). We specifically employ the HMA1 model class using two tables as inputs: a primary table with alerts (see Table ) and a secondary table with transactions (see Table ). A demonstration by the SDV developers is available online (; using data different from ours). Due to confidentiality, we do not share our code that (i) transforms the raw data so that it can be fed to the HMA1 model class and (ii) re-transforms and adds noise to the simulated data. The data-providing bank felt that providing this code would reveal sensitive information about its internal setup and the real data. All our transformations are, however, described in detail in our subsection “Implementation: Pre- and Postprocessing.” Our machine learning experiments were conducted with version 1.1.3 of the Scikit-learn library () and version 3.3.3 of the LightGBM library ().",,,,,,,
,2023-09-28,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02569-2,,,,,,,
,A synthetic data set to benchmark anti-money laundering methods,,,,,,,
,28,,,,,,,
,9,,,,,,,
,2023,,,,,,,
,"['https://sdv.dev', 'https://sdv.dev/SDV/user_guides/relational/hma1', 'https://scikit-learn.org', 'https://lightgbm.readthedocs.io/en/stable']",,,,,,,
,"All our simulations are made using version 0.14.1 of the SDV library (). We specifically employ the HMA1 model class using two tables as inputs: a primary table with alerts (see Table ) and a secondary table with transactions (see Table ). A demonstration by the SDV developers is available online (; using data different from ours). Due to confidentiality, we do not share our code that (i) transforms the raw data so that it can be fed to the HMA1 model class and (ii) re-transforms and adds noise to the simulated data. The data-providing bank felt that providing this code would reveal sensitive information about its internal setup and the real data. All our transformations are, however, described in detail in our subsection “Implementation: Pre- and Postprocessing.” Our machine learning experiments were conducted with version 1.1.3 of the Scikit-learn library () and version 3.3.3 of the LightGBM library ().",,,,,,,
,2023-09-28,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02551-y,,,,,,,
,Multi-view emotional expressions dataset using 2D pose estimation,,,,,,,
,22,,,,,,,
,9,,,,,,,
,2023,,,,,,,
,['https://doi.org/10.5281/zenodo.8185369'],,,,,,,
,The MATLAB code for parsing the JSON file and processing the coordinates can be found at .,,,,,,,
,2023-09-22,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02489-1,,,,,,,
,A pseudoproxy emulation of the PAGES 2k database using a hierarchy of proxy system models,,,,,,,
,14,,,,,,,
,9,,,,,,,
,2023,,,,,,,
,"['https://doi.org/10.5281/zenodo.7652533', 'https://github.com/fzhu2e/paper-pseudoPAGES2k']",,,,,,,
,The Jupyter notebooks illustrating the usage of the pseudoPAGES2k dataset can be accessed at  or .,,,,,,,
,2023-09-14,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02524-1,,,,,,,
,Open access dataset integrating EEG and fNIRS during Stroop tasks,,,,,,,
,12,,,,,,,
,9,,,,,,,
,2023,,,,,,,
,"['https://github.com/Yaaaaaaaaabby/fNIRS-data-pre-processing-from-Zemeng-Chen.git', 'https://gitee.com/chen-zemeng/f-nirs-data-pre-processing-from-zemeng-chen.git']",,,,,,,
,Scripts to import the fNIRS raw data (.tdms file format) into MATLAB and fNIRS data processing code used above are available at  or . A user guide describing the basic situation and usage of the dataset is uploaded together with the code. There are two files in the zip file. The MATLAB code file named “process_fNIRS_EEG_Stroop” is used to pre-process the fNIRS data of a subject. The folder used for MATLAB to load the .tdms file format is named “Matlab_read_tdms_file”. Please add the folder to the MATLAB search path before loading the .tdms file format.,,,,,,,
,2023-09-12,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02514-3,,,,,,,
,Ontology for the Avida digital evolution platform,,,,,,,
,9,,,,,,,
,9,,,,,,,
,2023,,,,,,,
,['https://graphdb.fortunalab.org'],,,,,,,
,"[{'ext-link': [{'@xlink:href': 'https://gitlab.com/fortunalab/ontoavida', '@ext-link-type': 'uri', '#text': 'https://gitlab.com/fortunalab/ontoavida'}, {'@xlink:href': 'https://gitlab.com/fortunalab/ontoavida/-/blob/master/ontoavida.obo', '@ext-link-type': 'uri', '#text': 'https://gitlab.com/fortunalab/ontoavida/-/blob/master/ontoavida.obo'}, {'@xlink:href': 'https://gitlab.com/fortunalab/ontoavida/-/blob/master/ontoavida.owl', '@ext-link-type': 'uri', '#text': 'https://gitlab.com/fortunalab/ontoavida/-/blob/master/ontoavida.owl'}, {'@xlink:href': 'http://www.obofoundry.org/ontology/ontoavida.html', '@ext-link-type': 'uri', '#text': 'http://www.obofoundry.org/ontology/ontoavida.html'}, {'@xlink:href': 'http://creativecommons.org/licenses/by/4.0/', '@ext-link-type': 'uri', '#text': 'http://creativecommons.org/licenses/by/4.0/'}], '#text': 'The ontology is available in both OBO and OWL format from the GitLab repository () and can be found at  and . OntoAvida OBO and OWL files are also available from the OBO Foundry (). All files are available under the Creative Commons Attribution 4.0 International License () which allows for the copying, redistribution and adaption of the ontology for any purpose.'}, {'ext-link': [{'@xlink:href': 'https://github.com/rdflib/pyLODE', '@ext-link-type': 'uri', '#text': 'https://github.com/rdflib/pyLODE'}, {'@xlink:href': 'https://gitlab.com/fortunalab/pyLODE', '@ext-link-type': 'uri', '#text': 'https://gitlab.com/fortunalab/pyLODE'}, {'@xlink:href': 'https://owl.fortunalab.org/ontoavida', '@ext-link-type': 'uri', '#text': 'https://owl.fortunalab.org/ontoavida'}], 'xref': {'@rid': 'Fig4', '@ref-type': 'fig', '#text': '4'}, '#text': 'pyLODE () was used to obtain a user-friendly visualization of the ontology. pyLODE is based on the OWL Documentation Environment tool (LODE), implemented in Python, and used to generate human-readable HTML documents for OWL and RDF ontologies. We have customized the original pyLODE templates () to convert a scheme of OntoAvida, in a HTML file so that its classes, object properties, and datatype properties can be easily visualized (Fig.\xa0). The pyLODE file of OntoAvida is available at .'}]",,,,,,,
,2023-09-09,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02470-y,,,,,,,
,Applying FAIR4RS principles to develop an integrated modeling environment for the magnetic confinement fusion,,,,,,,
,7,,,,,,,
,9,,,,,,,
,2023,,,,,,,
,['https://doi.org/10.5281/zenodo.8098117'],,,,,,,
,"We use the ZENODO service to keep a persistent archive of FyDev code, with .",,,,,,,
,2023-09-07,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02473-9,,,,,,,
,Monitoring of carbon-water fluxes at Eurasian meteorological stations using random forest and remote sensing,,,,,,,
,7,,,,,,,
,9,,,,,,,
,2023,,,,,,,
,['https://doi.org/10.6084/m9.figshare.21510183.v2'],,,,,,,
,The code to generate the carbon-water flux datasets is available at figshare ().,,,,,,,
,2023-09-07,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02494-4,,,,,,,
,SECURES-Met: A European meteorological data set suitable for electricity modelling applications,,,,,,,
,7,,,,,,,
,9,,,,,,,
,2023,,,,,,,
,['https://doi.org/10.5281/zenodo.8108927'],,,,,,,
,The data are stored as ASCII text (csv) and no specific software is necessary to access the data. The production of the data was done with Python. These scripts are mainly data manipulation routines and do not contribute in processing the data further. To assure repeatability all scripts are available at Zenodo ().,,,,,,,
,2023-09-07,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02483-7,,,,,,,
,Standardised images of novel objects created with generative adversarial networks,,,,,,,
,2,,,,,,,
,9,,,,,,,
,2023,,,,,,,
,['https://artbreeder.com'],,,,,,,
,"Software to generate novel objects is available at . Code to perform data collection (i.e., run the online experiment) was created using jsPsych (version 7.2.1), and a modified version of the  extension (all available at). Code to extract the objective properties of each object, and to compile the subjective ratings from our online study, was written in Python, MATLAB and Julia respectively (available at).",,,,,,,
,2023-09-02,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02475-7,,,,,,,
,Development of global monthly dataset of CMIP6 climate variables for estimating evapotranspiration,,,,,,,
,26,,,,,,,
,8,,,,,,,
,2023,,,,,,,
,['https://pyeto.readthedocs.io/en/latest/'],,,,,,,
,"The code to produce the data was written using Python, PyCharm 2022.2.2. The code is available in pyeto ().",,,,,,,
,2023-08-26,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02339-0,,,,,,,
,Three-dimensional topology dataset of folded radar stratigraphy in northern Greenland,,,,,,,
,7,,,,,,,
,8,,,,,,,
,2023,,,,,,,
,"['https://gitlab.com/openpolarradar/opr', 'https://gitlab.com/openpolarradar/opr/-/wikis/home']",,,,,,,
,"The CReSIS toolbox used to process the MCoRDS RES data is available at , and the main documentation can be found at .",,,,,,,
,2023-08-07,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02339-0,,,,,,,
,Three-dimensional topology dataset of folded radar stratigraphy in northern Greenland,,,,,,,
,7,,,,,,,
,8,,,,,,,
,2023,,,,,,,
,"['https://gitlab.com/openpolarradar/opr', 'https://gitlab.com/openpolarradar/opr/-/wikis/home']",,,,,,,
,"The CReSIS toolbox used to process the MCoRDS RES data is available at , and the main documentation can be found at .",,,,,,,
,2023-08-07,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02432-4,,,,,,,
,"FracAtlas: A Dataset for Fracture Classification, Localization and Segmentation of Musculoskeletal Radiographs",,,,,,,
,5,,,,,,,
,8,,,,,,,
,2023,,,,,,,
,"['https://doi.org/10.6084/m9.figshare.22363012', 'https://github.com/XLR8-07/FracAtlas']",,,,,,,
,"The conversion of DICOM to JPEG image format was done using proprietary software of the X-ray machines from brands like Fujifilm and Philips hence they could not be made available. The mask annotations for segmentation were done using an open-source web tool named makedsense.ai. It was also used for generating VGG annotations from COCO format. As explained in the Methods section, the annotation conversion procedures from COCO to YOLO and YOLO to PASCAL VOC were performed using Python 3.10.1 on a Windows 11 operating system using ‘coco2yolo.ipynb’ and ‘yolo2voc.ipynb’. Both the Jupyter notebooks can be found inside the ‘Utility’ folder along with the dataset at Figshare (). The code used for technical validation can be accessed from (). There are 2 notebooks inside ‘notebooks’ under the root folder called ‘Train_8s.ipynb’ and ‘Prediction_8s.ipynb’. The ‘Train_8s.ipynb’ is used to train 2 models of ‘YOLO8s_seg’ and ‘YOLO8s’ variants targeted toward segmentation and localization tasks respectively. ‘Prediction_8s.ipynb’ is used to generate predictions out of the 2 aforementioned models and view the results.",,,,,,,
,2023-08-05,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02405-7,,,,,,,
,ReCANVo: A database of real-world communicative and affective nonverbal vocalizations,,,,,,,
,5,,,,,,,
,8,,,,,,,
,2023,,,,,,,
,['https://doi.org/10.5281/zenodo.5786859'],,,,,,,
,We used the Python programming language for the data processing described above. Volume segmentation was implemented using the  libary. The label assignment algorithm is summarized in Fig. . The code is available as part of our dataset in Zenodo: .,,,,,,,
,2023-08-05,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02409-3,,,,,,,
,DIPS-Plus: The enhanced database of interacting protein structures for interface prediction,,,,,,,
,3,,,,,,,
,8,,,,,,,
,2023,,,,,,,
,"['Zenodo', 'GitHub', 'GitHub', 'DOI']",,,,,,,
,"Preprocessed data for DIPS-Plus as well as its associated source code and instructions for data processing and reproducibility can be found on  and , respectively. The GitHub instructions illustrate how users can install the Python programming language and build an Anaconda virtual environment containing the software dependencies required to preprocess and analyze DIPS-Plus using the provided Python scripts. Lastly, the GitHub instructions show users how to run such scripts and the order in which to do so to successfully rebuild DIPS-Plus from scratch, to featurize a given PDB file, or to train new machine learning models (e.g., NeiA) for protein interface prediction. For provenance, the original DIPS dataset’s source code can also be found on , along with a corresponding .",,,,,,,
,2023-08-03,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02409-3,,,,,,,
,DIPS-Plus: The enhanced database of interacting protein structures for interface prediction,,,,,,,
,3,,,,,,,
,8,,,,,,,
,2023,,,,,,,
,"['Zenodo', 'GitHub', 'GitHub', 'DOI']",,,,,,,
,"Preprocessed data for DIPS-Plus as well as its associated source code and instructions for data processing and reproducibility can be found on  and , respectively. The GitHub instructions illustrate how users can install the Python programming language and build an Anaconda virtual environment containing the software dependencies required to preprocess and analyze DIPS-Plus using the provided Python scripts. Lastly, the GitHub instructions show users how to run such scripts and the order in which to do so to successfully rebuild DIPS-Plus from scratch, to featurize a given PDB file, or to train new machine learning models (e.g., NeiA) for protein interface prediction. For provenance, the original DIPS dataset’s source code can also be found on , along with a corresponding .",,,,,,,
,2023-08-03,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02409-3,,,,,,,
,DIPS-Plus: The enhanced database of interacting protein structures for interface prediction,,,,,,,
,3,,,,,,,
,8,,,,,,,
,2023,,,,,,,
,"['Zenodo', 'GitHub', 'GitHub', 'DOI']",,,,,,,
,"Preprocessed data for DIPS-Plus as well as its associated source code and instructions for data processing and reproducibility can be found on  and , respectively. The GitHub instructions illustrate how users can install the Python programming language and build an Anaconda virtual environment containing the software dependencies required to preprocess and analyze DIPS-Plus using the provided Python scripts. Lastly, the GitHub instructions show users how to run such scripts and the order in which to do so to successfully rebuild DIPS-Plus from scratch, to featurize a given PDB file, or to train new machine learning models (e.g., NeiA) for protein interface prediction. For provenance, the original DIPS dataset’s source code can also be found on , along with a corresponding .",,,,,,,
,2023-08-03,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02409-3,,,,,,,
,DIPS-Plus: The enhanced database of interacting protein structures for interface prediction,,,,,,,
,3,,,,,,,
,8,,,,,,,
,2023,,,,,,,
,"['Zenodo', 'GitHub', 'GitHub', 'DOI']",,,,,,,
,"Preprocessed data for DIPS-Plus as well as its associated source code and instructions for data processing and reproducibility can be found on  and , respectively. The GitHub instructions illustrate how users can install the Python programming language and build an Anaconda virtual environment containing the software dependencies required to preprocess and analyze DIPS-Plus using the provided Python scripts. Lastly, the GitHub instructions show users how to run such scripts and the order in which to do so to successfully rebuild DIPS-Plus from scratch, to featurize a given PDB file, or to train new machine learning models (e.g., NeiA) for protein interface prediction. For provenance, the original DIPS dataset’s source code can also be found on , along with a corresponding .",,,,,,,
,2023-08-03,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02340-7,,,,,,,
,"A pipeline to further enhance quality, integrity and reusability of the NCCID clinical data",,,,,,,
,27,,,,,,,
,7,,,,,,,
,2023,,,,,,,
,['https://gitlab.developers.cam.ac.uk/maths/cia/covid-19-projects/nccidxclean'],,,,,,,
,"Our extended cleaning pipeline  is publicly available and can be accessed on GitLab:  with accompanying documentation available on the project website (package version at the time of publication: v1.0). The package will automatically download the necessary packages and requirements during installation, including the original NCCID cleaning pipeline upon which our pipeline builds. The python package is independent of the operating system and allows replication of our results using the command line interface, python scripts, or the included Jupyter notebooks. For full analysis, the  parameter can be set to ‘all_with_original’, returning all possible data features. Additional code is provided to allow for the original NHSx pipeline to be run using a single command in the command line. The data from the NHSx pipeline may be then used in the analysis subpackage to generate all figures and numerical results found in this work. An additional subpackage  is included for exploratory data analysis of the final cleaned data. Full step-by-step guidance and further details of the package are provided on the project’s website. .",,,,,,,
,2023-07-27,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02258-0,,,,,,,
,The Translational Data Catalog - discoverable biomedical datasets,,,,,,,
,20,,,,,,,
,7,,,,,,,
,2023,,,,,,,
,['https://datacatalog.elixir-luxembourg.org/'],,,,,,,
,"[{'ext-link': {'@xlink:href': 'https://github.com/FAIRplus/imi-data-catalogue', '@ext-link-type': 'uri', '#text': 'https://github.com/FAIRplus/imi-data-catalogue'}, '#text': 'All Data Catalog code is available in a dedicated repository of the FAIRplus Github organisation, at . The repository includes full documentation on how to deploy a stand-alone version of the Data Catalog.'}, {'ext-link': {'@xlink:href': 'https://github.com/datatagsuite/schema', '@ext-link-type': 'uri', '#text': 'https://github.com/datatagsuite/schema'}, '#text': 'The DATS model is available on Github at .'}]",,,,,,,
,2023-07-20,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02373-y,,,,,,,
,Single-nucleus chromatin landscapes during zebrafish early embryogenesis,,,,,,,
,19,,,,,,,
,7,,,,,,,
,2023,,,,,,,
,['https://figshare.com/articles/dataset/Code/22121171'],,,,,,,
,The codes used to analyze the data in this study were available online ().,,,,,,,
,2023-07-19,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02318-5,,,,,,,
,Helicopter-borne RGB orthomosaics and photogrammetric digital elevation models from the MOSAiC Expedition,,,,,,,
,3,,,,,,,
,7,,,,,,,
,2023,,,,,,,
,['https://gitlab.com/mosaic12/orthomosaics'],,,,,,,
,All processing developed within this study is wrapped in a python environment and is available at . For calculating image footprint locations we used the  python package. Note that part of the code is based on the commercial Agisoft Metashape software requiring licensing.,,,,,,,
,2023-07-03,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02230-y,,,,,,,
,Signing data citations enables data verification and citation persistence,,,,,,,
,27,,,,,,,
,6,,,,,,,
,2023,,,,,,,
,"['https://github.com/bio-guoda/preston', 'https://doi.org/10.5281/zenodo.1410543', 'https://doi.org/10.5281/zenodo.7005141']",,,,,,,
,The source code for the Preston software is available in GitHub at  and in Zenodo at . Preston version 4.4 has been assigned the . The MD5 content signature for the zip archive of the source code is .,,,,,,,
,2023-06-27,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02230-y,,,,,,,
,Signing data citations enables data verification and citation persistence,,,,,,,
,27,,,,,,,
,6,,,,,,,
,2023,,,,,,,
,"['https://github.com/bio-guoda/preston', 'https://doi.org/10.5281/zenodo.1410543', 'https://doi.org/10.5281/zenodo.7005141']",,,,,,,
,The source code for the Preston software is available in GitHub at  and in Zenodo at . Preston version 4.4 has been assigned the . The MD5 content signature for the zip archive of the source code is .,,,,,,,
,2023-06-27,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02321-w,,,,,,,
,A curated gene and biological system annotation of adverse outcome pathways related to human health,,,,,,,
,24,,,,,,,
,6,,,,,,,
,2023,,,,,,,
,['https://doi.org/10.5281/zenodo.7980953'],,,,,,,
,Custom code and data used in the NLP-based prioritisation of the gene set annotations is available in the data repository on Zenodo at  (file aop_mapping_nlp.tar.gz).,,,,,,,
,2023-06-24,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02256-2,,,,,,,
,A guide to sharing open healthcare data under the General Data Protection Regulation,,,,,,,
,24,,,,,,,
,6,,,,,,,
,2023,,,,,,,
,['https://doi.org/10.6084/m9.figshare.22643419'],,,,,,,
,No code was written or used for this paper.,,,,,,,
,2023-06-24,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02238-4,,,,,,,
,An Observation-Based Dataset of Global Sub-Daily Precipitation Indices (GSDR-I),,,,,,,
,22,,,,,,,
,6,,,,,,,
,2023,,,,,,,
,['https://doi.org/10.5281/zenodo.7492877'],,,,,,,
,The Python code for indices calculation based on gauge records and subsequent gridding is available in the code repository here: .,,,,,,,
,2023-06-22,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02282-0,,,,,,,
,"Population, land use and economic exposure estimates for Europe at 100 m resolution from 1870 to 2020",,,,,,,
,8,,,,,,,
,6,,,,,,,
,2023,,,,,,,
,"['https://doi.org/10.5281/zenodo.7556953', 'https://doi.org/10.5281/zenodo.6783023', 'https://doi.org/10.4121/collection:HANZE']",,,,,,,
,"The source code of HANZE v2.0 (implemented in Python 3.9) presented in the paper is archived at . All necessary input data are archived at . The flood impact data shown in Usage Notes, with a description of sources of the data, are available in the HANZE v1.0 repository, .",,,,,,,
,2023-06-08,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02282-0,,,,,,,
,"Population, land use and economic exposure estimates for Europe at 100 m resolution from 1870 to 2020",,,,,,,
,8,,,,,,,
,6,,,,,,,
,2023,,,,,,,
,"['https://doi.org/10.5281/zenodo.7556953', 'https://doi.org/10.5281/zenodo.6783023', 'https://doi.org/10.4121/collection:HANZE']",,,,,,,
,"The source code of HANZE v2.0 (implemented in Python 3.9) presented in the paper is archived at . All necessary input data are archived at . The flood impact data shown in Usage Notes, with a description of sources of the data, are available in the HANZE v1.0 repository, .",,,,,,,
,2023-06-08,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02282-0,,,,,,,
,"Population, land use and economic exposure estimates for Europe at 100 m resolution from 1870 to 2020",,,,,,,
,8,,,,,,,
,6,,,,,,,
,2023,,,,,,,
,"['https://doi.org/10.5281/zenodo.7556953', 'https://doi.org/10.5281/zenodo.6783023', 'https://doi.org/10.4121/collection:HANZE']",,,,,,,
,"The source code of HANZE v2.0 (implemented in Python 3.9) presented in the paper is archived at . All necessary input data are archived at . The flood impact data shown in Usage Notes, with a description of sources of the data, are available in the HANZE v1.0 repository, .",,,,,,,
,2023-06-08,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02208-w,,,,,,,
,CORE: A Global Aggregation Service for Open Access Papers,,,,,,,
,7,,,,,,,
,6,,,,,,,
,2023,,,,,,,
,['https://github.com/oacore/'],,,,,,,
,"CORE consists of multiple services. Most of our source code is open source and available in our public repository on GitHub (). As of today, we are unfortunately not yet able to provide the source code to our data ingestion module. However, as we want to be as transparent as possible with our community, we have documented in this paper the key algorithms and processes which we apply using pseudocode.",,,,,,,
,2023-06-07,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02242-8,,,,,,,
,How to establish and maintain a multimodal animal research dataset using DataLad,,,,,,,
,5,,,,,,,
,6,,,,,,,
,2023,,,,,,,
,['https://doi.org/10.12751/g-node.3yl5qi'],,,,,,,
,DataLad and GIN are freely available. The manuscript contains all code to reproduce the workflow.,,,,,,,
,2023-06-05,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02269-x,,,,,,,
,PANGAEA - Data Publisher for Earth & Environmental Science,,,,,,,
,2,,,,,,,
,6,,,,,,,
,2023,,,,,,,
,['https://github.com/pangaea-data-publisher'],,,,,,,
,"The code supporting the users with data retrieval and submission is freely available at . PANGAEA as a repository does not generate, test, or process data and metadata, therefore no custom code has been used.",,,,,,,
,2023-06-02,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02214-y,,,,,,,
,Responses of pyramidal cell somata and apical dendrites in mouse visual cortex over multiple days,,,,,,,
,17,,,,,,,
,5,,,,,,,
,2023,,,,,,,
,"['https://github.com/colleenjg/OpenScope_CA_Analysis', 'https://github.com/AllenInstitute/AllenSDK', 'https://github.com/colleenjg/cred_assign_stimuli', 'https://github.com/schnitzer-lab/EXTRACT-public', 'http://www.mackenziemathislab.org/deeplabcut', 'https://github.com/AllenInstitute/ophys_nway_matching', 'https://github.com/jeromelecoq/QC_2P/blob/master/Example%20use%20of%20QC_2P.ipynb']",,,,,,,
,"Data pre-processing was performed in Python 3.6 with custom scripts that are freely available on GitHub () and were developed using the following packages: NumPy, SciPy, Pandas, Matplotlib, Scikit-learn 0.21.1, and the AllenSDK 1.6.0. (). Stimuli were generated by Python 2.7 custom scripts based on PsychoPy 1.82.01 and CamStim 0.2.4. The code is freely available (along with instructions to reproduce the stimuli, and example videos) on GitHub (). Dendritic segmentation was run in Matlab 2019a using a robust estimation algorithm (). Pupil tracking was performed using DeepLabCut 2.0.5 (). ROIs were matched across sessions using a custom-modified version of the n-way cell matching package developed by the Allen Institute (). Code for estimating photon conversion statistics on the raw imaging stacks is available on GitHub ().",,,,,,,
,2023-05-17,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02196-x,,,,,,,
,Database covering the prayer movements which were not available previously,,,,,,,
,12,,,,,,,
,5,,,,,,,
,2023,,,,,,,
,['https://biomechlab.iyte.edu.tr/en/homepage/38'],,,,,,,
,"The code written for the development of the database is available upon request from the authors, but it is not open to the external users through the website to protect the database. The desired main functions of the database were created in Python and Django framework. Django was used for creating the model for the backend and Javascript for filtering functions. For the front-end, HTML, CSS, and JQuery were used. The database is available through website  and the public repository Database covering the previously excluded daily life activities | Aperta (ulakbim.gov.tr).",,,,,,,
,2023-05-12,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02100-7,,,,,,,
,VinDr-Mammo: A large-scale benchmark dataset for computer-aided diagnosis in full-field digital mammography,,,,,,,
,12,,,,,,,
,5,,,,,,,
,2023,,,,,,,
,"['https://www.python.org/', 'https://pydicom.github.io/', 'https://docs.python.org/3/library/hashlib.html', 'https://github.com/vinbigdata-medical/vindr-mammo']",,,,,,,
,The codes used in this study were made publicly available. The scripts used for loading and processing DICOM images are based on the following open-source repositories: Python 3.8.0 (); Pydicom 1.2.0 (); and Python hashlib (). The code for data pseudonymization and stratification was made publicly available at .,,,,,,,
,2023-05-12,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02100-7,,,,,,,
,VinDr-Mammo: A large-scale benchmark dataset for computer-aided diagnosis in full-field digital mammography,,,,,,,
,12,,,,,,,
,5,,,,,,,
,2023,,,,,,,
,"['https://www.python.org/', 'https://pydicom.github.io/', 'https://docs.python.org/3/library/hashlib.html', 'https://github.com/vinbigdata-medical/vindr-mammo']",,,,,,,
,The codes used in this study were made publicly available. The scripts used for loading and processing DICOM images are based on the following open-source repositories: Python 3.8.0 (); Pydicom 1.2.0 (); and Python hashlib (). The code for data pseudonymization and stratification was made publicly available at .,,,,,,,
,2023-05-12,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02100-7,,,,,,,
,VinDr-Mammo: A large-scale benchmark dataset for computer-aided diagnosis in full-field digital mammography,,,,,,,
,12,,,,,,,
,5,,,,,,,
,2023,,,,,,,
,"['https://www.python.org/', 'https://pydicom.github.io/', 'https://docs.python.org/3/library/hashlib.html', 'https://github.com/vinbigdata-medical/vindr-mammo']",,,,,,,
,The codes used in this study were made publicly available. The scripts used for loading and processing DICOM images are based on the following open-source repositories: Python 3.8.0 (); Pydicom 1.2.0 (); and Python hashlib (). The code for data pseudonymization and stratification was made publicly available at .,,,,,,,
,2023-05-12,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02176-1,,,,,,,
,Crowd cluster data in the USA for analysis of human response to COVID-19 events and policies,,,,,,,
,10,,,,,,,
,5,,,,,,,
,2023,,,,,,,
,['https://www.mpi-forum.org'],,,,,,,
,The codes were written in C++ and Python 3. They rely on Python DBSCAN package and on MPI () for parallelization. The code is available from the data site.,,,,,,,
,2023-05-10,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02150-x,,,,,,,
,Fatigue database of additively manufactured alloys,,,,,,,
,2,,,,,,,
,5,,,,,,,
,2023,,,,,,,
,"['ChemDataExtractor 2.0', 'table extractor', 'Simple Transformer', 'https://simpletransformers.ai/', 'https://github.com/xuzpgroup/ZianZhang/tree/main/FatigueData-AM2022']",,,,,,,
,"The scripts utilized to extract information from figures, tables, and text are mainly based on open-source codes such as , , and  (), respectively. The in-house scripts for data extraction and analysis are publicly released at the GitHub repository (), which can be used by acknowledging the current article and under the MIT license. These scripts include a detailed, step-by-step tutorial for loading and analyzing the dataset in the repository.",,,,,,,
,2023-05-02,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02150-x,,,,,,,
,Fatigue database of additively manufactured alloys,,,,,,,
,2,,,,,,,
,5,,,,,,,
,2023,,,,,,,
,"['ChemDataExtractor 2.0', 'table extractor', 'Simple Transformer', 'https://simpletransformers.ai/', 'https://github.com/xuzpgroup/ZianZhang/tree/main/FatigueData-AM2022']",,,,,,,
,"The scripts utilized to extract information from figures, tables, and text are mainly based on open-source codes such as , , and  (), respectively. The in-house scripts for data extraction and analysis are publicly released at the GitHub repository (), which can be used by acknowledging the current article and under the MIT license. These scripts include a detailed, step-by-step tutorial for loading and analyzing the dataset in the repository.",,,,,,,
,2023-05-02,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02150-x,,,,,,,
,Fatigue database of additively manufactured alloys,,,,,,,
,2,,,,,,,
,5,,,,,,,
,2023,,,,,,,
,"['ChemDataExtractor 2.0', 'table extractor', 'Simple Transformer', 'https://simpletransformers.ai/', 'https://github.com/xuzpgroup/ZianZhang/tree/main/FatigueData-AM2022']",,,,,,,
,"The scripts utilized to extract information from figures, tables, and text are mainly based on open-source codes such as , , and  (), respectively. The in-house scripts for data extraction and analysis are publicly released at the GitHub repository (), which can be used by acknowledging the current article and under the MIT license. These scripts include a detailed, step-by-step tutorial for loading and analyzing the dataset in the repository.",,,,,,,
,2023-05-02,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02150-x,,,,,,,
,Fatigue database of additively manufactured alloys,,,,,,,
,2,,,,,,,
,5,,,,,,,
,2023,,,,,,,
,"['ChemDataExtractor 2.0', 'table extractor', 'Simple Transformer', 'https://simpletransformers.ai/', 'https://github.com/xuzpgroup/ZianZhang/tree/main/FatigueData-AM2022']",,,,,,,
,"The scripts utilized to extract information from figures, tables, and text are mainly based on open-source codes such as , , and  (), respectively. The in-house scripts for data extraction and analysis are publicly released at the GitHub repository (), which can be used by acknowledging the current article and under the MIT license. These scripts include a detailed, step-by-step tutorial for loading and analyzing the dataset in the repository.",,,,,,,
,2023-05-02,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02134-x,,,,,,,
,OpCitance: Citation contexts identified from the PubMed Central open access articles,,,,,,,
,28,,,,,,,
,4,,,,,,,
,2023,,,,,,,
,['https://doi.org/10.13012/B2IDB-4353270_V2'],,,,,,,
,The code of our XML parser is provided in the Supplementary_File_1.zip on our data repository: .,,,,,,,
,2023-04-28,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02127-w,,,,,,,
,Dataset of theoretical multinary perovskite oxides,,,,,,,
,28,,,,,,,
,4,,,,,,,
,2023,,,,,,,
,"['https://www.unf.edu/~michael.lufaso/spuds/', 'https://github.com/zaba1157/PySPuDS', 'https://github.com/rymo1354/crystal_motifs']",,,,,,,
,"We used the pymatgen python package, which is open-source software under the Massachusetts Institute of Technology License, for materials analysis as well as the generation of VASP inputs and CIF files. The VASP DFT code used is accessible under a paid license, copyrighted by the University of Vienna, Austria. Initial structures were generated with SPuDS DOS version >2.20.08.06 () using a custom high-throughput python wrapper available on GitHub (). Perovskite/non-perovskite classifications were performed using a custom python package that is available on GitHub () and based on the pymatgen and NetworkX graph network python packages.",,,,,,,
,2023-04-28,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02102-5,,,,,,,
,"PediCXR: An open, large-scale chest radiograph dataset for interpretation of common thoracic diseases in children",,,,,,,
,27,,,,,,,
,4,,,,,,,
,2023,,,,,,,
,"['https://www.python.org/', 'https://pydicom.github.io/', 'https://pypi.org/project/opencv-python/', 'https://docs.python.org/3/library/hashlib.html', 'https://github.com/vinbigdata-medical/vindr-cxr', 'https://github.com/vinbigdata-medical/DICOM-Imaging-Router', 'https://vindr.ai/vindr-lab']",,,,,,,
,This study used the following open-source repositories to load and process DICOM scans: Python 3.7.0 (); Pydicom 1.2.0 (); OpenCV-Python 4.2.0.34 (); and Python hashlib (). The code for data de-identification was made publicly available at . The code to train CNN classifier for the out-of-distribution task was made publicly available at . The VinDr Lab is an open source software and can be found at .,,,,,,,
,2023-04-27,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02102-5,,,,,,,
,"PediCXR: An open, large-scale chest radiograph dataset for interpretation of common thoracic diseases in children",,,,,,,
,27,,,,,,,
,4,,,,,,,
,2023,,,,,,,
,"['https://www.python.org/', 'https://pydicom.github.io/', 'https://pypi.org/project/opencv-python/', 'https://docs.python.org/3/library/hashlib.html', 'https://github.com/vinbigdata-medical/vindr-cxr', 'https://github.com/vinbigdata-medical/DICOM-Imaging-Router', 'https://vindr.ai/vindr-lab']",,,,,,,
,This study used the following open-source repositories to load and process DICOM scans: Python 3.7.0 (); Pydicom 1.2.0 (); OpenCV-Python 4.2.0.34 (); and Python hashlib (). The code for data de-identification was made publicly available at . The code to train CNN classifier for the out-of-distribution task was made publicly available at . The VinDr Lab is an open source software and can be found at .,,,,,,,
,2023-04-27,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02102-5,,,,,,,
,"PediCXR: An open, large-scale chest radiograph dataset for interpretation of common thoracic diseases in children",,,,,,,
,27,,,,,,,
,4,,,,,,,
,2023,,,,,,,
,"['https://www.python.org/', 'https://pydicom.github.io/', 'https://pypi.org/project/opencv-python/', 'https://docs.python.org/3/library/hashlib.html', 'https://github.com/vinbigdata-medical/vindr-cxr', 'https://github.com/vinbigdata-medical/DICOM-Imaging-Router', 'https://vindr.ai/vindr-lab']",,,,,,,
,This study used the following open-source repositories to load and process DICOM scans: Python 3.7.0 (); Pydicom 1.2.0 (); OpenCV-Python 4.2.0.34 (); and Python hashlib (). The code for data de-identification was made publicly available at . The code to train CNN classifier for the out-of-distribution task was made publicly available at . The VinDr Lab is an open source software and can be found at .,,,,,,,
,2023-04-27,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02102-5,,,,,,,
,"PediCXR: An open, large-scale chest radiograph dataset for interpretation of common thoracic diseases in children",,,,,,,
,27,,,,,,,
,4,,,,,,,
,2023,,,,,,,
,"['https://www.python.org/', 'https://pydicom.github.io/', 'https://pypi.org/project/opencv-python/', 'https://docs.python.org/3/library/hashlib.html', 'https://github.com/vinbigdata-medical/vindr-cxr', 'https://github.com/vinbigdata-medical/DICOM-Imaging-Router', 'https://vindr.ai/vindr-lab']",,,,,,,
,This study used the following open-source repositories to load and process DICOM scans: Python 3.7.0 (); Pydicom 1.2.0 (); OpenCV-Python 4.2.0.34 (); and Python hashlib (). The code for data de-identification was made publicly available at . The code to train CNN classifier for the out-of-distribution task was made publicly available at . The VinDr Lab is an open source software and can be found at .,,,,,,,
,2023-04-27,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02102-5,,,,,,,
,"PediCXR: An open, large-scale chest radiograph dataset for interpretation of common thoracic diseases in children",,,,,,,
,27,,,,,,,
,4,,,,,,,
,2023,,,,,,,
,"['https://www.python.org/', 'https://pydicom.github.io/', 'https://pypi.org/project/opencv-python/', 'https://docs.python.org/3/library/hashlib.html', 'https://github.com/vinbigdata-medical/vindr-cxr', 'https://github.com/vinbigdata-medical/DICOM-Imaging-Router', 'https://vindr.ai/vindr-lab']",,,,,,,
,This study used the following open-source repositories to load and process DICOM scans: Python 3.7.0 (); Pydicom 1.2.0 (); OpenCV-Python 4.2.0.34 (); and Python hashlib (). The code for data de-identification was made publicly available at . The code to train CNN classifier for the out-of-distribution task was made publicly available at . The VinDr Lab is an open source software and can be found at .,,,,,,,
,2023-04-27,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02138-7,,,,,,,
,Complete Global Total Electron Content Map Dataset based on a Video Imputation Algorithm VISTA,,,,,,,
,25,,,,,,,
,4,,,,,,,
,2023,,,,,,,
,['https://vista-tec.shinyapps.io/VISTA-Dashboard/'],,,,,,,
,"Details about codes that generate the dataset as well as the usage notes on accessing, downloading and pre-processing the datasets are made available on the homepage of the dataset on the Deep Blue Data system of University of Michigan. Future updates of the codes and dataset will be made available on this website as well. Please contact the corresponding author for data request and questions. Additionally, our users can explore our interactive database dashboard () for more technical details and run the VISTA algorithm live.",,,,,,,
,2023-04-25,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02066-6,,,,,,,
,HIT-UAV: A high-altitude infrared thermal dataset for Unmanned Aerial Vehicle-based object detection,,,,,,,
,20,,,,,,,
,4,,,,,,,
,2023,,,,,,,
,['https://pegasus.ac.cn'],,,,,,,
,"The data processing code is available in the  folder of . The code is written in Python. The functions of the tools are as follows: (1) The  is to convert oriented bounding boxes to standard bounding boxes and generate the dataset, (2) The  is to visualize images with bounding boxes, (3) The  is to generate the label files with the YOLO format to help users train the YOLO, which is the representative object detection algorithm.",,,,,,,
,2023-04-20,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02094-2,,,,,,,
,CarbonMonitor-Power near-real-time monitoring of global power generation on hourly to daily scales,,,,,,,
,17,,,,,,,
,4,,,,,,,
,2023,,,,,,,
,"['https://github.com/KowComical/CM_Power_Data', 'https://power.carbonmonitor.org']",,,,,,,
,"The generated datasets and the codes for producing the datasets are available from  and . The most up-to-date, continuously updated data can be visualized and uploaded from . Codes are available upon reasonable requests.",,,,,,,
,2023-04-17,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02101-6,,,,,,,
,Unified access to up-to-date residue-level annotations from UniProtKB and other biological databases for PDB data,,,,,,,
,12,,,,,,,
,4,,,,,,,
,2023,,,,,,,
,"['https://colab.research.google.com/github/PDBe-KB/sifts_data_analysis/blob/main/sifts.ipynb', 'https://github.com/PDBe-KB/sifts_data_analysis']",,,,,,,
,"To assist users in utilising the updated PDBx/mmCIF files and SIFTS annotations, a Google Colab notebook is available at  or via GitHub at . This notebook provides information on how to parse, extract and filter SIFTS annotations from the updated PDBx/mmCIF files. Additionally, the notebook demonstrates how users can compare various numbering schemes of a given residue across different PDB structures of the same protein.",,,,,,,
,2023-04-12,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02096-0,,,,,,,
,A standardized catalogue of spectral indices to advance the use of remote sensing in Earth system research,,,,,,,
,8,,,,,,,
,4,,,,,,,
,2023,,,,,,,
,"['https://github.com/awesome-spectral-indices/awesome-spectral-indices', 'https://github.com/awesome-spectral-indices/spyndex', 'https://pypi.org/project/spyndex/', 'https://anaconda.org/conda-forge/spyndex', 'https://share.streamlit.io/davemlz/espectro/main/espectro.py', 'https://github.com/awesome-spectral-indices/espectro']",,,,,,,
,"The ASI Catalogue code is open-source and can be found at  and Zenodo. The  Python package is open-source and can be found at . It is also available through PyPI () and conda-forge (). The catalogue in CSV format can also be downloaded from the  Streamlit web app, which is available at . The  Streamlit web app code is open-source and can be found at .",,,,,,,
,2023-04-08,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02096-0,,,,,,,
,A standardized catalogue of spectral indices to advance the use of remote sensing in Earth system research,,,,,,,
,8,,,,,,,
,4,,,,,,,
,2023,,,,,,,
,"['https://github.com/awesome-spectral-indices/awesome-spectral-indices', 'https://github.com/awesome-spectral-indices/spyndex', 'https://pypi.org/project/spyndex/', 'https://anaconda.org/conda-forge/spyndex', 'https://share.streamlit.io/davemlz/espectro/main/espectro.py', 'https://github.com/awesome-spectral-indices/espectro']",,,,,,,
,"The ASI Catalogue code is open-source and can be found at  and Zenodo. The  Python package is open-source and can be found at . It is also available through PyPI () and conda-forge (). The catalogue in CSV format can also be downloaded from the  Streamlit web app, which is available at . The  Streamlit web app code is open-source and can be found at .",,,,,,,
,2023-04-08,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02096-0,,,,,,,
,A standardized catalogue of spectral indices to advance the use of remote sensing in Earth system research,,,,,,,
,8,,,,,,,
,4,,,,,,,
,2023,,,,,,,
,"['https://github.com/awesome-spectral-indices/awesome-spectral-indices', 'https://github.com/awesome-spectral-indices/spyndex', 'https://pypi.org/project/spyndex/', 'https://anaconda.org/conda-forge/spyndex', 'https://share.streamlit.io/davemlz/espectro/main/espectro.py', 'https://github.com/awesome-spectral-indices/espectro']",,,,,,,
,"The ASI Catalogue code is open-source and can be found at  and Zenodo. The  Python package is open-source and can be found at . It is also available through PyPI () and conda-forge (). The catalogue in CSV format can also be downloaded from the  Streamlit web app, which is available at . The  Streamlit web app code is open-source and can be found at .",,,,,,,
,2023-04-08,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02095-1,,,,,,,
,A century and a half precipitation oxygen isoscape for China generated using data fusion and bias correction,,,,,,,
,6,,,,,,,
,4,,,,,,,
,2023,,,,,,,
,['https://doi.org/10.5281/zenodo.7306199'],,,,,,,
,"The codes for two bias correction methods (LS and DT) and three neural network data fusion methods (BP, LSTM and CNN) are available at . The codes were programmed using MATLAB version 2022a and Python 3.8.",,,,,,,
,2023-04-06,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02049-7,,,,,,,
,Characterizing uncertainty in Community Land Model version 5 hydrological applications in the United States,,,,,,,
,6,,,,,,,
,4,,,,,,,
,2023,,,,,,,
,"['https://doi.org/10.57931/1922953', 'https://doi.org/10.5281/zenodo.6653704', 'https://github.com/ESCOMP/CTSM/tree/branch_tags/PPE.n11_ctsm5.1.dev030', 'https://doi.org/10.5281/zenodo.7039118', 'https://github.com/UW-Hydro/MetSim']",,,,,,,
,"The CLM5 hydrological datasets are available to the public at  in comma-separated value (.csv) and netcdf (.nc) formats. This experiment used a modified version of CLM5 designed to allow easier parameterization and support machine-specific compilation. The modified source code is available at , forked from . Source codes that were used to develop and analyze the data are available at . The MetSim disaggregation code is available at .",,,,,,,
,2023-04-06,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02049-7,,,,,,,
,Characterizing uncertainty in Community Land Model version 5 hydrological applications in the United States,,,,,,,
,6,,,,,,,
,4,,,,,,,
,2023,,,,,,,
,"['https://doi.org/10.57931/1922953', 'https://doi.org/10.5281/zenodo.6653704', 'https://github.com/ESCOMP/CTSM/tree/branch_tags/PPE.n11_ctsm5.1.dev030', 'https://doi.org/10.5281/zenodo.7039118', 'https://github.com/UW-Hydro/MetSim']",,,,,,,
,"The CLM5 hydrological datasets are available to the public at  in comma-separated value (.csv) and netcdf (.nc) formats. This experiment used a modified version of CLM5 designed to allow easier parameterization and support machine-specific compilation. The modified source code is available at , forked from . Source codes that were used to develop and analyze the data are available at . The MetSim disaggregation code is available at .",,,,,,,
,2023-04-06,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02049-7,,,,,,,
,Characterizing uncertainty in Community Land Model version 5 hydrological applications in the United States,,,,,,,
,6,,,,,,,
,4,,,,,,,
,2023,,,,,,,
,"['https://doi.org/10.57931/1922953', 'https://doi.org/10.5281/zenodo.6653704', 'https://github.com/ESCOMP/CTSM/tree/branch_tags/PPE.n11_ctsm5.1.dev030', 'https://doi.org/10.5281/zenodo.7039118', 'https://github.com/UW-Hydro/MetSim']",,,,,,,
,"The CLM5 hydrological datasets are available to the public at  in comma-separated value (.csv) and netcdf (.nc) formats. This experiment used a modified version of CLM5 designed to allow easier parameterization and support machine-specific compilation. The modified source code is available at , forked from . Source codes that were used to develop and analyze the data are available at . The MetSim disaggregation code is available at .",,,,,,,
,2023-04-06,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02089-z,,,,,,,
,A corpus of CO electrocatalytic reduction process extracted from the scientific literature,,,,,,,
,29,,,,,,,
,3,,,,,,,
,2023,,,,,,,
,"['https://github.com/kg4sci/electrocatalytic_db', 'chemdataextractor.org', 'radimrehurek.com', 'https://github.com/pymupdf/PyMuPDFPyMuPDF', 'www.pytorch.org', 'scikit-learn.org']",,,,,,,
,"The scripts utilized to parse articles and extract entities are home-written codes which are publicly available at the github repository . The underlying machine-learning libraries used in this project are all open-source: ChemDataExtractor (), gensim (), PyMuPDF(), Pytorch () and scikit-learn ().",,,,,,,
,2023-03-29,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02089-z,,,,,,,
,A corpus of CO electrocatalytic reduction process extracted from the scientific literature,,,,,,,
,29,,,,,,,
,3,,,,,,,
,2023,,,,,,,
,"['https://github.com/kg4sci/electrocatalytic_db', 'chemdataextractor.org', 'radimrehurek.com', 'https://github.com/pymupdf/PyMuPDFPyMuPDF', 'www.pytorch.org', 'scikit-learn.org']",,,,,,,
,"The scripts utilized to parse articles and extract entities are home-written codes which are publicly available at the github repository . The underlying machine-learning libraries used in this project are all open-source: ChemDataExtractor (), gensim (), PyMuPDF(), Pytorch () and scikit-learn ().",,,,,,,
,2023-03-29,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02089-z,,,,,,,
,A corpus of CO electrocatalytic reduction process extracted from the scientific literature,,,,,,,
,29,,,,,,,
,3,,,,,,,
,2023,,,,,,,
,"['https://github.com/kg4sci/electrocatalytic_db', 'chemdataextractor.org', 'radimrehurek.com', 'https://github.com/pymupdf/PyMuPDFPyMuPDF', 'www.pytorch.org', 'scikit-learn.org']",,,,,,,
,"The scripts utilized to parse articles and extract entities are home-written codes which are publicly available at the github repository . The underlying machine-learning libraries used in this project are all open-source: ChemDataExtractor (), gensim (), PyMuPDF(), Pytorch () and scikit-learn ().",,,,,,,
,2023-03-29,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02089-z,,,,,,,
,A corpus of CO electrocatalytic reduction process extracted from the scientific literature,,,,,,,
,29,,,,,,,
,3,,,,,,,
,2023,,,,,,,
,"['https://github.com/kg4sci/electrocatalytic_db', 'chemdataextractor.org', 'radimrehurek.com', 'https://github.com/pymupdf/PyMuPDFPyMuPDF', 'www.pytorch.org', 'scikit-learn.org']",,,,,,,
,"The scripts utilized to parse articles and extract entities are home-written codes which are publicly available at the github repository . The underlying machine-learning libraries used in this project are all open-source: ChemDataExtractor (), gensim (), PyMuPDF(), Pytorch () and scikit-learn ().",,,,,,,
,2023-03-29,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02030-4,,,,,,,
,A synthetic population for agent-based modelling in Canada,,,,,,,
,21,,,,,,,
,3,,,,,,,
,2023,,,,,,,
,['https://pypi.org/'],,,,,,,
,"The python scripts (python 3.10) developed for the generation and validation of the synthetic dataset are publicly and freely accessible on Zenodo. The scripts use the following python packages: pandas (1.4.4), numpy (1.23.2), pyreadstat (1.19), scipy (1.9.1) for the Pearson’s correlation coefficient computation, scikit-learn (1.1.2) for the RMSE computation, and matplotlib (3.5.3) to generate the charts. All these python packages are available from the Python Package Index: . The humanleague package (2.1.10) providing the QISI and IPF implementations is available from the Python Package Index and on Zenodo.",,,,,,,
,2023-03-21,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-01974-x,,,,,,,
,Evaluating explainability for graph neural networks,,,,,,,
,18,,,,,,,
,3,,,,,,,
,2023,,,,,,,
,"['https://zitniklab.hms.harvard.edu/projects/GraphXAI', 'https://github.com/mims-harvard/GraphXAI']",,,,,,,
,"Project website for GXAI is at . The code to reproduce results, documentation, and tutorials are available in GXAI ‘s Github repository at . The repository contains Python scripts to generate and evaluate explanations using performance metrics and also visualize explanationa. In addition, the repository contains information and Python scripts to build new versions of GXAI as the underlying primary resources get updated and new data become available.",,,,,,,
,2023-03-18,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02044-y,,,,,,,
,"LocalView, a database of public meetings for the study of local politics and policy-making in the United States",,,,,,,
,15,,,,,,,
,3,,,,,,,
,2023,,,,,,,
,"['https://doi.org/10.7910/DVN/NJTBEM', 'https://doi.org/10.7910/DVN/KHUXIN', 'https://localview.net']",,,,,,,
,"The  dataset is publicly available at . Code to replicate the main and supplementary analyses in this paper is available at . More information, including a codebook and related research, is linked on our companion website at .",,,,,,,
,2023-03-15,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02044-y,,,,,,,
,"LocalView, a database of public meetings for the study of local politics and policy-making in the United States",,,,,,,
,15,,,,,,,
,3,,,,,,,
,2023,,,,,,,
,"['https://doi.org/10.7910/DVN/NJTBEM', 'https://doi.org/10.7910/DVN/KHUXIN', 'https://localview.net']",,,,,,,
,"The  dataset is publicly available at . Code to replicate the main and supplementary analyses in this paper is available at . More information, including a codebook and related research, is linked on our companion website at .",,,,,,,
,2023-03-15,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02044-y,,,,,,,
,"LocalView, a database of public meetings for the study of local politics and policy-making in the United States",,,,,,,
,15,,,,,,,
,3,,,,,,,
,2023,,,,,,,
,"['https://doi.org/10.7910/DVN/NJTBEM', 'https://doi.org/10.7910/DVN/KHUXIN', 'https://localview.net']",,,,,,,
,"The  dataset is publicly available at . Code to replicate the main and supplementary analyses in this paper is available at . More information, including a codebook and related research, is linked on our companion website at .",,,,,,,
,2023-03-15,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02045-x,,,,,,,
,IceLines – A new data set of Antarctic ice shelf front positions,,,,,,,
,15,,,,,,,
,3,,,,,,,
,2023,,,,,,,
,"['https://github.com/khdlr/HED-UNet', 'https://download.geoservice.dlr.de/icelines/files/icelines_auxiliary_v1.zip']",,,,,,,
,"The processing of IceLines has been carried out in the Calvalus system and GPU-cluster available at DLR’s Earth Observation Center by means of proprietary software and dedicated Python (v3.6 and v3.7) scripts. Given the use of proprietary tools, the implemented processing pipeline cannot be openly released to the public. However, all processing steps can be accessed and reproduced as follows: The pre-processing of the Sentinel-1 imagery can be replicated with the open source ESA SNAP Toolbox 8.0 and the processing steps described in Fig. . The code of the HED-Unet based on Pytorch (v1.7) is available at  and the final post-processing script (Python v3.7) is available at . Additionally, this folder includes data used for the accuracy assessment, a Python script for bulk download (bulk-download-icelines.py) and an exemplary script (‘display-icelines-gee.js’) to display IceLines data with the corresponding Sentinel-1 scenes in Google Earth Engine.",,,,,,,
,2023-03-15,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02028-y,,,,,,,
,FAIRification of health-related data using semantic web technologies in the Swiss Personalized Health Network,,,,,,,
,10,,,,,,,
,3,,,,,,,
,2023,,,,,,,
,"['https://www.biomedit.ch/rdf/sphn-ontology/sphn/2022/2', 'https://git.dcc.sib.swiss/sphn-semantic-framework/sphn-ontology/-/tree/master/ontology', 'https://doi.org/10.5281/zenodo.7390281']",,,,,,,
,No custom code was used to generate this work.,,,,,,,
,2023-03-10,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02028-y,,,,,,,
,FAIRification of health-related data using semantic web technologies in the Swiss Personalized Health Network,,,,,,,
,10,,,,,,,
,3,,,,,,,
,2023,,,,,,,
,"['https://www.biomedit.ch/rdf/sphn-ontology/sphn/2022/2', 'https://git.dcc.sib.swiss/sphn-semantic-framework/sphn-ontology/-/tree/master/ontology', 'https://doi.org/10.5281/zenodo.7390281']",,,,,,,
,No custom code was used to generate this work.,,,,,,,
,2023-03-10,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02028-y,,,,,,,
,FAIRification of health-related data using semantic web technologies in the Swiss Personalized Health Network,,,,,,,
,10,,,,,,,
,3,,,,,,,
,2023,,,,,,,
,"['https://www.biomedit.ch/rdf/sphn-ontology/sphn/2022/2', 'https://git.dcc.sib.swiss/sphn-semantic-framework/sphn-ontology/-/tree/master/ontology', 'https://doi.org/10.5281/zenodo.7390281']",,,,,,,
,No custom code was used to generate this work.,,,,,,,
,2023-03-10,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02037-x,,,,,,,
,WAVES – The Lucile Packard Children’s Hospital Pediatric Physiological Waveforms Dataset,,,,,,,
,7,,,,,,,
,3,,,,,,,
,2023,,,,,,,
,"['https://bitbucket.org/surfstanfordmedicine/waves-utilities/src/main/', 'https://pypi.org/project/waves-utilities/']",,,,,,,
,"Redivis provides a visual drag-and-drop filtering user interface that allows the user to select columns of interest, filter on properties of interest, and limit output parameters before creating a downloadable CSV file. Sample scripts for working with data downloaded from Redivis and plotting sample waveforms are available in open-source repositories:  and .",,,,,,,
,2023-03-07,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02037-x,,,,,,,
,WAVES – The Lucile Packard Children’s Hospital Pediatric Physiological Waveforms Dataset,,,,,,,
,7,,,,,,,
,3,,,,,,,
,2023,,,,,,,
,"['https://bitbucket.org/surfstanfordmedicine/waves-utilities/src/main/', 'https://pypi.org/project/waves-utilities/']",,,,,,,
,"Redivis provides a visual drag-and-drop filtering user interface that allows the user to select columns of interest, filter on properties of interest, and limit output parameters before creating a downloadable CSV file. Sample scripts for working with data downloaded from Redivis and plotting sample waveforms are available in open-source repositories:  and .",,,,,,,
,2023-03-07,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02008-2,,,,,,,
,"Global Dam Tracker: A database of more than 35,000 dams with location, catchment, and attribute information",,,,,,,
,23,,,,,,,
,2,,,,,,,
,2023,,,,,,,
,"['https://doi.org/10.5281/zenodo.6784716', 'https://earthengine.google.com']",,,,,,,
,"Code for replicating results in this article is publicly available on Zenodo (). We use Python (versions 3.6 and 3.7), Stata MP (version 15.1), Google Earth Engine () to obtain dam catchment data and conduct analysis.",,,,,,,
,2023-02-23,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-02008-2,,,,,,,
,"Global Dam Tracker: A database of more than 35,000 dams with location, catchment, and attribute information",,,,,,,
,23,,,,,,,
,2,,,,,,,
,2023,,,,,,,
,"['https://doi.org/10.5281/zenodo.6784716', 'https://earthengine.google.com']",,,,,,,
,"Code for replicating results in this article is publicly available on Zenodo (). We use Python (versions 3.6 and 3.7), Stata MP (version 15.1), Google Earth Engine () to obtain dam catchment data and conduct analysis.",,,,,,,
,2023-02-23,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-01968-9,,,,,,,
,Developing a standardized but extendable framework to increase the findability of infectious disease datasets,,,,,,,
,23,,,,,,,
,2,,,,,,,
,2023,,,,,,,
,"['https://github.com/Hughes-Lab/niaid-schema-publication', 'https://doi.org/10.5281/zenodo.6816052', 'https://zenodo.org/record/681605292', 'https://crawler.biothings.io/', 'https://github.com/biothings/biothings.crawler', 'https://metadataplus.biothings.io/', 'https://github.com/biothings/metadataplus', 'https://github.com/biothings/discovery-app', 'https://github.com/Hughes-Lab/niaid-schema-publication/blob/main/figures-code/Table%203%20-%20Datasets%20by%20Grant.qmd']",,,,,,,
,"Code used to analyze the prevalence of Schema.org properties and create figures is available at GitHub () and archived on Zenodo (): . Code used to harvest metadata via Metadata Crawler (, ) and Metadata Plus (, ) are available on GitHub, and the code to create the Data Discovery Engine, including the NIAID SysBio Dataset and ComputationalTool registration guides, is available on GitHub (). Code to dynamically generate File 5 is available at .",,,,,,,
,2023-02-23,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-01968-9,,,,,,,
,Developing a standardized but extendable framework to increase the findability of infectious disease datasets,,,,,,,
,23,,,,,,,
,2,,,,,,,
,2023,,,,,,,
,"['https://github.com/Hughes-Lab/niaid-schema-publication', 'https://doi.org/10.5281/zenodo.6816052', 'https://zenodo.org/record/681605292', 'https://crawler.biothings.io/', 'https://github.com/biothings/biothings.crawler', 'https://metadataplus.biothings.io/', 'https://github.com/biothings/metadataplus', 'https://github.com/biothings/discovery-app', 'https://github.com/Hughes-Lab/niaid-schema-publication/blob/main/figures-code/Table%203%20-%20Datasets%20by%20Grant.qmd']",,,,,,,
,"Code used to analyze the prevalence of Schema.org properties and create figures is available at GitHub () and archived on Zenodo (): . Code used to harvest metadata via Metadata Crawler (, ) and Metadata Plus (, ) are available on GitHub, and the code to create the Data Discovery Engine, including the NIAID SysBio Dataset and ComputationalTool registration guides, is available on GitHub (). Code to dynamically generate File 5 is available at .",,,,,,,
,2023-02-23,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-01968-9,,,,,,,
,Developing a standardized but extendable framework to increase the findability of infectious disease datasets,,,,,,,
,23,,,,,,,
,2,,,,,,,
,2023,,,,,,,
,"['https://github.com/Hughes-Lab/niaid-schema-publication', 'https://doi.org/10.5281/zenodo.6816052', 'https://zenodo.org/record/681605292', 'https://crawler.biothings.io/', 'https://github.com/biothings/biothings.crawler', 'https://metadataplus.biothings.io/', 'https://github.com/biothings/metadataplus', 'https://github.com/biothings/discovery-app', 'https://github.com/Hughes-Lab/niaid-schema-publication/blob/main/figures-code/Table%203%20-%20Datasets%20by%20Grant.qmd']",,,,,,,
,"Code used to analyze the prevalence of Schema.org properties and create figures is available at GitHub () and archived on Zenodo (): . Code used to harvest metadata via Metadata Crawler (, ) and Metadata Plus (, ) are available on GitHub, and the code to create the Data Discovery Engine, including the NIAID SysBio Dataset and ComputationalTool registration guides, is available on GitHub (). Code to dynamically generate File 5 is available at .",,,,,,,
,2023-02-23,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-01968-9,,,,,,,
,Developing a standardized but extendable framework to increase the findability of infectious disease datasets,,,,,,,
,23,,,,,,,
,2,,,,,,,
,2023,,,,,,,
,"['https://github.com/Hughes-Lab/niaid-schema-publication', 'https://doi.org/10.5281/zenodo.6816052', 'https://zenodo.org/record/681605292', 'https://crawler.biothings.io/', 'https://github.com/biothings/biothings.crawler', 'https://metadataplus.biothings.io/', 'https://github.com/biothings/metadataplus', 'https://github.com/biothings/discovery-app', 'https://github.com/Hughes-Lab/niaid-schema-publication/blob/main/figures-code/Table%203%20-%20Datasets%20by%20Grant.qmd']",,,,,,,
,"Code used to analyze the prevalence of Schema.org properties and create figures is available at GitHub () and archived on Zenodo (): . Code used to harvest metadata via Metadata Crawler (, ) and Metadata Plus (, ) are available on GitHub, and the code to create the Data Discovery Engine, including the NIAID SysBio Dataset and ComputationalTool registration guides, is available on GitHub (). Code to dynamically generate File 5 is available at .",,,,,,,
,2023-02-23,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-01942-5,,,,,,,
,A longitudinal microstructural MRI dataset in healthy C57Bl/6 mice at 9.4 Tesla,,,,,,,
,14,,,,,,,
,2,,,,,,,
,2023,,,,,,,
,"['https://doi.org/10.20383/103.0594', 'https://gitlab.com/cfmm/pipelines/mouse_dmri_MT_dicomTOscalarMaps', 'https://osf.io/5eusw/']",,,,,,,
,"As mentioned previously, all code required to process dicoms to the final scalar maps is available: . The code is also available publicly through GitLab: . This includes a Snakemake pipeline, which includes FSL, MRtrix3, and ANTs commands, and MATLAB functions. The custom dMRI pulse sequences used in this work are available as binary methods: , and the source code is available upon reasonable request.",,,,,,,
,2023-02-14,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-01942-5,,,,,,,
,A longitudinal microstructural MRI dataset in healthy C57Bl/6 mice at 9.4 Tesla,,,,,,,
,14,,,,,,,
,2,,,,,,,
,2023,,,,,,,
,"['https://doi.org/10.20383/103.0594', 'https://gitlab.com/cfmm/pipelines/mouse_dmri_MT_dicomTOscalarMaps', 'https://osf.io/5eusw/']",,,,,,,
,"As mentioned previously, all code required to process dicoms to the final scalar maps is available: . The code is also available publicly through GitLab: . This includes a Snakemake pipeline, which includes FSL, MRtrix3, and ANTs commands, and MATLAB functions. The custom dMRI pulse sequences used in this work are available as binary methods: , and the source code is available upon reasonable request.",,,,,,,
,2023-02-14,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-01942-5,,,,,,,
,A longitudinal microstructural MRI dataset in healthy C57Bl/6 mice at 9.4 Tesla,,,,,,,
,14,,,,,,,
,2,,,,,,,
,2023,,,,,,,
,"['https://doi.org/10.20383/103.0594', 'https://gitlab.com/cfmm/pipelines/mouse_dmri_MT_dicomTOscalarMaps', 'https://osf.io/5eusw/']",,,,,,,
,"As mentioned previously, all code required to process dicoms to the final scalar maps is available: . The code is also available publicly through GitLab: . This includes a Snakemake pipeline, which includes FSL, MRtrix3, and ANTs commands, and MATLAB functions. The custom dMRI pulse sequences used in this work are available as binary methods: , and the source code is available upon reasonable request.",,,,,,,
,2023-02-14,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-01972-z,,,,,,,
,Data Archive for the BRAIN Initiative (DABI),,,,,,,
,9,,,,,,,
,2,,,,,,,
,2023,,,,,,,
,"['https://doi.org/10.18120/sr2n-gz34', 'https://doi.org/10.18120/x7mj-am06']",,,,,,,
,"['Open-source software was used for analysis, including the open-source H2O library for machine learning frameworks, Jupyter notebooks for python analysis, and R Analysis and Visualization of intracranial EEG (RAVE).', 'This project is supported by the NIH/NINDS under award number R24MH114796. Data used in the sample analyses presented were supported by the NIH/NINDS under award numbers UH3NS100553, R01NS119520 and U01NS098961 and the Michael J. Fox Foundation under grant number 15098.']",,,,,,,
,2023-02-09,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-01972-z,,,,,,,
,Data Archive for the BRAIN Initiative (DABI),,,,,,,
,9,,,,,,,
,2,,,,,,,
,2023,,,,,,,
,"['https://doi.org/10.18120/sr2n-gz34', 'https://doi.org/10.18120/x7mj-am06']",,,,,,,
,"['Open-source software was used for analysis, including the open-source H2O library for machine learning frameworks, Jupyter notebooks for python analysis, and R Analysis and Visualization of intracranial EEG (RAVE).', 'This project is supported by the NIH/NINDS under award number R24MH114796. Data used in the sample analyses presented were supported by the NIH/NINDS under award numbers UH3NS100553, R01NS119520 and U01NS098961 and the Michael J. Fox Foundation under grant number 15098.']",,,,,,,
,2023-02-09,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-01960-3,,,,,,,
,Building a knowledge graph to enable precision medicine,,,,,,,
,2,,,,,,,
,2,,,,,,,
,2023,,,,,,,
,"['https://zitniklab.hms.harvard.edu/projects/PrimeKG', 'https://github.com/mims-harvard/PrimeKG', 'Harvard Dataverse', 'https://doi.org/10.7910/DVN/IXA7BM']",,,,,,,
,"The PrimeKG’s project website is at . The code to reproduce results, together with documentation and tutorials, is available in PrimeKG’s GitHub repository at . In addition, the repository contains information and Python scripts to build new versions of PrimeKG as the underlying primary resources get updated and new data become available. PrimeKG data resource is hosted on  under a persistent identifier . We have deposited the knowledge graph and all relevant intermediate files at this repository.",,,,,,,
,2023-02-02,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-01960-3,,,,,,,
,Building a knowledge graph to enable precision medicine,,,,,,,
,2,,,,,,,
,2,,,,,,,
,2023,,,,,,,
,"['https://zitniklab.hms.harvard.edu/projects/PrimeKG', 'https://github.com/mims-harvard/PrimeKG', 'Harvard Dataverse', 'https://doi.org/10.7910/DVN/IXA7BM']",,,,,,,
,"The PrimeKG’s project website is at . The code to reproduce results, together with documentation and tutorials, is available in PrimeKG’s GitHub repository at . In addition, the repository contains information and Python scripts to build new versions of PrimeKG as the underlying primary resources get updated and new data become available. PrimeKG data resource is hosted on  under a persistent identifier . We have deposited the knowledge graph and all relevant intermediate files at this repository.",,,,,,,
,2023-02-02,,,,,,,
,0,,,,,,,
,Scientific Data,,,,,,,
,41597,,,,,,,
,10.1038/s41597-023-01960-3,,,,,,,
,Building a knowledge graph to enable precision medicine,,,,,,,
,2,,,,,,,
,2,,,,,,,
,2023,,,,,,,
,"['https://zitniklab.hms.harvard.edu/projects/PrimeKG', 'https://github.com/mims-harvard/PrimeKG', 'Harvard Dataverse', 'https://doi.org/10.7910/DVN/IXA7BM']",,,,,,,
,"The PrimeKG’s project website is at . The code to reproduce results, together with documentation and tutorials, is available in PrimeKG’s GitHub repository at . In addition, the repository contains information and Python scripts to build new versions of PrimeKG as the underlying primary resources get updated and new data become available. PrimeKG data resource is hosted on  under a persistent identifier . We have deposited the knowledge graph and all relevant intermediate files at this repository.",,,,,,,
,2023-02-02,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02808-6,,,,,,,,
Labelled dataset for Ultra-Low Temperature Freezer to aid dynamic modelling & fault detection and diagnostics,,,,,,,,
9,,,,,,,,
12,,,,,,,,
2023,,,,,,,,
['https://lab.compute.dtu.dk/taohu/ult-freezers-labelled-dataset-sci-data.git'],,,,,,,,
The Python code for ETL pipelines is available on the open-access GitLab at .,,,,,,,,
2023-12-09,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02809-5,,,,,,,,
A dataset of proteomic changes during human heat stress and heat acclimation,,,,,,,,
7,,,,,,,,
12,,,,,,,,
2023,,,,,,,,
['https://acclimation.statgen.org/'],,,,,,,,
The source code for this dataset is publicly available through the Figshare repository. The data may also be visualized with an interactive web visualization tool:,,,,,,,,
2023-12-07,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02757-0,,,,,,,,
The OREGANO knowledge graph for computational drug repurposing,,,,,,,,
6,,,,,,,,
12,,,,,,,,
2023,,,,,,,,
['https://gitub.u-bordeaux.fr/erias/oregano'],,,,,,,,
The code for the integration and the knowledge graph are available on the GitHub of the OREGANO project in the  folder ().,,,,,,,,
2023-12-06,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02755-2,,,,,,,,
High-throughput density functional theory screening of double transition metal MXene precursors,,,,,,,,
25,,,,,,,,
11,,,,,,,,
2023,,,,,,,,
"['https://nanohub.org/tools/vaspingestor', 'https://github.com/katnykiel/vasp_ingestor']",,,,,,,,
"The DFT results database, code used to generate figures, and tool for ingesting new data into this database are available at . This code is also available at .",,,,,,,,
2023-11-25,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02756-1,,,,,,,,
A global daily evapotranspiration deficit index dataset for quantifying drought severity from 1979 to 2022,,,,,,,,
24,,,,,,,,
11,,,,,,,,
2023,,,,,,,,
"['https://github.com/XiaZhang1113/Daily-drought-index–DEDI', 'https://www.python.org/', 'https://code.mpimet.mpg.de/']",,,,,,,,
The code used to calculate the DEDI dataset is available via GitHub () under the MIT license. The scripts are written with the open-source Python language version 3.8.6 () and the Climate Data Operators (CDO) version 1.9.10 (). Any updates will be published on GitHub.,,,,,,,,
2023-11-24,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02756-1,,,,,,,,
A global daily evapotranspiration deficit index dataset for quantifying drought severity from 1979 to 2022,,,,,,,,
24,,,,,,,,
11,,,,,,,,
2023,,,,,,,,
"['https://github.com/XiaZhang1113/Daily-drought-index–DEDI', 'https://www.python.org/', 'https://code.mpimet.mpg.de/']",,,,,,,,
The code used to calculate the DEDI dataset is available via GitHub () under the MIT license. The scripts are written with the open-source Python language version 3.8.6 () and the Climate Data Operators (CDO) version 1.9.10 (). Any updates will be published on GitHub.,,,,,,,,
2023-11-24,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02682-2,,,,,,,,
Photos and rendered images of LEGO bricks,,,,,,,,
18,,,,,,,,
11,,,,,,,,
2023,,,,,,,,
['https://github.com/LegoSorter'],,,,,,,,
"Custom tools used to take photos, generate renders, annotate photos, and extract annotated bricks from the complete scene, including the trained neural networks, are publicly available through the Lego Sorter project and its repositories available at .",,,,,,,,
2023-11-18,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02673-3,,,,,,,,
Daylong acoustic recordings of grazing and rumination activities in dairy cows,,,,,,,,
8,,,,,,,,
11,,,,,,,,
2023,,,,,,,,
['https://gitlab.com/luciano.mrau/acoustic_dairy_cow_dataset'],,,,,,,,
"The code for automatically adjusting the timesteps of JM labels, computing the JM labels of the audio recordings and for technical validation is available at Gitlab (). All code was written in Python 3.8.10 and distributed under the MIT license. Small changes should be made to the scripts by specifying the path of the audio files of the execution environment.",,,,,,,,
2023-11-08,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02697-9,,,,,,,,
The EPOS multi-disciplinary Data Portal for integrated access to solid Earth science datasets,,,,,,,,
8,,,,,,,,
11,,,,,,,,
2023,,,,,,,,
"['https://epos-eu.github.io/epos-open-source/', 'https://github.com/epos-eu/SHAPEness-Metadata-Editor', 'https://github.com/epos-eu/EPOS-DCAT-AP']",,,,,,,,
"The code developed for the EPOS Data Portal system is available on the GitHub repository of EPOS . Other code that contributed to the development of the system is already available on such repository, as in the case of the SHAPEness metadata Editor () and the specifications for EPOS-DCAT-AP extension ().",,,,,,,,
2023-11-08,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02634-w,,,,,,,,
Land cover and forest health indicator datasets for central India using very-high resolution satellite data,,,,,,,,
25,,,,,,,,
10,,,,,,,,
2023,,,,,,,,
['https://doi.org/10.57760/sciencedb.10422/'],,,,,,,,
The code classifying land cover from PlanetScope imagery and deriving the BGI was written in Google Earth Engine. The JavaScript language to classify land covers from Planetscope imagery and derive the BGI from the land cover is available as the ‘Code’ text file from Science Data Bank at .,,,,,,,,
2023-10-25,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02650-w,,,,,,,,
A Large Finer-grained Affective Computing EEG Dataset,,,,,,,,
25,,,,,,,,
10,,,,,,,,
2023,,,,,,,,
['https://doi.org/10.7303/syn50614194'],,,,,,,,
All the codes used for the data pre-processing and the technical validation are publicly available together with the FACED datasets in Synapse (). The codes were developed in Python 3.10. These codes can be executed on Linux and Windows. All required packages are listed in the torch_ubuntu.yml and torch_win.yml files. The README file under the Code file provides a detailed explanation of the procedure to reproduce the validation results using the codes and data.,,,,,,,,
2023-10-25,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02316-7,,,,,,,,
Restructuring and serving web-accessible streamflow data from the NOAA National Water Model historic simulations,,,,,,,,
20,,,,,,,,
10,,,,,,,,
2023,,,,,,,,
['https://www.hydroshare.org/resource/84c2b029f97343a59d0739115d4087f1/'],,,,,,,,
"All code for data download and reformatting can be found in the appropriate USGS repository. The  R package is available on GitHub and the dataset is documented and published via HydroShare. All the data are currently open, and publicly available at this URL: .",,,,,,,,
2023-10-20,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02612-2,,,,,,,,
A benchmark dataset for machine learning in ecotoxicology,,,,,,,,
18,,,,,,,,
10,,,,,,,,
2023,,,,,,,,
['https://renkulab.io/gitlab/mltox/adore'],,,,,,,,
"The code used to load and process the input data and generate the output dataset was created and run in Python 3.9 and is made available on . The repository contains code on how to load the data, prepare it for modeling, ., create one-hot and multi-hot-encodings for categorical features, and apply the train-test-split for 5-fold cross-validation. A good starting point are the files in the folder  for random forests ( and ).",,,,,,,,
2023-10-18,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02599-w,,,,,,,,
Global database of cement production assets and upstream suppliers,,,,,,,,
13,,,,,,,,
10,,,,,,,,
2023,,,,,,,,
"['Spatial macrolocalisation model', 'Cement production types and capacity estimation models', 'Sentinel-2 cement assets deployment models']",,,,,,,,
The deployment phase code notebooks are available from the GitHub Spatial Finance repository: [1] ; [2] ; [3] .,,,,,,,,
2023-10-13,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02599-w,,,,,,,,
Global database of cement production assets and upstream suppliers,,,,,,,,
13,,,,,,,,
10,,,,,,,,
2023,,,,,,,,
"['Spatial macrolocalisation model', 'Cement production types and capacity estimation models', 'Sentinel-2 cement assets deployment models']",,,,,,,,
The deployment phase code notebooks are available from the GitHub Spatial Finance repository: [1] ; [2] ; [3] .,,,,,,,,
2023-10-13,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02599-w,,,,,,,,
Global database of cement production assets and upstream suppliers,,,,,,,,
13,,,,,,,,
10,,,,,,,,
2023,,,,,,,,
"['Spatial macrolocalisation model', 'Cement production types and capacity estimation models', 'Sentinel-2 cement assets deployment models']",,,,,,,,
The deployment phase code notebooks are available from the GitHub Spatial Finance repository: [1] ; [2] ; [3] .,,,,,,,,
2023-10-13,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02555-8,,,,,,,,
CherryChèvre: A fine-grained dataset for goat detection in natural environments,,,,,,,,
11,,,,,,,,
10,,,,,,,,
2023,,,,,,,,
['https://doi.org/10.57745/QEZBNA'],,,,,,,,
"The Python 3.10 scripts used for converting the VGG VIA csv format to YOLO format, as well as other scripts used for generating statistics presented in the article, are available at .",,,,,,,,
2023-10-11,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02583-4,,,,,,,,
The R package for DICOM to brain imaging data structure conversion,,,,,,,,
4,,,,,,,,
10,,,,,,,,
2023,,,,,,,,
"['https://github.com/bidsconvertr/bidsconvertr', 'https://bidsconvertr.github.io']",,,,,,,,
"All code is hosted freely and open-source on a GitHub repository, accessible via . The documentation is hosted on the GitHub page: .",,,,,,,,
2023-10-04,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02569-2,,,,,,,,
A synthetic data set to benchmark anti-money laundering methods,,,,,,,,
28,,,,,,,,
9,,,,,,,,
2023,,,,,,,,
"['https://sdv.dev', 'https://sdv.dev/SDV/user_guides/relational/hma1', 'https://scikit-learn.org', 'https://lightgbm.readthedocs.io/en/stable']",,,,,,,,
"All our simulations are made using version 0.14.1 of the SDV library (). We specifically employ the HMA1 model class using two tables as inputs: a primary table with alerts (see Table ) and a secondary table with transactions (see Table ). A demonstration by the SDV developers is available online (; using data different from ours). Due to confidentiality, we do not share our code that (i) transforms the raw data so that it can be fed to the HMA1 model class and (ii) re-transforms and adds noise to the simulated data. The data-providing bank felt that providing this code would reveal sensitive information about its internal setup and the real data. All our transformations are, however, described in detail in our subsection “Implementation: Pre- and Postprocessing.” Our machine learning experiments were conducted with version 1.1.3 of the Scikit-learn library () and version 3.3.3 of the LightGBM library ().",,,,,,,,
2023-09-28,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02569-2,,,,,,,,
A synthetic data set to benchmark anti-money laundering methods,,,,,,,,
28,,,,,,,,
9,,,,,,,,
2023,,,,,,,,
"['https://sdv.dev', 'https://sdv.dev/SDV/user_guides/relational/hma1', 'https://scikit-learn.org', 'https://lightgbm.readthedocs.io/en/stable']",,,,,,,,
"All our simulations are made using version 0.14.1 of the SDV library (). We specifically employ the HMA1 model class using two tables as inputs: a primary table with alerts (see Table ) and a secondary table with transactions (see Table ). A demonstration by the SDV developers is available online (; using data different from ours). Due to confidentiality, we do not share our code that (i) transforms the raw data so that it can be fed to the HMA1 model class and (ii) re-transforms and adds noise to the simulated data. The data-providing bank felt that providing this code would reveal sensitive information about its internal setup and the real data. All our transformations are, however, described in detail in our subsection “Implementation: Pre- and Postprocessing.” Our machine learning experiments were conducted with version 1.1.3 of the Scikit-learn library () and version 3.3.3 of the LightGBM library ().",,,,,,,,
2023-09-28,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02569-2,,,,,,,,
A synthetic data set to benchmark anti-money laundering methods,,,,,,,,
28,,,,,,,,
9,,,,,,,,
2023,,,,,,,,
"['https://sdv.dev', 'https://sdv.dev/SDV/user_guides/relational/hma1', 'https://scikit-learn.org', 'https://lightgbm.readthedocs.io/en/stable']",,,,,,,,
"All our simulations are made using version 0.14.1 of the SDV library (). We specifically employ the HMA1 model class using two tables as inputs: a primary table with alerts (see Table ) and a secondary table with transactions (see Table ). A demonstration by the SDV developers is available online (; using data different from ours). Due to confidentiality, we do not share our code that (i) transforms the raw data so that it can be fed to the HMA1 model class and (ii) re-transforms and adds noise to the simulated data. The data-providing bank felt that providing this code would reveal sensitive information about its internal setup and the real data. All our transformations are, however, described in detail in our subsection “Implementation: Pre- and Postprocessing.” Our machine learning experiments were conducted with version 1.1.3 of the Scikit-learn library () and version 3.3.3 of the LightGBM library ().",,,,,,,,
2023-09-28,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02569-2,,,,,,,,
A synthetic data set to benchmark anti-money laundering methods,,,,,,,,
28,,,,,,,,
9,,,,,,,,
2023,,,,,,,,
"['https://sdv.dev', 'https://sdv.dev/SDV/user_guides/relational/hma1', 'https://scikit-learn.org', 'https://lightgbm.readthedocs.io/en/stable']",,,,,,,,
"All our simulations are made using version 0.14.1 of the SDV library (). We specifically employ the HMA1 model class using two tables as inputs: a primary table with alerts (see Table ) and a secondary table with transactions (see Table ). A demonstration by the SDV developers is available online (; using data different from ours). Due to confidentiality, we do not share our code that (i) transforms the raw data so that it can be fed to the HMA1 model class and (ii) re-transforms and adds noise to the simulated data. The data-providing bank felt that providing this code would reveal sensitive information about its internal setup and the real data. All our transformations are, however, described in detail in our subsection “Implementation: Pre- and Postprocessing.” Our machine learning experiments were conducted with version 1.1.3 of the Scikit-learn library () and version 3.3.3 of the LightGBM library ().",,,,,,,,
2023-09-28,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02551-y,,,,,,,,
Multi-view emotional expressions dataset using 2D pose estimation,,,,,,,,
22,,,,,,,,
9,,,,,,,,
2023,,,,,,,,
['https://doi.org/10.5281/zenodo.8185369'],,,,,,,,
The MATLAB code for parsing the JSON file and processing the coordinates can be found at .,,,,,,,,
2023-09-22,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02489-1,,,,,,,,
A pseudoproxy emulation of the PAGES 2k database using a hierarchy of proxy system models,,,,,,,,
14,,,,,,,,
9,,,,,,,,
2023,,,,,,,,
"['https://doi.org/10.5281/zenodo.7652533', 'https://github.com/fzhu2e/paper-pseudoPAGES2k']",,,,,,,,
The Jupyter notebooks illustrating the usage of the pseudoPAGES2k dataset can be accessed at  or .,,,,,,,,
2023-09-14,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02524-1,,,,,,,,
Open access dataset integrating EEG and fNIRS during Stroop tasks,,,,,,,,
12,,,,,,,,
9,,,,,,,,
2023,,,,,,,,
"['https://github.com/Yaaaaaaaaabby/fNIRS-data-pre-processing-from-Zemeng-Chen.git', 'https://gitee.com/chen-zemeng/f-nirs-data-pre-processing-from-zemeng-chen.git']",,,,,,,,
Scripts to import the fNIRS raw data (.tdms file format) into MATLAB and fNIRS data processing code used above are available at  or . A user guide describing the basic situation and usage of the dataset is uploaded together with the code. There are two files in the zip file. The MATLAB code file named “process_fNIRS_EEG_Stroop” is used to pre-process the fNIRS data of a subject. The folder used for MATLAB to load the .tdms file format is named “Matlab_read_tdms_file”. Please add the folder to the MATLAB search path before loading the .tdms file format.,,,,,,,,
2023-09-12,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02514-3,,,,,,,,
Ontology for the Avida digital evolution platform,,,,,,,,
9,,,,,,,,
9,,,,,,,,
2023,,,,,,,,
['https://graphdb.fortunalab.org'],,,,,,,,
"[{'ext-link': [{'@xlink:href': 'https://gitlab.com/fortunalab/ontoavida', '@ext-link-type': 'uri', '#text': 'https://gitlab.com/fortunalab/ontoavida'}, {'@xlink:href': 'https://gitlab.com/fortunalab/ontoavida/-/blob/master/ontoavida.obo', '@ext-link-type': 'uri', '#text': 'https://gitlab.com/fortunalab/ontoavida/-/blob/master/ontoavida.obo'}, {'@xlink:href': 'https://gitlab.com/fortunalab/ontoavida/-/blob/master/ontoavida.owl', '@ext-link-type': 'uri', '#text': 'https://gitlab.com/fortunalab/ontoavida/-/blob/master/ontoavida.owl'}, {'@xlink:href': 'http://www.obofoundry.org/ontology/ontoavida.html', '@ext-link-type': 'uri', '#text': 'http://www.obofoundry.org/ontology/ontoavida.html'}, {'@xlink:href': 'http://creativecommons.org/licenses/by/4.0/', '@ext-link-type': 'uri', '#text': 'http://creativecommons.org/licenses/by/4.0/'}], '#text': 'The ontology is available in both OBO and OWL format from the GitLab repository () and can be found at  and . OntoAvida OBO and OWL files are also available from the OBO Foundry (). All files are available under the Creative Commons Attribution 4.0 International License () which allows for the copying, redistribution and adaption of the ontology for any purpose.'}, {'ext-link': [{'@xlink:href': 'https://github.com/rdflib/pyLODE', '@ext-link-type': 'uri', '#text': 'https://github.com/rdflib/pyLODE'}, {'@xlink:href': 'https://gitlab.com/fortunalab/pyLODE', '@ext-link-type': 'uri', '#text': 'https://gitlab.com/fortunalab/pyLODE'}, {'@xlink:href': 'https://owl.fortunalab.org/ontoavida', '@ext-link-type': 'uri', '#text': 'https://owl.fortunalab.org/ontoavida'}], 'xref': {'@rid': 'Fig4', '@ref-type': 'fig', '#text': '4'}, '#text': 'pyLODE () was used to obtain a user-friendly visualization of the ontology. pyLODE is based on the OWL Documentation Environment tool (LODE), implemented in Python, and used to generate human-readable HTML documents for OWL and RDF ontologies. We have customized the original pyLODE templates () to convert a scheme of OntoAvida, in a HTML file so that its classes, object properties, and datatype properties can be easily visualized (Fig.\xa0). The pyLODE file of OntoAvida is available at .'}]",,,,,,,,
2023-09-09,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02470-y,,,,,,,,
Applying FAIR4RS principles to develop an integrated modeling environment for the magnetic confinement fusion,,,,,,,,
7,,,,,,,,
9,,,,,,,,
2023,,,,,,,,
['https://doi.org/10.5281/zenodo.8098117'],,,,,,,,
"We use the ZENODO service to keep a persistent archive of FyDev code, with .",,,,,,,,
2023-09-07,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02473-9,,,,,,,,
Monitoring of carbon-water fluxes at Eurasian meteorological stations using random forest and remote sensing,,,,,,,,
7,,,,,,,,
9,,,,,,,,
2023,,,,,,,,
['https://doi.org/10.6084/m9.figshare.21510183.v2'],,,,,,,,
The code to generate the carbon-water flux datasets is available at figshare ().,,,,,,,,
2023-09-07,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02494-4,,,,,,,,
SECURES-Met: A European meteorological data set suitable for electricity modelling applications,,,,,,,,
7,,,,,,,,
9,,,,,,,,
2023,,,,,,,,
['https://doi.org/10.5281/zenodo.8108927'],,,,,,,,
The data are stored as ASCII text (csv) and no specific software is necessary to access the data. The production of the data was done with Python. These scripts are mainly data manipulation routines and do not contribute in processing the data further. To assure repeatability all scripts are available at Zenodo ().,,,,,,,,
2023-09-07,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02483-7,,,,,,,,
Standardised images of novel objects created with generative adversarial networks,,,,,,,,
2,,,,,,,,
9,,,,,,,,
2023,,,,,,,,
['https://artbreeder.com'],,,,,,,,
"Software to generate novel objects is available at . Code to perform data collection (i.e., run the online experiment) was created using jsPsych (version 7.2.1), and a modified version of the  extension (all available at). Code to extract the objective properties of each object, and to compile the subjective ratings from our online study, was written in Python, MATLAB and Julia respectively (available at).",,,,,,,,
2023-09-02,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02475-7,,,,,,,,
Development of global monthly dataset of CMIP6 climate variables for estimating evapotranspiration,,,,,,,,
26,,,,,,,,
8,,,,,,,,
2023,,,,,,,,
['https://pyeto.readthedocs.io/en/latest/'],,,,,,,,
"The code to produce the data was written using Python, PyCharm 2022.2.2. The code is available in pyeto ().",,,,,,,,
2023-08-26,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02339-0,,,,,,,,
Three-dimensional topology dataset of folded radar stratigraphy in northern Greenland,,,,,,,,
7,,,,,,,,
8,,,,,,,,
2023,,,,,,,,
"['https://gitlab.com/openpolarradar/opr', 'https://gitlab.com/openpolarradar/opr/-/wikis/home']",,,,,,,,
"The CReSIS toolbox used to process the MCoRDS RES data is available at , and the main documentation can be found at .",,,,,,,,
2023-08-07,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02339-0,,,,,,,,
Three-dimensional topology dataset of folded radar stratigraphy in northern Greenland,,,,,,,,
7,,,,,,,,
8,,,,,,,,
2023,,,,,,,,
"['https://gitlab.com/openpolarradar/opr', 'https://gitlab.com/openpolarradar/opr/-/wikis/home']",,,,,,,,
"The CReSIS toolbox used to process the MCoRDS RES data is available at , and the main documentation can be found at .",,,,,,,,
2023-08-07,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02432-4,,,,,,,,
"FracAtlas: A Dataset for Fracture Classification, Localization and Segmentation of Musculoskeletal Radiographs",,,,,,,,
5,,,,,,,,
8,,,,,,,,
2023,,,,,,,,
"['https://doi.org/10.6084/m9.figshare.22363012', 'https://github.com/XLR8-07/FracAtlas']",,,,,,,,
"The conversion of DICOM to JPEG image format was done using proprietary software of the X-ray machines from brands like Fujifilm and Philips hence they could not be made available. The mask annotations for segmentation were done using an open-source web tool named makedsense.ai. It was also used for generating VGG annotations from COCO format. As explained in the Methods section, the annotation conversion procedures from COCO to YOLO and YOLO to PASCAL VOC were performed using Python 3.10.1 on a Windows 11 operating system using ‘coco2yolo.ipynb’ and ‘yolo2voc.ipynb’. Both the Jupyter notebooks can be found inside the ‘Utility’ folder along with the dataset at Figshare (). The code used for technical validation can be accessed from (). There are 2 notebooks inside ‘notebooks’ under the root folder called ‘Train_8s.ipynb’ and ‘Prediction_8s.ipynb’. The ‘Train_8s.ipynb’ is used to train 2 models of ‘YOLO8s_seg’ and ‘YOLO8s’ variants targeted toward segmentation and localization tasks respectively. ‘Prediction_8s.ipynb’ is used to generate predictions out of the 2 aforementioned models and view the results.",,,,,,,,
2023-08-05,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02405-7,,,,,,,,
ReCANVo: A database of real-world communicative and affective nonverbal vocalizations,,,,,,,,
5,,,,,,,,
8,,,,,,,,
2023,,,,,,,,
['https://doi.org/10.5281/zenodo.5786859'],,,,,,,,
We used the Python programming language for the data processing described above. Volume segmentation was implemented using the  libary. The label assignment algorithm is summarized in Fig. . The code is available as part of our dataset in Zenodo: .,,,,,,,,
2023-08-05,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02409-3,,,,,,,,
DIPS-Plus: The enhanced database of interacting protein structures for interface prediction,,,,,,,,
3,,,,,,,,
8,,,,,,,,
2023,,,,,,,,
"['Zenodo', 'GitHub', 'GitHub', 'DOI']",,,,,,,,
"Preprocessed data for DIPS-Plus as well as its associated source code and instructions for data processing and reproducibility can be found on  and , respectively. The GitHub instructions illustrate how users can install the Python programming language and build an Anaconda virtual environment containing the software dependencies required to preprocess and analyze DIPS-Plus using the provided Python scripts. Lastly, the GitHub instructions show users how to run such scripts and the order in which to do so to successfully rebuild DIPS-Plus from scratch, to featurize a given PDB file, or to train new machine learning models (e.g., NeiA) for protein interface prediction. For provenance, the original DIPS dataset’s source code can also be found on , along with a corresponding .",,,,,,,,
2023-08-03,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02409-3,,,,,,,,
DIPS-Plus: The enhanced database of interacting protein structures for interface prediction,,,,,,,,
3,,,,,,,,
8,,,,,,,,
2023,,,,,,,,
"['Zenodo', 'GitHub', 'GitHub', 'DOI']",,,,,,,,
"Preprocessed data for DIPS-Plus as well as its associated source code and instructions for data processing and reproducibility can be found on  and , respectively. The GitHub instructions illustrate how users can install the Python programming language and build an Anaconda virtual environment containing the software dependencies required to preprocess and analyze DIPS-Plus using the provided Python scripts. Lastly, the GitHub instructions show users how to run such scripts and the order in which to do so to successfully rebuild DIPS-Plus from scratch, to featurize a given PDB file, or to train new machine learning models (e.g., NeiA) for protein interface prediction. For provenance, the original DIPS dataset’s source code can also be found on , along with a corresponding .",,,,,,,,
2023-08-03,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02409-3,,,,,,,,
DIPS-Plus: The enhanced database of interacting protein structures for interface prediction,,,,,,,,
3,,,,,,,,
8,,,,,,,,
2023,,,,,,,,
"['Zenodo', 'GitHub', 'GitHub', 'DOI']",,,,,,,,
"Preprocessed data for DIPS-Plus as well as its associated source code and instructions for data processing and reproducibility can be found on  and , respectively. The GitHub instructions illustrate how users can install the Python programming language and build an Anaconda virtual environment containing the software dependencies required to preprocess and analyze DIPS-Plus using the provided Python scripts. Lastly, the GitHub instructions show users how to run such scripts and the order in which to do so to successfully rebuild DIPS-Plus from scratch, to featurize a given PDB file, or to train new machine learning models (e.g., NeiA) for protein interface prediction. For provenance, the original DIPS dataset’s source code can also be found on , along with a corresponding .",,,,,,,,
2023-08-03,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02409-3,,,,,,,,
DIPS-Plus: The enhanced database of interacting protein structures for interface prediction,,,,,,,,
3,,,,,,,,
8,,,,,,,,
2023,,,,,,,,
"['Zenodo', 'GitHub', 'GitHub', 'DOI']",,,,,,,,
"Preprocessed data for DIPS-Plus as well as its associated source code and instructions for data processing and reproducibility can be found on  and , respectively. The GitHub instructions illustrate how users can install the Python programming language and build an Anaconda virtual environment containing the software dependencies required to preprocess and analyze DIPS-Plus using the provided Python scripts. Lastly, the GitHub instructions show users how to run such scripts and the order in which to do so to successfully rebuild DIPS-Plus from scratch, to featurize a given PDB file, or to train new machine learning models (e.g., NeiA) for protein interface prediction. For provenance, the original DIPS dataset’s source code can also be found on , along with a corresponding .",,,,,,,,
2023-08-03,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02340-7,,,,,,,,
"A pipeline to further enhance quality, integrity and reusability of the NCCID clinical data",,,,,,,,
27,,,,,,,,
7,,,,,,,,
2023,,,,,,,,
['https://gitlab.developers.cam.ac.uk/maths/cia/covid-19-projects/nccidxclean'],,,,,,,,
"Our extended cleaning pipeline  is publicly available and can be accessed on GitLab:  with accompanying documentation available on the project website (package version at the time of publication: v1.0). The package will automatically download the necessary packages and requirements during installation, including the original NCCID cleaning pipeline upon which our pipeline builds. The python package is independent of the operating system and allows replication of our results using the command line interface, python scripts, or the included Jupyter notebooks. For full analysis, the  parameter can be set to ‘all_with_original’, returning all possible data features. Additional code is provided to allow for the original NHSx pipeline to be run using a single command in the command line. The data from the NHSx pipeline may be then used in the analysis subpackage to generate all figures and numerical results found in this work. An additional subpackage  is included for exploratory data analysis of the final cleaned data. Full step-by-step guidance and further details of the package are provided on the project’s website. .",,,,,,,,
2023-07-27,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02258-0,,,,,,,,
The Translational Data Catalog - discoverable biomedical datasets,,,,,,,,
20,,,,,,,,
7,,,,,,,,
2023,,,,,,,,
['https://datacatalog.elixir-luxembourg.org/'],,,,,,,,
"[{'ext-link': {'@xlink:href': 'https://github.com/FAIRplus/imi-data-catalogue', '@ext-link-type': 'uri', '#text': 'https://github.com/FAIRplus/imi-data-catalogue'}, '#text': 'All Data Catalog code is available in a dedicated repository of the FAIRplus Github organisation, at . The repository includes full documentation on how to deploy a stand-alone version of the Data Catalog.'}, {'ext-link': {'@xlink:href': 'https://github.com/datatagsuite/schema', '@ext-link-type': 'uri', '#text': 'https://github.com/datatagsuite/schema'}, '#text': 'The DATS model is available on Github at .'}]",,,,,,,,
2023-07-20,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02373-y,,,,,,,,
Single-nucleus chromatin landscapes during zebrafish early embryogenesis,,,,,,,,
19,,,,,,,,
7,,,,,,,,
2023,,,,,,,,
['https://figshare.com/articles/dataset/Code/22121171'],,,,,,,,
The codes used to analyze the data in this study were available online ().,,,,,,,,
2023-07-19,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02318-5,,,,,,,,
Helicopter-borne RGB orthomosaics and photogrammetric digital elevation models from the MOSAiC Expedition,,,,,,,,
3,,,,,,,,
7,,,,,,,,
2023,,,,,,,,
['https://gitlab.com/mosaic12/orthomosaics'],,,,,,,,
All processing developed within this study is wrapped in a python environment and is available at . For calculating image footprint locations we used the  python package. Note that part of the code is based on the commercial Agisoft Metashape software requiring licensing.,,,,,,,,
2023-07-03,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02230-y,,,,,,,,
Signing data citations enables data verification and citation persistence,,,,,,,,
27,,,,,,,,
6,,,,,,,,
2023,,,,,,,,
"['https://github.com/bio-guoda/preston', 'https://doi.org/10.5281/zenodo.1410543', 'https://doi.org/10.5281/zenodo.7005141']",,,,,,,,
The source code for the Preston software is available in GitHub at  and in Zenodo at . Preston version 4.4 has been assigned the . The MD5 content signature for the zip archive of the source code is .,,,,,,,,
2023-06-27,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02230-y,,,,,,,,
Signing data citations enables data verification and citation persistence,,,,,,,,
27,,,,,,,,
6,,,,,,,,
2023,,,,,,,,
"['https://github.com/bio-guoda/preston', 'https://doi.org/10.5281/zenodo.1410543', 'https://doi.org/10.5281/zenodo.7005141']",,,,,,,,
The source code for the Preston software is available in GitHub at  and in Zenodo at . Preston version 4.4 has been assigned the . The MD5 content signature for the zip archive of the source code is .,,,,,,,,
2023-06-27,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02321-w,,,,,,,,
A curated gene and biological system annotation of adverse outcome pathways related to human health,,,,,,,,
24,,,,,,,,
6,,,,,,,,
2023,,,,,,,,
['https://doi.org/10.5281/zenodo.7980953'],,,,,,,,
Custom code and data used in the NLP-based prioritisation of the gene set annotations is available in the data repository on Zenodo at  (file aop_mapping_nlp.tar.gz).,,,,,,,,
2023-06-24,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02256-2,,,,,,,,
A guide to sharing open healthcare data under the General Data Protection Regulation,,,,,,,,
24,,,,,,,,
6,,,,,,,,
2023,,,,,,,,
['https://doi.org/10.6084/m9.figshare.22643419'],,,,,,,,
No code was written or used for this paper.,,,,,,,,
2023-06-24,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02238-4,,,,,,,,
An Observation-Based Dataset of Global Sub-Daily Precipitation Indices (GSDR-I),,,,,,,,
22,,,,,,,,
6,,,,,,,,
2023,,,,,,,,
['https://doi.org/10.5281/zenodo.7492877'],,,,,,,,
The Python code for indices calculation based on gauge records and subsequent gridding is available in the code repository here: .,,,,,,,,
2023-06-22,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02282-0,,,,,,,,
"Population, land use and economic exposure estimates for Europe at 100 m resolution from 1870 to 2020",,,,,,,,
8,,,,,,,,
6,,,,,,,,
2023,,,,,,,,
"['https://doi.org/10.5281/zenodo.7556953', 'https://doi.org/10.5281/zenodo.6783023', 'https://doi.org/10.4121/collection:HANZE']",,,,,,,,
"The source code of HANZE v2.0 (implemented in Python 3.9) presented in the paper is archived at . All necessary input data are archived at . The flood impact data shown in Usage Notes, with a description of sources of the data, are available in the HANZE v1.0 repository, .",,,,,,,,
2023-06-08,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02282-0,,,,,,,,
"Population, land use and economic exposure estimates for Europe at 100 m resolution from 1870 to 2020",,,,,,,,
8,,,,,,,,
6,,,,,,,,
2023,,,,,,,,
"['https://doi.org/10.5281/zenodo.7556953', 'https://doi.org/10.5281/zenodo.6783023', 'https://doi.org/10.4121/collection:HANZE']",,,,,,,,
"The source code of HANZE v2.0 (implemented in Python 3.9) presented in the paper is archived at . All necessary input data are archived at . The flood impact data shown in Usage Notes, with a description of sources of the data, are available in the HANZE v1.0 repository, .",,,,,,,,
2023-06-08,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02282-0,,,,,,,,
"Population, land use and economic exposure estimates for Europe at 100 m resolution from 1870 to 2020",,,,,,,,
8,,,,,,,,
6,,,,,,,,
2023,,,,,,,,
"['https://doi.org/10.5281/zenodo.7556953', 'https://doi.org/10.5281/zenodo.6783023', 'https://doi.org/10.4121/collection:HANZE']",,,,,,,,
"The source code of HANZE v2.0 (implemented in Python 3.9) presented in the paper is archived at . All necessary input data are archived at . The flood impact data shown in Usage Notes, with a description of sources of the data, are available in the HANZE v1.0 repository, .",,,,,,,,
2023-06-08,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02208-w,,,,,,,,
CORE: A Global Aggregation Service for Open Access Papers,,,,,,,,
7,,,,,,,,
6,,,,,,,,
2023,,,,,,,,
['https://github.com/oacore/'],,,,,,,,
"CORE consists of multiple services. Most of our source code is open source and available in our public repository on GitHub (). As of today, we are unfortunately not yet able to provide the source code to our data ingestion module. However, as we want to be as transparent as possible with our community, we have documented in this paper the key algorithms and processes which we apply using pseudocode.",,,,,,,,
2023-06-07,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02242-8,,,,,,,,
How to establish and maintain a multimodal animal research dataset using DataLad,,,,,,,,
5,,,,,,,,
6,,,,,,,,
2023,,,,,,,,
['https://doi.org/10.12751/g-node.3yl5qi'],,,,,,,,
DataLad and GIN are freely available. The manuscript contains all code to reproduce the workflow.,,,,,,,,
2023-06-05,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02269-x,,,,,,,,
PANGAEA - Data Publisher for Earth & Environmental Science,,,,,,,,
2,,,,,,,,
6,,,,,,,,
2023,,,,,,,,
['https://github.com/pangaea-data-publisher'],,,,,,,,
"The code supporting the users with data retrieval and submission is freely available at . PANGAEA as a repository does not generate, test, or process data and metadata, therefore no custom code has been used.",,,,,,,,
2023-06-02,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02196-x,,,,,,,,
Database covering the prayer movements which were not available previously,,,,,,,,
12,,,,,,,,
5,,,,,,,,
2023,,,,,,,,
['https://biomechlab.iyte.edu.tr/en/homepage/38'],,,,,,,,
"The code written for the development of the database is available upon request from the authors, but it is not open to the external users through the website to protect the database. The desired main functions of the database were created in Python and Django framework. Django was used for creating the model for the backend and Javascript for filtering functions. For the front-end, HTML, CSS, and JQuery were used. The database is available through website  and the public repository Database covering the previously excluded daily life activities | Aperta (ulakbim.gov.tr).",,,,,,,,
2023-05-12,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02176-1,,,,,,,,
Crowd cluster data in the USA for analysis of human response to COVID-19 events and policies,,,,,,,,
10,,,,,,,,
5,,,,,,,,
2023,,,,,,,,
['https://www.mpi-forum.org'],,,,,,,,
The codes were written in C++ and Python 3. They rely on Python DBSCAN package and on MPI () for parallelization. The code is available from the data site.,,,,,,,,
2023-05-10,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02134-x,,,,,,,,
OpCitance: Citation contexts identified from the PubMed Central open access articles,,,,,,,,
28,,,,,,,,
4,,,,,,,,
2023,,,,,,,,
['https://doi.org/10.13012/B2IDB-4353270_V2'],,,,,,,,
The code of our XML parser is provided in the Supplementary_File_1.zip on our data repository: .,,,,,,,,
2023-04-28,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02138-7,,,,,,,,
Complete Global Total Electron Content Map Dataset based on a Video Imputation Algorithm VISTA,,,,,,,,
25,,,,,,,,
4,,,,,,,,
2023,,,,,,,,
['https://vista-tec.shinyapps.io/VISTA-Dashboard/'],,,,,,,,
"Details about codes that generate the dataset as well as the usage notes on accessing, downloading and pre-processing the datasets are made available on the homepage of the dataset on the Deep Blue Data system of University of Michigan. Future updates of the codes and dataset will be made available on this website as well. Please contact the corresponding author for data request and questions. Additionally, our users can explore our interactive database dashboard () for more technical details and run the VISTA algorithm live.",,,,,,,,
2023-04-25,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02066-6,,,,,,,,
HIT-UAV: A high-altitude infrared thermal dataset for Unmanned Aerial Vehicle-based object detection,,,,,,,,
20,,,,,,,,
4,,,,,,,,
2023,,,,,,,,
['https://pegasus.ac.cn'],,,,,,,,
"The data processing code is available in the  folder of . The code is written in Python. The functions of the tools are as follows: (1) The  is to convert oriented bounding boxes to standard bounding boxes and generate the dataset, (2) The  is to visualize images with bounding boxes, (3) The  is to generate the label files with the YOLO format to help users train the YOLO, which is the representative object detection algorithm.",,,,,,,,
2023-04-20,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02101-6,,,,,,,,
Unified access to up-to-date residue-level annotations from UniProtKB and other biological databases for PDB data,,,,,,,,
12,,,,,,,,
4,,,,,,,,
2023,,,,,,,,
"['https://colab.research.google.com/github/PDBe-KB/sifts_data_analysis/blob/main/sifts.ipynb', 'https://github.com/PDBe-KB/sifts_data_analysis']",,,,,,,,
"To assist users in utilising the updated PDBx/mmCIF files and SIFTS annotations, a Google Colab notebook is available at  or via GitHub at . This notebook provides information on how to parse, extract and filter SIFTS annotations from the updated PDBx/mmCIF files. Additionally, the notebook demonstrates how users can compare various numbering schemes of a given residue across different PDB structures of the same protein.",,,,,,,,
2023-04-12,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02095-1,,,,,,,,
A century and a half precipitation oxygen isoscape for China generated using data fusion and bias correction,,,,,,,,
6,,,,,,,,
4,,,,,,,,
2023,,,,,,,,
['https://doi.org/10.5281/zenodo.7306199'],,,,,,,,
"The codes for two bias correction methods (LS and DT) and three neural network data fusion methods (BP, LSTM and CNN) are available at . The codes were programmed using MATLAB version 2022a and Python 3.8.",,,,,,,,
2023-04-06,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02030-4,,,,,,,,
A synthetic population for agent-based modelling in Canada,,,,,,,,
21,,,,,,,,
3,,,,,,,,
2023,,,,,,,,
['https://pypi.org/'],,,,,,,,
"The python scripts (python 3.10) developed for the generation and validation of the synthetic dataset are publicly and freely accessible on Zenodo. The scripts use the following python packages: pandas (1.4.4), numpy (1.23.2), pyreadstat (1.19), scipy (1.9.1) for the Pearson’s correlation coefficient computation, scikit-learn (1.1.2) for the RMSE computation, and matplotlib (3.5.3) to generate the charts. All these python packages are available from the Python Package Index: . The humanleague package (2.1.10) providing the QISI and IPF implementations is available from the Python Package Index and on Zenodo.",,,,,,,,
2023-03-21,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02044-y,,,,,,,,
"LocalView, a database of public meetings for the study of local politics and policy-making in the United States",,,,,,,,
15,,,,,,,,
3,,,,,,,,
2023,,,,,,,,
"['https://doi.org/10.7910/DVN/NJTBEM', 'https://doi.org/10.7910/DVN/KHUXIN', 'https://localview.net']",,,,,,,,
"The  dataset is publicly available at . Code to replicate the main and supplementary analyses in this paper is available at . More information, including a codebook and related research, is linked on our companion website at .",,,,,,,,
2023-03-15,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02044-y,,,,,,,,
"LocalView, a database of public meetings for the study of local politics and policy-making in the United States",,,,,,,,
15,,,,,,,,
3,,,,,,,,
2023,,,,,,,,
"['https://doi.org/10.7910/DVN/NJTBEM', 'https://doi.org/10.7910/DVN/KHUXIN', 'https://localview.net']",,,,,,,,
"The  dataset is publicly available at . Code to replicate the main and supplementary analyses in this paper is available at . More information, including a codebook and related research, is linked on our companion website at .",,,,,,,,
2023-03-15,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02044-y,,,,,,,,
"LocalView, a database of public meetings for the study of local politics and policy-making in the United States",,,,,,,,
15,,,,,,,,
3,,,,,,,,
2023,,,,,,,,
"['https://doi.org/10.7910/DVN/NJTBEM', 'https://doi.org/10.7910/DVN/KHUXIN', 'https://localview.net']",,,,,,,,
"The  dataset is publicly available at . Code to replicate the main and supplementary analyses in this paper is available at . More information, including a codebook and related research, is linked on our companion website at .",,,,,,,,
2023-03-15,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02028-y,,,,,,,,
FAIRification of health-related data using semantic web technologies in the Swiss Personalized Health Network,,,,,,,,
10,,,,,,,,
3,,,,,,,,
2023,,,,,,,,
"['https://www.biomedit.ch/rdf/sphn-ontology/sphn/2022/2', 'https://git.dcc.sib.swiss/sphn-semantic-framework/sphn-ontology/-/tree/master/ontology', 'https://doi.org/10.5281/zenodo.7390281']",,,,,,,,
No custom code was used to generate this work.,,,,,,,,
2023-03-10,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02028-y,,,,,,,,
FAIRification of health-related data using semantic web technologies in the Swiss Personalized Health Network,,,,,,,,
10,,,,,,,,
3,,,,,,,,
2023,,,,,,,,
"['https://www.biomedit.ch/rdf/sphn-ontology/sphn/2022/2', 'https://git.dcc.sib.swiss/sphn-semantic-framework/sphn-ontology/-/tree/master/ontology', 'https://doi.org/10.5281/zenodo.7390281']",,,,,,,,
No custom code was used to generate this work.,,,,,,,,
2023-03-10,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02028-y,,,,,,,,
FAIRification of health-related data using semantic web technologies in the Swiss Personalized Health Network,,,,,,,,
10,,,,,,,,
3,,,,,,,,
2023,,,,,,,,
"['https://www.biomedit.ch/rdf/sphn-ontology/sphn/2022/2', 'https://git.dcc.sib.swiss/sphn-semantic-framework/sphn-ontology/-/tree/master/ontology', 'https://doi.org/10.5281/zenodo.7390281']",,,,,,,,
No custom code was used to generate this work.,,,,,,,,
2023-03-10,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02037-x,,,,,,,,
WAVES – The Lucile Packard Children’s Hospital Pediatric Physiological Waveforms Dataset,,,,,,,,
7,,,,,,,,
3,,,,,,,,
2023,,,,,,,,
"['https://bitbucket.org/surfstanfordmedicine/waves-utilities/src/main/', 'https://pypi.org/project/waves-utilities/']",,,,,,,,
"Redivis provides a visual drag-and-drop filtering user interface that allows the user to select columns of interest, filter on properties of interest, and limit output parameters before creating a downloadable CSV file. Sample scripts for working with data downloaded from Redivis and plotting sample waveforms are available in open-source repositories:  and .",,,,,,,,
2023-03-07,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02037-x,,,,,,,,
WAVES – The Lucile Packard Children’s Hospital Pediatric Physiological Waveforms Dataset,,,,,,,,
7,,,,,,,,
3,,,,,,,,
2023,,,,,,,,
"['https://bitbucket.org/surfstanfordmedicine/waves-utilities/src/main/', 'https://pypi.org/project/waves-utilities/']",,,,,,,,
"Redivis provides a visual drag-and-drop filtering user interface that allows the user to select columns of interest, filter on properties of interest, and limit output parameters before creating a downloadable CSV file. Sample scripts for working with data downloaded from Redivis and plotting sample waveforms are available in open-source repositories:  and .",,,,,,,,
2023-03-07,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02008-2,,,,,,,,
"Global Dam Tracker: A database of more than 35,000 dams with location, catchment, and attribute information",,,,,,,,
23,,,,,,,,
2,,,,,,,,
2023,,,,,,,,
"['https://doi.org/10.5281/zenodo.6784716', 'https://earthengine.google.com']",,,,,,,,
"Code for replicating results in this article is publicly available on Zenodo (). We use Python (versions 3.6 and 3.7), Stata MP (version 15.1), Google Earth Engine () to obtain dam catchment data and conduct analysis.",,,,,,,,
2023-02-23,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-02008-2,,,,,,,,
"Global Dam Tracker: A database of more than 35,000 dams with location, catchment, and attribute information",,,,,,,,
23,,,,,,,,
2,,,,,,,,
2023,,,,,,,,
"['https://doi.org/10.5281/zenodo.6784716', 'https://earthengine.google.com']",,,,,,,,
"Code for replicating results in this article is publicly available on Zenodo (). We use Python (versions 3.6 and 3.7), Stata MP (version 15.1), Google Earth Engine () to obtain dam catchment data and conduct analysis.",,,,,,,,
2023-02-23,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-01942-5,,,,,,,,
A longitudinal microstructural MRI dataset in healthy C57Bl/6 mice at 9.4 Tesla,,,,,,,,
14,,,,,,,,
2,,,,,,,,
2023,,,,,,,,
"['https://doi.org/10.20383/103.0594', 'https://gitlab.com/cfmm/pipelines/mouse_dmri_MT_dicomTOscalarMaps', 'https://osf.io/5eusw/']",,,,,,,,
"As mentioned previously, all code required to process dicoms to the final scalar maps is available: . The code is also available publicly through GitLab: . This includes a Snakemake pipeline, which includes FSL, MRtrix3, and ANTs commands, and MATLAB functions. The custom dMRI pulse sequences used in this work are available as binary methods: , and the source code is available upon reasonable request.",,,,,,,,
2023-02-14,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-01942-5,,,,,,,,
A longitudinal microstructural MRI dataset in healthy C57Bl/6 mice at 9.4 Tesla,,,,,,,,
14,,,,,,,,
2,,,,,,,,
2023,,,,,,,,
"['https://doi.org/10.20383/103.0594', 'https://gitlab.com/cfmm/pipelines/mouse_dmri_MT_dicomTOscalarMaps', 'https://osf.io/5eusw/']",,,,,,,,
"As mentioned previously, all code required to process dicoms to the final scalar maps is available: . The code is also available publicly through GitLab: . This includes a Snakemake pipeline, which includes FSL, MRtrix3, and ANTs commands, and MATLAB functions. The custom dMRI pulse sequences used in this work are available as binary methods: , and the source code is available upon reasonable request.",,,,,,,,
2023-02-14,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-01942-5,,,,,,,,
A longitudinal microstructural MRI dataset in healthy C57Bl/6 mice at 9.4 Tesla,,,,,,,,
14,,,,,,,,
2,,,,,,,,
2023,,,,,,,,
"['https://doi.org/10.20383/103.0594', 'https://gitlab.com/cfmm/pipelines/mouse_dmri_MT_dicomTOscalarMaps', 'https://osf.io/5eusw/']",,,,,,,,
"As mentioned previously, all code required to process dicoms to the final scalar maps is available: . The code is also available publicly through GitLab: . This includes a Snakemake pipeline, which includes FSL, MRtrix3, and ANTs commands, and MATLAB functions. The custom dMRI pulse sequences used in this work are available as binary methods: , and the source code is available upon reasonable request.",,,,,,,,
2023-02-14,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-01972-z,,,,,,,,
Data Archive for the BRAIN Initiative (DABI),,,,,,,,
9,,,,,,,,
2,,,,,,,,
2023,,,,,,,,
"['https://doi.org/10.18120/sr2n-gz34', 'https://doi.org/10.18120/x7mj-am06']",,,,,,,,
"['Open-source software was used for analysis, including the open-source H2O library for machine learning frameworks, Jupyter notebooks for python analysis, and R Analysis and Visualization of intracranial EEG (RAVE).', 'This project is supported by the NIH/NINDS under award number R24MH114796. Data used in the sample analyses presented were supported by the NIH/NINDS under award numbers UH3NS100553, R01NS119520 and U01NS098961 and the Michael J. Fox Foundation under grant number 15098.']",,,,,,,,
2023-02-09,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-01972-z,,,,,,,,
Data Archive for the BRAIN Initiative (DABI),,,,,,,,
9,,,,,,,,
2,,,,,,,,
2023,,,,,,,,
"['https://doi.org/10.18120/sr2n-gz34', 'https://doi.org/10.18120/x7mj-am06']",,,,,,,,
"['Open-source software was used for analysis, including the open-source H2O library for machine learning frameworks, Jupyter notebooks for python analysis, and R Analysis and Visualization of intracranial EEG (RAVE).', 'This project is supported by the NIH/NINDS under award number R24MH114796. Data used in the sample analyses presented were supported by the NIH/NINDS under award numbers UH3NS100553, R01NS119520 and U01NS098961 and the Michael J. Fox Foundation under grant number 15098.']",,,,,,,,
2023-02-09,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-01965-y,,,,,,,,
An open database on global coal and metal mine production,,,,,,,,
24,,,,,,,,
1,,,,,,,,
2023,,,,,,,,
['www.github.com/fineprint-global/compilation_mining_data'],,,,,,,,
"The code used to derive the final data product from the raw input data file is available under the licence GNU General Public License v3.0 (GPL-v3) from the GitHub repository . All processing scripts were written in R, and geoprocessing was conducted with the R package sf.",,,,,,,,
2023-01-24,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-01929-2,,,,,,,,
A 1.2 Billion Pixel Human-Labeled Dataset for Data-Driven Classification of Coastal Environments,,,,,,,,
20,,,,,,,,
1,,,,,,,,
2023,,,,,,,,
"['https://github.com/CoastTrain/CoastTrainMetaPlots', 'https://github.com/kvos/CoastSat', 'https://github.com/giswqs/geemap', 'https://coasttrain.github.io/CoastTrain/']",,,,,,,,
"All the figures presented in this manuscript may be generated using computational notebooks provided (). Utilities for npz file variable extraction and class remapping are provided in the Doodler and Segmentation Gym software packages. All labels were created with Doodler. Imagery was downloaded using CoastSat () and Geemap () functionality. For more information, please see the Coast Train project website ().",,,,,,,,
2023-01-20,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-01930-9,,,,,,,,
Mobility recorded by wearable devices and gold standards: the Mobilise-D procedure for data standardization,,,,,,,,
19,,,,,,,,
1,,,,,,,,
2023,,,,,,,,
['https://doi.org/10.5281/zenodo.7185429'],,,,,,,,
"[{'ext-link': {'@xlink:href': 'https://github.com/luca-palmerini/Procedure-wearable-data-standardization-Mobilise-D', '@ext-link-type': 'uri', '#text': 'https://github.com/luca-palmerini/Procedure-wearable-data-standardization-Mobilise-D'}, '#text': 'The presented data structure and the related example subjects were created in MATLAB R2021b. Example of the data structures in Matlab are available (see Data Availability Section). To illustrate how to standardize raw data, we provide the Matlab code to standardize the raw data of the example subject from the ICICLE dataset (see Data Availability). The code is in the “MATLAB code for standardization” folder in the following GitHub repository (release version v1.0.0 upon article submission): .'}, {'italic': ['data.mat', 'infoforalgo.mat'], '#text': 'The code was used to obtain the standardized  and  of the corresponding example subject. In the same repository, in the “Python code for access” and in the “R code for access”, we provide code to access the standardized data with Python and R programming languages, respectively.'}]",,,,,,,,
2023-01-19,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-022-01889-z,,,,,,,,
Satellite-derived multivariate world-wide lake physical variable timeseries for climate studies,,,,,,,,
14,,,,,,,,
1,,,,,,,,
2023,,,,,,,,
['https://climate.esa.int/en/explore/analyse-climate-data/'],,,,,,,,
An option to visualise the dataset is to use the software Climate Analysis Toolbox (Cate) see ).,,,,,,,,
2023-01-14,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-022-01719-2,,,,,,,,
The Dresden Surgical Anatomy Dataset for Abdominal Organ Segmentation in Surgical Data Science,,,,,,,,
12,,,,,,,,
1,,,,,,,,
2023,,,,,,,,
"['https://gitlab.com/nct_tso_public/dsad', 'https://zenodo.org/record/6958337#.YvIsP3ZBxaQ']",,,,,,,,
"The scripts for frame extraction, annotation merging, and statistical analysis, as well as the results of the statistical analysis are made public on  and via . All code is written in  and freely accessible.",,,,,,,,
2023-01-12,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-022-01719-2,,,,,,,,
The Dresden Surgical Anatomy Dataset for Abdominal Organ Segmentation in Surgical Data Science,,,,,,,,
12,,,,,,,,
1,,,,,,,,
2023,,,,,,,,
"['https://gitlab.com/nct_tso_public/dsad', 'https://zenodo.org/record/6958337#.YvIsP3ZBxaQ']",,,,,,,,
"The scripts for frame extraction, annotation merging, and statistical analysis, as well as the results of the statistical analysis are made public on  and via . All code is written in  and freely accessible.",,,,,,,,
2023-01-12,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-023-01939-0,,,,,,,,
Dataset on child vaccination in Brazil from 1996 to 2021,,,,,,,,
11,,,,,,,,
1,,,,,,,,
2023,,,,,,,,
['https://doi.org/10.7303/syn26453964'],,,,,,,,
"We automated all data processing and curation in the free and open software R (4.2.1, current version for windows). The data resources described in this paper, including R codes, can be accessed with no restrictions on the Synapse repository at . Anyone can browse the content on the Synapse website, but you must register for an account using your email address to download the files and datasets. Please see Table  and references for details and links to the data resources.",,,,,,,,
2023-01-11,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-022-01916-z,,,,,,,,
Citizen science helps in the study of fungal diversity in New Jersey,,,,,,,,
4,,,,,,,,
1,,,,,,,,
2023,,,,,,,,
['www.gbif.org/ipt'],,,,,,,,
"Figures were prepared using R Statistical Software (v4.1.2; R Core Team 2021), packages  and. The dataset was shared via GBIF.org using Integrated Publishing Toolkit (IPT, ). No original code was created to generate the dataset.",,,,,,,,
2023-01-04,,,,,,,,
0,,,,,,,,
Scientific Data,,,,,,,,
41597,,,,,,,,
10.1038/s41597-022-01893-3,,,,,,,,
A dataset to assess mobility changes in Chile following local quarantines,,,,,,,,
3,,,,,,,,
1,,,,,,,,
2023,,,,,,,,
"['https://raw.githubusercontent.com/MinCiencia/Datos-COVID19/master/output/producto33/IndiceDeMovilidad.csv', 'https://github.com/MinCiencia/Datos-COVID19/blob/master/output/producto29/Cuarentenas-Activas.csv']",,,,,,,,
"The up-to-date data are available from the general repository of the Ministry of Science of Chile at:  (IM indeces), and  (quarantines). The code to download the up-to-date data automatically and to reproduce the analysis in our paper is available at.",,,,,,,,
2023-01-03,,,,,,,,
